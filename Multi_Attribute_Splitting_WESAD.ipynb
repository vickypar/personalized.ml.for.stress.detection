{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b6546a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from os import listdir\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b6f821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.20 in c:\\users\\vasil\\appdata\\roaming\\python\\python38\\site-packages (1.20.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\envs\\env_python8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\envs\\env_python8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\envs\\env_python8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\envs\\env_python8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\envs\\env_python8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\envs\\env_python8\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade numpy==1.20 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(data, participants): \n",
    "    \n",
    "    cost = []\n",
    "    silhouette = []\n",
    "\n",
    "    for i in range(2, participants):\n",
    "        kmeans = KMeans(n_clusters = i, max_iter = 500, random_state = 0)\n",
    "        kmeans.fit_predict(data)\n",
    "        \n",
    "        # Calculate Silhoutte Score\n",
    "        score = silhouette_score(data, kmeans.labels_, metric='euclidean')\n",
    "        silhouette.append(score)\n",
    "    \n",
    "        # Calculates squared error for the clustered points\n",
    "        cost.append(kmeans.inertia_)    \n",
    "        \n",
    "    # Plot the cost against K values\n",
    "    plt.plot(range(2, participants), cost, color ='g', linewidth ='3')\n",
    "    plt.xlabel(\"Value of K\")\n",
    "    plt.ylabel(\"Squared Error (Cost)\")\n",
    "    plt.show() # clear the plot\n",
    "    \n",
    "    # Plot the Silhouette Score against K values\n",
    "    plt.plot(range(2, participants), silhouette, color ='b', linewidth ='3')\n",
    "    plt.xlabel(\"Value of K\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.show() # clear the plot\n",
    "    # the point of the elbow is the\n",
    "    # most optimal value for choosing k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69cdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette(data, clusters):\n",
    "    kmeans = KMeans(n_clusters = clusters, max_iter = 500, random_state = 0)\n",
    "    kmeans.fit_predict(data)\n",
    "    \n",
    "    #Create SilhouetteVisualizer instance with KMeans instance\n",
    "    visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')#ax[q-1][mod])#, ax = ax[x%3][y%2]\n",
    "    \n",
    "    #Fit the visualizer\n",
    "    visualizer.fit(data)\n",
    "    #fig, ax = plt.subplots(3,2, figsize = (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cfe1cb",
   "metadata": {},
   "source": [
    "## WESAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbabf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_dataset = pd.read_csv('Final_CSVs/wesad_new_with1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558bf019",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'WESAD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a49ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "age = []\n",
    "gender = []\n",
    "dominant_hand = []\n",
    "coffee_today = []\n",
    "coffee_hour = []\n",
    "sports = []\n",
    "smoker = []\n",
    "smoke_hour = []\n",
    "ill = []\n",
    "\n",
    "for x in range(2, 18):\n",
    "    #subjects 1 and 12 were not included in the published data\n",
    "    if x != 12:\n",
    "        f = open(data_dir + \"S\" + str(x) + '/S' + str(x) +\"_readme.txt\", \"r\")\n",
    "        content = f.readlines()\n",
    "        ids.append(x)\n",
    "        age.append(int(re.findall(\"\\d+\", content[1])[0]))\n",
    "        gender.append(re.findall(\"male|female\", content[4])[0])\n",
    "        dominant_hand.append(re.findall(\"right|left\", content[5])[0])\n",
    "        coffee_today.append(re.findall(\"YES|NO\", content[8])[0])\n",
    "        coffee_hour.append(re.findall(\"YES|NO\", content[9])[0])\n",
    "        sports.append(re.findall(\"YES|NO\", content[10])[0])\n",
    "        smoker.append(re.findall(\"YES|NO\", content[11])[0])\n",
    "        smoke_hour.append(re.findall(\"YES|NO\", content[12])[0])\n",
    "        ill.append(re.findall(\"YES|NO\", content[13])[0])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7984ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_extra = pd.DataFrame(\n",
    "                            {'id': ids,\n",
    "                             'age': age,\n",
    "                             'gender': gender,\n",
    "                             'dominant_hand': dominant_hand,\n",
    "                             'coffee_today': coffee_today,\n",
    "                             'coffee_hour': coffee_hour,\n",
    "                             'sports': sports,\n",
    "                             'smoker': smoker,\n",
    "                             'smoke_hour': smoke_hour,\n",
    "                             'ill': ill\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a5f75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>dominant_hand</th>\n",
       "      <th>coffee_today</th>\n",
       "      <th>coffee_hour</th>\n",
       "      <th>sports</th>\n",
       "      <th>smoker</th>\n",
       "      <th>smoke_hour</th>\n",
       "      <th>ill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>female</td>\n",
       "      <td>left</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age  gender dominant_hand coffee_today coffee_hour sports smoker  \\\n",
       "0    2   27    male         right           NO          NO     NO     NO   \n",
       "1    3   27    male         right           NO          NO     NO     NO   \n",
       "2    4   25    male         right           NO          NO     NO     NO   \n",
       "3    5   35    male         right          YES          NO     NO     NO   \n",
       "4    6   27    male         right          YES          NO     NO    YES   \n",
       "5    7   28    male         right           NO          NO    YES     NO   \n",
       "6    8   27  female          left          YES          NO    YES     NO   \n",
       "7    9   26    male         right           NO          NO     NO     NO   \n",
       "8   10   28    male         right           NO          NO     NO     NO   \n",
       "9   11   26  female         right          YES          NO     NO     NO   \n",
       "10  13   28    male         right           NO          NO     NO     NO   \n",
       "11  14   27    male         right           NO          NO     NO     NO   \n",
       "12  15   28    male         right           NO          NO     NO     NO   \n",
       "13  16   24    male         right           NO          NO     NO     NO   \n",
       "14  17   29  female         right           NO          NO     NO     NO   \n",
       "\n",
       "   smoke_hour  ill  \n",
       "0          NO   NO  \n",
       "1          NO   NO  \n",
       "2          NO   NO  \n",
       "3          NO   NO  \n",
       "4          NO   NO  \n",
       "5          NO   NO  \n",
       "6          NO   NO  \n",
       "7          NO  YES  \n",
       "8          NO   NO  \n",
       "9          NO   NO  \n",
       "10         NO   NO  \n",
       "11         NO   NO  \n",
       "12         NO   NO  \n",
       "13         NO   NO  \n",
       "14         NO   NO  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wesad_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da9f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary features\n",
    "\n",
    "wesad_extra.drop(['smoke_hour'], axis=1, inplace=True)\n",
    "wesad_extra.drop(['coffee_hour'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "458f6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding categorical features\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "wesad_extra['gender'] = le.fit_transform(wesad_extra['gender'])\n",
    "wesad_extra['dominant_hand'] = le.fit_transform(wesad_extra['dominant_hand'])\n",
    "wesad_extra['coffee_today'] = le.fit_transform(wesad_extra['coffee_today'])\n",
    "wesad_extra['sports'] = le.fit_transform(wesad_extra['sports'])\n",
    "wesad_extra['smoker'] = le.fit_transform(wesad_extra['smoker'])\n",
    "wesad_extra['ill'] = le.fit_transform(wesad_extra['ill'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75777d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "wesad_extra[['age']] = scaler.fit_transform(wesad_extra[['age']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d85edd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC110</th>\n",
       "      <th>ACC111</th>\n",
       "      <th>ACC1110</th>\n",
       "      <th>ACC1111</th>\n",
       "      <th>ACC1112</th>\n",
       "      <th>ACC1113</th>\n",
       "      <th>ACC1114</th>\n",
       "      <th>ACC1115</th>\n",
       "      <th>ACC1116</th>\n",
       "      <th>ACC1117</th>\n",
       "      <th>...</th>\n",
       "      <th>EDA11</th>\n",
       "      <th>EDA12</th>\n",
       "      <th>EDA13</th>\n",
       "      <th>TEMP10</th>\n",
       "      <th>TEMP11</th>\n",
       "      <th>TEMP12</th>\n",
       "      <th>TEMP13</th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>stress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275.095822</td>\n",
       "      <td>257.706505</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>335.958433</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>214.233212</td>\n",
       "      <td>257.706505</td>\n",
       "      <td>344.653091</td>\n",
       "      <td>370.737067</td>\n",
       "      <td>335.958433</td>\n",
       "      <td>...</td>\n",
       "      <td>-2015.464571</td>\n",
       "      <td>-944.071437</td>\n",
       "      <td>-904.398252</td>\n",
       "      <td>5090.486004</td>\n",
       "      <td>5090.486004</td>\n",
       "      <td>4860.722884</td>\n",
       "      <td>4860.722884</td>\n",
       "      <td>11</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-627.837227</td>\n",
       "      <td>-680.813133</td>\n",
       "      <td>-866.228804</td>\n",
       "      <td>-707.301086</td>\n",
       "      <td>-548.373367</td>\n",
       "      <td>-548.373367</td>\n",
       "      <td>-521.885414</td>\n",
       "      <td>-574.861320</td>\n",
       "      <td>-601.349274</td>\n",
       "      <td>-627.837227</td>\n",
       "      <td>...</td>\n",
       "      <td>-4070.109389</td>\n",
       "      <td>-4059.534817</td>\n",
       "      <td>-4080.683961</td>\n",
       "      <td>5464.362128</td>\n",
       "      <td>5464.362128</td>\n",
       "      <td>5704.145133</td>\n",
       "      <td>5704.145133</td>\n",
       "      <td>4</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-495.397461</td>\n",
       "      <td>-495.397461</td>\n",
       "      <td>-495.397461</td>\n",
       "      <td>-521.885414</td>\n",
       "      <td>-495.397461</td>\n",
       "      <td>-495.397461</td>\n",
       "      <td>-495.397461</td>\n",
       "      <td>-495.397461</td>\n",
       "      <td>-495.397461</td>\n",
       "      <td>-495.397461</td>\n",
       "      <td>...</td>\n",
       "      <td>-4799.854061</td>\n",
       "      <td>-4810.428633</td>\n",
       "      <td>-4799.854061</td>\n",
       "      <td>11219.154268</td>\n",
       "      <td>11219.154268</td>\n",
       "      <td>11219.154268</td>\n",
       "      <td>11219.154268</td>\n",
       "      <td>4</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-312.637815</td>\n",
       "      <td>-336.365602</td>\n",
       "      <td>-348.229495</td>\n",
       "      <td>-360.093388</td>\n",
       "      <td>-300.773921</td>\n",
       "      <td>-336.365602</td>\n",
       "      <td>-348.229495</td>\n",
       "      <td>-348.229495</td>\n",
       "      <td>-383.821175</td>\n",
       "      <td>-371.957282</td>\n",
       "      <td>...</td>\n",
       "      <td>1496.828511</td>\n",
       "      <td>1513.949214</td>\n",
       "      <td>1502.533927</td>\n",
       "      <td>-5817.578991</td>\n",
       "      <td>-5817.578991</td>\n",
       "      <td>-5632.552542</td>\n",
       "      <td>-5632.552542</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>-7.235882</td>\n",
       "      <td>...</td>\n",
       "      <td>-7228.750604</td>\n",
       "      <td>-7167.131140</td>\n",
       "      <td>-7167.131140</td>\n",
       "      <td>1150.312933</td>\n",
       "      <td>1049.554349</td>\n",
       "      <td>1049.554349</td>\n",
       "      <td>1049.554349</td>\n",
       "      <td>15</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78761</th>\n",
       "      <td>161.917923</td>\n",
       "      <td>161.917923</td>\n",
       "      <td>161.917923</td>\n",
       "      <td>161.917923</td>\n",
       "      <td>161.917923</td>\n",
       "      <td>173.781816</td>\n",
       "      <td>161.917923</td>\n",
       "      <td>161.917923</td>\n",
       "      <td>161.917923</td>\n",
       "      <td>161.917923</td>\n",
       "      <td>...</td>\n",
       "      <td>-1082.687849</td>\n",
       "      <td>-1076.982432</td>\n",
       "      <td>-1082.687849</td>\n",
       "      <td>11204.854337</td>\n",
       "      <td>11204.854337</td>\n",
       "      <td>11204.854337</td>\n",
       "      <td>11204.854337</td>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78762</th>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>168.767337</td>\n",
       "      <td>...</td>\n",
       "      <td>-3958.932937</td>\n",
       "      <td>-3969.283320</td>\n",
       "      <td>-3979.641795</td>\n",
       "      <td>6846.752098</td>\n",
       "      <td>6846.752098</td>\n",
       "      <td>6846.752098</td>\n",
       "      <td>6846.752098</td>\n",
       "      <td>16</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78763</th>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>-174.028735</td>\n",
       "      <td>...</td>\n",
       "      <td>-3702.069290</td>\n",
       "      <td>-3702.069290</td>\n",
       "      <td>-3702.069290</td>\n",
       "      <td>-410.537962</td>\n",
       "      <td>-410.537962</td>\n",
       "      <td>-410.537962</td>\n",
       "      <td>-410.537962</td>\n",
       "      <td>10</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78764</th>\n",
       "      <td>40.669509</td>\n",
       "      <td>48.001085</td>\n",
       "      <td>-61.972554</td>\n",
       "      <td>26.006357</td>\n",
       "      <td>-142.619890</td>\n",
       "      <td>-142.619890</td>\n",
       "      <td>55.332661</td>\n",
       "      <td>91.990541</td>\n",
       "      <td>187.301029</td>\n",
       "      <td>172.637877</td>\n",
       "      <td>...</td>\n",
       "      <td>3965.861145</td>\n",
       "      <td>3949.758752</td>\n",
       "      <td>4416.816285</td>\n",
       "      <td>-2681.315127</td>\n",
       "      <td>-2681.315127</td>\n",
       "      <td>-2681.315127</td>\n",
       "      <td>-2681.315127</td>\n",
       "      <td>8</td>\n",
       "      <td>Test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78765</th>\n",
       "      <td>-500.117448</td>\n",
       "      <td>-539.463612</td>\n",
       "      <td>-578.809776</td>\n",
       "      <td>-460.771284</td>\n",
       "      <td>-421.425120</td>\n",
       "      <td>-460.771284</td>\n",
       "      <td>-539.463612</td>\n",
       "      <td>-539.463612</td>\n",
       "      <td>-539.463612</td>\n",
       "      <td>-539.463612</td>\n",
       "      <td>...</td>\n",
       "      <td>7071.232425</td>\n",
       "      <td>7060.882043</td>\n",
       "      <td>7102.291665</td>\n",
       "      <td>10760.314222</td>\n",
       "      <td>10760.314222</td>\n",
       "      <td>10480.774070</td>\n",
       "      <td>10480.774070</td>\n",
       "      <td>16</td>\n",
       "      <td>Test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78766 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ACC110      ACC111     ACC1110     ACC1111     ACC1112     ACC1113  \\\n",
       "0      275.095822  257.706505  309.874457  335.958433  309.874457  214.233212   \n",
       "1     -627.837227 -680.813133 -866.228804 -707.301086 -548.373367 -548.373367   \n",
       "2     -495.397461 -495.397461 -495.397461 -521.885414 -495.397461 -495.397461   \n",
       "3     -312.637815 -336.365602 -348.229495 -360.093388 -300.773921 -336.365602   \n",
       "4       -7.235882   -7.235882   -7.235882   -7.235882   -7.235882   -7.235882   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  161.917923  161.917923  161.917923  161.917923  161.917923  173.781816   \n",
       "78762  168.767337  168.767337  168.767337  168.767337  168.767337  168.767337   \n",
       "78763 -174.028735 -174.028735 -174.028735 -174.028735 -174.028735 -174.028735   \n",
       "78764   40.669509   48.001085  -61.972554   26.006357 -142.619890 -142.619890   \n",
       "78765 -500.117448 -539.463612 -578.809776 -460.771284 -421.425120 -460.771284   \n",
       "\n",
       "          ACC1114     ACC1115     ACC1116     ACC1117  ...        EDA11  \\\n",
       "0      257.706505  344.653091  370.737067  335.958433  ... -2015.464571   \n",
       "1     -521.885414 -574.861320 -601.349274 -627.837227  ... -4070.109389   \n",
       "2     -495.397461 -495.397461 -495.397461 -495.397461  ... -4799.854061   \n",
       "3     -348.229495 -348.229495 -383.821175 -371.957282  ...  1496.828511   \n",
       "4       -7.235882   -7.235882   -7.235882   -7.235882  ... -7228.750604   \n",
       "...           ...         ...         ...         ...  ...          ...   \n",
       "78761  161.917923  161.917923  161.917923  161.917923  ... -1082.687849   \n",
       "78762  168.767337  168.767337  168.767337  168.767337  ... -3958.932937   \n",
       "78763 -174.028735 -174.028735 -174.028735 -174.028735  ... -3702.069290   \n",
       "78764   55.332661   91.990541  187.301029  172.637877  ...  3965.861145   \n",
       "78765 -539.463612 -539.463612 -539.463612 -539.463612  ...  7071.232425   \n",
       "\n",
       "             EDA12        EDA13        TEMP10        TEMP11        TEMP12  \\\n",
       "0      -944.071437  -904.398252   5090.486004   5090.486004   4860.722884   \n",
       "1     -4059.534817 -4080.683961   5464.362128   5464.362128   5704.145133   \n",
       "2     -4810.428633 -4799.854061  11219.154268  11219.154268  11219.154268   \n",
       "3      1513.949214  1502.533927  -5817.578991  -5817.578991  -5632.552542   \n",
       "4     -7167.131140 -7167.131140   1150.312933   1049.554349   1049.554349   \n",
       "...            ...          ...           ...           ...           ...   \n",
       "78761 -1076.982432 -1082.687849  11204.854337  11204.854337  11204.854337   \n",
       "78762 -3969.283320 -3979.641795   6846.752098   6846.752098   6846.752098   \n",
       "78763 -3702.069290 -3702.069290   -410.537962   -410.537962   -410.537962   \n",
       "78764  3949.758752  4416.816285  -2681.315127  -2681.315127  -2681.315127   \n",
       "78765  7060.882043  7102.291665  10760.314222  10760.314222  10480.774070   \n",
       "\n",
       "             TEMP13  id  dataset  stress  \n",
       "0       4860.722884  11    Train     1.0  \n",
       "1       5704.145133   4    Train     0.0  \n",
       "2      11219.154268   4    Train     0.0  \n",
       "3      -5632.552542   2    Train     1.0  \n",
       "4       1049.554349  15    Train     0.0  \n",
       "...             ...  ..      ...     ...  \n",
       "78761  11204.854337   2     Test     0.0  \n",
       "78762   6846.752098  16     Test     0.0  \n",
       "78763   -410.537962  10     Test     0.0  \n",
       "78764  -2681.315127   8     Test     1.0  \n",
       "78765  10480.774070  16     Test     1.0  \n",
       "\n",
       "[78766 rows x 171 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wesad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58610fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_features = pd.DataFrame()\n",
    "wesad_features['id'] = wesad_dataset['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e99fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cols = [col for col in wesad_dataset.columns if 'ACC' in col]\n",
    "bvp_cols = [col for col in wesad_dataset.columns if 'BVP' in col]\n",
    "eda_cols = [col for col in wesad_dataset.columns if 'EDA' in col]\n",
    "temp_cols = [col for col in wesad_dataset.columns if 'TEMP' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "295e7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_features[\"ACC\"] = wesad_dataset[acc_cols].apply(lambda x: statistics.mean(x), axis =1)\n",
    "wesad_features[\"BVP\"] = wesad_dataset[bvp_cols].apply(lambda x: statistics.mean(x), axis =1)\n",
    "wesad_features[\"EDA\"] = wesad_dataset[eda_cols].apply(lambda x: statistics.mean(x), axis =1)\n",
    "wesad_features[\"TEMP\"] = wesad_dataset[temp_cols].apply(lambda x: statistics.mean(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856012b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0831e3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ACC</th>\n",
       "      <th>BVP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>TEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>-11.378692</td>\n",
       "      <td>20.311753</td>\n",
       "      <td>-1466.543609</td>\n",
       "      <td>4975.604444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-347.879380</td>\n",
       "      <td>15.940493</td>\n",
       "      <td>-4070.109389</td>\n",
       "      <td>5584.253630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>-348.862397</td>\n",
       "      <td>-19.024111</td>\n",
       "      <td>-4799.854061</td>\n",
       "      <td>11219.154268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-701.771327</td>\n",
       "      <td>0.997293</td>\n",
       "      <td>1495.401043</td>\n",
       "      <td>-5725.065767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>203.402497</td>\n",
       "      <td>1.623619</td>\n",
       "      <td>-7197.940872</td>\n",
       "      <td>1074.743995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78761</th>\n",
       "      <td>2</td>\n",
       "      <td>376.430602</td>\n",
       "      <td>0.419619</td>\n",
       "      <td>-1081.261495</td>\n",
       "      <td>11204.854337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78762</th>\n",
       "      <td>16</td>\n",
       "      <td>407.115692</td>\n",
       "      <td>-0.705260</td>\n",
       "      <td>-3969.285343</td>\n",
       "      <td>6846.752098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78763</th>\n",
       "      <td>10</td>\n",
       "      <td>475.350796</td>\n",
       "      <td>1.908681</td>\n",
       "      <td>-3700.304886</td>\n",
       "      <td>-410.537962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78764</th>\n",
       "      <td>8</td>\n",
       "      <td>78.582199</td>\n",
       "      <td>22.516438</td>\n",
       "      <td>4058.468791</td>\n",
       "      <td>-2681.315127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78765</th>\n",
       "      <td>16</td>\n",
       "      <td>-1495.204557</td>\n",
       "      <td>3.403335</td>\n",
       "      <td>7084.174449</td>\n",
       "      <td>10620.544146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78766 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          ACC        BVP          EDA          TEMP\n",
       "0      11   -11.378692  20.311753 -1466.543609   4975.604444\n",
       "1       4  -347.879380  15.940493 -4070.109389   5584.253630\n",
       "2       4  -348.862397 -19.024111 -4799.854061  11219.154268\n",
       "3       2  -701.771327   0.997293  1495.401043  -5725.065767\n",
       "4      15   203.402497   1.623619 -7197.940872   1074.743995\n",
       "...    ..          ...        ...          ...           ...\n",
       "78761   2   376.430602   0.419619 -1081.261495  11204.854337\n",
       "78762  16   407.115692  -0.705260 -3969.285343   6846.752098\n",
       "78763  10   475.350796   1.908681 -3700.304886   -410.537962\n",
       "78764   8    78.582199  22.516438  4058.468791  -2681.315127\n",
       "78765  16 -1495.204557   3.403335  7084.174449  10620.544146\n",
       "\n",
       "[78766 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wesad_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23789dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ACC_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>502.045422</td>\n",
       "      <td>8.621274</td>\n",
       "      <td>1565.524486</td>\n",
       "      <td>9833.841562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>348.669329</td>\n",
       "      <td>13.589607</td>\n",
       "      <td>1619.742066</td>\n",
       "      <td>13895.165208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>370.881501</td>\n",
       "      <td>21.902177</td>\n",
       "      <td>7910.193229</td>\n",
       "      <td>13123.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>697.334405</td>\n",
       "      <td>23.424893</td>\n",
       "      <td>3226.998622</td>\n",
       "      <td>7628.694799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>237.185010</td>\n",
       "      <td>18.639532</td>\n",
       "      <td>34943.846434</td>\n",
       "      <td>13295.477323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>75.455974</td>\n",
       "      <td>8.603391</td>\n",
       "      <td>7464.238351</td>\n",
       "      <td>26954.510152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>476.180921</td>\n",
       "      <td>20.460376</td>\n",
       "      <td>2823.491721</td>\n",
       "      <td>6843.442197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>278.579577</td>\n",
       "      <td>18.516263</td>\n",
       "      <td>3019.595459</td>\n",
       "      <td>23193.261691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>529.447035</td>\n",
       "      <td>13.188113</td>\n",
       "      <td>5117.328689</td>\n",
       "      <td>3924.886808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>231.055206</td>\n",
       "      <td>38.583819</td>\n",
       "      <td>7234.572573</td>\n",
       "      <td>13222.537402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>257.480857</td>\n",
       "      <td>13.841357</td>\n",
       "      <td>4529.804209</td>\n",
       "      <td>4004.667906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>338.080956</td>\n",
       "      <td>26.784146</td>\n",
       "      <td>330.120216</td>\n",
       "      <td>16649.852572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>143.848260</td>\n",
       "      <td>22.178697</td>\n",
       "      <td>10608.990479</td>\n",
       "      <td>1540.163406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>821.253347</td>\n",
       "      <td>35.094980</td>\n",
       "      <td>5983.117488</td>\n",
       "      <td>7690.558549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>440.267344</td>\n",
       "      <td>16.187133</td>\n",
       "      <td>1040.932509</td>\n",
       "      <td>10284.874706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     ACC_std    BVP_std       EDA_std      TEMP_std\n",
       "0    2  502.045422   8.621274   1565.524486   9833.841562\n",
       "1    3  348.669329  13.589607   1619.742066  13895.165208\n",
       "2    4  370.881501  21.902177   7910.193229  13123.172200\n",
       "3    5  697.334405  23.424893   3226.998622   7628.694799\n",
       "4    6  237.185010  18.639532  34943.846434  13295.477323\n",
       "5    7   75.455974   8.603391   7464.238351  26954.510152\n",
       "6    8  476.180921  20.460376   2823.491721   6843.442197\n",
       "7    9  278.579577  18.516263   3019.595459  23193.261691\n",
       "8   10  529.447035  13.188113   5117.328689   3924.886808\n",
       "9   11  231.055206  38.583819   7234.572573  13222.537402\n",
       "10  13  257.480857  13.841357   4529.804209   4004.667906\n",
       "11  14  338.080956  26.784146    330.120216  16649.852572\n",
       "12  15  143.848260  22.178697  10608.990479   1540.163406\n",
       "13  16  821.253347  35.094980   5983.117488   7690.558549\n",
       "14  17  440.267344  16.187133   1040.932509  10284.874706"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wesad_mean = wesad_features.groupby('id', as_index = False, group_keys = True).mean()\n",
    "wesad_mean.columns = map(lambda x: x + '_mean', wesad_mean.columns)\n",
    "wesad_mean = wesad_mean.rename({'id_mean': 'id'}, axis='columns')\n",
    "wesad_mean\n",
    "\n",
    "wesad_min = wesad_features.groupby('id', as_index = False, group_keys = True).min()\n",
    "wesad_min.columns = map(lambda x: x + '_min', wesad_min.columns)\n",
    "wesad_min = wesad_min.rename({'id_min': 'id'}, axis='columns')\n",
    "wesad_min\n",
    "\n",
    "wesad_std = wesad_features.groupby('id', as_index = False, group_keys = True).std()\n",
    "wesad_std.columns = map(lambda x: x + '_std', wesad_std.columns)\n",
    "wesad_std = wesad_std.rename({'id_std': 'id'}, axis='columns')\n",
    "wesad_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b81b6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "wesad_mean.loc[:, wesad_mean.columns!='id'] = scaler.fit_transform(wesad_mean.loc[:, wesad_mean.columns!='id'])\n",
    "wesad_min.loc[:, wesad_min.columns!='id'] = scaler.fit_transform(wesad_min.loc[:, wesad_min.columns!='id'])\n",
    "wesad_std.loc[:, wesad_std.columns!='id'] = scaler.fit_transform(wesad_std.loc[:, wesad_std.columns!='id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3fbd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_all_grouped = pd.merge(wesad_extra, wesad_mean, on='id')\n",
    "wesad_all_grouped = pd.merge(wesad_min, wesad_all_grouped, on='id')\n",
    "wesad_all_grouped = pd.merge(wesad_std, wesad_all_grouped, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfea9cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ACC_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>ACC_min</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>EDA_min</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>dominant_hand</th>\n",
       "      <th>coffee_today</th>\n",
       "      <th>sports</th>\n",
       "      <th>smoker</th>\n",
       "      <th>ill</th>\n",
       "      <th>ACC_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>TEMP_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.571991</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.035691</td>\n",
       "      <td>0.326338</td>\n",
       "      <td>0.592250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975537</td>\n",
       "      <td>0.756190</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.613435</td>\n",
       "      <td>0.647604</td>\n",
       "      <td>0.535588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.366337</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.037258</td>\n",
       "      <td>0.486143</td>\n",
       "      <td>0.811092</td>\n",
       "      <td>0.446881</td>\n",
       "      <td>0.981260</td>\n",
       "      <td>0.689573</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.559606</td>\n",
       "      <td>0.650124</td>\n",
       "      <td>0.616729</td>\n",
       "      <td>0.752105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.396120</td>\n",
       "      <td>0.443582</td>\n",
       "      <td>0.218990</td>\n",
       "      <td>0.455767</td>\n",
       "      <td>0.849084</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.872678</td>\n",
       "      <td>0.588313</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.355065</td>\n",
       "      <td>0.234184</td>\n",
       "      <td>0.654142</td>\n",
       "      <td>0.260498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.833844</td>\n",
       "      <td>0.494373</td>\n",
       "      <td>0.083692</td>\n",
       "      <td>0.239571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576034</td>\n",
       "      <td>0.932509</td>\n",
       "      <td>0.762333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.104599</td>\n",
       "      <td>0.476529</td>\n",
       "      <td>0.811299</td>\n",
       "      <td>0.521898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.216854</td>\n",
       "      <td>0.334756</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462546</td>\n",
       "      <td>0.320005</td>\n",
       "      <td>0.669746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654837</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.784146</td>\n",
       "      <td>0.576738</td>\n",
       "      <td>0.929398</td>\n",
       "      <td>0.586744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997203</td>\n",
       "      <td>0.723089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.470799</td>\n",
       "      <td>0.442501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.645543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.537311</td>\n",
       "      <td>0.395491</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.208673</td>\n",
       "      <td>0.620727</td>\n",
       "      <td>0.658468</td>\n",
       "      <td>0.943927</td>\n",
       "      <td>0.870573</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.314885</td>\n",
       "      <td>0.520319</td>\n",
       "      <td>0.505342</td>\n",
       "      <td>0.503070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.272358</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>0.852003</td>\n",
       "      <td>0.156006</td>\n",
       "      <td>0.799970</td>\n",
       "      <td>0.935449</td>\n",
       "      <td>0.436242</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710407</td>\n",
       "      <td>0.690369</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.608732</td>\n",
       "      <td>0.152924</td>\n",
       "      <td>0.138304</td>\n",
       "      <td>0.093834</td>\n",
       "      <td>0.367236</td>\n",
       "      <td>0.913051</td>\n",
       "      <td>0.905989</td>\n",
       "      <td>0.911635</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236879</td>\n",
       "      <td>0.600611</td>\n",
       "      <td>0.685666</td>\n",
       "      <td>0.479416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.208635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.199472</td>\n",
       "      <td>0.459676</td>\n",
       "      <td>0.716657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676996</td>\n",
       "      <td>0.755713</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244917</td>\n",
       "      <td>0.533140</td>\n",
       "      <td>0.156897</td>\n",
       "      <td>0.506410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0.244067</td>\n",
       "      <td>0.174713</td>\n",
       "      <td>0.121330</td>\n",
       "      <td>0.096973</td>\n",
       "      <td>0.690240</td>\n",
       "      <td>0.997059</td>\n",
       "      <td>0.877005</td>\n",
       "      <td>0.932686</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.161804</td>\n",
       "      <td>0.808773</td>\n",
       "      <td>0.619289</td>\n",
       "      <td>0.910672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>0.352140</td>\n",
       "      <td>0.606421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594534</td>\n",
       "      <td>0.896906</td>\n",
       "      <td>0.418909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565476</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958525</td>\n",
       "      <td>0.277592</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>0.091704</td>\n",
       "      <td>0.452806</td>\n",
       "      <td>0.296959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969406</td>\n",
       "      <td>0.718206</td>\n",
       "      <td>0.737034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.722320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883629</td>\n",
       "      <td>0.163317</td>\n",
       "      <td>0.242005</td>\n",
       "      <td>0.079057</td>\n",
       "      <td>0.033250</td>\n",
       "      <td>0.894303</td>\n",
       "      <td>0.842271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447085</td>\n",
       "      <td>0.752585</td>\n",
       "      <td>0.709978</td>\n",
       "      <td>0.373644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>0.489156</td>\n",
       "      <td>0.252956</td>\n",
       "      <td>0.020536</td>\n",
       "      <td>0.344086</td>\n",
       "      <td>0.685858</td>\n",
       "      <td>0.885359</td>\n",
       "      <td>0.980889</td>\n",
       "      <td>0.829027</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587600</td>\n",
       "      <td>0.873670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   ACC_std   BVP_std   EDA_std  TEMP_std   ACC_min   BVP_min   EDA_min  \\\n",
       "0    2  0.571991  0.000596  0.035691  0.326338  0.592250  1.000000  0.975537   \n",
       "1    3  0.366337  0.166316  0.037258  0.486143  0.811092  0.446881  0.981260   \n",
       "2    4  0.396120  0.443582  0.218990  0.455767  0.849084  0.834331  0.872678   \n",
       "3    5  0.833844  0.494373  0.083692  0.239571  0.000000  0.576034  0.932509   \n",
       "4    6  0.216854  0.334756  1.000000  0.462546  0.320005  0.669746  0.000000   \n",
       "5    7  0.000000  0.000000  0.206107  1.000000  1.000000  0.997203  0.723089   \n",
       "6    8  0.537311  0.395491  0.072034  0.208673  0.620727  0.658468  0.943927   \n",
       "7    9  0.272358  0.330645  0.077700  0.852003  0.156006  0.799970  0.935449   \n",
       "8   10  0.608732  0.152924  0.138304  0.093834  0.367236  0.913051  0.905989   \n",
       "9   11  0.208635  1.000000  0.199472  0.459676  0.716657  0.000000  0.676996   \n",
       "10  13  0.244067  0.174713  0.121330  0.096973  0.690240  0.997059  0.877005   \n",
       "11  14  0.352140  0.606421  0.000000  0.594534  0.896906  0.418909  1.000000   \n",
       "12  15  0.091704  0.452806  0.296959  0.000000  0.969406  0.718206  0.737034   \n",
       "13  16  1.000000  0.883629  0.163317  0.242005  0.079057  0.033250  0.894303   \n",
       "14  17  0.489156  0.252956  0.020536  0.344086  0.685858  0.885359  0.980889   \n",
       "\n",
       "    TEMP_min       age  gender  dominant_hand  coffee_today  sports  smoker  \\\n",
       "0   0.756190  0.272727       1              1             0       0       0   \n",
       "1   0.689573  0.272727       1              1             0       0       0   \n",
       "2   0.588313  0.090909       1              1             0       0       0   \n",
       "3   0.762333  1.000000       1              1             1       0       0   \n",
       "4   0.654837  0.272727       1              1             1       0       1   \n",
       "5   0.000000  0.363636       1              1             0       1       0   \n",
       "6   0.870573  0.272727       0              0             1       1       0   \n",
       "7   0.436242  0.181818       1              1             0       0       0   \n",
       "8   0.911635  0.363636       1              1             0       0       0   \n",
       "9   0.755713  0.181818       0              1             1       0       0   \n",
       "10  0.932686  0.363636       1              1             0       0       0   \n",
       "11  0.565476  0.272727       1              1             0       0       0   \n",
       "12  1.000000  0.363636       1              1             0       0       0   \n",
       "13  0.842271  0.000000       1              1             0       0       0   \n",
       "14  0.829027  0.454545       0              1             0       0       0   \n",
       "\n",
       "    ill  ACC_mean  BVP_mean  EDA_mean  TEMP_mean  \n",
       "0     0  0.716677  0.613435  0.647604   0.535588  \n",
       "1     0  0.559606  0.650124  0.616729   0.752105  \n",
       "2     0  0.355065  0.234184  0.654142   0.260498  \n",
       "3     0  0.104599  0.476529  0.811299   0.521898  \n",
       "4     0  0.784146  0.576738  0.929398   0.586744  \n",
       "5     0  0.470799  0.442501  0.000000   0.645543  \n",
       "6     0  0.314885  0.520319  0.505342   0.503070  \n",
       "7     1  0.000000  0.710407  0.690369   1.000000  \n",
       "8     0  0.236879  0.600611  0.685666   0.479416  \n",
       "9     0  0.244917  0.533140  0.156897   0.506410  \n",
       "10    0  0.161804  0.808773  0.619289   0.910672  \n",
       "11    0  1.000000  0.958525  0.277592   0.000000  \n",
       "12    0  0.722320  1.000000  1.000000   0.424200  \n",
       "13    0  0.447085  0.752585  0.709978   0.373644  \n",
       "14    0  0.014397  0.000000  0.587600   0.873670  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wesad_all_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbaf03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = wesad_all_grouped[\"id\"]\n",
    "wesad_all_grouped = wesad_all_grouped.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15e7a0",
   "metadata": {},
   "source": [
    "## Hard Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "112b949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering(wesad_all_grouped, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b43901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "silhouette(wesad_all_grouped, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07c6ad3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster\n",
       "0          5\n",
       "1          5\n",
       "2          5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 3, max_iter = 500, random_state = 0)\n",
    "y = kmeans.fit_predict(wesad_all_grouped)\n",
    "y = pd.DataFrame(y, columns=[\"Cluster\"])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b9fb9",
   "metadata": {},
   "source": [
    "####  Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ab9a7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b2909ff9d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqkAAAHTCAYAAAAEd4H9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8vUlEQVR4nO3de3RU9b3//9fkPiFAIIQYiCXlakBIYlLiBUUQFRArBuxF/SKlHm0N8Fs/LbTxcupREMHTVmsjimJtS1W+GK0etLZqaxUQpIEEgQBJMBgNxEQIATLJ5LK/f3gcHXKbDZk9O5PnY60sVvbnM5k3L2fCyz2THYdhGIYAAAAAGwkJ9AAAAADA6SipAAAAsB1KKgAAAGyHkgoAAADboaQCAADAdiipAAAAsB1KKgAAAGyHkgoAAADbCQv0AN1l586dMgxD4eHhgR4FAAAA7WhqapLD4VB6enqXe4PmTKphGArEL88yDENutzsg991TkZl5ZGYemZlHZuaRmXlkZl4wZWamrwXNmdSvzqCOHz/e0vutr69XcXGxRo4cqejoaEvvu6ciM/PIzDwyM4/MzCMz88jMvGDK7KOPPvJ5b7efSa2qqtLixYs1ceJEXXrppVqxYoUaGxslSRUVFZo/f77S0tI0c+ZMbdq0yeu2W7Zs0axZs5Samqp58+apoqKiu8cDAABAD9CtJdUwDC1evFgul0t//vOf9Zvf/Eb//Oc/9eijj8owDOXk5GjQoEHKz8/Xddddp4ULF6qyslKSVFlZqZycHGVnZ+ull17SwIEDdccddwTFqW0AAACY060v9x88eFCFhYXavHmzBg0aJElavHixVq5cqcsuu0wVFRV68cUXFR0drREjRuiDDz5Qfn6+Fi1apA0bNuj888/XggULJEkrVqzQJZdcog8//FBZWVndOSYAAABsrlvPpMbHx+uZZ57xFNSvnDx5UkVFRRo7dqzXeykyMjJUWFgoSSoqKlJmZqZnzel0aty4cZ51AAAA9B7deia1X79+uvTSSz2ft7a2at26dbrwwgtVXV2twYMHe+2Pi4vTkSNHJKnLdV8YhqH6+vqz+BuY53K5vP5E18jMPDIzj8zMIzPzyMw8MjMvmDIzDEMOh8OnvX796f5HHnlEe/fu1UsvvaTnnntOERERXusRERFyu92Svgy+s3VfNDU1qbi4+OwHPwPl5eUBud+ejMzMIzPzyMw8MjOPzMwjM/OCJbPT+15H/FZSH3nkEf3hD3/Qb37zG40ePVqRkZGqra312uN2uxUVFSVJioyMbFNI3W63+vXr5/N9hoeHa+TIkWc9uxkul0vl5eVKTk6W0+m09L57KjIzj8zMIzPzyMw8MjOPzMwLpsxKS0t93uuXkvrggw/qhRde0COPPKKrr75akpSQkNBmsJqaGs9L/AkJCaqpqWmznpKS4vP9OhyOgF0/zOl09vhrl1mNzMwjM/PIzDwyM4/MzCMz84IhM19f6pf8cJ3U3/3ud3rxxRf161//Wtdcc43neGpqqvbs2aOGhgbPsYKCAqWmpnrWCwoKPGsul0t79+71rAMAAKD36NaSWlZWpieeeEL/8R//oYyMDFVXV3s+Jk6cqMTEROXm5qqkpERr1qzRrl27NHfuXEnSnDlztGPHDq1Zs0YlJSXKzc1VUlISl58CAADohbq1pL7zzjtqaWnR6tWrNWnSJK+P0NBQPfHEE6qurlZ2drZee+015eXlaciQIZKkpKQkPf7448rPz9fcuXNVW1urvLw8U6eFAQAAEBy69T2pt912m2677bYO14cNG6Z169Z1uD558mRNnjy5O0cCAMDWjNZWqalJjsjIQI8C2IpfL0EFAADa1/DBVp165hk1FRdL7iY5Bg5QZNaF6rfkLoWYuLINEKwoqQAAWOzkn9bpxCP/rdYvvvj64OHDat6zV+7t2zVw3R8VdtpvbwR6m27/6X4AANCxlqoqnXjsMe+C+g1NH32k40uWWjwVYD+UVAAALHTi8Ty1Hu78V3437ixUc+VhiyYC7ImSCgCAhZrKuv6NO0Z1tRre+rsF0wD2RUkFAMCOWloCPQEQUJRUAAAsFDZsWJd7QuLiFDltmgXTAPZFSQUAwEJ9Fy1USHx8p3vCUyco/FvfsmgiwJ4oqYBNtLa2qqysTNu2bVNRUZEaGxsDPRIAPwgbOlQxP/2JHLGx7a+fd55iH37Y2qEAG+I6qYAN7N+/Xx999JGOHTsmwzAkSXv27FFSUpImTZoU4OkAdLe+t9+msORknfrTn9S8b79a3W6FDByoyMwM9f3Fz7lGKiBKKhBwBw4c0NatW9ucOT158qT27dunxsZGXXLJJQGaDoC/OK++Ss6rr1KryyXD5VJIv35yhPHPMvAVng1AABmGoY8++qjTl/YrKipUVVVl4VQArBTidEpOZ6DHAGyH96QCAXTo0CEdO3as0z3Nzc06cOCARRMBAGAPlFQggI4eParW1tYu97ndbgumAQDAPiipQAA5fXyJLzQ01M+TAABgL5RUIIBGjhypfv36dblv6NChFkwDAIB9UFKBAAoPD9e3urhg96BBgzRixAiLJgIAwB746X4gwC666CI1Njbq0KFDXu89dTgcGjRokK644gqFhPD/kwCA3oWSCgSYw+HQlClTVFNTo927d6uhoUGhoaEaNmyYRo4cqZCQENXX1wd6TAAALEVJBWxi0KBBuvzyywM9BgAAtsBriAAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA2/FbSXW73Zo1a5a2bdvmObZs2TKNGTPG62PdunWe9Y0bN2ratGlKTU1VTk6Ojh496q/xAAAAYGN+KamNjY268847VVJS4nW8rKxMd911lzZt2uT5mDNnjiRp165duueee7Rw4UKtX79edXV1ys3N9cd4AAAAsLmw7v6CpaWluuuuu2QYRpu1srIy/fjHP1Z8fHybtXXr1mnGjBmaPXu2JGnVqlWaMmWKKioqdO6553b3mAAAALCxbj+T+uGHHyorK0vr16/3On7y5ElVVVUpOTm53dsVFRUpMzPT83liYqKGDBmioqKi7h4RAAAANtftZ1JvvPHGdo+XlZXJ4XDoySef1HvvvafY2Fj96Ec/0vXXXy9J+vzzzzV48GCv28TFxenIkSM+37dhGKqvrz/z4c+Ay+Xy+hNdIzPzyMw8MjOPzMwjM/PIzLxgyswwDDkcDp/2dntJ7cjBgwflcDg0fPhw3Xzzzdq+fbvuu+8+xcTE6Morr1RDQ4MiIiK8bhMRESG32+3zfTQ1Nam4uLi7R/dJeXl5QO63JyMz88jMPDIzj8zMIzPzyMy8YMns9L7XEctK6uzZszVlyhTFxsZKks477zyVl5frhRde0JVXXqnIyMg2hdTtdsvpdPp8H+Hh4Ro5cmR3jt0ll8ul8vJyJScnm5q1NyMz88jMPDIzj8zMIzPzyMy8YMqstLTU572WlVSHw+EpqF8ZPny4tm7dKklKSEhQTU2N13pNTU27P2TV2X1ER0ef9axnwul0Buy+eyoyM4/MzCMz88jMPDIzj8zMC4bMfH2pX7LwYv6PPfaY5s+f73Vs3759Gj58uCQpNTVVBQUFnrXDhw/r8OHDSk1NtWpEAAAA2IRlJXXKlCnavn271q5dq08++UTPP/+8/vKXv2jBggWSpB/+8Id69dVXtWHDBu3bt09Lly7V5ZdfzuWnAAAAeiHLXu6fMGGCHnvsMf32t7/VY489pqFDh+pXv/qV0tPTJUnp6el64IEH9Nvf/lbHjx/XJZdcogcffNCq8QAAAGAjfi2p+/fv9/p82rRpmjZtWof7s7OzlZ2d7c+RAAAA0ANY9nI/AAAA4CtKKgAAAGyHkgoAAADboaQCAADAdiipAAAAsB1KKgAAAGyHkgoAAADboaQCAADAdiipAAAAsB1KKgAAAGyHkgoAAADboaQCAADAdiipAAAAsB1KKgAAAGyHkgoAAADboaQCAADAdiipAAAAsB1KKgAAAGyHkgoAAADboaQCAADAdiipAGBThmHIMIxAjwEAAREW6AEAAN7q//qm6l94Qc1lB6WWFoUmJirqqqsUc9utcoSGBno8ALAEJRUAbOT4iod18rnnpJOnPMdaKirk3r5d7q0faODaZ+QI41s3gODHy/0AYBMN776rk79/zqugehiGGt5+R8cfftjyuQAgECipAGATp/74J+lUOwX1GxrffU9GS4tFEwFA4FBSAcAmmj8u73pPebmaP/7Y/8MAQIBRUgGgJzEMqbU10FMAgN9RUgHAJkKHDulyT1hSksKGDbNgGgAILEoqANhE9Nw5UkREp3sisrLkiIy0aCIACBxKKgDYhPO66+ScfZ0UHt7uevh3vqP+9/+nxVMBQGBwsT0AsAmHw6EBv/6VwseMkeuNN778QarWFoWec44iL7pY/e7+hUKiowM9JgBYgpIKADbicDjU9ye3K+b229RaUyM1NyskPp4L+APodfiuBwA25HA4FBofH+gxACBgKKkAAAC9jMvlUmFhoerq6iRJ8fHxGj9+vMI7eE98IFBSAQAAepFdu3Zp165dqq+v9xw7dOiQDhw4oAsvvFDJycmBG+4b+Ol+AACAXuLgwYPasWOHV0H9Sl1dnTZv3qza2lrrB2sHJRUAAKCXKC4ultvt7nD91KlT2rlzp4UTdYySCgAA0Au4XC4dPXq0y301NTUWTNM1SioAAEAv4Ha71dzc3OW+lpYWC6bpGiUVAACgF4iOjlakD79W2Zc9VqCkAgAA9ALh4eGK9+H6y0OGDLFgmq5RUgEAAHqJzMxM9evXr8P1QYMGKT093cKJOkZJBQAA6CUGDBigK664Quecc47XhfsjIyOVlJSk6dOnKyIiIoATfs1vJdXtdmvWrFnatm2b51hFRYXmz5+vtLQ0zZw5U5s2bfK6zZYtWzRr1iylpqZq3rx5qqio8Nd4AAAAvVJ8fLy++93vasaMGUpLS9MFF1yg6667TjNnzlR0dHSgx/PwS0ltbGzUnXfeqZKSEs8xwzCUk5OjQYMGKT8/X9ddd50WLlyoyspKSVJlZaVycnKUnZ2tl156SQMHDtQdd9whwzD8MSIAAECvds4552jixInKzMxUbGxsoMdpo9tLamlpqb73ve/pk08+8Tq+detWVVRU6IEHHtCIESN0++23Ky0tTfn5+ZKkDRs26Pzzz9eCBQs0atQorVixQp999pk+/PDD7h4RAAAANtftJfXDDz9UVlaW1q9f73W8qKhIY8eO9TqNnJGRocLCQs96ZmamZ83pdGrcuHGedQAAAPQeYd39BW+88cZ2j1dXV2vw4MFex+Li4nTkyBGf1n1hGEa7v4vWn1wul9ef6BqZmUdm5pGZeWRmHpmZR2bmBVNmhmHI4XD4tLfbS2pHXC5Xm58Wi4iI8Pz+2K7WfdHU1KTi4uKzH/YMlJeXB+R+ezIyM4/MzCMz88jMPDIzj8zMC5bMfL16gGUlNTIyUrW1tV7H3G63oqKiPOunF1K3293ptbxOFx4erpEjR571rGa4XC6Vl5crOTlZTqfT0vvuqcjMPDIzj8zMIzPzyMw8MjMvmDIrLS31ea9lJTUhIaHNYDU1NZ6X+BMSElRTU9NmPSUlxef7cDgcAbt0gtPptNVlG3oCMjOPzMwjM/PIzDwyM4/MzAuGzHx9qV+y8GL+qamp2rNnjxoaGjzHCgoKlJqa6lkvKCjwrLlcLu3du9ezbhuGIe17TXr5/0gbfqDwd36usIaarm8HAAAAn1l2JnXixIlKTExUbm6u7rjjDv3zn//Url27tGLFCknSnDlztHbtWq1Zs0ZTpkxRXl6ekpKSlJWVZdWIXTteIb30A6myQGpplCSFS0qJfFGO4/9HmvnrwM4HAAAQJCw7kxoaGqonnnhC1dXVys7O1muvvaa8vDwNGTJEkpSUlKTHH39c+fn5mjt3rmpra5WXl2fqtLBfNbul9XOkii2egvqViMYahe14Unp/RYCGAwAACC5+PZO6f/9+r8+HDRumdevWdbh/8uTJmjx5sj9HOnM7npYq/93hsqPZJe1+Ubp4iRRq2QlqAACAoGTZmdQer+QNSV38itbP90glf7VkHAAAgGBGSfWV24dfEmC0SHUV/p8FAAAgyFFSfRXVv+s9oZHSIN8vmQUAAID2UVJ9Ne77UkgX7zVNmCB9+3IrpgEAAAhqlFRfnf99adhlHS4bUbHSd+6Q7HI1AgAAgB6MkuqrkBDpB69K510vRQ/yHDbkUH3Mt+W+9H4pfX7AxgMAAAgmXCvJjMgY6QcvS1+USQVPSU31cseO0r6ILJ03bkKgpwMAAAgalNQzETdCumqVJKmlvl5GcXGABwIAAAguvNwPAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALAdSioAAABsh5IKAAAA26GkAgAAwHYoqQAAALCdsEAPAAAAupfR3KymfftlNDYofPhwhQwYEOiRANMoqQAABAmjtVV1qx5R4zv/UFNJidTUpNAhQxSenqb+99+vsCGJgR4R8BklFQCAIGAYho4tXCzXxo1SS4vneEtlpVoqK9VcUqq4dX9U2NChAZwS8B3vSQUAIAi4Xn9drr++4VVQv6n5wAEd/+V/WTwVcOYoqQAABIH6DS9J7qZO97gLd6rl6FGLJgLODiUVAIAg0Fr1edd7Dh9R0/79FkwDnD1KKgAAwSA01Kc9DqfT/7MA3YCSCgBAEAgfParLPWGjRipi3DgLpgHOnuUl9a233tKYMWO8PhYvXixJ2rt3r2644QalpqZqzpw52r17t9XjAQDQI8Xk3KGQwYM73hASoqgpl8sRHm7ZTMDZsLyklpaWasqUKdq0aZPnY9myZaqvr9dtt92mzMxMvfzyy0pPT9ftt9+u+vp6q0cELNFSeViuv/1NDZs2y3C7Az0OgB4ufORI9Vu6RCHx8W0Xw8IUNX26+uXmWj8YcIYsv05qWVmZRo8erfjTnkQvvfSSIiMjtXTpUjkcDt1zzz1677339Oabbyo7O9vqMQG/adq3X8eXL5e7aJeML76QHA6FjRqlyCmXq/89d8vhy/vKAKAdfX74A0VkZujk755QU0mJjOZmhcbHy5l9vaKvny1HCO/yQ88RkJJ68cUXtzleVFSkjIwMORwOSZLD4dAFF1ygwsJCSiqCRtO+/frix7eqpbz864OGoeYDB9RcWqqWykoNXP2E53kAAGaFjxqlAY/9JtBjAGfN0pJqGIY+/vhjbdq0SU899ZRaWlo0ffp0LV68WNXV1Ro5cqTX/ri4OJWUlJj6+la/PcDlcnn9ia715sxO/NcD3gX1m1pb1fC3v+v4xo2KuOIKr6XenNmZIjPzyMw8MjOPzMwLpswMw/D5RIylJbWyslIul0sRERF69NFH9emnn2rZsmVqaGjwHP+miIgIuU28V6+pqUnFxcXdPbZPyjsqHuhQb8sspKZGcUVF6vTFfLdbXzz7nGqHDGl3ubdl1h3IzDwyM4/MzCMz84Ils9P7XkcsLalDhw7Vtm3b1L9/fzkcDqWkpKi1tVVLlizRxIkT2xRSt9utqKgon79+eHh4m7Ox/uZyuVReXq7k5GQ5ufacT3prZu6339Gp48e73NensVGJKSlex3prZmeDzMwjM/PIzDwyMy+YMistLfV5r+XvSY2NjfX6fMSIEWpsbFR8fLxqamq81mpqajS4s8tpnMbhcCg6Oro7xjTN6XQG7L57qt6WWUhcnE45HJJhdLovNCK8w1x6W2bdgczMIzPzyMw8MjMvGDIz8zMXlv6Y3/vvv6+srCyv91QUFxcrNjZWGRkZ2rlzp4z//QfcMAzt2LFDqampVo4I+E3kdzIVNsqHi21zoW0AAKwtqenp6YqMjNS9996rgwcP6l//+pdWrVqlW2+9VdOnT1ddXZ2WL1+u0tJSLV++XC6XSzNmzLByRMBvHBERipo6RerkEjAhSUPVd9FCC6cCAMCeLC2pMTExWrt2rY4ePao5c+bonnvu0fe//33deuutiomJ0VNPPaWCggJlZ2erqKhIa9as6fGntYFv6nd3rqKumSm186bxkKShir3/foWdc04AJgMAwF4sf0/qqFGj9Pvf/77dtQkTJuiVV16xeCLAOo7QUA1c/YQa/v6W6l9cr5bqajnCwxQ2dqz6LsxRWGJioEcEAMAWLC+pQG/ncDjkvPoqOa++KtCjAABgW/x+NAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANhOWKAHAAD46Fi5tGmF9MUByTCk2GHSxUulhHGBngwAuh0lFQB6gg+fkN57UDp55OtjhyQd2ChNXCRNuT9QkwGAX/ByPwDYXfl70rv3exfUr7iOSh/8WipaZ/lYAOBPlFQAsLutj0r11R2vu09Ihc9ZNQ0AWIKSCgB29/nurvdU75Fcx/w/CwBYhJIKAHbW2iq1uLve1+yWmlz+nwcALEJJBQA7CwmR+gzuel+feCl6kP/nAQCLUFIBwO6GT+t6z7kXS2ER/p8FACxCSQUAu7vsHinpoo7XEyZI0x62bh4AsAAlFQDsLqKPdPNfpXHfl/qd+/XxPgnSmO9KN/1VivHhLQEA0INwMX8A6Ami+ks3vCjVfyGVvim1tkjDr5D6DQ30ZADgF5RUAOhJouOkCTcFegoA8Dte7gcAAIDtUFIBAABgO5RUAAAA2A4lFQAAALZDSQUAAIDtUFIBAABgO5RUAAAA2A4lFQAAALZDSQUAAIDt2KqkNjY26u6771ZmZqYmTZqkZ599NtAjAQAAIABs9WtRV61apd27d+sPf/iDKisr9fOf/1xDhgzR9OnTAz0aAAAALGSbklpfX68NGzbo6aef1rhx4zRu3DiVlJToz3/+MyUVAACgl7HNy/379u1Tc3Oz0tPTPccyMjJUVFSk1tbWAE4GAAAAq9nmTGp1dbUGDBigiIgIz7FBgwapsbFRtbW1GjhwYJdfwzAM1dfX+3PMNlwul9ef6BqZmUdm5pGZeWRmHpmZR2bmBVNmhmHI4XD4tNc2JdXlcnkVVEmez91ut09fo6mpScXFxd0+my/Ky8sDcr89GZmZR2bmkZl5ZGYemZlHZuYFS2an972O2KakRkZGtimjX30eFRXl09cIDw/XyJEju322zrhcLpWXlys5OVlOp9PS++6pyMw8MjOPzMwjM/PIzDwyMy+YMistLfV5r21KakJCgo4dO6bm5maFhX05VnV1taKiotSvXz+fvobD4VB0dLQ/x+yQ0+kM2H33VGRmHpmZR2bmkZl5ZGYemZkXDJn5+lK/ZKMfnEpJSVFYWJgKCws9xwoKCjR+/HiFhNhmTAAAAFjANu3P6XRq9uzZuv/++7Vr1y69/fbbevbZZzVv3rxAjwYAAACL2eblfknKzc3V/fffr1tuuUUxMTFatGiRrrrqqkCPBQAAAIvZqqQ6nU6tXLlSK1euDPQoAAAACCDbvNwPAAAAfIWSCgAAANuhpAIAAMB2KKkAAACwHUoqAAAAbIeSCgAAANuhpAIAAMB2KKkAAACwHUoqAAAAbIeSCgAAANuhpAIAAMB2wgI9AAAAAPzDMAx98sknKi0tVXNzsyIiIjR27FglJCQEerQuUVIBAACCUGNjo/7+97/r888/V0tLi+d4eXm5hg4dqiuuuEKhoaEBnLBzvNwPAAAQZAzD0FtvvaXDhw97FVRJampqUnl5ud57770ATecbSioAAECQqaysVFVVVad7PvvsM9XX11s0kXmUVAAAgCBz4MCBNmdQT1dfX689e/ZYNJF5lFQAAIAg01VB/UpjY6OfJzlzlFQAAIAgExUV5dO+fv36+XmSM0dJBQAACDLjx4/vsqj2799fY8eOtWgi8yipAAAAQaZ///5KTk7ucD00NFSjRo1SWJh9r0Zq38kAAABwxi699FKFh4ervLxcJ06c8ByPjY3V6NGjlZaWFrjhfEBJBQAACEIOh0MXXXSRMjIytG/fPp06dcpTUO18Ef+vUFIBAACCWEREhCZMmBDoMUzjPakAAACwHUoqAAAAbIeSCgAAANuhpAIAAMB2KKkAAACwHUoqAAAAbIdLUNlYq8ulk2ufVdNHH0mGFJ6Sopj/+LFCYmICPRoAAIBfUVJtqv6Nv6puxcNqOXjQc6zh9ddV/9IG9bvrLkVnXx/A6QAAAPyLl/ttyL3rIx3/z//0KqhfaSk/pOMPLlPjtm0BmAwAAMAalFQbOpH3hFoPH+lwvfXzz3XyqTUWTgQAAGAtSqrNGIahpj17utzn3rNHRkODBRMBAABYj5JqN42NMlyuLrcZDQ1qra+3YCAAAADrUVLtJjJSIf37dbkttG8/hfTta8FAAAAA1qOk2ozD4VBERkaX+8IvSJMjPNyCiQAAAKxHSbWhvnfdqbBRozpcDx0+XH3vusvCiQAAAKxFSbWhsHPO0cBn1ijiwiypT5+vF6KjFZ6ZobinVit82LDADQgAAOBnXMzfpsJHjlR8/kty/7tA9a+/IclQ1FVXKeqiCwM9GgAAgN9RUm0uIjNDEZldv0cVAAAgmPByPwAAAGyHkgoAAADbsbSk7t27V2PGjPH6yM7O9qxXVFRo/vz5SktL08yZM7Vp0yYrxwMAAIBNWPqe1NLSUqWkpOjpp5/+eoCwL0cwDEM5OTkaPXq08vPz9fbbb2vhwoV64403NGTIECvHBAAAQIBZWlLLyso0YsQIxcfHt1nbunWrKioq9OKLLyo6OlojRozQBx98oPz8fC1atMjKMQEAABBglr7cX1ZWpuTk5HbXioqKNHbsWEVHR3uOZWRkqLCw0JrhAAAAYBuWn0ltbW3VtddeqxMnTuiyyy7T0qVLFRMTo+rqag0ePNhrf1xcnI4cOeLz1zcMQ/X19d09dqdcLpfXn+gamZlHZuaRmXlkZh6ZmUdm5gVTZoZhyOFw+LS3W0tqQ0ODqqqq2l0bOHCgKioqlJSUpIceekh1dXVasWKFlixZotWrV8vlcikiIsLrNhEREXK73T7ff1NTk4qLi8/q73CmysvLA3K/PRmZmUdm5pGZeWRmHpmZR2bmBUtmp/e9jnRrSS0qKtK8efPaXcvLy9PWrVsVGRmp8PBwSdLDDz+sOXPmqKqqSpGRkaqtrfW6jdvtVlRUlM/3Hx4erpEjR57x/GfC5XKpvLxcycnJcjqdlt53T0Vm5pGZeWRmHpn5xlH7sULL/ibDEaJTSdP08ReNZGYCjzPzgimz0tJSn/d2a0nNysrS/v37fd4/YsQISVJVVZUSEhLaDF5TU9PmLQCdcTgcXu9ptZLT6QzYffdUZGYemZlHZuaRWQdqD0kbfyJV/luqr5EkhfdJ0PC+5yky+U+Kjo4L8IA9C48z84IhM19f6pcs/MGp0tJSpaenq6KiwnOsuLhYYWFhGjZsmFJTU7Vnzx41NDR41gsKCpSammrViAAAtK/usPTnWVLpm56CKkkhp6o04Mi/FPl/r5Ma6gI4IBB8LCupw4cP17Bhw3TffffpwIED+ve//6377rtPN9xwg/r376+JEycqMTFRubm5Kikp0Zo1a7Rr1y7NnTvXqhEBAGjfO3dL1bs7XA6t2im9+18WDgQEP8tKakhIiFavXq2YmBjddNNNysnJ0UUXXaS7775bkhQaGqonnnhC1dXVys7O1muvvaa8vDwu5A8ACKyWJunTD7red+hdv48C9CaWXoIqMTFRv/vd7zpcHzZsmNatW2fhRAAAdKH+C6+X+Dt0qlpqbZFCQv0/E9ALWHoxfwAAepzwaCkssut9YZGSg39Wge7CswmAdQxDam0N9BSAOVH9pMHnd70vYYJk4ieXAXTO0pf7AfRSB96Q/r1aqt775cuhfROlEdOlS3/h2xkqINAybpM+2y41HGt3uTV6sEIu/P8tHgoIbpRUAP713kPSlkekhtqvjx0/JH26VTr0L+mm16Xwnn1xavQCY+dIRw9KH/xKOuX9mxUbowZLl/6nIodNCtBwQHCipALwn8oCact/exfUbyr/p/TX/0/67hpLxwLOyKQl0rgbpE0PS8fKJDnUFDtK++K+q1GplwZ6OiDoUFIB+M/m/+7w5VGPQ/+S3PVSRM/+LSroJQYkS9c+6fm0qb5ezcXFgZsHCGL84BQA/zlW1vWeLw5Ihwv8PwsAoEehpALwH8PwcR8/8Q8A8EZJBeA/sd/yYc+3pXPS/T8LAKBHoaQC8J/vLJTCYzrfc+5FX16HEgCAb6CkAvCf4VOkC26Vwjq4xNSQ70gz86ydCQDQI/DT/QD8a8ZvpPgU6aPnpZp9UmuzFHOONOxSadrDUlT/QE8IALAhSioA/8u87cuP+qNSS6MUHS+F8u0HANAx/pUAYJ3ogYGeAADQQ/CeVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANiOX0qqYRhasGCBXn75Za/jx44d06JFi5Senq6pU6fq1Vdf9Vrfu3evbrjhBqWmpmrOnDnavXu3P8YDAACAzXV7SW1tbdWyZcu0efPmNmu5ubk6ceKE1q9fr5/+9Ke69957tWvXLklSfX29brvtNmVmZurll19Wenq6br/9dtXX13f3iAAAALC5sO78YlVVVfrZz36mTz/9VP369fNa++STT/TPf/5T77zzjpKSkjR69GgVFhbq+eef14QJE/TGG28oMjJSS5culcPh0D333KP33ntPb775prKzs7tzTAAAANhct55J3bNnjxITE5Wfn6++fft6rRUVFSkxMVFJSUmeYxkZGdq5c6dnPSMjQw6HQ5LkcDh0wQUXqLCwsDtHBAAAQA/QrWdSp06dqqlTp7a7Vl1drcGDB3sdi4uLU1VVlWd95MiRbdZLSkq6c0QAAAD0AKZKakNDg6dUni4+Pl7R0dEd3tblcikiIsLrWEREhNxut0/rvjAMw/L3sLpcLq8/0TUyM4/MzCMz88jMPDIzj8zMC6bMDMPwvGreFVMltaioSPPmzWt3LS8vT9OmTevwtpGRkW0Kp9vtVlRUlE/rvmhqalJxcbHP+7tTeXl5QO63JyMz88jMPDIzj8zMIzPzyMy8YMns9JOSHTFVUrOysrR///4zGighIUE1NTVex2pqahQfH9/p+ulvEehMeHh4m7cM+JvL5VJ5ebmSk5PldDotve+eiszMIzPzyMw8MjOPzMwjM/OCKbPS0lKf93bre1I7k5aWps8++0xHjhzROeecI0kqKChQWlqaJCk1NVVPP/205zSwYRjasWOHfvKTn/h8Hw6Ho9O3HPiT0+kM2H33VGRmHpmZR2bmkZl5ZGYemZkXDJn5+lK/ZOFvnDr33HM1adIkLVmyRPv27dOGDRu0ceNG3XTTTZKk6dOnq66uTsuXL1dpaamWL18ul8ulGTNmWDUiAAAAbMLSX4u6atUq9enTR9/73vf05JNP6qGHHtKECRMkSTExMXrqqadUUFCg7OxsFRUVac2aNT3+/xgAAABgnt9e7v/HP/7R5lhcXJyefPLJDm8zYcIEvfLKK/4aCQAAAD2EpWdSAQAAAF9QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYTligBwAAAOguX3zxhQoLC3Xs2DG1tLTI6XTqW9/6lsaPH6/Q0NBAjwcTKKkAACAoHDhwQNu2bZPL5fIcO378uI4cOaKKigrNmDFDYWFUn56Cl/sBAECPd/LkSW3fvt2roH7T4cOH9f7771s8Fc4GJRUAAPR4hYWFOnXqVKd7jhw5IrfbbdFEOFuUVAAA0OMdO3asyz0nTpxQRUWFBdOgO1BSAQBAj2cYhk/7Wlpa/DwJugslFQAA9Hh9+vTpck9UVJQSExMtmAbdgZIKAAB6vAkTJigiIqLTPfHx8erbt69FE+FsUVIBAECPFx8frzFjxnR4ianY2FhdcsklFk+Fs8HFwgAAQFC46KKL1LdvX5WWlqq2tlYtLS2Kjo5WfHy8srKyOIvaw1BSAQBA0Dj//PM1btw4nTx5Us3NzerTp0+XbwOAPVFSAQBAUHE4HJw1DQJ+eU+qYRhasGCBXn75Za/jzz33nMaMGeP1sXLlSs/6li1bNGvWLKWmpmrevHlcywzdzjAM1dXV6dixY2pubg70OAAAoAPdfia1tbVVy5cv1+bNmzVr1iyvtdLSUt1444264447PMecTqckqbKyUjk5OVq0aJEuvfRS5eXl6Y477tBrr70mh8PR3WOilzEMQzt37lR5ebmOHz+u1tZW9enTRwkJCbrooosUFRUV6BEBAMA3dGtJraqq0s9+9jN9+umn6tevX5v1srIyzZ49W/Hx8W3WNmzYoPPPP18LFiyQJK1YsUKXXHKJPvzwQ2VlZXXnmOhlDMPQu+++q9LSUq+LPdfV1XnOqs6cOZOiCgCAjXTry/179uxRYmKi8vPz230vyMGDB5WcnNzubYuKipSZmen53Ol0aty4cSosLOzOEdELHTx4UGVlZR3+NpKamhpt2bLF4qkAAEBnurWkTp06VatWrdLAgQPbrNXU1Ki2tlavvPKKpk6dqhkzZmjt2rWe4lBdXa3Bgwd73SYuLk5HjhzpzhHRCx04cECtra2d7qmqqpLb7bZoIgAA0BVTL/c3NDSoqqqq3bX4+HhFR0d3eNuDBw9K+rJ4rl69WsXFxVq2bJlCQ0M1f/58uVyuNpeIiIiIMFUcDMNQfX29z/u7g8vl8voTXbM6sxMnTvi0p6qqSnFxcRZMZB6PM/PIzDwyM4/MzCMz84IpM8MwfP5ZI1MltaioSPPmzWt3LS8vT9OmTevwthMnTtTWrVs1YMAASdKYMWN09OhRvfDCC5o/f74iIyPbFFK3293ue1s70tTUpOLiYp/3d6fy8vKA3G9PZlVmjY2NPu0rLy/X559/7udpzg6PM/PIzDwyM4/MzCMz84IlM1+vW2uqpGZlZWn//v1nNJAkT0H9yogRIzxnZhMSElRTU+O1XlNTo5SUFJ+/fnh4uEaOHHnG850Jl8ul8vJyJScne65UgM5ZnVlNTY0OHTrU6Z7Y2Filp6crJMSevymYx5l5ZGYemZlHZuaRmXnBlFlpaanPey27mP+GDRv0zDPP6M033/Sc5i0uLtbw4cMlSampqSooKPDsd7lc2rt3rxYuXOjzfTgcjk7fcuBPTqczYPfdU1mVWVpamo4cOdLpGdVzzz1XMTExfp/lbPE4M4/MzCMz88jMPDIzLxgyM3NZUctOG1188cWqrq7WypUrdejQIb3++ut6+umndeutt0qS5syZox07dmjNmjUqKSlRbm6ukpKSuPwUzlpCQoLS0tI6fHlh2LBhPM4AALAZy86kDh06VGvWrNEjjzyiF154QXFxcfrZz36mmTNnSpKSkpL0+OOP66GHHlJeXp7S09OVl5fHhfzRLVJTUzV48GDt3r1btbW1MgxD0dHR+va3v62UlBTbvswPAEBv5beS+o9//KPNsczMTK1fv77D20yePFmTJ0/210jo5RITE5WYmBjoMYDAMQypxS2FRkicAABgc5adSQUABMin26Utq6QjhVJTg+QcIJ17sTTlASlmcJc3B4BAoKQCQDD76EXpb3dJJyu/PnbiU+nzj6RPNks/fE0a+O3AzQcAHeCNeAAQrBrqpH/c611Qv6l6t/Q/t1o7EwD4iJIKAMHqg19Jx8o631O5QzpSZM08AGACJRUAgtWRXV3vaayVdv9fv48CAGZRUgEgWPn8A/yt/pwCAM4IJRUAgtWg87reE9FXSsn2/ywAYBIlFQCC1SVLpf7Jne9JTJeGfseScQDADEoqAAQr5wBp8n1Snw6uhRo3RrpmtbUzAYCPuE4qAASzCxZI/b8lbXtMqtolNbm+LK9Ds6SpD0qxwwI9IQC0i5IKAMFuxLQvP5pckvukFBUrhYYHeioA6BQlFQB6i3Dnlx8A0APwnlQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO1QUgEAAGA7lFQAAADYDiUVAAAAtkNJBQAAgO04DMMwAj1Ed9ixY4cMw1BERISl92sYhpqamhQeHi6Hw2HpffdUZGYemZlHZuaRmXlkZh6ZmRdMmbndbjkcDl1wwQVd7g2zYB5LBOo/msPhsLwY93RkZh6ZmUdm5pGZeWRmHpmZF0yZORwOnztb0JxJBQAAQPDgPakAAACwHUoqAAAAbIeSCgAAANuhpAIAAMB2KKkAAACwHUoqAAAAbIeSCgAAANuhpAIAAMB2KKk+qqur0z333KOLL75YF154oX7xi1+orq7Os37s2DEtWrRI6enpmjp1ql599VWv2+/du1c33HCDUlNTNWfOHO3evdvqv0LAGIahBQsW6OWXX/Y6/txzz2nMmDFeHytXrvSsb9myRbNmzVJqaqrmzZuniooKq0cPmI4y43HWtb1797Z5XGVnZ3vWKyoqNH/+fKWlpWnmzJnatGlTAKe1j8bGRt19993KzMzUpEmT9OyzzwZ6JNt566232jy2Fi9eLInn3uncbrdmzZqlbdu2eY519dzrzd/zpfYzW7ZsWZvH3Lp16zzrGzdu1LRp05SamqqcnBwdPXo0EKP7DSXVR7/85S+1b98+rVmzRmvXrlVZWZnuvfdez3pubq5OnDih9evX66c//anuvfde7dq1S5JUX1+v2267TZmZmXr55ZeVnp6u22+/XfX19YH661imtbVVy5Yt0+bNm9uslZaW6sYbb9SmTZs8Hzk5OZKkyspK5eTkKDs7Wy+99JIGDhyoO+64Q73hF6R1lhmPs66VlpYqJSXF63G1du1aSV+W/5ycHA0aNEj5+fm67rrrtHDhQlVWVgZ46sBbtWqVdu/erT/84Q/65S9/qd/97nd68803Az2WrZSWlmrKlClej61ly5bx3DtNY2Oj7rzzTpWUlHiOdfXc683f86X2M5OksrIy3XXXXV6PuTlz5kiSdu3apXvuuUcLFy7U+vXrVVdXp9zc3ECM7z8GunTq1CkjJSXFKCws9BzbsWOHkZKSYjQ0NBiHDh0yRo8ebVRUVHjW7777buPnP/+5YRiGsWHDBmPq1KlGa2urYRiG0draalx55ZVGfn6+tX8Rix05csS4+eabjcsvv9zIzMxs8/f9wQ9+YLz44ovt3vbRRx81br75Zs/n9fX1Rnp6urF161a/zhxonWXG48w3v/71r40777yz3bUtW7YYaWlpxqlTpzzHbrnlFuO3v/2tVePZ0qlTp4zx48d7Pb/y8vK8noMwjLvuusv41a9+1eY4z72vlZSUGN/97neNa6+91hg9erTnMdXVc6+3fs83jI4zMwzDuPTSS43333+/3dstWbLE8/3fMAyjsrLSGDNmjPHJJ5/4fWarcCbVByEhIXryySeVkpLidbylpUWnTp1SUVGREhMTlZSU5FnLyMjQzp07JUlFRUXKyMiQw+GQJDkcDl1wwQUqLCy07O8QCHv27FFiYqLy8/PVt2/fNusHDx5UcnJyu7ctKipSZmam53On06lx48b16sx4nPmmrKys08fV2LFjFR0d7TmWkZHR6zI63b59+9Tc3Kz09HTPsYyMDBUVFam1tTWAk9lLR48tnntf+/DDD5WVlaX169d7He/quddbv+dLHWd28uRJVVVV+fzvZGJiooYMGaKioiJ/jmupsEAP0BNERUXpsssu8zr2xz/+UWPGjNHAgQNVXV2twYMHe63HxcWpqqpKklRdXa2RI0e2WT/9tH6wmTp1qqZOndruWk1NjWpra/XKK68oNzdXkZGRmjt3rhYsWCCHw9FhpkeOHLFi9IDpLDMeZ74pKytTa2urrr32Wp04cUKXXXaZli5dqpiYmF77uOpKdXW1BgwYoIiICM+xQYMGqbGxUbW1tRo4cGAAp7MHwzD08ccfa9OmTXrqqafU0tKi6dOna/HixTz3vuHGG29s93hXz73e/NzsKLOysjI5HA49+eSTeu+99xQbG6sf/ehHuv766yVJn3/+edBnRkn9Xw0NDZ5/7E8XHx/v9X9/69at01//+lc988wzkiSXy+X1zV2SIiIi5Ha7fVrvqcxkdrqDBw9K+vIJtXr1ahUXF2vZsmUKDQ3V/PnzyawdvfVxdrrOMhw4cKAqKiqUlJSkhx56SHV1dVqxYoWWLFmi1atX95qMzOooF0m9PpuvVFZWenJ69NFH9emnn2rZsmVqaGjgceUDvn+Zd/DgQTkcDg0fPlw333yztm/frvvuu08xMTG68sor1dDQEPSZUVL/V1FRkebNm9fuWl5enqZNmyZJ+vOf/6xly5YpNzdXkyZNkiRFRka2eVC43W5FRUX5tN5T+ZpZeyZOnKitW7dqwIABkqQxY8bo6NGjeuGFFzR//vwOM+vXr1/3/QUC4Gwy662Ps9N1leHWrVsVGRmp8PBwSdLDDz+sOXPmqKqqSpGRkaqtrfW6TTBmZFZHjx1JvT6brwwdOlTbtm1T//795XA4lJKSotbWVi1ZskQTJ07sFc+9s9HVcy9Yv+efjdmzZ2vKlCmKjY2VJJ133nkqLy/XCy+8oCuvvLLDzJxOZwCm9Q9K6v/KysrS/v37O92zdu1arVq1SkuXLtUtt9ziOZ6QkKCamhqvvTU1NYqPj+90/fTT9D2NL5l15quC+pURI0Z4zpB1lNnp7wvuac4ms976ODud2QxHjBghSaqqqlJCQoJKS0u91oMxI7MSEhJ07NgxNTc3Kyzsy38WqqurFRUV1atLwum+KgtfGTFihBobGxUfH98rnntno6vnXrB+zz8bDoejzWNu+PDh2rp1q6Su/00IBvzglI9eeeUVrVq1Srm5ufrxj3/stZaWlqbPPvvM630gBQUFSktLkySlpqZq586dnktpGIahHTt2KDU11bL57WbDhg26+uqrvS4vUlxcrOHDh0v6MrOCggLPmsvl0t69e3t1ZjzOulZaWqr09HSv6ysWFxcrLCxMw4YNU2pqqvbs2aOGhgbPekFBQa/KqD0pKSkKCwvz+iGVgoICjR8/XiEh/DMhSe+//76ysrLkcrk8x4qLixUbG+v5Acbe/NzrSlfPPb7nt/XYY49p/vz5Xsf27dvX4b+Thw8f1uHDh4MqM777+KC2tlYPPPCArr/+el1zzTWqrq72fLS0tOjcc8/VpEmTtGTJEu3bt08bNmzQxo0bddNNN0mSpk+frrq6Oi1fvlylpaVavny5XC6XZsyYEeC/WeBcfPHFqq6u1sqVK3Xo0CG9/vrrevrpp3XrrbdKkubMmaMdO3ZozZo1KikpUW5urpKSkpSVlRXgyQOHx1nXhg8frmHDhum+++7TgQMH9O9//1v33XefbrjhBvXv318TJ05UYmKicnNzVVJSojVr1mjXrl2aO3duoEcPKKfTqdmzZ+v+++/Xrl279Pbbb+vZZ5/t8G0VvVF6eroiIyN177336uDBg/rXv/6lVatW6dZbb+W554Ounnt8z29rypQp2r59u9auXatPPvlEzz//vP7yl79owYIFkqQf/vCHevXVV7Vhwwbt27dPS5cu1eWXX65zzz03wJN3o8Bd/arn2LhxozF69Oh2P766ZmVNTY1x++23G+PHjzemTp1q/M///I/X1ygqKjJmz55tjB8/3pg7d66xZ8+eQPxVAmbKlCltrhm4fft243vf+54xYcIEY8qUKcbzzz/vtf7uu+8aV111lTFhwgTjlltuCaprv/mivcx4nHWtsrLSyMnJMTIzM42JEycaDz74oNHY2OhZLy8vN2666Sbj/PPPN6655hpj8+bNAZzWPurr642lS5caaWlpxqRJk4zf//73gR7Jdg4cOGDMnz/fSEtLMy655BLj8ccf91wbledeW6df87Or515v/55vGG0ze+utt4xrr73WGD9+vDF9+nTjb3/7m9f+/Px8Y/LkyUZaWpqRk5NjHD161OqR/cphGL3k1zkAAACgx+DlfgAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDuUVAAAANgOJRUAAAC2Q0kFAACA7VBSAQAAYDv/DyBAbnVw1jEFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3, max_iter = 500, random_state = 0)\n",
    "model = kmeans.fit(wesad_all_grouped)\n",
    "tsne = TSNE().fit_transform(wesad_all_grouped)\n",
    "plt.scatter(x = tsne[:, 0], y = tsne[:, 1], c=model.labels_, cmap='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ba11737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Cluster\n",
       "0    2        1\n",
       "1    3        1\n",
       "2    4        1\n",
       "3    5        1\n",
       "4    6        1\n",
       "5    7        2\n",
       "6    8        2\n",
       "7    9        2\n",
       "8   10        2\n",
       "9   11        2\n",
       "10  13        0\n",
       "11  14        0\n",
       "12  15        0\n",
       "13  16        0\n",
       "14  17        0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = pd.concat([ids, y], axis=1)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f089b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC110</th>\n",
       "      <th>ACC111</th>\n",
       "      <th>ACC1110</th>\n",
       "      <th>ACC1111</th>\n",
       "      <th>ACC1112</th>\n",
       "      <th>ACC1113</th>\n",
       "      <th>ACC1114</th>\n",
       "      <th>ACC1115</th>\n",
       "      <th>ACC1116</th>\n",
       "      <th>ACC1117</th>\n",
       "      <th>ACC1118</th>\n",
       "      <th>ACC1119</th>\n",
       "      <th>ACC112</th>\n",
       "      <th>ACC1120</th>\n",
       "      <th>ACC1121</th>\n",
       "      <th>ACC1122</th>\n",
       "      <th>ACC1123</th>\n",
       "      <th>ACC1124</th>\n",
       "      <th>ACC1125</th>\n",
       "      <th>ACC1126</th>\n",
       "      <th>ACC1127</th>\n",
       "      <th>ACC1128</th>\n",
       "      <th>ACC1129</th>\n",
       "      <th>ACC113</th>\n",
       "      <th>ACC1130</th>\n",
       "      <th>ACC1131</th>\n",
       "      <th>ACC114</th>\n",
       "      <th>ACC115</th>\n",
       "      <th>ACC116</th>\n",
       "      <th>ACC117</th>\n",
       "      <th>ACC118</th>\n",
       "      <th>ACC119</th>\n",
       "      <th>ACC210</th>\n",
       "      <th>ACC211</th>\n",
       "      <th>ACC2110</th>\n",
       "      <th>ACC2111</th>\n",
       "      <th>ACC2112</th>\n",
       "      <th>ACC2113</th>\n",
       "      <th>ACC2114</th>\n",
       "      <th>ACC2115</th>\n",
       "      <th>ACC2116</th>\n",
       "      <th>ACC2117</th>\n",
       "      <th>ACC2118</th>\n",
       "      <th>ACC2119</th>\n",
       "      <th>ACC212</th>\n",
       "      <th>ACC2120</th>\n",
       "      <th>ACC2121</th>\n",
       "      <th>ACC2122</th>\n",
       "      <th>ACC2123</th>\n",
       "      <th>ACC2124</th>\n",
       "      <th>ACC2125</th>\n",
       "      <th>ACC2126</th>\n",
       "      <th>ACC2127</th>\n",
       "      <th>ACC2128</th>\n",
       "      <th>ACC2129</th>\n",
       "      <th>ACC213</th>\n",
       "      <th>ACC2130</th>\n",
       "      <th>ACC2131</th>\n",
       "      <th>ACC214</th>\n",
       "      <th>ACC215</th>\n",
       "      <th>ACC216</th>\n",
       "      <th>ACC217</th>\n",
       "      <th>ACC218</th>\n",
       "      <th>ACC219</th>\n",
       "      <th>ACC310</th>\n",
       "      <th>ACC311</th>\n",
       "      <th>ACC3110</th>\n",
       "      <th>ACC3111</th>\n",
       "      <th>ACC3112</th>\n",
       "      <th>ACC3113</th>\n",
       "      <th>ACC3114</th>\n",
       "      <th>ACC3115</th>\n",
       "      <th>ACC3116</th>\n",
       "      <th>ACC3117</th>\n",
       "      <th>ACC3118</th>\n",
       "      <th>ACC3119</th>\n",
       "      <th>ACC312</th>\n",
       "      <th>ACC3120</th>\n",
       "      <th>ACC3121</th>\n",
       "      <th>ACC3122</th>\n",
       "      <th>ACC3123</th>\n",
       "      <th>ACC3124</th>\n",
       "      <th>ACC3125</th>\n",
       "      <th>ACC3126</th>\n",
       "      <th>ACC3127</th>\n",
       "      <th>ACC3128</th>\n",
       "      <th>ACC3129</th>\n",
       "      <th>ACC313</th>\n",
       "      <th>ACC3130</th>\n",
       "      <th>ACC3131</th>\n",
       "      <th>ACC314</th>\n",
       "      <th>ACC315</th>\n",
       "      <th>ACC316</th>\n",
       "      <th>ACC317</th>\n",
       "      <th>ACC318</th>\n",
       "      <th>ACC319</th>\n",
       "      <th>BVP10</th>\n",
       "      <th>BVP11</th>\n",
       "      <th>BVP110</th>\n",
       "      <th>BVP111</th>\n",
       "      <th>BVP112</th>\n",
       "      <th>BVP113</th>\n",
       "      <th>BVP114</th>\n",
       "      <th>BVP115</th>\n",
       "      <th>BVP116</th>\n",
       "      <th>BVP117</th>\n",
       "      <th>BVP118</th>\n",
       "      <th>BVP119</th>\n",
       "      <th>BVP12</th>\n",
       "      <th>BVP120</th>\n",
       "      <th>BVP121</th>\n",
       "      <th>BVP122</th>\n",
       "      <th>BVP123</th>\n",
       "      <th>BVP124</th>\n",
       "      <th>BVP125</th>\n",
       "      <th>BVP126</th>\n",
       "      <th>BVP127</th>\n",
       "      <th>BVP128</th>\n",
       "      <th>BVP129</th>\n",
       "      <th>BVP13</th>\n",
       "      <th>BVP130</th>\n",
       "      <th>BVP131</th>\n",
       "      <th>BVP132</th>\n",
       "      <th>BVP133</th>\n",
       "      <th>BVP134</th>\n",
       "      <th>BVP135</th>\n",
       "      <th>BVP136</th>\n",
       "      <th>BVP137</th>\n",
       "      <th>BVP138</th>\n",
       "      <th>BVP139</th>\n",
       "      <th>BVP14</th>\n",
       "      <th>BVP140</th>\n",
       "      <th>BVP141</th>\n",
       "      <th>BVP142</th>\n",
       "      <th>BVP143</th>\n",
       "      <th>BVP144</th>\n",
       "      <th>BVP145</th>\n",
       "      <th>BVP146</th>\n",
       "      <th>BVP147</th>\n",
       "      <th>BVP148</th>\n",
       "      <th>BVP149</th>\n",
       "      <th>BVP15</th>\n",
       "      <th>BVP150</th>\n",
       "      <th>BVP151</th>\n",
       "      <th>BVP152</th>\n",
       "      <th>BVP153</th>\n",
       "      <th>BVP154</th>\n",
       "      <th>BVP155</th>\n",
       "      <th>BVP156</th>\n",
       "      <th>BVP157</th>\n",
       "      <th>BVP158</th>\n",
       "      <th>BVP159</th>\n",
       "      <th>BVP16</th>\n",
       "      <th>BVP160</th>\n",
       "      <th>BVP161</th>\n",
       "      <th>BVP162</th>\n",
       "      <th>BVP163</th>\n",
       "      <th>BVP17</th>\n",
       "      <th>BVP18</th>\n",
       "      <th>BVP19</th>\n",
       "      <th>EDA10</th>\n",
       "      <th>EDA11</th>\n",
       "      <th>EDA12</th>\n",
       "      <th>EDA13</th>\n",
       "      <th>TEMP10</th>\n",
       "      <th>TEMP11</th>\n",
       "      <th>TEMP12</th>\n",
       "      <th>TEMP13</th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>stress</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275.095822</td>\n",
       "      <td>257.706505</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>335.958433</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>214.233212</td>\n",
       "      <td>257.706505</td>\n",
       "      <td>344.653091</td>\n",
       "      <td>370.737067</td>\n",
       "      <td>335.958433</td>\n",
       "      <td>318.569115</td>\n",
       "      <td>301.179798</td>\n",
       "      <td>249.011847</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>327.263774</td>\n",
       "      <td>335.958433</td>\n",
       "      <td>283.790481</td>\n",
       "      <td>301.179798</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>196.843895</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>379.431726</td>\n",
       "      <td>-55.301204</td>\n",
       "      <td>283.790481</td>\n",
       "      <td>283.790481</td>\n",
       "      <td>275.095822</td>\n",
       "      <td>275.095822</td>\n",
       "      <td>301.179798</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>414.950315</td>\n",
       "      <td>414.950315</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>130.448984</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>414.950315</td>\n",
       "      <td>414.950315</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>1065.239070</td>\n",
       "      <td>1024.596022</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>-649.907797</td>\n",
       "      <td>-649.907797</td>\n",
       "      <td>-661.972758</td>\n",
       "      <td>-698.167639</td>\n",
       "      <td>-782.622362</td>\n",
       "      <td>-601.647955</td>\n",
       "      <td>-613.712916</td>\n",
       "      <td>-698.167639</td>\n",
       "      <td>-661.972758</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-746.427481</td>\n",
       "      <td>-698.167639</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-746.427481</td>\n",
       "      <td>-722.297560</td>\n",
       "      <td>-649.907797</td>\n",
       "      <td>-625.777876</td>\n",
       "      <td>-613.712916</td>\n",
       "      <td>-686.102678</td>\n",
       "      <td>-722.297560</td>\n",
       "      <td>-734.362520</td>\n",
       "      <td>-698.167639</td>\n",
       "      <td>-649.907797</td>\n",
       "      <td>339.418959</td>\n",
       "      <td>-927.401887</td>\n",
       "      <td>-613.712916</td>\n",
       "      <td>-625.777876</td>\n",
       "      <td>-770.557401</td>\n",
       "      <td>-794.687322</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-1.033617</td>\n",
       "      <td>10.019713</td>\n",
       "      <td>38.208016</td>\n",
       "      <td>40.427932</td>\n",
       "      <td>42.624723</td>\n",
       "      <td>44.266536</td>\n",
       "      <td>45.168377</td>\n",
       "      <td>45.492114</td>\n",
       "      <td>45.422742</td>\n",
       "      <td>45.122128</td>\n",
       "      <td>44.590274</td>\n",
       "      <td>43.642185</td>\n",
       "      <td>19.454354</td>\n",
       "      <td>42.139117</td>\n",
       "      <td>40.011698</td>\n",
       "      <td>37.329300</td>\n",
       "      <td>34.323164</td>\n",
       "      <td>31.132035</td>\n",
       "      <td>28.033403</td>\n",
       "      <td>25.096640</td>\n",
       "      <td>22.391117</td>\n",
       "      <td>19.801216</td>\n",
       "      <td>17.234438</td>\n",
       "      <td>26.992817</td>\n",
       "      <td>14.667661</td>\n",
       "      <td>12.147132</td>\n",
       "      <td>9.811596</td>\n",
       "      <td>7.892294</td>\n",
       "      <td>6.435474</td>\n",
       "      <td>5.394888</td>\n",
       "      <td>4.493048</td>\n",
       "      <td>3.568083</td>\n",
       "      <td>2.550622</td>\n",
       "      <td>1.648781</td>\n",
       "      <td>32.288241</td>\n",
       "      <td>1.116926</td>\n",
       "      <td>1.209423</td>\n",
       "      <td>1.949394</td>\n",
       "      <td>3.244345</td>\n",
       "      <td>4.839910</td>\n",
       "      <td>6.458598</td>\n",
       "      <td>7.961666</td>\n",
       "      <td>9.187244</td>\n",
       "      <td>10.042837</td>\n",
       "      <td>10.435947</td>\n",
       "      <td>35.201880</td>\n",
       "      <td>10.412823</td>\n",
       "      <td>10.158457</td>\n",
       "      <td>10.019713</td>\n",
       "      <td>10.297202</td>\n",
       "      <td>11.152795</td>\n",
       "      <td>12.424621</td>\n",
       "      <td>13.765820</td>\n",
       "      <td>14.783282</td>\n",
       "      <td>15.268888</td>\n",
       "      <td>15.153268</td>\n",
       "      <td>36.173093</td>\n",
       "      <td>14.552040</td>\n",
       "      <td>13.580827</td>\n",
       "      <td>12.332125</td>\n",
       "      <td>10.852181</td>\n",
       "      <td>36.057473</td>\n",
       "      <td>35.941852</td>\n",
       "      <td>36.589328</td>\n",
       "      <td>-2002.240176</td>\n",
       "      <td>-2015.464571</td>\n",
       "      <td>-944.071437</td>\n",
       "      <td>-904.398252</td>\n",
       "      <td>5090.486004</td>\n",
       "      <td>5090.486004</td>\n",
       "      <td>4860.722884</td>\n",
       "      <td>4860.722884</td>\n",
       "      <td>11</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-408.608588</td>\n",
       "      <td>-408.608588</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-432.738509</td>\n",
       "      <td>-432.738509</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-432.738509</td>\n",
       "      <td>-408.608588</td>\n",
       "      <td>-432.738509</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-384.478667</td>\n",
       "      <td>-384.478667</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-384.478667</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-384.478667</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>1.556284</td>\n",
       "      <td>3.729952</td>\n",
       "      <td>10.435947</td>\n",
       "      <td>10.366574</td>\n",
       "      <td>10.019713</td>\n",
       "      <td>9.256617</td>\n",
       "      <td>8.169783</td>\n",
       "      <td>7.013577</td>\n",
       "      <td>6.065488</td>\n",
       "      <td>5.256144</td>\n",
       "      <td>4.308055</td>\n",
       "      <td>2.804987</td>\n",
       "      <td>6.042364</td>\n",
       "      <td>0.561947</td>\n",
       "      <td>-2.305444</td>\n",
       "      <td>-5.473448</td>\n",
       "      <td>-8.618329</td>\n",
       "      <td>-11.531968</td>\n",
       "      <td>-14.214366</td>\n",
       "      <td>-16.642399</td>\n",
       "      <td>-18.561701</td>\n",
       "      <td>-19.671659</td>\n",
       "      <td>-19.625410</td>\n",
       "      <td>8.192907</td>\n",
       "      <td>-18.307336</td>\n",
       "      <td>-15.994923</td>\n",
       "      <td>-13.173781</td>\n",
       "      <td>-10.468259</td>\n",
       "      <td>-8.317715</td>\n",
       "      <td>-6.907144</td>\n",
       "      <td>-6.190296</td>\n",
       "      <td>-5.982179</td>\n",
       "      <td>-6.051551</td>\n",
       "      <td>-6.190296</td>\n",
       "      <td>9.927216</td>\n",
       "      <td>-6.375289</td>\n",
       "      <td>-6.652779</td>\n",
       "      <td>-7.022764</td>\n",
       "      <td>-7.485247</td>\n",
       "      <td>-8.017102</td>\n",
       "      <td>-8.456460</td>\n",
       "      <td>-8.710825</td>\n",
       "      <td>-8.780198</td>\n",
       "      <td>-8.687701</td>\n",
       "      <td>-8.618329</td>\n",
       "      <td>10.990926</td>\n",
       "      <td>-8.664577</td>\n",
       "      <td>-8.872694</td>\n",
       "      <td>-9.173308</td>\n",
       "      <td>-9.497045</td>\n",
       "      <td>-9.797659</td>\n",
       "      <td>-10.190769</td>\n",
       "      <td>-10.722624</td>\n",
       "      <td>-11.393223</td>\n",
       "      <td>-11.994451</td>\n",
       "      <td>-12.086947</td>\n",
       "      <td>11.337788</td>\n",
       "      <td>-11.231355</td>\n",
       "      <td>-9.196432</td>\n",
       "      <td>-6.074676</td>\n",
       "      <td>-2.444189</td>\n",
       "      <td>11.175919</td>\n",
       "      <td>10.782809</td>\n",
       "      <td>10.528443</td>\n",
       "      <td>10508.161649</td>\n",
       "      <td>10349.427549</td>\n",
       "      <td>10124.571474</td>\n",
       "      <td>10018.755634</td>\n",
       "      <td>-5248.854417</td>\n",
       "      <td>-5248.854417</td>\n",
       "      <td>-5248.854417</td>\n",
       "      <td>-5248.854417</td>\n",
       "      <td>11</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-82.854657</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>9.904092</td>\n",
       "      <td>9.719099</td>\n",
       "      <td>7.452935</td>\n",
       "      <td>7.476059</td>\n",
       "      <td>7.314191</td>\n",
       "      <td>6.921080</td>\n",
       "      <td>6.250481</td>\n",
       "      <td>5.348640</td>\n",
       "      <td>4.331179</td>\n",
       "      <td>3.359966</td>\n",
       "      <td>2.481249</td>\n",
       "      <td>1.602533</td>\n",
       "      <td>9.279741</td>\n",
       "      <td>0.446327</td>\n",
       "      <td>-1.264858</td>\n",
       "      <td>-3.716015</td>\n",
       "      <td>-6.837772</td>\n",
       "      <td>-10.352638</td>\n",
       "      <td>-13.728760</td>\n",
       "      <td>-16.526778</td>\n",
       "      <td>-18.284211</td>\n",
       "      <td>-18.700446</td>\n",
       "      <td>-17.544240</td>\n",
       "      <td>8.724762</td>\n",
       "      <td>-14.746221</td>\n",
       "      <td>-10.514507</td>\n",
       "      <td>-5.265331</td>\n",
       "      <td>0.284458</td>\n",
       "      <td>5.464261</td>\n",
       "      <td>9.695975</td>\n",
       "      <td>12.887104</td>\n",
       "      <td>15.153268</td>\n",
       "      <td>16.818204</td>\n",
       "      <td>18.205651</td>\n",
       "      <td>8.146659</td>\n",
       "      <td>19.384982</td>\n",
       "      <td>20.379319</td>\n",
       "      <td>21.073042</td>\n",
       "      <td>21.419904</td>\n",
       "      <td>21.373656</td>\n",
       "      <td>20.911174</td>\n",
       "      <td>20.078705</td>\n",
       "      <td>18.876251</td>\n",
       "      <td>17.396307</td>\n",
       "      <td>15.846991</td>\n",
       "      <td>7.661052</td>\n",
       "      <td>14.459544</td>\n",
       "      <td>13.488331</td>\n",
       "      <td>12.979600</td>\n",
       "      <td>12.863980</td>\n",
       "      <td>12.863980</td>\n",
       "      <td>12.678987</td>\n",
       "      <td>12.054635</td>\n",
       "      <td>10.967802</td>\n",
       "      <td>9.441610</td>\n",
       "      <td>7.637928</td>\n",
       "      <td>7.337315</td>\n",
       "      <td>5.626130</td>\n",
       "      <td>3.475586</td>\n",
       "      <td>1.209423</td>\n",
       "      <td>-1.172362</td>\n",
       "      <td>7.221694</td>\n",
       "      <td>7.244818</td>\n",
       "      <td>7.360439</td>\n",
       "      <td>-7230.053448</td>\n",
       "      <td>-7296.185763</td>\n",
       "      <td>-7243.277843</td>\n",
       "      <td>-7203.604658</td>\n",
       "      <td>-17885.826043</td>\n",
       "      <td>-17885.826043</td>\n",
       "      <td>-17885.826043</td>\n",
       "      <td>-17885.826043</td>\n",
       "      <td>11</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-58.724736</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-58.724736</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-5.797186</td>\n",
       "      <td>-8.155846</td>\n",
       "      <td>-31.927443</td>\n",
       "      <td>-31.488084</td>\n",
       "      <td>-29.753775</td>\n",
       "      <td>-26.724516</td>\n",
       "      <td>-22.377181</td>\n",
       "      <td>-16.989261</td>\n",
       "      <td>-10.907617</td>\n",
       "      <td>-4.710352</td>\n",
       "      <td>0.978181</td>\n",
       "      <td>5.695502</td>\n",
       "      <td>-10.722624</td>\n",
       "      <td>9.233493</td>\n",
       "      <td>11.522781</td>\n",
       "      <td>12.817731</td>\n",
       "      <td>13.372710</td>\n",
       "      <td>13.580827</td>\n",
       "      <td>13.650200</td>\n",
       "      <td>13.673324</td>\n",
       "      <td>13.557703</td>\n",
       "      <td>13.187717</td>\n",
       "      <td>12.632738</td>\n",
       "      <td>-13.474394</td>\n",
       "      <td>12.054635</td>\n",
       "      <td>11.892766</td>\n",
       "      <td>12.447745</td>\n",
       "      <td>13.835193</td>\n",
       "      <td>15.708246</td>\n",
       "      <td>17.581300</td>\n",
       "      <td>18.945623</td>\n",
       "      <td>19.523726</td>\n",
       "      <td>19.361858</td>\n",
       "      <td>18.598762</td>\n",
       "      <td>-16.457406</td>\n",
       "      <td>17.442556</td>\n",
       "      <td>15.939488</td>\n",
       "      <td>14.274551</td>\n",
       "      <td>12.563366</td>\n",
       "      <td>11.175919</td>\n",
       "      <td>10.274078</td>\n",
       "      <td>9.904092</td>\n",
       "      <td>9.719099</td>\n",
       "      <td>9.279741</td>\n",
       "      <td>8.169783</td>\n",
       "      <td>-19.741031</td>\n",
       "      <td>6.227357</td>\n",
       "      <td>3.614331</td>\n",
       "      <td>0.631320</td>\n",
       "      <td>-2.444189</td>\n",
       "      <td>-5.427200</td>\n",
       "      <td>-8.178971</td>\n",
       "      <td>-10.653251</td>\n",
       "      <td>-12.803795</td>\n",
       "      <td>-14.723097</td>\n",
       "      <td>-16.642399</td>\n",
       "      <td>-23.163401</td>\n",
       "      <td>-18.862314</td>\n",
       "      <td>-21.660333</td>\n",
       "      <td>-25.152075</td>\n",
       "      <td>-29.036928</td>\n",
       "      <td>-26.447026</td>\n",
       "      <td>-29.198797</td>\n",
       "      <td>-31.118099</td>\n",
       "      <td>-6515.801698</td>\n",
       "      <td>-6515.801698</td>\n",
       "      <td>-6515.801698</td>\n",
       "      <td>-6542.250488</td>\n",
       "      <td>-16737.010440</td>\n",
       "      <td>-16737.010440</td>\n",
       "      <td>-16737.010440</td>\n",
       "      <td>-16737.010440</td>\n",
       "      <td>11</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-116.163814</td>\n",
       "      <td>-107.469155</td>\n",
       "      <td>40.340041</td>\n",
       "      <td>75.118675</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-81.385180</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-55.301204</td>\n",
       "      <td>-20.522569</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>31.645382</td>\n",
       "      <td>-229.194376</td>\n",
       "      <td>75.118675</td>\n",
       "      <td>49.034699</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>31.645382</td>\n",
       "      <td>75.118675</td>\n",
       "      <td>101.202651</td>\n",
       "      <td>49.034699</td>\n",
       "      <td>40.340041</td>\n",
       "      <td>49.034699</td>\n",
       "      <td>31.645382</td>\n",
       "      <td>-194.415741</td>\n",
       "      <td>31.645382</td>\n",
       "      <td>14.256065</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>75.118675</td>\n",
       "      <td>118.591968</td>\n",
       "      <td>-81.385180</td>\n",
       "      <td>-142.247790</td>\n",
       "      <td>-55.301204</td>\n",
       "      <td>-763.698054</td>\n",
       "      <td>-844.984148</td>\n",
       "      <td>-1048.199384</td>\n",
       "      <td>-1007.556337</td>\n",
       "      <td>-1088.842431</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-1170.128525</td>\n",
       "      <td>-1292.057667</td>\n",
       "      <td>-1251.414620</td>\n",
       "      <td>-1251.414620</td>\n",
       "      <td>-1170.128525</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-966.913290</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-1048.199384</td>\n",
       "      <td>-966.913290</td>\n",
       "      <td>-966.913290</td>\n",
       "      <td>-844.984148</td>\n",
       "      <td>-804.341101</td>\n",
       "      <td>-763.698054</td>\n",
       "      <td>-763.698054</td>\n",
       "      <td>-723.055006</td>\n",
       "      <td>-804.341101</td>\n",
       "      <td>-1088.842431</td>\n",
       "      <td>-844.984148</td>\n",
       "      <td>-926.270242</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-1007.556337</td>\n",
       "      <td>-885.627195</td>\n",
       "      <td>-1007.556337</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>1.600066</td>\n",
       "      <td>1.600066</td>\n",
       "      <td>399.743761</td>\n",
       "      <td>484.198484</td>\n",
       "      <td>508.328405</td>\n",
       "      <td>448.003603</td>\n",
       "      <td>230.834315</td>\n",
       "      <td>182.574473</td>\n",
       "      <td>206.704394</td>\n",
       "      <td>218.769354</td>\n",
       "      <td>242.899275</td>\n",
       "      <td>327.353998</td>\n",
       "      <td>-82.854657</td>\n",
       "      <td>375.613840</td>\n",
       "      <td>399.743761</td>\n",
       "      <td>508.328405</td>\n",
       "      <td>508.328405</td>\n",
       "      <td>604.848088</td>\n",
       "      <td>641.042970</td>\n",
       "      <td>580.718167</td>\n",
       "      <td>616.913049</td>\n",
       "      <td>616.913049</td>\n",
       "      <td>568.653207</td>\n",
       "      <td>-22.529854</td>\n",
       "      <td>544.523286</td>\n",
       "      <td>472.133523</td>\n",
       "      <td>218.769354</td>\n",
       "      <td>351.483919</td>\n",
       "      <td>472.133523</td>\n",
       "      <td>327.353998</td>\n",
       "      <td>254.964236</td>\n",
       "      <td>303.224077</td>\n",
       "      <td>-148.010530</td>\n",
       "      <td>-133.673575</td>\n",
       "      <td>116.598786</td>\n",
       "      <td>124.692229</td>\n",
       "      <td>126.588407</td>\n",
       "      <td>122.888547</td>\n",
       "      <td>115.442580</td>\n",
       "      <td>107.233517</td>\n",
       "      <td>101.406239</td>\n",
       "      <td>100.435026</td>\n",
       "      <td>105.568581</td>\n",
       "      <td>116.691283</td>\n",
       "      <td>-108.468283</td>\n",
       "      <td>132.577554</td>\n",
       "      <td>151.354340</td>\n",
       "      <td>170.847974</td>\n",
       "      <td>189.023533</td>\n",
       "      <td>204.077336</td>\n",
       "      <td>214.899425</td>\n",
       "      <td>221.327930</td>\n",
       "      <td>224.218445</td>\n",
       "      <td>225.074038</td>\n",
       "      <td>225.397775</td>\n",
       "      <td>-75.539535</td>\n",
       "      <td>225.883382</td>\n",
       "      <td>225.883382</td>\n",
       "      <td>223.478473</td>\n",
       "      <td>216.217499</td>\n",
       "      <td>202.158034</td>\n",
       "      <td>180.953215</td>\n",
       "      <td>154.337352</td>\n",
       "      <td>126.172172</td>\n",
       "      <td>101.336866</td>\n",
       "      <td>84.363762</td>\n",
       "      <td>-38.934051</td>\n",
       "      <td>78.258994</td>\n",
       "      <td>83.531293</td>\n",
       "      <td>98.492600</td>\n",
       "      <td>119.743667</td>\n",
       "      <td>143.353394</td>\n",
       "      <td>165.714419</td>\n",
       "      <td>183.820606</td>\n",
       "      <td>195.428915</td>\n",
       "      <td>198.828160</td>\n",
       "      <td>192.792765</td>\n",
       "      <td>-2.652306</td>\n",
       "      <td>176.860246</td>\n",
       "      <td>151.562457</td>\n",
       "      <td>118.911198</td>\n",
       "      <td>82.005101</td>\n",
       "      <td>44.451529</td>\n",
       "      <td>9.603478</td>\n",
       "      <td>-20.642872</td>\n",
       "      <td>-45.825039</td>\n",
       "      <td>-66.729245</td>\n",
       "      <td>-84.465446</td>\n",
       "      <td>30.553932</td>\n",
       "      <td>-99.681117</td>\n",
       "      <td>-112.145018</td>\n",
       "      <td>-121.024681</td>\n",
       "      <td>-125.603257</td>\n",
       "      <td>59.343463</td>\n",
       "      <td>83.392549</td>\n",
       "      <td>102.585569</td>\n",
       "      <td>8881.240525</td>\n",
       "      <td>9013.505155</td>\n",
       "      <td>9979.072109</td>\n",
       "      <td>10217.162919</td>\n",
       "      <td>10604.800896</td>\n",
       "      <td>10604.800896</td>\n",
       "      <td>10834.564016</td>\n",
       "      <td>10834.564016</td>\n",
       "      <td>11</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78761</th>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-84.810776</td>\n",
       "      <td>-84.810776</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-84.810776</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-31.870103</td>\n",
       "      <td>-31.870103</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-7.094508</td>\n",
       "      <td>-7.652662</td>\n",
       "      <td>-8.895005</td>\n",
       "      <td>-7.445605</td>\n",
       "      <td>-5.510071</td>\n",
       "      <td>-3.223440</td>\n",
       "      <td>-0.801772</td>\n",
       "      <td>1.538874</td>\n",
       "      <td>3.600442</td>\n",
       "      <td>5.247897</td>\n",
       "      <td>6.364205</td>\n",
       "      <td>6.931361</td>\n",
       "      <td>-8.129794</td>\n",
       "      <td>7.030389</td>\n",
       "      <td>6.760314</td>\n",
       "      <td>6.274180</td>\n",
       "      <td>5.698021</td>\n",
       "      <td>5.103857</td>\n",
       "      <td>4.527698</td>\n",
       "      <td>3.996552</td>\n",
       "      <td>3.519420</td>\n",
       "      <td>3.132313</td>\n",
       "      <td>2.853236</td>\n",
       "      <td>-8.543908</td>\n",
       "      <td>2.736204</td>\n",
       "      <td>2.808224</td>\n",
       "      <td>3.069296</td>\n",
       "      <td>3.492413</td>\n",
       "      <td>4.014557</td>\n",
       "      <td>4.509693</td>\n",
       "      <td>4.815778</td>\n",
       "      <td>4.815778</td>\n",
       "      <td>4.428671</td>\n",
       "      <td>3.699470</td>\n",
       "      <td>-8.940017</td>\n",
       "      <td>2.763211</td>\n",
       "      <td>1.754933</td>\n",
       "      <td>0.827677</td>\n",
       "      <td>0.035459</td>\n",
       "      <td>-0.630725</td>\n",
       "      <td>-1.233891</td>\n",
       "      <td>-1.837058</td>\n",
       "      <td>-2.476234</td>\n",
       "      <td>-3.187430</td>\n",
       "      <td>-3.934636</td>\n",
       "      <td>-9.354132</td>\n",
       "      <td>-4.708850</td>\n",
       "      <td>-5.456056</td>\n",
       "      <td>-6.140245</td>\n",
       "      <td>-6.725406</td>\n",
       "      <td>-7.238548</td>\n",
       "      <td>-7.733685</td>\n",
       "      <td>-8.291839</td>\n",
       "      <td>-8.958022</td>\n",
       "      <td>-9.714231</td>\n",
       "      <td>-10.416425</td>\n",
       "      <td>-9.741238</td>\n",
       "      <td>-10.839541</td>\n",
       "      <td>-10.767522</td>\n",
       "      <td>-10.074330</td>\n",
       "      <td>-8.714955</td>\n",
       "      <td>-10.047323</td>\n",
       "      <td>-10.110340</td>\n",
       "      <td>-9.768246</td>\n",
       "      <td>-6148.589357</td>\n",
       "      <td>-6083.072448</td>\n",
       "      <td>-6101.794701</td>\n",
       "      <td>-6111.152169</td>\n",
       "      <td>19000.821193</td>\n",
       "      <td>19000.821193</td>\n",
       "      <td>19000.821193</td>\n",
       "      <td>19000.821193</td>\n",
       "      <td>7</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78762</th>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>132.547427</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>-13.729339</td>\n",
       "      <td>-9.894281</td>\n",
       "      <td>-1.927083</td>\n",
       "      <td>-2.296184</td>\n",
       "      <td>-2.656284</td>\n",
       "      <td>-2.989376</td>\n",
       "      <td>-3.259450</td>\n",
       "      <td>-3.340472</td>\n",
       "      <td>-3.070398</td>\n",
       "      <td>-2.314189</td>\n",
       "      <td>-1.017832</td>\n",
       "      <td>0.746655</td>\n",
       "      <td>-6.734409</td>\n",
       "      <td>2.763211</td>\n",
       "      <td>4.770765</td>\n",
       "      <td>6.490240</td>\n",
       "      <td>7.723580</td>\n",
       "      <td>8.398766</td>\n",
       "      <td>8.560811</td>\n",
       "      <td>8.371759</td>\n",
       "      <td>8.002657</td>\n",
       "      <td>7.633555</td>\n",
       "      <td>7.318468</td>\n",
       "      <td>-4.330746</td>\n",
       "      <td>7.075401</td>\n",
       "      <td>6.832334</td>\n",
       "      <td>6.499242</td>\n",
       "      <td>5.977098</td>\n",
       "      <td>5.229892</td>\n",
       "      <td>4.248621</td>\n",
       "      <td>3.087301</td>\n",
       "      <td>1.799946</td>\n",
       "      <td>0.440571</td>\n",
       "      <td>-0.972819</td>\n",
       "      <td>-2.647281</td>\n",
       "      <td>-2.485237</td>\n",
       "      <td>-4.177703</td>\n",
       "      <td>-6.104235</td>\n",
       "      <td>-8.282836</td>\n",
       "      <td>-10.614479</td>\n",
       "      <td>-12.874103</td>\n",
       "      <td>-14.737617</td>\n",
       "      <td>-15.808912</td>\n",
       "      <td>-15.736893</td>\n",
       "      <td>-14.341508</td>\n",
       "      <td>-1.602993</td>\n",
       "      <td>-11.622757</td>\n",
       "      <td>-7.886727</td>\n",
       "      <td>-3.601544</td>\n",
       "      <td>0.719648</td>\n",
       "      <td>4.608721</td>\n",
       "      <td>7.768592</td>\n",
       "      <td>10.082231</td>\n",
       "      <td>11.513626</td>\n",
       "      <td>12.071780</td>\n",
       "      <td>11.810708</td>\n",
       "      <td>-1.107857</td>\n",
       "      <td>10.847442</td>\n",
       "      <td>9.353030</td>\n",
       "      <td>7.633555</td>\n",
       "      <td>5.995103</td>\n",
       "      <td>-1.017832</td>\n",
       "      <td>-1.215886</td>\n",
       "      <td>-1.557981</td>\n",
       "      <td>-2792.550625</td>\n",
       "      <td>-2783.193157</td>\n",
       "      <td>-2858.067534</td>\n",
       "      <td>-2876.789787</td>\n",
       "      <td>23376.495391</td>\n",
       "      <td>23376.495391</td>\n",
       "      <td>23376.495391</td>\n",
       "      <td>23376.495391</td>\n",
       "      <td>7</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78763</th>\n",
       "      <td>20.319082</td>\n",
       "      <td>14.758692</td>\n",
       "      <td>14.758692</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>14.758692</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>14.758692</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>6.301188</td>\n",
       "      <td>6.571262</td>\n",
       "      <td>4.356651</td>\n",
       "      <td>4.185604</td>\n",
       "      <td>4.131589</td>\n",
       "      <td>4.158596</td>\n",
       "      <td>4.212611</td>\n",
       "      <td>4.185604</td>\n",
       "      <td>4.014557</td>\n",
       "      <td>3.681465</td>\n",
       "      <td>3.231341</td>\n",
       "      <td>2.745207</td>\n",
       "      <td>6.751312</td>\n",
       "      <td>2.250070</td>\n",
       "      <td>1.763936</td>\n",
       "      <td>1.241792</td>\n",
       "      <td>0.674635</td>\n",
       "      <td>0.116481</td>\n",
       "      <td>-0.351648</td>\n",
       "      <td>-0.639727</td>\n",
       "      <td>-0.702745</td>\n",
       "      <td>-0.594715</td>\n",
       "      <td>-0.405663</td>\n",
       "      <td>6.742309</td>\n",
       "      <td>-0.261623</td>\n",
       "      <td>-0.225613</td>\n",
       "      <td>-0.288630</td>\n",
       "      <td>-0.414665</td>\n",
       "      <td>-0.522695</td>\n",
       "      <td>-0.594715</td>\n",
       "      <td>-0.684740</td>\n",
       "      <td>-0.855787</td>\n",
       "      <td>-1.161871</td>\n",
       "      <td>-1.630001</td>\n",
       "      <td>6.544255</td>\n",
       "      <td>-2.206160</td>\n",
       "      <td>-2.818328</td>\n",
       "      <td>-3.412492</td>\n",
       "      <td>-3.943639</td>\n",
       "      <td>-4.420771</td>\n",
       "      <td>-4.888900</td>\n",
       "      <td>-5.348026</td>\n",
       "      <td>-5.807153</td>\n",
       "      <td>-6.239272</td>\n",
       "      <td>-6.608374</td>\n",
       "      <td>6.184155</td>\n",
       "      <td>-6.905456</td>\n",
       "      <td>-7.166528</td>\n",
       "      <td>-7.481615</td>\n",
       "      <td>-7.913734</td>\n",
       "      <td>-8.480891</td>\n",
       "      <td>-9.066052</td>\n",
       "      <td>-9.480166</td>\n",
       "      <td>-9.471164</td>\n",
       "      <td>-8.867997</td>\n",
       "      <td>-7.607650</td>\n",
       "      <td>5.761039</td>\n",
       "      <td>-5.843163</td>\n",
       "      <td>-3.817604</td>\n",
       "      <td>-1.783043</td>\n",
       "      <td>0.017454</td>\n",
       "      <td>5.328919</td>\n",
       "      <td>4.941813</td>\n",
       "      <td>4.617723</td>\n",
       "      <td>-4042.749331</td>\n",
       "      <td>-4070.821735</td>\n",
       "      <td>-4033.384547</td>\n",
       "      <td>-4070.821735</td>\n",
       "      <td>-61219.872439</td>\n",
       "      <td>-61219.872439</td>\n",
       "      <td>-61219.872439</td>\n",
       "      <td>-61219.872439</td>\n",
       "      <td>7</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78764</th>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>4.986825</td>\n",
       "      <td>4.473683</td>\n",
       "      <td>-2.926358</td>\n",
       "      <td>-4.393763</td>\n",
       "      <td>-5.780146</td>\n",
       "      <td>-7.076503</td>\n",
       "      <td>-8.354856</td>\n",
       "      <td>-9.732236</td>\n",
       "      <td>-11.289666</td>\n",
       "      <td>-13.054152</td>\n",
       "      <td>-14.944674</td>\n",
       "      <td>-16.853201</td>\n",
       "      <td>3.825505</td>\n",
       "      <td>-18.608685</td>\n",
       "      <td>-19.986065</td>\n",
       "      <td>-20.715266</td>\n",
       "      <td>-20.517211</td>\n",
       "      <td>-19.157836</td>\n",
       "      <td>-16.574124</td>\n",
       "      <td>-12.928118</td>\n",
       "      <td>-8.534906</td>\n",
       "      <td>-3.898626</td>\n",
       "      <td>0.521593</td>\n",
       "      <td>3.177326</td>\n",
       "      <td>4.356651</td>\n",
       "      <td>7.408493</td>\n",
       "      <td>9.713129</td>\n",
       "      <td>11.396593</td>\n",
       "      <td>12.584921</td>\n",
       "      <td>13.386142</td>\n",
       "      <td>13.782252</td>\n",
       "      <td>13.719234</td>\n",
       "      <td>13.143075</td>\n",
       "      <td>12.161804</td>\n",
       "      <td>2.583162</td>\n",
       "      <td>10.946469</td>\n",
       "      <td>9.812156</td>\n",
       "      <td>9.001933</td>\n",
       "      <td>8.605823</td>\n",
       "      <td>8.560811</td>\n",
       "      <td>8.650836</td>\n",
       "      <td>8.632831</td>\n",
       "      <td>8.362756</td>\n",
       "      <td>7.831610</td>\n",
       "      <td>7.129416</td>\n",
       "      <td>2.070020</td>\n",
       "      <td>6.373207</td>\n",
       "      <td>5.598994</td>\n",
       "      <td>4.788770</td>\n",
       "      <td>3.843510</td>\n",
       "      <td>2.700194</td>\n",
       "      <td>1.385831</td>\n",
       "      <td>-0.000551</td>\n",
       "      <td>-1.287906</td>\n",
       "      <td>-2.368204</td>\n",
       "      <td>-3.214438</td>\n",
       "      <td>1.511866</td>\n",
       "      <td>-3.961644</td>\n",
       "      <td>-4.798875</td>\n",
       "      <td>-5.879173</td>\n",
       "      <td>-7.274558</td>\n",
       "      <td>0.782665</td>\n",
       "      <td>-0.216611</td>\n",
       "      <td>-1.485961</td>\n",
       "      <td>-6962.850037</td>\n",
       "      <td>-7019.002162</td>\n",
       "      <td>-6990.929757</td>\n",
       "      <td>-7019.002162</td>\n",
       "      <td>23793.226267</td>\n",
       "      <td>23793.226267</td>\n",
       "      <td>24209.957143</td>\n",
       "      <td>24209.957143</td>\n",
       "      <td>7</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78765</th>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>7.255451</td>\n",
       "      <td>7.318468</td>\n",
       "      <td>4.536701</td>\n",
       "      <td>4.185604</td>\n",
       "      <td>4.032562</td>\n",
       "      <td>4.149594</td>\n",
       "      <td>4.473683</td>\n",
       "      <td>4.905803</td>\n",
       "      <td>5.292909</td>\n",
       "      <td>5.490964</td>\n",
       "      <td>5.445952</td>\n",
       "      <td>5.148870</td>\n",
       "      <td>7.192433</td>\n",
       "      <td>4.671738</td>\n",
       "      <td>4.122587</td>\n",
       "      <td>3.591440</td>\n",
       "      <td>3.132313</td>\n",
       "      <td>2.790219</td>\n",
       "      <td>2.529147</td>\n",
       "      <td>2.295082</td>\n",
       "      <td>2.007003</td>\n",
       "      <td>1.619896</td>\n",
       "      <td>1.133762</td>\n",
       "      <td>6.958369</td>\n",
       "      <td>0.620620</td>\n",
       "      <td>0.152491</td>\n",
       "      <td>-0.243618</td>\n",
       "      <td>-0.594715</td>\n",
       "      <td>-0.999827</td>\n",
       "      <td>-1.512968</td>\n",
       "      <td>-2.116135</td>\n",
       "      <td>-2.701296</td>\n",
       "      <td>-3.133415</td>\n",
       "      <td>-3.331470</td>\n",
       "      <td>6.688294</td>\n",
       "      <td>-3.349475</td>\n",
       "      <td>-3.322468</td>\n",
       "      <td>-3.403490</td>\n",
       "      <td>-3.673564</td>\n",
       "      <td>-4.132691</td>\n",
       "      <td>-4.690845</td>\n",
       "      <td>-5.221992</td>\n",
       "      <td>-5.699123</td>\n",
       "      <td>-6.113237</td>\n",
       "      <td>-6.536354</td>\n",
       "      <td>6.409217</td>\n",
       "      <td>-7.004483</td>\n",
       "      <td>-7.562637</td>\n",
       "      <td>-8.192811</td>\n",
       "      <td>-8.904007</td>\n",
       "      <td>-9.660216</td>\n",
       "      <td>-10.425427</td>\n",
       "      <td>-11.136623</td>\n",
       "      <td>-11.712782</td>\n",
       "      <td>-12.036872</td>\n",
       "      <td>-11.955849</td>\n",
       "      <td>6.121138</td>\n",
       "      <td>-11.316673</td>\n",
       "      <td>-10.011313</td>\n",
       "      <td>-8.021764</td>\n",
       "      <td>-5.465059</td>\n",
       "      <td>5.806051</td>\n",
       "      <td>5.427947</td>\n",
       "      <td>4.986825</td>\n",
       "      <td>-4024.027079</td>\n",
       "      <td>-3967.874954</td>\n",
       "      <td>-4024.027079</td>\n",
       "      <td>-3995.947358</td>\n",
       "      <td>-54968.909299</td>\n",
       "      <td>-54968.909299</td>\n",
       "      <td>-54968.909299</td>\n",
       "      <td>-54968.909299</td>\n",
       "      <td>7</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78766 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ACC110      ACC111     ACC1110     ACC1111     ACC1112     ACC1113  \\\n",
       "0      275.095822  257.706505  309.874457  335.958433  309.874457  214.233212   \n",
       "1       -3.133252   -3.133252    5.561406    5.561406    5.561406   -3.133252   \n",
       "2      -90.079838  -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -90.079838  -90.079838  -90.079838  -98.774497  -90.079838  -90.079838   \n",
       "4     -116.163814 -107.469155   40.340041   75.118675   -3.133252  -11.827911   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    9.198303    3.637913    9.198303    9.198303    3.637913    9.198303   \n",
       "78762   -1.922477   -1.922477   -7.482866   -1.922477   -1.922477   -1.922477   \n",
       "78763   20.319082   14.758692   14.758692   20.319082   20.319082   20.319082   \n",
       "78764  -29.724425  -24.164035  -24.164035  -29.724425  -24.164035  -24.164035   \n",
       "78765   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "          ACC1114     ACC1115     ACC1116     ACC1117     ACC1118     ACC1119  \\\n",
       "0      257.706505  344.653091  370.737067  335.958433  318.569115  301.179798   \n",
       "1        5.561406    5.561406   -3.133252    5.561406   -3.133252   -3.133252   \n",
       "2      -90.079838  -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -90.079838  -98.774497  -98.774497  -90.079838  -98.774497  -98.774497   \n",
       "4      -81.385180  -98.774497  -55.301204  -20.522569   -3.133252   31.645382   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    3.637913    9.198303    9.198303    3.637913    9.198303    3.637913   \n",
       "78762   -1.922477   -1.922477   -7.482866   -1.922477   -1.922477   -1.922477   \n",
       "78763   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "78764  -24.164035  -29.724425  -29.724425  -24.164035  -24.164035  -24.164035   \n",
       "78765   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "           ACC112     ACC1120     ACC1121     ACC1122     ACC1123     ACC1124  \\\n",
       "0      249.011847  266.401164  266.401164  266.401164  309.874457  327.263774   \n",
       "1      -11.827911  -11.827911   -3.133252   -3.133252  -11.827911   -3.133252   \n",
       "2      -90.079838  -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -98.774497  -90.079838  -90.079838  -98.774497  -90.079838  -98.774497   \n",
       "4     -229.194376   75.118675   49.034699   -3.133252   31.645382   75.118675   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    9.198303    3.637913    9.198303    9.198303    9.198303    3.637913   \n",
       "78762   -7.482866   -1.922477   -1.922477   -1.922477   -7.482866   -7.482866   \n",
       "78763   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "78764  -29.724425  -24.164035  -24.164035  -29.724425  -29.724425  -29.724425   \n",
       "78765   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "          ACC1125     ACC1126     ACC1127     ACC1128     ACC1129      ACC113  \\\n",
       "0      335.958433  283.790481  301.179798  266.401164  196.843895  266.401164   \n",
       "1       -3.133252   -3.133252   -3.133252   -3.133252  -11.827911    5.561406   \n",
       "2      -90.079838  -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -90.079838  -90.079838  -98.774497  -90.079838  -98.774497  -90.079838   \n",
       "4      101.202651   49.034699   40.340041   49.034699   31.645382 -194.415741   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    9.198303    3.637913    3.637913    3.637913    3.637913    9.198303   \n",
       "78762   -1.922477   -1.922477   -1.922477   -1.922477   -1.922477   -1.922477   \n",
       "78763   20.319082   14.758692   20.319082   14.758692   20.319082   20.319082   \n",
       "78764  -24.164035  -24.164035  -29.724425  -29.724425  -29.724425  -29.724425   \n",
       "78765   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "          ACC1130    ACC1131      ACC114      ACC115      ACC116      ACC117  \\\n",
       "0      379.431726 -55.301204  283.790481  283.790481  275.095822  275.095822   \n",
       "1      -11.827911 -11.827911   -3.133252  -11.827911    5.561406    5.561406   \n",
       "2      -90.079838 -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -90.079838 -90.079838  -90.079838  -98.774497  -98.774497  -98.774497   \n",
       "4       31.645382  14.256065   -3.133252   75.118675  118.591968  -81.385180   \n",
       "...           ...        ...         ...         ...         ...         ...   \n",
       "78761    3.637913   3.637913    3.637913    9.198303    9.198303    3.637913   \n",
       "78762   -7.482866  -1.922477   -1.922477   -1.922477   -1.922477   -1.922477   \n",
       "78763   20.319082  20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "78764  -24.164035 -24.164035  -29.724425  -24.164035  -29.724425  -24.164035   \n",
       "78765   20.319082  20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "           ACC118      ACC119      ACC210      ACC211      ACC2110  \\\n",
       "0      301.179798  309.874457  252.378126  293.021173   333.664220   \n",
       "1       -3.133252   -3.133252  252.378126  252.378126   293.021173   \n",
       "2      -90.079838  -90.079838  699.451645  699.451645   658.808598   \n",
       "3      -90.079838  -98.774497  618.165551  618.165551   618.165551   \n",
       "4     -142.247790  -55.301204 -763.698054 -844.984148 -1048.199384   \n",
       "...           ...         ...         ...         ...          ...   \n",
       "78761    9.198303    9.198303 -104.570612 -104.570612  -104.570612   \n",
       "78762   -1.922477   -1.922477  132.547427  112.787590    93.027754   \n",
       "78763   20.319082   20.319082   73.267917   73.267917    73.267917   \n",
       "78764  -29.724425  -29.724425 -203.369795 -203.369795  -203.369795   \n",
       "78765   20.319082   20.319082   93.027754   93.027754    93.027754   \n",
       "\n",
       "           ACC2111      ACC2112      ACC2113      ACC2114      ACC2115  \\\n",
       "0       333.664220   374.307267   252.378126   414.950315   414.950315   \n",
       "1       252.378126   252.378126   293.021173   333.664220   293.021173   \n",
       "2       658.808598   699.451645   658.808598   658.808598   658.808598   \n",
       "3       618.165551   618.165551   618.165551   618.165551   618.165551   \n",
       "4     -1007.556337 -1088.842431 -1129.485478 -1170.128525 -1292.057667   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "78761   -84.810776   -84.810776  -104.570612  -104.570612  -104.570612   \n",
       "78762   112.787590   112.787590   112.787590   112.787590    93.027754   \n",
       "78763    73.267917    73.267917    73.267917    73.267917    73.267917   \n",
       "78764  -203.369795  -203.369795  -203.369795  -203.369795  -203.369795   \n",
       "78765    93.027754    93.027754    93.027754    93.027754    93.027754   \n",
       "\n",
       "           ACC2116      ACC2117      ACC2118      ACC2119      ACC212  \\\n",
       "0       374.307267   374.307267   293.021173   211.735079  293.021173   \n",
       "1       333.664220   293.021173   252.378126   252.378126  252.378126   \n",
       "2       658.808598   658.808598   658.808598   658.808598  658.808598   \n",
       "3       618.165551   618.165551   618.165551   618.165551  618.165551   \n",
       "4     -1251.414620 -1251.414620 -1170.128525 -1129.485478 -966.913290   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "78761  -104.570612  -104.570612  -104.570612  -104.570612 -104.570612   \n",
       "78762    93.027754    93.027754    93.027754    93.027754  112.787590   \n",
       "78763    73.267917    73.267917    73.267917    73.267917   73.267917   \n",
       "78764  -203.369795  -203.369795  -203.369795  -203.369795 -203.369795   \n",
       "78765    93.027754    93.027754    93.027754    93.027754   93.027754   \n",
       "\n",
       "           ACC2120      ACC2121     ACC2122     ACC2123     ACC2124  \\\n",
       "0       293.021173   252.378126  293.021173  171.092032  130.448984   \n",
       "1       252.378126   252.378126  252.378126  252.378126  252.378126   \n",
       "2       658.808598   658.808598  658.808598  658.808598  658.808598   \n",
       "3       618.165551   618.165551  618.165551  618.165551  618.165551   \n",
       "4     -1129.485478 -1048.199384 -966.913290 -966.913290 -844.984148   \n",
       "...            ...          ...         ...         ...         ...   \n",
       "78761  -104.570612  -104.570612 -104.570612 -104.570612 -104.570612   \n",
       "78762   112.787590   112.787590   93.027754   93.027754   93.027754   \n",
       "78763    73.267917    73.267917   73.267917   73.267917   73.267917   \n",
       "78764  -203.369795  -203.369795 -203.369795 -203.369795 -203.369795   \n",
       "78765    93.027754    93.027754   93.027754   93.027754   93.027754   \n",
       "\n",
       "          ACC2125     ACC2126     ACC2127     ACC2128     ACC2129  \\\n",
       "0      171.092032  171.092032  171.092032  414.950315  414.950315   \n",
       "1      252.378126  293.021173  252.378126  293.021173  252.378126   \n",
       "2      658.808598  658.808598  658.808598  658.808598  658.808598   \n",
       "3      618.165551  618.165551  618.165551  618.165551  618.165551   \n",
       "4     -804.341101 -763.698054 -763.698054 -723.055006 -804.341101   \n",
       "...           ...         ...         ...         ...         ...   \n",
       "78761 -104.570612 -104.570612 -104.570612 -104.570612 -104.570612   \n",
       "78762  112.787590  112.787590   93.027754   93.027754   93.027754   \n",
       "78763   73.267917   73.267917   73.267917   73.267917   73.267917   \n",
       "78764 -203.369795 -203.369795 -203.369795 -203.369795 -203.369795   \n",
       "78765   93.027754   93.027754   93.027754   93.027754   93.027754   \n",
       "\n",
       "            ACC213      ACC2130      ACC2131       ACC214       ACC215  \\\n",
       "0       333.664220  1065.239070  1024.596022   374.307267   374.307267   \n",
       "1       211.735079   252.378126   252.378126   171.092032   211.735079   \n",
       "2       658.808598   658.808598   658.808598   658.808598   658.808598   \n",
       "3       618.165551   618.165551   618.165551   618.165551   618.165551   \n",
       "4     -1088.842431  -844.984148  -926.270242 -1129.485478 -1129.485478   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "78761  -104.570612  -104.570612  -104.570612  -104.570612   -84.810776   \n",
       "78762   112.787590    93.027754    93.027754   112.787590   112.787590   \n",
       "78763    73.267917    73.267917    73.267917    73.267917    73.267917   \n",
       "78764  -203.369795  -203.369795  -203.369795  -203.369795  -203.369795   \n",
       "78765    93.027754    93.027754    93.027754    93.027754    93.027754   \n",
       "\n",
       "            ACC216      ACC217       ACC218       ACC219      ACC310  \\\n",
       "0       252.378126  211.735079   293.021173   333.664220 -649.907797   \n",
       "1       171.092032  211.735079   252.378126   252.378126 -420.673549   \n",
       "2       658.808598  699.451645   699.451645   658.808598  -70.789696   \n",
       "3       618.165551  618.165551   618.165551   618.165551  -46.659775   \n",
       "4     -1007.556337 -885.627195 -1007.556337 -1129.485478    1.600066   \n",
       "...            ...         ...          ...          ...         ...   \n",
       "78761  -104.570612 -104.570612  -104.570612  -104.570612  -40.043709   \n",
       "78762   112.787590  112.787590   112.787590    93.027754  131.602023   \n",
       "78763    73.267917   73.267917    73.267917    73.267917    0.824322   \n",
       "78764  -203.369795 -203.369795  -203.369795  -203.369795   90.733992   \n",
       "78765    93.027754   93.027754    93.027754    93.027754   -7.349284   \n",
       "\n",
       "           ACC311     ACC3110     ACC3111     ACC3112     ACC3113     ACC3114  \\\n",
       "0     -649.907797 -661.972758 -698.167639 -782.622362 -601.647955 -613.712916   \n",
       "1     -420.673549 -408.608588 -408.608588 -420.673549 -420.673549 -420.673549   \n",
       "2      -82.854657  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -46.659775  -46.659775  -46.659775   \n",
       "4        1.600066  399.743761  484.198484  508.328405  448.003603  230.834315   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  123.428417  123.428417  123.428417  131.602023  131.602023  131.602023   \n",
       "78763   -7.349284    0.824322    0.824322    0.824322    0.824322    0.824322   \n",
       "78764   90.733992   82.560385   82.560385   90.733992   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284   -7.349284  -15.522890   -7.349284   -7.349284   \n",
       "\n",
       "          ACC3115     ACC3116     ACC3117     ACC3118     ACC3119      ACC312  \\\n",
       "0     -698.167639 -661.972758 -674.037718 -746.427481 -698.167639 -674.037718   \n",
       "1     -432.738509 -432.738509 -420.673549 -420.673549 -432.738509 -408.608588   \n",
       "2      -70.789696  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -46.659775  -58.724736  -46.659775   \n",
       "4      182.574473  206.704394  218.769354  242.899275  327.353998  -82.854657   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  123.428417  123.428417  123.428417  123.428417  123.428417  131.602023   \n",
       "78763    0.824322   -7.349284   -7.349284   -7.349284    0.824322   -7.349284   \n",
       "78764   82.560385   90.733992   82.560385   90.733992   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284  -15.522890   -7.349284  -15.522890   -7.349284   \n",
       "\n",
       "          ACC3120     ACC3121     ACC3122     ACC3123     ACC3124     ACC3125  \\\n",
       "0     -674.037718 -746.427481 -722.297560 -649.907797 -625.777876 -613.712916   \n",
       "1     -432.738509 -396.543628 -396.543628 -420.673549 -396.543628 -396.543628   \n",
       "2      -70.789696  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -46.659775  -46.659775  -46.659775   \n",
       "4      375.613840  399.743761  508.328405  508.328405  604.848088  641.042970   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  131.602023  123.428417  123.428417  123.428417  123.428417  123.428417   \n",
       "78763   -7.349284   -7.349284    0.824322   -7.349284   -7.349284    0.824322   \n",
       "78764   90.733992   82.560385   82.560385   82.560385   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284  -15.522890   -7.349284   -7.349284   -7.349284   \n",
       "\n",
       "          ACC3126     ACC3127     ACC3128     ACC3129      ACC313     ACC3130  \\\n",
       "0     -686.102678 -722.297560 -734.362520 -698.167639 -649.907797  339.418959   \n",
       "1     -396.543628 -384.478667 -384.478667 -396.543628 -396.543628 -396.543628   \n",
       "2      -70.789696  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -58.724736  -46.659775  -46.659775   \n",
       "4      580.718167  616.913049  616.913049  568.653207  -22.529854  544.523286   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -31.870103  -31.870103  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  123.428417  123.428417  123.428417  123.428417  131.602023  123.428417   \n",
       "78763    0.824322    0.824322   -7.349284   -7.349284    0.824322   -7.349284   \n",
       "78764   82.560385   90.733992   90.733992   90.733992   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284  -15.522890   -7.349284   -7.349284   -7.349284   \n",
       "\n",
       "          ACC3131      ACC314      ACC315      ACC316      ACC317      ACC318  \\\n",
       "0     -927.401887 -613.712916 -625.777876 -770.557401 -794.687322 -674.037718   \n",
       "1     -384.478667 -420.673549 -396.543628 -384.478667 -396.543628 -420.673549   \n",
       "2      -70.789696  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -46.659775  -46.659775  -46.659775   \n",
       "4      472.133523  218.769354  351.483919  472.133523  327.353998  254.964236   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  123.428417  131.602023  123.428417  123.428417  123.428417  123.428417   \n",
       "78763    0.824322   -7.349284    0.824322   -7.349284   -7.349284    0.824322   \n",
       "78764   90.733992   90.733992   82.560385   82.560385   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284   -7.349284   -7.349284   -7.349284   -7.349284   \n",
       "\n",
       "           ACC319       BVP10       BVP11      BVP110      BVP111      BVP112  \\\n",
       "0     -674.037718   -1.033617   10.019713   38.208016   40.427932   42.624723   \n",
       "1     -420.673549    1.556284    3.729952   10.435947   10.366574   10.019713   \n",
       "2      -70.789696    9.904092    9.719099    7.452935    7.476059    7.314191   \n",
       "3      -46.659775   -5.797186   -8.155846  -31.927443  -31.488084  -29.753775   \n",
       "4      303.224077 -148.010530 -133.673575  116.598786  124.692229  126.588407   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709   -7.094508   -7.652662   -8.895005   -7.445605   -5.510071   \n",
       "78762  123.428417  -13.729339   -9.894281   -1.927083   -2.296184   -2.656284   \n",
       "78763    0.824322    6.301188    6.571262    4.356651    4.185604    4.131589   \n",
       "78764   90.733992    4.986825    4.473683   -2.926358   -4.393763   -5.780146   \n",
       "78765   -7.349284    7.255451    7.318468    4.536701    4.185604    4.032562   \n",
       "\n",
       "           BVP113      BVP114      BVP115      BVP116      BVP117      BVP118  \\\n",
       "0       44.266536   45.168377   45.492114   45.422742   45.122128   44.590274   \n",
       "1        9.256617    8.169783    7.013577    6.065488    5.256144    4.308055   \n",
       "2        6.921080    6.250481    5.348640    4.331179    3.359966    2.481249   \n",
       "3      -26.724516  -22.377181  -16.989261  -10.907617   -4.710352    0.978181   \n",
       "4      122.888547  115.442580  107.233517  101.406239  100.435026  105.568581   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761   -3.223440   -0.801772    1.538874    3.600442    5.247897    6.364205   \n",
       "78762   -2.989376   -3.259450   -3.340472   -3.070398   -2.314189   -1.017832   \n",
       "78763    4.158596    4.212611    4.185604    4.014557    3.681465    3.231341   \n",
       "78764   -7.076503   -8.354856   -9.732236  -11.289666  -13.054152  -14.944674   \n",
       "78765    4.149594    4.473683    4.905803    5.292909    5.490964    5.445952   \n",
       "\n",
       "           BVP119       BVP12      BVP120      BVP121      BVP122      BVP123  \\\n",
       "0       43.642185   19.454354   42.139117   40.011698   37.329300   34.323164   \n",
       "1        2.804987    6.042364    0.561947   -2.305444   -5.473448   -8.618329   \n",
       "2        1.602533    9.279741    0.446327   -1.264858   -3.716015   -6.837772   \n",
       "3        5.695502  -10.722624    9.233493   11.522781   12.817731   13.372710   \n",
       "4      116.691283 -108.468283  132.577554  151.354340  170.847974  189.023533   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    6.931361   -8.129794    7.030389    6.760314    6.274180    5.698021   \n",
       "78762    0.746655   -6.734409    2.763211    4.770765    6.490240    7.723580   \n",
       "78763    2.745207    6.751312    2.250070    1.763936    1.241792    0.674635   \n",
       "78764  -16.853201    3.825505  -18.608685  -19.986065  -20.715266  -20.517211   \n",
       "78765    5.148870    7.192433    4.671738    4.122587    3.591440    3.132313   \n",
       "\n",
       "           BVP124      BVP125      BVP126      BVP127      BVP128      BVP129  \\\n",
       "0       31.132035   28.033403   25.096640   22.391117   19.801216   17.234438   \n",
       "1      -11.531968  -14.214366  -16.642399  -18.561701  -19.671659  -19.625410   \n",
       "2      -10.352638  -13.728760  -16.526778  -18.284211  -18.700446  -17.544240   \n",
       "3       13.580827   13.650200   13.673324   13.557703   13.187717   12.632738   \n",
       "4      204.077336  214.899425  221.327930  224.218445  225.074038  225.397775   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    5.103857    4.527698    3.996552    3.519420    3.132313    2.853236   \n",
       "78762    8.398766    8.560811    8.371759    8.002657    7.633555    7.318468   \n",
       "78763    0.116481   -0.351648   -0.639727   -0.702745   -0.594715   -0.405663   \n",
       "78764  -19.157836  -16.574124  -12.928118   -8.534906   -3.898626    0.521593   \n",
       "78765    2.790219    2.529147    2.295082    2.007003    1.619896    1.133762   \n",
       "\n",
       "           BVP13      BVP130      BVP131      BVP132      BVP133      BVP134  \\\n",
       "0      26.992817   14.667661   12.147132    9.811596    7.892294    6.435474   \n",
       "1       8.192907  -18.307336  -15.994923  -13.173781  -10.468259   -8.317715   \n",
       "2       8.724762  -14.746221  -10.514507   -5.265331    0.284458    5.464261   \n",
       "3     -13.474394   12.054635   11.892766   12.447745   13.835193   15.708246   \n",
       "4     -75.539535  225.883382  225.883382  223.478473  216.217499  202.158034   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "78761  -8.543908    2.736204    2.808224    3.069296    3.492413    4.014557   \n",
       "78762  -4.330746    7.075401    6.832334    6.499242    5.977098    5.229892   \n",
       "78763   6.742309   -0.261623   -0.225613   -0.288630   -0.414665   -0.522695   \n",
       "78764   3.177326    4.356651    7.408493    9.713129   11.396593   12.584921   \n",
       "78765   6.958369    0.620620    0.152491   -0.243618   -0.594715   -0.999827   \n",
       "\n",
       "           BVP135      BVP136      BVP137      BVP138     BVP139      BVP14  \\\n",
       "0        5.394888    4.493048    3.568083    2.550622   1.648781  32.288241   \n",
       "1       -6.907144   -6.190296   -5.982179   -6.051551  -6.190296   9.927216   \n",
       "2        9.695975   12.887104   15.153268   16.818204  18.205651   8.146659   \n",
       "3       17.581300   18.945623   19.523726   19.361858  18.598762 -16.457406   \n",
       "4      180.953215  154.337352  126.172172  101.336866  84.363762 -38.934051   \n",
       "...           ...         ...         ...         ...        ...        ...   \n",
       "78761    4.509693    4.815778    4.815778    4.428671   3.699470  -8.940017   \n",
       "78762    4.248621    3.087301    1.799946    0.440571  -0.972819  -2.647281   \n",
       "78763   -0.594715   -0.684740   -0.855787   -1.161871  -1.630001   6.544255   \n",
       "78764   13.386142   13.782252   13.719234   13.143075  12.161804   2.583162   \n",
       "78765   -1.512968   -2.116135   -2.701296   -3.133415  -3.331470   6.688294   \n",
       "\n",
       "          BVP140     BVP141     BVP142      BVP143      BVP144      BVP145  \\\n",
       "0       1.116926   1.209423   1.949394    3.244345    4.839910    6.458598   \n",
       "1      -6.375289  -6.652779  -7.022764   -7.485247   -8.017102   -8.456460   \n",
       "2      19.384982  20.379319  21.073042   21.419904   21.373656   20.911174   \n",
       "3      17.442556  15.939488  14.274551   12.563366   11.175919   10.274078   \n",
       "4      78.258994  83.531293  98.492600  119.743667  143.353394  165.714419   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "78761   2.763211   1.754933   0.827677    0.035459   -0.630725   -1.233891   \n",
       "78762  -2.485237  -4.177703  -6.104235   -8.282836  -10.614479  -12.874103   \n",
       "78763  -2.206160  -2.818328  -3.412492   -3.943639   -4.420771   -4.888900   \n",
       "78764  10.946469   9.812156   9.001933    8.605823    8.560811    8.650836   \n",
       "78765  -3.349475  -3.322468  -3.403490   -3.673564   -4.132691   -4.690845   \n",
       "\n",
       "           BVP146      BVP147      BVP148      BVP149      BVP15      BVP150  \\\n",
       "0        7.961666    9.187244   10.042837   10.435947  35.201880   10.412823   \n",
       "1       -8.710825   -8.780198   -8.687701   -8.618329  10.990926   -8.664577   \n",
       "2       20.078705   18.876251   17.396307   15.846991   7.661052   14.459544   \n",
       "3        9.904092    9.719099    9.279741    8.169783 -19.741031    6.227357   \n",
       "4      183.820606  195.428915  198.828160  192.792765  -2.652306  176.860246   \n",
       "...           ...         ...         ...         ...        ...         ...   \n",
       "78761   -1.837058   -2.476234   -3.187430   -3.934636  -9.354132   -4.708850   \n",
       "78762  -14.737617  -15.808912  -15.736893  -14.341508  -1.602993  -11.622757   \n",
       "78763   -5.348026   -5.807153   -6.239272   -6.608374   6.184155   -6.905456   \n",
       "78764    8.632831    8.362756    7.831610    7.129416   2.070020    6.373207   \n",
       "78765   -5.221992   -5.699123   -6.113237   -6.536354   6.409217   -7.004483   \n",
       "\n",
       "           BVP151      BVP152     BVP153     BVP154     BVP155     BVP156  \\\n",
       "0       10.158457   10.019713  10.297202  11.152795  12.424621  13.765820   \n",
       "1       -8.872694   -9.173308  -9.497045  -9.797659 -10.190769 -10.722624   \n",
       "2       13.488331   12.979600  12.863980  12.863980  12.678987  12.054635   \n",
       "3        3.614331    0.631320  -2.444189  -5.427200  -8.178971 -10.653251   \n",
       "4      151.562457  118.911198  82.005101  44.451529   9.603478 -20.642872   \n",
       "...           ...         ...        ...        ...        ...        ...   \n",
       "78761   -5.456056   -6.140245  -6.725406  -7.238548  -7.733685  -8.291839   \n",
       "78762   -7.886727   -3.601544   0.719648   4.608721   7.768592  10.082231   \n",
       "78763   -7.166528   -7.481615  -7.913734  -8.480891  -9.066052  -9.480166   \n",
       "78764    5.598994    4.788770   3.843510   2.700194   1.385831  -0.000551   \n",
       "78765   -7.562637   -8.192811  -8.904007  -9.660216 -10.425427 -11.136623   \n",
       "\n",
       "          BVP157     BVP158     BVP159      BVP16     BVP160      BVP161  \\\n",
       "0      14.783282  15.268888  15.153268  36.173093  14.552040   13.580827   \n",
       "1     -11.393223 -11.994451 -12.086947  11.337788 -11.231355   -9.196432   \n",
       "2      10.967802   9.441610   7.637928   7.337315   5.626130    3.475586   \n",
       "3     -12.803795 -14.723097 -16.642399 -23.163401 -18.862314  -21.660333   \n",
       "4     -45.825039 -66.729245 -84.465446  30.553932 -99.681117 -112.145018   \n",
       "...          ...        ...        ...        ...        ...         ...   \n",
       "78761  -8.958022  -9.714231 -10.416425  -9.741238 -10.839541  -10.767522   \n",
       "78762  11.513626  12.071780  11.810708  -1.107857  10.847442    9.353030   \n",
       "78763  -9.471164  -8.867997  -7.607650   5.761039  -5.843163   -3.817604   \n",
       "78764  -1.287906  -2.368204  -3.214438   1.511866  -3.961644   -4.798875   \n",
       "78765 -11.712782 -12.036872 -11.955849   6.121138 -11.316673  -10.011313   \n",
       "\n",
       "           BVP162      BVP163      BVP17      BVP18       BVP19         EDA10  \\\n",
       "0       12.332125   10.852181  36.057473  35.941852   36.589328  -2002.240176   \n",
       "1       -6.074676   -2.444189  11.175919  10.782809   10.528443  10508.161649   \n",
       "2        1.209423   -1.172362   7.221694   7.244818    7.360439  -7230.053448   \n",
       "3      -25.152075  -29.036928 -26.447026 -29.198797  -31.118099  -6515.801698   \n",
       "4     -121.024681 -125.603257  59.343463  83.392549  102.585569   8881.240525   \n",
       "...           ...         ...        ...        ...         ...           ...   \n",
       "78761  -10.074330   -8.714955 -10.047323 -10.110340   -9.768246  -6148.589357   \n",
       "78762    7.633555    5.995103  -1.017832  -1.215886   -1.557981  -2792.550625   \n",
       "78763   -1.783043    0.017454   5.328919   4.941813    4.617723  -4042.749331   \n",
       "78764   -5.879173   -7.274558   0.782665  -0.216611   -1.485961  -6962.850037   \n",
       "78765   -8.021764   -5.465059   5.806051   5.427947    4.986825  -4024.027079   \n",
       "\n",
       "              EDA11         EDA12         EDA13        TEMP10        TEMP11  \\\n",
       "0      -2015.464571   -944.071437   -904.398252   5090.486004   5090.486004   \n",
       "1      10349.427549  10124.571474  10018.755634  -5248.854417  -5248.854417   \n",
       "2      -7296.185763  -7243.277843  -7203.604658 -17885.826043 -17885.826043   \n",
       "3      -6515.801698  -6515.801698  -6542.250488 -16737.010440 -16737.010440   \n",
       "4       9013.505155   9979.072109  10217.162919  10604.800896  10604.800896   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "78761  -6083.072448  -6101.794701  -6111.152169  19000.821193  19000.821193   \n",
       "78762  -2783.193157  -2858.067534  -2876.789787  23376.495391  23376.495391   \n",
       "78763  -4070.821735  -4033.384547  -4070.821735 -61219.872439 -61219.872439   \n",
       "78764  -7019.002162  -6990.929757  -7019.002162  23793.226267  23793.226267   \n",
       "78765  -3967.874954  -4024.027079  -3995.947358 -54968.909299 -54968.909299   \n",
       "\n",
       "             TEMP12        TEMP13  id dataset  stress  Cluster  \n",
       "0       4860.722884   4860.722884  11   Train     1.0        2  \n",
       "1      -5248.854417  -5248.854417  11   Train     1.0        2  \n",
       "2     -17885.826043 -17885.826043  11   Train     0.0        2  \n",
       "3     -16737.010440 -16737.010440  11   Train     0.0        2  \n",
       "4      10834.564016  10834.564016  11   Train     0.0        2  \n",
       "...             ...           ...  ..     ...     ...      ...  \n",
       "78761  19000.821193  19000.821193   7    Test     0.0        2  \n",
       "78762  23376.495391  23376.495391   7    Test     0.0        2  \n",
       "78763 -61219.872439 -61219.872439   7    Test     0.0        2  \n",
       "78764  24209.957143  24209.957143   7    Test     0.0        2  \n",
       "78765 -54968.909299 -54968.909299   7    Test     0.0        2  \n",
       "\n",
       "[78766 rows x 172 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wesad_grouped_all = pd.merge(wesad_dataset, clusters, on = \"id\")\n",
    "wesad_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b5de8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wesad_grouped_all.to_csv(\"Final_CSVs/wesad_clusters_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "478244e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC110</th>\n",
       "      <th>ACC111</th>\n",
       "      <th>ACC1110</th>\n",
       "      <th>ACC1111</th>\n",
       "      <th>ACC1112</th>\n",
       "      <th>ACC1113</th>\n",
       "      <th>ACC1114</th>\n",
       "      <th>ACC1115</th>\n",
       "      <th>ACC1116</th>\n",
       "      <th>ACC1117</th>\n",
       "      <th>ACC1118</th>\n",
       "      <th>ACC1119</th>\n",
       "      <th>ACC112</th>\n",
       "      <th>ACC1120</th>\n",
       "      <th>ACC1121</th>\n",
       "      <th>ACC1122</th>\n",
       "      <th>ACC1123</th>\n",
       "      <th>ACC1124</th>\n",
       "      <th>ACC1125</th>\n",
       "      <th>ACC1126</th>\n",
       "      <th>ACC1127</th>\n",
       "      <th>ACC1128</th>\n",
       "      <th>ACC1129</th>\n",
       "      <th>ACC113</th>\n",
       "      <th>ACC1130</th>\n",
       "      <th>ACC1131</th>\n",
       "      <th>ACC114</th>\n",
       "      <th>ACC115</th>\n",
       "      <th>ACC116</th>\n",
       "      <th>ACC117</th>\n",
       "      <th>ACC118</th>\n",
       "      <th>ACC119</th>\n",
       "      <th>ACC210</th>\n",
       "      <th>ACC211</th>\n",
       "      <th>ACC2110</th>\n",
       "      <th>ACC2111</th>\n",
       "      <th>ACC2112</th>\n",
       "      <th>ACC2113</th>\n",
       "      <th>ACC2114</th>\n",
       "      <th>ACC2115</th>\n",
       "      <th>ACC2116</th>\n",
       "      <th>ACC2117</th>\n",
       "      <th>ACC2118</th>\n",
       "      <th>ACC2119</th>\n",
       "      <th>ACC212</th>\n",
       "      <th>ACC2120</th>\n",
       "      <th>ACC2121</th>\n",
       "      <th>ACC2122</th>\n",
       "      <th>ACC2123</th>\n",
       "      <th>ACC2124</th>\n",
       "      <th>ACC2125</th>\n",
       "      <th>ACC2126</th>\n",
       "      <th>ACC2127</th>\n",
       "      <th>ACC2128</th>\n",
       "      <th>ACC2129</th>\n",
       "      <th>ACC213</th>\n",
       "      <th>ACC2130</th>\n",
       "      <th>ACC2131</th>\n",
       "      <th>ACC214</th>\n",
       "      <th>ACC215</th>\n",
       "      <th>ACC216</th>\n",
       "      <th>ACC217</th>\n",
       "      <th>ACC218</th>\n",
       "      <th>ACC219</th>\n",
       "      <th>ACC310</th>\n",
       "      <th>ACC311</th>\n",
       "      <th>ACC3110</th>\n",
       "      <th>ACC3111</th>\n",
       "      <th>ACC3112</th>\n",
       "      <th>ACC3113</th>\n",
       "      <th>ACC3114</th>\n",
       "      <th>ACC3115</th>\n",
       "      <th>ACC3116</th>\n",
       "      <th>ACC3117</th>\n",
       "      <th>ACC3118</th>\n",
       "      <th>ACC3119</th>\n",
       "      <th>ACC312</th>\n",
       "      <th>ACC3120</th>\n",
       "      <th>ACC3121</th>\n",
       "      <th>ACC3122</th>\n",
       "      <th>ACC3123</th>\n",
       "      <th>ACC3124</th>\n",
       "      <th>ACC3125</th>\n",
       "      <th>ACC3126</th>\n",
       "      <th>ACC3127</th>\n",
       "      <th>ACC3128</th>\n",
       "      <th>ACC3129</th>\n",
       "      <th>ACC313</th>\n",
       "      <th>ACC3130</th>\n",
       "      <th>ACC3131</th>\n",
       "      <th>ACC314</th>\n",
       "      <th>ACC315</th>\n",
       "      <th>ACC316</th>\n",
       "      <th>ACC317</th>\n",
       "      <th>ACC318</th>\n",
       "      <th>ACC319</th>\n",
       "      <th>BVP10</th>\n",
       "      <th>BVP11</th>\n",
       "      <th>BVP110</th>\n",
       "      <th>BVP111</th>\n",
       "      <th>BVP112</th>\n",
       "      <th>BVP113</th>\n",
       "      <th>BVP114</th>\n",
       "      <th>BVP115</th>\n",
       "      <th>BVP116</th>\n",
       "      <th>BVP117</th>\n",
       "      <th>BVP118</th>\n",
       "      <th>BVP119</th>\n",
       "      <th>BVP12</th>\n",
       "      <th>BVP120</th>\n",
       "      <th>BVP121</th>\n",
       "      <th>BVP122</th>\n",
       "      <th>BVP123</th>\n",
       "      <th>BVP124</th>\n",
       "      <th>BVP125</th>\n",
       "      <th>BVP126</th>\n",
       "      <th>BVP127</th>\n",
       "      <th>BVP128</th>\n",
       "      <th>BVP129</th>\n",
       "      <th>BVP13</th>\n",
       "      <th>BVP130</th>\n",
       "      <th>BVP131</th>\n",
       "      <th>BVP132</th>\n",
       "      <th>BVP133</th>\n",
       "      <th>BVP134</th>\n",
       "      <th>BVP135</th>\n",
       "      <th>BVP136</th>\n",
       "      <th>BVP137</th>\n",
       "      <th>BVP138</th>\n",
       "      <th>BVP139</th>\n",
       "      <th>BVP14</th>\n",
       "      <th>BVP140</th>\n",
       "      <th>BVP141</th>\n",
       "      <th>BVP142</th>\n",
       "      <th>BVP143</th>\n",
       "      <th>BVP144</th>\n",
       "      <th>BVP145</th>\n",
       "      <th>BVP146</th>\n",
       "      <th>BVP147</th>\n",
       "      <th>BVP148</th>\n",
       "      <th>BVP149</th>\n",
       "      <th>BVP15</th>\n",
       "      <th>BVP150</th>\n",
       "      <th>BVP151</th>\n",
       "      <th>BVP152</th>\n",
       "      <th>BVP153</th>\n",
       "      <th>BVP154</th>\n",
       "      <th>BVP155</th>\n",
       "      <th>BVP156</th>\n",
       "      <th>BVP157</th>\n",
       "      <th>BVP158</th>\n",
       "      <th>BVP159</th>\n",
       "      <th>BVP16</th>\n",
       "      <th>BVP160</th>\n",
       "      <th>BVP161</th>\n",
       "      <th>BVP162</th>\n",
       "      <th>BVP163</th>\n",
       "      <th>BVP17</th>\n",
       "      <th>BVP18</th>\n",
       "      <th>BVP19</th>\n",
       "      <th>EDA10</th>\n",
       "      <th>EDA11</th>\n",
       "      <th>EDA12</th>\n",
       "      <th>EDA13</th>\n",
       "      <th>TEMP10</th>\n",
       "      <th>TEMP11</th>\n",
       "      <th>TEMP12</th>\n",
       "      <th>TEMP13</th>\n",
       "      <th>id</th>\n",
       "      <th>stress</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275.095822</td>\n",
       "      <td>257.706505</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>335.958433</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>214.233212</td>\n",
       "      <td>257.706505</td>\n",
       "      <td>344.653091</td>\n",
       "      <td>370.737067</td>\n",
       "      <td>335.958433</td>\n",
       "      <td>318.569115</td>\n",
       "      <td>301.179798</td>\n",
       "      <td>249.011847</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>327.263774</td>\n",
       "      <td>335.958433</td>\n",
       "      <td>283.790481</td>\n",
       "      <td>301.179798</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>196.843895</td>\n",
       "      <td>266.401164</td>\n",
       "      <td>379.431726</td>\n",
       "      <td>-55.301204</td>\n",
       "      <td>283.790481</td>\n",
       "      <td>283.790481</td>\n",
       "      <td>275.095822</td>\n",
       "      <td>275.095822</td>\n",
       "      <td>301.179798</td>\n",
       "      <td>309.874457</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>414.950315</td>\n",
       "      <td>414.950315</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>130.448984</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>414.950315</td>\n",
       "      <td>414.950315</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>1065.239070</td>\n",
       "      <td>1024.596022</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>374.307267</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>-649.907797</td>\n",
       "      <td>-649.907797</td>\n",
       "      <td>-661.972758</td>\n",
       "      <td>-698.167639</td>\n",
       "      <td>-782.622362</td>\n",
       "      <td>-601.647955</td>\n",
       "      <td>-613.712916</td>\n",
       "      <td>-698.167639</td>\n",
       "      <td>-661.972758</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-746.427481</td>\n",
       "      <td>-698.167639</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-746.427481</td>\n",
       "      <td>-722.297560</td>\n",
       "      <td>-649.907797</td>\n",
       "      <td>-625.777876</td>\n",
       "      <td>-613.712916</td>\n",
       "      <td>-686.102678</td>\n",
       "      <td>-722.297560</td>\n",
       "      <td>-734.362520</td>\n",
       "      <td>-698.167639</td>\n",
       "      <td>-649.907797</td>\n",
       "      <td>339.418959</td>\n",
       "      <td>-927.401887</td>\n",
       "      <td>-613.712916</td>\n",
       "      <td>-625.777876</td>\n",
       "      <td>-770.557401</td>\n",
       "      <td>-794.687322</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-674.037718</td>\n",
       "      <td>-1.033617</td>\n",
       "      <td>10.019713</td>\n",
       "      <td>38.208016</td>\n",
       "      <td>40.427932</td>\n",
       "      <td>42.624723</td>\n",
       "      <td>44.266536</td>\n",
       "      <td>45.168377</td>\n",
       "      <td>45.492114</td>\n",
       "      <td>45.422742</td>\n",
       "      <td>45.122128</td>\n",
       "      <td>44.590274</td>\n",
       "      <td>43.642185</td>\n",
       "      <td>19.454354</td>\n",
       "      <td>42.139117</td>\n",
       "      <td>40.011698</td>\n",
       "      <td>37.329300</td>\n",
       "      <td>34.323164</td>\n",
       "      <td>31.132035</td>\n",
       "      <td>28.033403</td>\n",
       "      <td>25.096640</td>\n",
       "      <td>22.391117</td>\n",
       "      <td>19.801216</td>\n",
       "      <td>17.234438</td>\n",
       "      <td>26.992817</td>\n",
       "      <td>14.667661</td>\n",
       "      <td>12.147132</td>\n",
       "      <td>9.811596</td>\n",
       "      <td>7.892294</td>\n",
       "      <td>6.435474</td>\n",
       "      <td>5.394888</td>\n",
       "      <td>4.493048</td>\n",
       "      <td>3.568083</td>\n",
       "      <td>2.550622</td>\n",
       "      <td>1.648781</td>\n",
       "      <td>32.288241</td>\n",
       "      <td>1.116926</td>\n",
       "      <td>1.209423</td>\n",
       "      <td>1.949394</td>\n",
       "      <td>3.244345</td>\n",
       "      <td>4.839910</td>\n",
       "      <td>6.458598</td>\n",
       "      <td>7.961666</td>\n",
       "      <td>9.187244</td>\n",
       "      <td>10.042837</td>\n",
       "      <td>10.435947</td>\n",
       "      <td>35.201880</td>\n",
       "      <td>10.412823</td>\n",
       "      <td>10.158457</td>\n",
       "      <td>10.019713</td>\n",
       "      <td>10.297202</td>\n",
       "      <td>11.152795</td>\n",
       "      <td>12.424621</td>\n",
       "      <td>13.765820</td>\n",
       "      <td>14.783282</td>\n",
       "      <td>15.268888</td>\n",
       "      <td>15.153268</td>\n",
       "      <td>36.173093</td>\n",
       "      <td>14.552040</td>\n",
       "      <td>13.580827</td>\n",
       "      <td>12.332125</td>\n",
       "      <td>10.852181</td>\n",
       "      <td>36.057473</td>\n",
       "      <td>35.941852</td>\n",
       "      <td>36.589328</td>\n",
       "      <td>-2002.240176</td>\n",
       "      <td>-2015.464571</td>\n",
       "      <td>-944.071437</td>\n",
       "      <td>-904.398252</td>\n",
       "      <td>5090.486004</td>\n",
       "      <td>5090.486004</td>\n",
       "      <td>4860.722884</td>\n",
       "      <td>4860.722884</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>5.561406</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>333.664220</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>293.021173</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>171.092032</td>\n",
       "      <td>211.735079</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>252.378126</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-408.608588</td>\n",
       "      <td>-408.608588</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-432.738509</td>\n",
       "      <td>-432.738509</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-432.738509</td>\n",
       "      <td>-408.608588</td>\n",
       "      <td>-432.738509</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-384.478667</td>\n",
       "      <td>-384.478667</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-384.478667</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-384.478667</td>\n",
       "      <td>-396.543628</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>-420.673549</td>\n",
       "      <td>1.556284</td>\n",
       "      <td>3.729952</td>\n",
       "      <td>10.435947</td>\n",
       "      <td>10.366574</td>\n",
       "      <td>10.019713</td>\n",
       "      <td>9.256617</td>\n",
       "      <td>8.169783</td>\n",
       "      <td>7.013577</td>\n",
       "      <td>6.065488</td>\n",
       "      <td>5.256144</td>\n",
       "      <td>4.308055</td>\n",
       "      <td>2.804987</td>\n",
       "      <td>6.042364</td>\n",
       "      <td>0.561947</td>\n",
       "      <td>-2.305444</td>\n",
       "      <td>-5.473448</td>\n",
       "      <td>-8.618329</td>\n",
       "      <td>-11.531968</td>\n",
       "      <td>-14.214366</td>\n",
       "      <td>-16.642399</td>\n",
       "      <td>-18.561701</td>\n",
       "      <td>-19.671659</td>\n",
       "      <td>-19.625410</td>\n",
       "      <td>8.192907</td>\n",
       "      <td>-18.307336</td>\n",
       "      <td>-15.994923</td>\n",
       "      <td>-13.173781</td>\n",
       "      <td>-10.468259</td>\n",
       "      <td>-8.317715</td>\n",
       "      <td>-6.907144</td>\n",
       "      <td>-6.190296</td>\n",
       "      <td>-5.982179</td>\n",
       "      <td>-6.051551</td>\n",
       "      <td>-6.190296</td>\n",
       "      <td>9.927216</td>\n",
       "      <td>-6.375289</td>\n",
       "      <td>-6.652779</td>\n",
       "      <td>-7.022764</td>\n",
       "      <td>-7.485247</td>\n",
       "      <td>-8.017102</td>\n",
       "      <td>-8.456460</td>\n",
       "      <td>-8.710825</td>\n",
       "      <td>-8.780198</td>\n",
       "      <td>-8.687701</td>\n",
       "      <td>-8.618329</td>\n",
       "      <td>10.990926</td>\n",
       "      <td>-8.664577</td>\n",
       "      <td>-8.872694</td>\n",
       "      <td>-9.173308</td>\n",
       "      <td>-9.497045</td>\n",
       "      <td>-9.797659</td>\n",
       "      <td>-10.190769</td>\n",
       "      <td>-10.722624</td>\n",
       "      <td>-11.393223</td>\n",
       "      <td>-11.994451</td>\n",
       "      <td>-12.086947</td>\n",
       "      <td>11.337788</td>\n",
       "      <td>-11.231355</td>\n",
       "      <td>-9.196432</td>\n",
       "      <td>-6.074676</td>\n",
       "      <td>-2.444189</td>\n",
       "      <td>11.175919</td>\n",
       "      <td>10.782809</td>\n",
       "      <td>10.528443</td>\n",
       "      <td>10508.161649</td>\n",
       "      <td>10349.427549</td>\n",
       "      <td>10124.571474</td>\n",
       "      <td>10018.755634</td>\n",
       "      <td>-5248.854417</td>\n",
       "      <td>-5248.854417</td>\n",
       "      <td>-5248.854417</td>\n",
       "      <td>-5248.854417</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>699.451645</td>\n",
       "      <td>658.808598</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-82.854657</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>-70.789696</td>\n",
       "      <td>9.904092</td>\n",
       "      <td>9.719099</td>\n",
       "      <td>7.452935</td>\n",
       "      <td>7.476059</td>\n",
       "      <td>7.314191</td>\n",
       "      <td>6.921080</td>\n",
       "      <td>6.250481</td>\n",
       "      <td>5.348640</td>\n",
       "      <td>4.331179</td>\n",
       "      <td>3.359966</td>\n",
       "      <td>2.481249</td>\n",
       "      <td>1.602533</td>\n",
       "      <td>9.279741</td>\n",
       "      <td>0.446327</td>\n",
       "      <td>-1.264858</td>\n",
       "      <td>-3.716015</td>\n",
       "      <td>-6.837772</td>\n",
       "      <td>-10.352638</td>\n",
       "      <td>-13.728760</td>\n",
       "      <td>-16.526778</td>\n",
       "      <td>-18.284211</td>\n",
       "      <td>-18.700446</td>\n",
       "      <td>-17.544240</td>\n",
       "      <td>8.724762</td>\n",
       "      <td>-14.746221</td>\n",
       "      <td>-10.514507</td>\n",
       "      <td>-5.265331</td>\n",
       "      <td>0.284458</td>\n",
       "      <td>5.464261</td>\n",
       "      <td>9.695975</td>\n",
       "      <td>12.887104</td>\n",
       "      <td>15.153268</td>\n",
       "      <td>16.818204</td>\n",
       "      <td>18.205651</td>\n",
       "      <td>8.146659</td>\n",
       "      <td>19.384982</td>\n",
       "      <td>20.379319</td>\n",
       "      <td>21.073042</td>\n",
       "      <td>21.419904</td>\n",
       "      <td>21.373656</td>\n",
       "      <td>20.911174</td>\n",
       "      <td>20.078705</td>\n",
       "      <td>18.876251</td>\n",
       "      <td>17.396307</td>\n",
       "      <td>15.846991</td>\n",
       "      <td>7.661052</td>\n",
       "      <td>14.459544</td>\n",
       "      <td>13.488331</td>\n",
       "      <td>12.979600</td>\n",
       "      <td>12.863980</td>\n",
       "      <td>12.863980</td>\n",
       "      <td>12.678987</td>\n",
       "      <td>12.054635</td>\n",
       "      <td>10.967802</td>\n",
       "      <td>9.441610</td>\n",
       "      <td>7.637928</td>\n",
       "      <td>7.337315</td>\n",
       "      <td>5.626130</td>\n",
       "      <td>3.475586</td>\n",
       "      <td>1.209423</td>\n",
       "      <td>-1.172362</td>\n",
       "      <td>7.221694</td>\n",
       "      <td>7.244818</td>\n",
       "      <td>7.360439</td>\n",
       "      <td>-7230.053448</td>\n",
       "      <td>-7296.185763</td>\n",
       "      <td>-7243.277843</td>\n",
       "      <td>-7203.604658</td>\n",
       "      <td>-17885.826043</td>\n",
       "      <td>-17885.826043</td>\n",
       "      <td>-17885.826043</td>\n",
       "      <td>-17885.826043</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-90.079838</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>618.165551</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-58.724736</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-58.724736</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-46.659775</td>\n",
       "      <td>-5.797186</td>\n",
       "      <td>-8.155846</td>\n",
       "      <td>-31.927443</td>\n",
       "      <td>-31.488084</td>\n",
       "      <td>-29.753775</td>\n",
       "      <td>-26.724516</td>\n",
       "      <td>-22.377181</td>\n",
       "      <td>-16.989261</td>\n",
       "      <td>-10.907617</td>\n",
       "      <td>-4.710352</td>\n",
       "      <td>0.978181</td>\n",
       "      <td>5.695502</td>\n",
       "      <td>-10.722624</td>\n",
       "      <td>9.233493</td>\n",
       "      <td>11.522781</td>\n",
       "      <td>12.817731</td>\n",
       "      <td>13.372710</td>\n",
       "      <td>13.580827</td>\n",
       "      <td>13.650200</td>\n",
       "      <td>13.673324</td>\n",
       "      <td>13.557703</td>\n",
       "      <td>13.187717</td>\n",
       "      <td>12.632738</td>\n",
       "      <td>-13.474394</td>\n",
       "      <td>12.054635</td>\n",
       "      <td>11.892766</td>\n",
       "      <td>12.447745</td>\n",
       "      <td>13.835193</td>\n",
       "      <td>15.708246</td>\n",
       "      <td>17.581300</td>\n",
       "      <td>18.945623</td>\n",
       "      <td>19.523726</td>\n",
       "      <td>19.361858</td>\n",
       "      <td>18.598762</td>\n",
       "      <td>-16.457406</td>\n",
       "      <td>17.442556</td>\n",
       "      <td>15.939488</td>\n",
       "      <td>14.274551</td>\n",
       "      <td>12.563366</td>\n",
       "      <td>11.175919</td>\n",
       "      <td>10.274078</td>\n",
       "      <td>9.904092</td>\n",
       "      <td>9.719099</td>\n",
       "      <td>9.279741</td>\n",
       "      <td>8.169783</td>\n",
       "      <td>-19.741031</td>\n",
       "      <td>6.227357</td>\n",
       "      <td>3.614331</td>\n",
       "      <td>0.631320</td>\n",
       "      <td>-2.444189</td>\n",
       "      <td>-5.427200</td>\n",
       "      <td>-8.178971</td>\n",
       "      <td>-10.653251</td>\n",
       "      <td>-12.803795</td>\n",
       "      <td>-14.723097</td>\n",
       "      <td>-16.642399</td>\n",
       "      <td>-23.163401</td>\n",
       "      <td>-18.862314</td>\n",
       "      <td>-21.660333</td>\n",
       "      <td>-25.152075</td>\n",
       "      <td>-29.036928</td>\n",
       "      <td>-26.447026</td>\n",
       "      <td>-29.198797</td>\n",
       "      <td>-31.118099</td>\n",
       "      <td>-6515.801698</td>\n",
       "      <td>-6515.801698</td>\n",
       "      <td>-6515.801698</td>\n",
       "      <td>-6542.250488</td>\n",
       "      <td>-16737.010440</td>\n",
       "      <td>-16737.010440</td>\n",
       "      <td>-16737.010440</td>\n",
       "      <td>-16737.010440</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-116.163814</td>\n",
       "      <td>-107.469155</td>\n",
       "      <td>40.340041</td>\n",
       "      <td>75.118675</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>-11.827911</td>\n",
       "      <td>-81.385180</td>\n",
       "      <td>-98.774497</td>\n",
       "      <td>-55.301204</td>\n",
       "      <td>-20.522569</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>31.645382</td>\n",
       "      <td>-229.194376</td>\n",
       "      <td>75.118675</td>\n",
       "      <td>49.034699</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>31.645382</td>\n",
       "      <td>75.118675</td>\n",
       "      <td>101.202651</td>\n",
       "      <td>49.034699</td>\n",
       "      <td>40.340041</td>\n",
       "      <td>49.034699</td>\n",
       "      <td>31.645382</td>\n",
       "      <td>-194.415741</td>\n",
       "      <td>31.645382</td>\n",
       "      <td>14.256065</td>\n",
       "      <td>-3.133252</td>\n",
       "      <td>75.118675</td>\n",
       "      <td>118.591968</td>\n",
       "      <td>-81.385180</td>\n",
       "      <td>-142.247790</td>\n",
       "      <td>-55.301204</td>\n",
       "      <td>-763.698054</td>\n",
       "      <td>-844.984148</td>\n",
       "      <td>-1048.199384</td>\n",
       "      <td>-1007.556337</td>\n",
       "      <td>-1088.842431</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-1170.128525</td>\n",
       "      <td>-1292.057667</td>\n",
       "      <td>-1251.414620</td>\n",
       "      <td>-1251.414620</td>\n",
       "      <td>-1170.128525</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-966.913290</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-1048.199384</td>\n",
       "      <td>-966.913290</td>\n",
       "      <td>-966.913290</td>\n",
       "      <td>-844.984148</td>\n",
       "      <td>-804.341101</td>\n",
       "      <td>-763.698054</td>\n",
       "      <td>-763.698054</td>\n",
       "      <td>-723.055006</td>\n",
       "      <td>-804.341101</td>\n",
       "      <td>-1088.842431</td>\n",
       "      <td>-844.984148</td>\n",
       "      <td>-926.270242</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>-1007.556337</td>\n",
       "      <td>-885.627195</td>\n",
       "      <td>-1007.556337</td>\n",
       "      <td>-1129.485478</td>\n",
       "      <td>1.600066</td>\n",
       "      <td>1.600066</td>\n",
       "      <td>399.743761</td>\n",
       "      <td>484.198484</td>\n",
       "      <td>508.328405</td>\n",
       "      <td>448.003603</td>\n",
       "      <td>230.834315</td>\n",
       "      <td>182.574473</td>\n",
       "      <td>206.704394</td>\n",
       "      <td>218.769354</td>\n",
       "      <td>242.899275</td>\n",
       "      <td>327.353998</td>\n",
       "      <td>-82.854657</td>\n",
       "      <td>375.613840</td>\n",
       "      <td>399.743761</td>\n",
       "      <td>508.328405</td>\n",
       "      <td>508.328405</td>\n",
       "      <td>604.848088</td>\n",
       "      <td>641.042970</td>\n",
       "      <td>580.718167</td>\n",
       "      <td>616.913049</td>\n",
       "      <td>616.913049</td>\n",
       "      <td>568.653207</td>\n",
       "      <td>-22.529854</td>\n",
       "      <td>544.523286</td>\n",
       "      <td>472.133523</td>\n",
       "      <td>218.769354</td>\n",
       "      <td>351.483919</td>\n",
       "      <td>472.133523</td>\n",
       "      <td>327.353998</td>\n",
       "      <td>254.964236</td>\n",
       "      <td>303.224077</td>\n",
       "      <td>-148.010530</td>\n",
       "      <td>-133.673575</td>\n",
       "      <td>116.598786</td>\n",
       "      <td>124.692229</td>\n",
       "      <td>126.588407</td>\n",
       "      <td>122.888547</td>\n",
       "      <td>115.442580</td>\n",
       "      <td>107.233517</td>\n",
       "      <td>101.406239</td>\n",
       "      <td>100.435026</td>\n",
       "      <td>105.568581</td>\n",
       "      <td>116.691283</td>\n",
       "      <td>-108.468283</td>\n",
       "      <td>132.577554</td>\n",
       "      <td>151.354340</td>\n",
       "      <td>170.847974</td>\n",
       "      <td>189.023533</td>\n",
       "      <td>204.077336</td>\n",
       "      <td>214.899425</td>\n",
       "      <td>221.327930</td>\n",
       "      <td>224.218445</td>\n",
       "      <td>225.074038</td>\n",
       "      <td>225.397775</td>\n",
       "      <td>-75.539535</td>\n",
       "      <td>225.883382</td>\n",
       "      <td>225.883382</td>\n",
       "      <td>223.478473</td>\n",
       "      <td>216.217499</td>\n",
       "      <td>202.158034</td>\n",
       "      <td>180.953215</td>\n",
       "      <td>154.337352</td>\n",
       "      <td>126.172172</td>\n",
       "      <td>101.336866</td>\n",
       "      <td>84.363762</td>\n",
       "      <td>-38.934051</td>\n",
       "      <td>78.258994</td>\n",
       "      <td>83.531293</td>\n",
       "      <td>98.492600</td>\n",
       "      <td>119.743667</td>\n",
       "      <td>143.353394</td>\n",
       "      <td>165.714419</td>\n",
       "      <td>183.820606</td>\n",
       "      <td>195.428915</td>\n",
       "      <td>198.828160</td>\n",
       "      <td>192.792765</td>\n",
       "      <td>-2.652306</td>\n",
       "      <td>176.860246</td>\n",
       "      <td>151.562457</td>\n",
       "      <td>118.911198</td>\n",
       "      <td>82.005101</td>\n",
       "      <td>44.451529</td>\n",
       "      <td>9.603478</td>\n",
       "      <td>-20.642872</td>\n",
       "      <td>-45.825039</td>\n",
       "      <td>-66.729245</td>\n",
       "      <td>-84.465446</td>\n",
       "      <td>30.553932</td>\n",
       "      <td>-99.681117</td>\n",
       "      <td>-112.145018</td>\n",
       "      <td>-121.024681</td>\n",
       "      <td>-125.603257</td>\n",
       "      <td>59.343463</td>\n",
       "      <td>83.392549</td>\n",
       "      <td>102.585569</td>\n",
       "      <td>8881.240525</td>\n",
       "      <td>9013.505155</td>\n",
       "      <td>9979.072109</td>\n",
       "      <td>10217.162919</td>\n",
       "      <td>10604.800896</td>\n",
       "      <td>10604.800896</td>\n",
       "      <td>10834.564016</td>\n",
       "      <td>10834.564016</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78761</th>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>3.637913</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>9.198303</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-84.810776</td>\n",
       "      <td>-84.810776</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-84.810776</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-104.570612</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-31.870103</td>\n",
       "      <td>-31.870103</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-40.043709</td>\n",
       "      <td>-7.094508</td>\n",
       "      <td>-7.652662</td>\n",
       "      <td>-8.895005</td>\n",
       "      <td>-7.445605</td>\n",
       "      <td>-5.510071</td>\n",
       "      <td>-3.223440</td>\n",
       "      <td>-0.801772</td>\n",
       "      <td>1.538874</td>\n",
       "      <td>3.600442</td>\n",
       "      <td>5.247897</td>\n",
       "      <td>6.364205</td>\n",
       "      <td>6.931361</td>\n",
       "      <td>-8.129794</td>\n",
       "      <td>7.030389</td>\n",
       "      <td>6.760314</td>\n",
       "      <td>6.274180</td>\n",
       "      <td>5.698021</td>\n",
       "      <td>5.103857</td>\n",
       "      <td>4.527698</td>\n",
       "      <td>3.996552</td>\n",
       "      <td>3.519420</td>\n",
       "      <td>3.132313</td>\n",
       "      <td>2.853236</td>\n",
       "      <td>-8.543908</td>\n",
       "      <td>2.736204</td>\n",
       "      <td>2.808224</td>\n",
       "      <td>3.069296</td>\n",
       "      <td>3.492413</td>\n",
       "      <td>4.014557</td>\n",
       "      <td>4.509693</td>\n",
       "      <td>4.815778</td>\n",
       "      <td>4.815778</td>\n",
       "      <td>4.428671</td>\n",
       "      <td>3.699470</td>\n",
       "      <td>-8.940017</td>\n",
       "      <td>2.763211</td>\n",
       "      <td>1.754933</td>\n",
       "      <td>0.827677</td>\n",
       "      <td>0.035459</td>\n",
       "      <td>-0.630725</td>\n",
       "      <td>-1.233891</td>\n",
       "      <td>-1.837058</td>\n",
       "      <td>-2.476234</td>\n",
       "      <td>-3.187430</td>\n",
       "      <td>-3.934636</td>\n",
       "      <td>-9.354132</td>\n",
       "      <td>-4.708850</td>\n",
       "      <td>-5.456056</td>\n",
       "      <td>-6.140245</td>\n",
       "      <td>-6.725406</td>\n",
       "      <td>-7.238548</td>\n",
       "      <td>-7.733685</td>\n",
       "      <td>-8.291839</td>\n",
       "      <td>-8.958022</td>\n",
       "      <td>-9.714231</td>\n",
       "      <td>-10.416425</td>\n",
       "      <td>-9.741238</td>\n",
       "      <td>-10.839541</td>\n",
       "      <td>-10.767522</td>\n",
       "      <td>-10.074330</td>\n",
       "      <td>-8.714955</td>\n",
       "      <td>-10.047323</td>\n",
       "      <td>-10.110340</td>\n",
       "      <td>-9.768246</td>\n",
       "      <td>-6148.589357</td>\n",
       "      <td>-6083.072448</td>\n",
       "      <td>-6101.794701</td>\n",
       "      <td>-6111.152169</td>\n",
       "      <td>19000.821193</td>\n",
       "      <td>19000.821193</td>\n",
       "      <td>19000.821193</td>\n",
       "      <td>19000.821193</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78762</th>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-7.482866</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>-1.922477</td>\n",
       "      <td>132.547427</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>112.787590</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>131.602023</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>123.428417</td>\n",
       "      <td>-13.729339</td>\n",
       "      <td>-9.894281</td>\n",
       "      <td>-1.927083</td>\n",
       "      <td>-2.296184</td>\n",
       "      <td>-2.656284</td>\n",
       "      <td>-2.989376</td>\n",
       "      <td>-3.259450</td>\n",
       "      <td>-3.340472</td>\n",
       "      <td>-3.070398</td>\n",
       "      <td>-2.314189</td>\n",
       "      <td>-1.017832</td>\n",
       "      <td>0.746655</td>\n",
       "      <td>-6.734409</td>\n",
       "      <td>2.763211</td>\n",
       "      <td>4.770765</td>\n",
       "      <td>6.490240</td>\n",
       "      <td>7.723580</td>\n",
       "      <td>8.398766</td>\n",
       "      <td>8.560811</td>\n",
       "      <td>8.371759</td>\n",
       "      <td>8.002657</td>\n",
       "      <td>7.633555</td>\n",
       "      <td>7.318468</td>\n",
       "      <td>-4.330746</td>\n",
       "      <td>7.075401</td>\n",
       "      <td>6.832334</td>\n",
       "      <td>6.499242</td>\n",
       "      <td>5.977098</td>\n",
       "      <td>5.229892</td>\n",
       "      <td>4.248621</td>\n",
       "      <td>3.087301</td>\n",
       "      <td>1.799946</td>\n",
       "      <td>0.440571</td>\n",
       "      <td>-0.972819</td>\n",
       "      <td>-2.647281</td>\n",
       "      <td>-2.485237</td>\n",
       "      <td>-4.177703</td>\n",
       "      <td>-6.104235</td>\n",
       "      <td>-8.282836</td>\n",
       "      <td>-10.614479</td>\n",
       "      <td>-12.874103</td>\n",
       "      <td>-14.737617</td>\n",
       "      <td>-15.808912</td>\n",
       "      <td>-15.736893</td>\n",
       "      <td>-14.341508</td>\n",
       "      <td>-1.602993</td>\n",
       "      <td>-11.622757</td>\n",
       "      <td>-7.886727</td>\n",
       "      <td>-3.601544</td>\n",
       "      <td>0.719648</td>\n",
       "      <td>4.608721</td>\n",
       "      <td>7.768592</td>\n",
       "      <td>10.082231</td>\n",
       "      <td>11.513626</td>\n",
       "      <td>12.071780</td>\n",
       "      <td>11.810708</td>\n",
       "      <td>-1.107857</td>\n",
       "      <td>10.847442</td>\n",
       "      <td>9.353030</td>\n",
       "      <td>7.633555</td>\n",
       "      <td>5.995103</td>\n",
       "      <td>-1.017832</td>\n",
       "      <td>-1.215886</td>\n",
       "      <td>-1.557981</td>\n",
       "      <td>-2792.550625</td>\n",
       "      <td>-2783.193157</td>\n",
       "      <td>-2858.067534</td>\n",
       "      <td>-2876.789787</td>\n",
       "      <td>23376.495391</td>\n",
       "      <td>23376.495391</td>\n",
       "      <td>23376.495391</td>\n",
       "      <td>23376.495391</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78763</th>\n",
       "      <td>20.319082</td>\n",
       "      <td>14.758692</td>\n",
       "      <td>14.758692</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>14.758692</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>14.758692</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>73.267917</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>0.824322</td>\n",
       "      <td>6.301188</td>\n",
       "      <td>6.571262</td>\n",
       "      <td>4.356651</td>\n",
       "      <td>4.185604</td>\n",
       "      <td>4.131589</td>\n",
       "      <td>4.158596</td>\n",
       "      <td>4.212611</td>\n",
       "      <td>4.185604</td>\n",
       "      <td>4.014557</td>\n",
       "      <td>3.681465</td>\n",
       "      <td>3.231341</td>\n",
       "      <td>2.745207</td>\n",
       "      <td>6.751312</td>\n",
       "      <td>2.250070</td>\n",
       "      <td>1.763936</td>\n",
       "      <td>1.241792</td>\n",
       "      <td>0.674635</td>\n",
       "      <td>0.116481</td>\n",
       "      <td>-0.351648</td>\n",
       "      <td>-0.639727</td>\n",
       "      <td>-0.702745</td>\n",
       "      <td>-0.594715</td>\n",
       "      <td>-0.405663</td>\n",
       "      <td>6.742309</td>\n",
       "      <td>-0.261623</td>\n",
       "      <td>-0.225613</td>\n",
       "      <td>-0.288630</td>\n",
       "      <td>-0.414665</td>\n",
       "      <td>-0.522695</td>\n",
       "      <td>-0.594715</td>\n",
       "      <td>-0.684740</td>\n",
       "      <td>-0.855787</td>\n",
       "      <td>-1.161871</td>\n",
       "      <td>-1.630001</td>\n",
       "      <td>6.544255</td>\n",
       "      <td>-2.206160</td>\n",
       "      <td>-2.818328</td>\n",
       "      <td>-3.412492</td>\n",
       "      <td>-3.943639</td>\n",
       "      <td>-4.420771</td>\n",
       "      <td>-4.888900</td>\n",
       "      <td>-5.348026</td>\n",
       "      <td>-5.807153</td>\n",
       "      <td>-6.239272</td>\n",
       "      <td>-6.608374</td>\n",
       "      <td>6.184155</td>\n",
       "      <td>-6.905456</td>\n",
       "      <td>-7.166528</td>\n",
       "      <td>-7.481615</td>\n",
       "      <td>-7.913734</td>\n",
       "      <td>-8.480891</td>\n",
       "      <td>-9.066052</td>\n",
       "      <td>-9.480166</td>\n",
       "      <td>-9.471164</td>\n",
       "      <td>-8.867997</td>\n",
       "      <td>-7.607650</td>\n",
       "      <td>5.761039</td>\n",
       "      <td>-5.843163</td>\n",
       "      <td>-3.817604</td>\n",
       "      <td>-1.783043</td>\n",
       "      <td>0.017454</td>\n",
       "      <td>5.328919</td>\n",
       "      <td>4.941813</td>\n",
       "      <td>4.617723</td>\n",
       "      <td>-4042.749331</td>\n",
       "      <td>-4070.821735</td>\n",
       "      <td>-4033.384547</td>\n",
       "      <td>-4070.821735</td>\n",
       "      <td>-61219.872439</td>\n",
       "      <td>-61219.872439</td>\n",
       "      <td>-61219.872439</td>\n",
       "      <td>-61219.872439</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78764</th>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-24.164035</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-29.724425</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>-203.369795</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>82.560385</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>90.733992</td>\n",
       "      <td>4.986825</td>\n",
       "      <td>4.473683</td>\n",
       "      <td>-2.926358</td>\n",
       "      <td>-4.393763</td>\n",
       "      <td>-5.780146</td>\n",
       "      <td>-7.076503</td>\n",
       "      <td>-8.354856</td>\n",
       "      <td>-9.732236</td>\n",
       "      <td>-11.289666</td>\n",
       "      <td>-13.054152</td>\n",
       "      <td>-14.944674</td>\n",
       "      <td>-16.853201</td>\n",
       "      <td>3.825505</td>\n",
       "      <td>-18.608685</td>\n",
       "      <td>-19.986065</td>\n",
       "      <td>-20.715266</td>\n",
       "      <td>-20.517211</td>\n",
       "      <td>-19.157836</td>\n",
       "      <td>-16.574124</td>\n",
       "      <td>-12.928118</td>\n",
       "      <td>-8.534906</td>\n",
       "      <td>-3.898626</td>\n",
       "      <td>0.521593</td>\n",
       "      <td>3.177326</td>\n",
       "      <td>4.356651</td>\n",
       "      <td>7.408493</td>\n",
       "      <td>9.713129</td>\n",
       "      <td>11.396593</td>\n",
       "      <td>12.584921</td>\n",
       "      <td>13.386142</td>\n",
       "      <td>13.782252</td>\n",
       "      <td>13.719234</td>\n",
       "      <td>13.143075</td>\n",
       "      <td>12.161804</td>\n",
       "      <td>2.583162</td>\n",
       "      <td>10.946469</td>\n",
       "      <td>9.812156</td>\n",
       "      <td>9.001933</td>\n",
       "      <td>8.605823</td>\n",
       "      <td>8.560811</td>\n",
       "      <td>8.650836</td>\n",
       "      <td>8.632831</td>\n",
       "      <td>8.362756</td>\n",
       "      <td>7.831610</td>\n",
       "      <td>7.129416</td>\n",
       "      <td>2.070020</td>\n",
       "      <td>6.373207</td>\n",
       "      <td>5.598994</td>\n",
       "      <td>4.788770</td>\n",
       "      <td>3.843510</td>\n",
       "      <td>2.700194</td>\n",
       "      <td>1.385831</td>\n",
       "      <td>-0.000551</td>\n",
       "      <td>-1.287906</td>\n",
       "      <td>-2.368204</td>\n",
       "      <td>-3.214438</td>\n",
       "      <td>1.511866</td>\n",
       "      <td>-3.961644</td>\n",
       "      <td>-4.798875</td>\n",
       "      <td>-5.879173</td>\n",
       "      <td>-7.274558</td>\n",
       "      <td>0.782665</td>\n",
       "      <td>-0.216611</td>\n",
       "      <td>-1.485961</td>\n",
       "      <td>-6962.850037</td>\n",
       "      <td>-7019.002162</td>\n",
       "      <td>-6990.929757</td>\n",
       "      <td>-7019.002162</td>\n",
       "      <td>23793.226267</td>\n",
       "      <td>23793.226267</td>\n",
       "      <td>24209.957143</td>\n",
       "      <td>24209.957143</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78765</th>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>20.319082</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>93.027754</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-15.522890</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>-7.349284</td>\n",
       "      <td>7.255451</td>\n",
       "      <td>7.318468</td>\n",
       "      <td>4.536701</td>\n",
       "      <td>4.185604</td>\n",
       "      <td>4.032562</td>\n",
       "      <td>4.149594</td>\n",
       "      <td>4.473683</td>\n",
       "      <td>4.905803</td>\n",
       "      <td>5.292909</td>\n",
       "      <td>5.490964</td>\n",
       "      <td>5.445952</td>\n",
       "      <td>5.148870</td>\n",
       "      <td>7.192433</td>\n",
       "      <td>4.671738</td>\n",
       "      <td>4.122587</td>\n",
       "      <td>3.591440</td>\n",
       "      <td>3.132313</td>\n",
       "      <td>2.790219</td>\n",
       "      <td>2.529147</td>\n",
       "      <td>2.295082</td>\n",
       "      <td>2.007003</td>\n",
       "      <td>1.619896</td>\n",
       "      <td>1.133762</td>\n",
       "      <td>6.958369</td>\n",
       "      <td>0.620620</td>\n",
       "      <td>0.152491</td>\n",
       "      <td>-0.243618</td>\n",
       "      <td>-0.594715</td>\n",
       "      <td>-0.999827</td>\n",
       "      <td>-1.512968</td>\n",
       "      <td>-2.116135</td>\n",
       "      <td>-2.701296</td>\n",
       "      <td>-3.133415</td>\n",
       "      <td>-3.331470</td>\n",
       "      <td>6.688294</td>\n",
       "      <td>-3.349475</td>\n",
       "      <td>-3.322468</td>\n",
       "      <td>-3.403490</td>\n",
       "      <td>-3.673564</td>\n",
       "      <td>-4.132691</td>\n",
       "      <td>-4.690845</td>\n",
       "      <td>-5.221992</td>\n",
       "      <td>-5.699123</td>\n",
       "      <td>-6.113237</td>\n",
       "      <td>-6.536354</td>\n",
       "      <td>6.409217</td>\n",
       "      <td>-7.004483</td>\n",
       "      <td>-7.562637</td>\n",
       "      <td>-8.192811</td>\n",
       "      <td>-8.904007</td>\n",
       "      <td>-9.660216</td>\n",
       "      <td>-10.425427</td>\n",
       "      <td>-11.136623</td>\n",
       "      <td>-11.712782</td>\n",
       "      <td>-12.036872</td>\n",
       "      <td>-11.955849</td>\n",
       "      <td>6.121138</td>\n",
       "      <td>-11.316673</td>\n",
       "      <td>-10.011313</td>\n",
       "      <td>-8.021764</td>\n",
       "      <td>-5.465059</td>\n",
       "      <td>5.806051</td>\n",
       "      <td>5.427947</td>\n",
       "      <td>4.986825</td>\n",
       "      <td>-4024.027079</td>\n",
       "      <td>-3967.874954</td>\n",
       "      <td>-4024.027079</td>\n",
       "      <td>-3995.947358</td>\n",
       "      <td>-54968.909299</td>\n",
       "      <td>-54968.909299</td>\n",
       "      <td>-54968.909299</td>\n",
       "      <td>-54968.909299</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78766 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ACC110      ACC111     ACC1110     ACC1111     ACC1112     ACC1113  \\\n",
       "0      275.095822  257.706505  309.874457  335.958433  309.874457  214.233212   \n",
       "1       -3.133252   -3.133252    5.561406    5.561406    5.561406   -3.133252   \n",
       "2      -90.079838  -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -90.079838  -90.079838  -90.079838  -98.774497  -90.079838  -90.079838   \n",
       "4     -116.163814 -107.469155   40.340041   75.118675   -3.133252  -11.827911   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    9.198303    3.637913    9.198303    9.198303    3.637913    9.198303   \n",
       "78762   -1.922477   -1.922477   -7.482866   -1.922477   -1.922477   -1.922477   \n",
       "78763   20.319082   14.758692   14.758692   20.319082   20.319082   20.319082   \n",
       "78764  -29.724425  -24.164035  -24.164035  -29.724425  -24.164035  -24.164035   \n",
       "78765   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "          ACC1114     ACC1115     ACC1116     ACC1117     ACC1118     ACC1119  \\\n",
       "0      257.706505  344.653091  370.737067  335.958433  318.569115  301.179798   \n",
       "1        5.561406    5.561406   -3.133252    5.561406   -3.133252   -3.133252   \n",
       "2      -90.079838  -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -90.079838  -98.774497  -98.774497  -90.079838  -98.774497  -98.774497   \n",
       "4      -81.385180  -98.774497  -55.301204  -20.522569   -3.133252   31.645382   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    3.637913    9.198303    9.198303    3.637913    9.198303    3.637913   \n",
       "78762   -1.922477   -1.922477   -7.482866   -1.922477   -1.922477   -1.922477   \n",
       "78763   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "78764  -24.164035  -29.724425  -29.724425  -24.164035  -24.164035  -24.164035   \n",
       "78765   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "           ACC112     ACC1120     ACC1121     ACC1122     ACC1123     ACC1124  \\\n",
       "0      249.011847  266.401164  266.401164  266.401164  309.874457  327.263774   \n",
       "1      -11.827911  -11.827911   -3.133252   -3.133252  -11.827911   -3.133252   \n",
       "2      -90.079838  -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -98.774497  -90.079838  -90.079838  -98.774497  -90.079838  -98.774497   \n",
       "4     -229.194376   75.118675   49.034699   -3.133252   31.645382   75.118675   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    9.198303    3.637913    9.198303    9.198303    9.198303    3.637913   \n",
       "78762   -7.482866   -1.922477   -1.922477   -1.922477   -7.482866   -7.482866   \n",
       "78763   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "78764  -29.724425  -24.164035  -24.164035  -29.724425  -29.724425  -29.724425   \n",
       "78765   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "          ACC1125     ACC1126     ACC1127     ACC1128     ACC1129      ACC113  \\\n",
       "0      335.958433  283.790481  301.179798  266.401164  196.843895  266.401164   \n",
       "1       -3.133252   -3.133252   -3.133252   -3.133252  -11.827911    5.561406   \n",
       "2      -90.079838  -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -90.079838  -90.079838  -98.774497  -90.079838  -98.774497  -90.079838   \n",
       "4      101.202651   49.034699   40.340041   49.034699   31.645382 -194.415741   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    9.198303    3.637913    3.637913    3.637913    3.637913    9.198303   \n",
       "78762   -1.922477   -1.922477   -1.922477   -1.922477   -1.922477   -1.922477   \n",
       "78763   20.319082   14.758692   20.319082   14.758692   20.319082   20.319082   \n",
       "78764  -24.164035  -24.164035  -29.724425  -29.724425  -29.724425  -29.724425   \n",
       "78765   20.319082   20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "          ACC1130    ACC1131      ACC114      ACC115      ACC116      ACC117  \\\n",
       "0      379.431726 -55.301204  283.790481  283.790481  275.095822  275.095822   \n",
       "1      -11.827911 -11.827911   -3.133252  -11.827911    5.561406    5.561406   \n",
       "2      -90.079838 -90.079838  -90.079838  -90.079838  -90.079838  -90.079838   \n",
       "3      -90.079838 -90.079838  -90.079838  -98.774497  -98.774497  -98.774497   \n",
       "4       31.645382  14.256065   -3.133252   75.118675  118.591968  -81.385180   \n",
       "...           ...        ...         ...         ...         ...         ...   \n",
       "78761    3.637913   3.637913    3.637913    9.198303    9.198303    3.637913   \n",
       "78762   -7.482866  -1.922477   -1.922477   -1.922477   -1.922477   -1.922477   \n",
       "78763   20.319082  20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "78764  -24.164035 -24.164035  -29.724425  -24.164035  -29.724425  -24.164035   \n",
       "78765   20.319082  20.319082   20.319082   20.319082   20.319082   20.319082   \n",
       "\n",
       "           ACC118      ACC119      ACC210      ACC211      ACC2110  \\\n",
       "0      301.179798  309.874457  252.378126  293.021173   333.664220   \n",
       "1       -3.133252   -3.133252  252.378126  252.378126   293.021173   \n",
       "2      -90.079838  -90.079838  699.451645  699.451645   658.808598   \n",
       "3      -90.079838  -98.774497  618.165551  618.165551   618.165551   \n",
       "4     -142.247790  -55.301204 -763.698054 -844.984148 -1048.199384   \n",
       "...           ...         ...         ...         ...          ...   \n",
       "78761    9.198303    9.198303 -104.570612 -104.570612  -104.570612   \n",
       "78762   -1.922477   -1.922477  132.547427  112.787590    93.027754   \n",
       "78763   20.319082   20.319082   73.267917   73.267917    73.267917   \n",
       "78764  -29.724425  -29.724425 -203.369795 -203.369795  -203.369795   \n",
       "78765   20.319082   20.319082   93.027754   93.027754    93.027754   \n",
       "\n",
       "           ACC2111      ACC2112      ACC2113      ACC2114      ACC2115  \\\n",
       "0       333.664220   374.307267   252.378126   414.950315   414.950315   \n",
       "1       252.378126   252.378126   293.021173   333.664220   293.021173   \n",
       "2       658.808598   699.451645   658.808598   658.808598   658.808598   \n",
       "3       618.165551   618.165551   618.165551   618.165551   618.165551   \n",
       "4     -1007.556337 -1088.842431 -1129.485478 -1170.128525 -1292.057667   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "78761   -84.810776   -84.810776  -104.570612  -104.570612  -104.570612   \n",
       "78762   112.787590   112.787590   112.787590   112.787590    93.027754   \n",
       "78763    73.267917    73.267917    73.267917    73.267917    73.267917   \n",
       "78764  -203.369795  -203.369795  -203.369795  -203.369795  -203.369795   \n",
       "78765    93.027754    93.027754    93.027754    93.027754    93.027754   \n",
       "\n",
       "           ACC2116      ACC2117      ACC2118      ACC2119      ACC212  \\\n",
       "0       374.307267   374.307267   293.021173   211.735079  293.021173   \n",
       "1       333.664220   293.021173   252.378126   252.378126  252.378126   \n",
       "2       658.808598   658.808598   658.808598   658.808598  658.808598   \n",
       "3       618.165551   618.165551   618.165551   618.165551  618.165551   \n",
       "4     -1251.414620 -1251.414620 -1170.128525 -1129.485478 -966.913290   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "78761  -104.570612  -104.570612  -104.570612  -104.570612 -104.570612   \n",
       "78762    93.027754    93.027754    93.027754    93.027754  112.787590   \n",
       "78763    73.267917    73.267917    73.267917    73.267917   73.267917   \n",
       "78764  -203.369795  -203.369795  -203.369795  -203.369795 -203.369795   \n",
       "78765    93.027754    93.027754    93.027754    93.027754   93.027754   \n",
       "\n",
       "           ACC2120      ACC2121     ACC2122     ACC2123     ACC2124  \\\n",
       "0       293.021173   252.378126  293.021173  171.092032  130.448984   \n",
       "1       252.378126   252.378126  252.378126  252.378126  252.378126   \n",
       "2       658.808598   658.808598  658.808598  658.808598  658.808598   \n",
       "3       618.165551   618.165551  618.165551  618.165551  618.165551   \n",
       "4     -1129.485478 -1048.199384 -966.913290 -966.913290 -844.984148   \n",
       "...            ...          ...         ...         ...         ...   \n",
       "78761  -104.570612  -104.570612 -104.570612 -104.570612 -104.570612   \n",
       "78762   112.787590   112.787590   93.027754   93.027754   93.027754   \n",
       "78763    73.267917    73.267917   73.267917   73.267917   73.267917   \n",
       "78764  -203.369795  -203.369795 -203.369795 -203.369795 -203.369795   \n",
       "78765    93.027754    93.027754   93.027754   93.027754   93.027754   \n",
       "\n",
       "          ACC2125     ACC2126     ACC2127     ACC2128     ACC2129  \\\n",
       "0      171.092032  171.092032  171.092032  414.950315  414.950315   \n",
       "1      252.378126  293.021173  252.378126  293.021173  252.378126   \n",
       "2      658.808598  658.808598  658.808598  658.808598  658.808598   \n",
       "3      618.165551  618.165551  618.165551  618.165551  618.165551   \n",
       "4     -804.341101 -763.698054 -763.698054 -723.055006 -804.341101   \n",
       "...           ...         ...         ...         ...         ...   \n",
       "78761 -104.570612 -104.570612 -104.570612 -104.570612 -104.570612   \n",
       "78762  112.787590  112.787590   93.027754   93.027754   93.027754   \n",
       "78763   73.267917   73.267917   73.267917   73.267917   73.267917   \n",
       "78764 -203.369795 -203.369795 -203.369795 -203.369795 -203.369795   \n",
       "78765   93.027754   93.027754   93.027754   93.027754   93.027754   \n",
       "\n",
       "            ACC213      ACC2130      ACC2131       ACC214       ACC215  \\\n",
       "0       333.664220  1065.239070  1024.596022   374.307267   374.307267   \n",
       "1       211.735079   252.378126   252.378126   171.092032   211.735079   \n",
       "2       658.808598   658.808598   658.808598   658.808598   658.808598   \n",
       "3       618.165551   618.165551   618.165551   618.165551   618.165551   \n",
       "4     -1088.842431  -844.984148  -926.270242 -1129.485478 -1129.485478   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "78761  -104.570612  -104.570612  -104.570612  -104.570612   -84.810776   \n",
       "78762   112.787590    93.027754    93.027754   112.787590   112.787590   \n",
       "78763    73.267917    73.267917    73.267917    73.267917    73.267917   \n",
       "78764  -203.369795  -203.369795  -203.369795  -203.369795  -203.369795   \n",
       "78765    93.027754    93.027754    93.027754    93.027754    93.027754   \n",
       "\n",
       "            ACC216      ACC217       ACC218       ACC219      ACC310  \\\n",
       "0       252.378126  211.735079   293.021173   333.664220 -649.907797   \n",
       "1       171.092032  211.735079   252.378126   252.378126 -420.673549   \n",
       "2       658.808598  699.451645   699.451645   658.808598  -70.789696   \n",
       "3       618.165551  618.165551   618.165551   618.165551  -46.659775   \n",
       "4     -1007.556337 -885.627195 -1007.556337 -1129.485478    1.600066   \n",
       "...            ...         ...          ...          ...         ...   \n",
       "78761  -104.570612 -104.570612  -104.570612  -104.570612  -40.043709   \n",
       "78762   112.787590  112.787590   112.787590    93.027754  131.602023   \n",
       "78763    73.267917   73.267917    73.267917    73.267917    0.824322   \n",
       "78764  -203.369795 -203.369795  -203.369795  -203.369795   90.733992   \n",
       "78765    93.027754   93.027754    93.027754    93.027754   -7.349284   \n",
       "\n",
       "           ACC311     ACC3110     ACC3111     ACC3112     ACC3113     ACC3114  \\\n",
       "0     -649.907797 -661.972758 -698.167639 -782.622362 -601.647955 -613.712916   \n",
       "1     -420.673549 -408.608588 -408.608588 -420.673549 -420.673549 -420.673549   \n",
       "2      -82.854657  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -46.659775  -46.659775  -46.659775   \n",
       "4        1.600066  399.743761  484.198484  508.328405  448.003603  230.834315   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  123.428417  123.428417  123.428417  131.602023  131.602023  131.602023   \n",
       "78763   -7.349284    0.824322    0.824322    0.824322    0.824322    0.824322   \n",
       "78764   90.733992   82.560385   82.560385   90.733992   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284   -7.349284  -15.522890   -7.349284   -7.349284   \n",
       "\n",
       "          ACC3115     ACC3116     ACC3117     ACC3118     ACC3119      ACC312  \\\n",
       "0     -698.167639 -661.972758 -674.037718 -746.427481 -698.167639 -674.037718   \n",
       "1     -432.738509 -432.738509 -420.673549 -420.673549 -432.738509 -408.608588   \n",
       "2      -70.789696  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -46.659775  -58.724736  -46.659775   \n",
       "4      182.574473  206.704394  218.769354  242.899275  327.353998  -82.854657   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  123.428417  123.428417  123.428417  123.428417  123.428417  131.602023   \n",
       "78763    0.824322   -7.349284   -7.349284   -7.349284    0.824322   -7.349284   \n",
       "78764   82.560385   90.733992   82.560385   90.733992   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284  -15.522890   -7.349284  -15.522890   -7.349284   \n",
       "\n",
       "          ACC3120     ACC3121     ACC3122     ACC3123     ACC3124     ACC3125  \\\n",
       "0     -674.037718 -746.427481 -722.297560 -649.907797 -625.777876 -613.712916   \n",
       "1     -432.738509 -396.543628 -396.543628 -420.673549 -396.543628 -396.543628   \n",
       "2      -70.789696  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -46.659775  -46.659775  -46.659775   \n",
       "4      375.613840  399.743761  508.328405  508.328405  604.848088  641.042970   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  131.602023  123.428417  123.428417  123.428417  123.428417  123.428417   \n",
       "78763   -7.349284   -7.349284    0.824322   -7.349284   -7.349284    0.824322   \n",
       "78764   90.733992   82.560385   82.560385   82.560385   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284  -15.522890   -7.349284   -7.349284   -7.349284   \n",
       "\n",
       "          ACC3126     ACC3127     ACC3128     ACC3129      ACC313     ACC3130  \\\n",
       "0     -686.102678 -722.297560 -734.362520 -698.167639 -649.907797  339.418959   \n",
       "1     -396.543628 -384.478667 -384.478667 -396.543628 -396.543628 -396.543628   \n",
       "2      -70.789696  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -58.724736  -46.659775  -46.659775   \n",
       "4      580.718167  616.913049  616.913049  568.653207  -22.529854  544.523286   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -31.870103  -31.870103  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  123.428417  123.428417  123.428417  123.428417  131.602023  123.428417   \n",
       "78763    0.824322    0.824322   -7.349284   -7.349284    0.824322   -7.349284   \n",
       "78764   82.560385   90.733992   90.733992   90.733992   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284  -15.522890   -7.349284   -7.349284   -7.349284   \n",
       "\n",
       "          ACC3131      ACC314      ACC315      ACC316      ACC317      ACC318  \\\n",
       "0     -927.401887 -613.712916 -625.777876 -770.557401 -794.687322 -674.037718   \n",
       "1     -384.478667 -420.673549 -396.543628 -384.478667 -396.543628 -420.673549   \n",
       "2      -70.789696  -70.789696  -70.789696  -70.789696  -70.789696  -70.789696   \n",
       "3      -46.659775  -46.659775  -46.659775  -46.659775  -46.659775  -46.659775   \n",
       "4      472.133523  218.769354  351.483919  472.133523  327.353998  254.964236   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709  -40.043709   \n",
       "78762  123.428417  131.602023  123.428417  123.428417  123.428417  123.428417   \n",
       "78763    0.824322   -7.349284    0.824322   -7.349284   -7.349284    0.824322   \n",
       "78764   90.733992   90.733992   82.560385   82.560385   90.733992   90.733992   \n",
       "78765   -7.349284   -7.349284   -7.349284   -7.349284   -7.349284   -7.349284   \n",
       "\n",
       "           ACC319       BVP10       BVP11      BVP110      BVP111      BVP112  \\\n",
       "0     -674.037718   -1.033617   10.019713   38.208016   40.427932   42.624723   \n",
       "1     -420.673549    1.556284    3.729952   10.435947   10.366574   10.019713   \n",
       "2      -70.789696    9.904092    9.719099    7.452935    7.476059    7.314191   \n",
       "3      -46.659775   -5.797186   -8.155846  -31.927443  -31.488084  -29.753775   \n",
       "4      303.224077 -148.010530 -133.673575  116.598786  124.692229  126.588407   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761  -40.043709   -7.094508   -7.652662   -8.895005   -7.445605   -5.510071   \n",
       "78762  123.428417  -13.729339   -9.894281   -1.927083   -2.296184   -2.656284   \n",
       "78763    0.824322    6.301188    6.571262    4.356651    4.185604    4.131589   \n",
       "78764   90.733992    4.986825    4.473683   -2.926358   -4.393763   -5.780146   \n",
       "78765   -7.349284    7.255451    7.318468    4.536701    4.185604    4.032562   \n",
       "\n",
       "           BVP113      BVP114      BVP115      BVP116      BVP117      BVP118  \\\n",
       "0       44.266536   45.168377   45.492114   45.422742   45.122128   44.590274   \n",
       "1        9.256617    8.169783    7.013577    6.065488    5.256144    4.308055   \n",
       "2        6.921080    6.250481    5.348640    4.331179    3.359966    2.481249   \n",
       "3      -26.724516  -22.377181  -16.989261  -10.907617   -4.710352    0.978181   \n",
       "4      122.888547  115.442580  107.233517  101.406239  100.435026  105.568581   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761   -3.223440   -0.801772    1.538874    3.600442    5.247897    6.364205   \n",
       "78762   -2.989376   -3.259450   -3.340472   -3.070398   -2.314189   -1.017832   \n",
       "78763    4.158596    4.212611    4.185604    4.014557    3.681465    3.231341   \n",
       "78764   -7.076503   -8.354856   -9.732236  -11.289666  -13.054152  -14.944674   \n",
       "78765    4.149594    4.473683    4.905803    5.292909    5.490964    5.445952   \n",
       "\n",
       "           BVP119       BVP12      BVP120      BVP121      BVP122      BVP123  \\\n",
       "0       43.642185   19.454354   42.139117   40.011698   37.329300   34.323164   \n",
       "1        2.804987    6.042364    0.561947   -2.305444   -5.473448   -8.618329   \n",
       "2        1.602533    9.279741    0.446327   -1.264858   -3.716015   -6.837772   \n",
       "3        5.695502  -10.722624    9.233493   11.522781   12.817731   13.372710   \n",
       "4      116.691283 -108.468283  132.577554  151.354340  170.847974  189.023533   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    6.931361   -8.129794    7.030389    6.760314    6.274180    5.698021   \n",
       "78762    0.746655   -6.734409    2.763211    4.770765    6.490240    7.723580   \n",
       "78763    2.745207    6.751312    2.250070    1.763936    1.241792    0.674635   \n",
       "78764  -16.853201    3.825505  -18.608685  -19.986065  -20.715266  -20.517211   \n",
       "78765    5.148870    7.192433    4.671738    4.122587    3.591440    3.132313   \n",
       "\n",
       "           BVP124      BVP125      BVP126      BVP127      BVP128      BVP129  \\\n",
       "0       31.132035   28.033403   25.096640   22.391117   19.801216   17.234438   \n",
       "1      -11.531968  -14.214366  -16.642399  -18.561701  -19.671659  -19.625410   \n",
       "2      -10.352638  -13.728760  -16.526778  -18.284211  -18.700446  -17.544240   \n",
       "3       13.580827   13.650200   13.673324   13.557703   13.187717   12.632738   \n",
       "4      204.077336  214.899425  221.327930  224.218445  225.074038  225.397775   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78761    5.103857    4.527698    3.996552    3.519420    3.132313    2.853236   \n",
       "78762    8.398766    8.560811    8.371759    8.002657    7.633555    7.318468   \n",
       "78763    0.116481   -0.351648   -0.639727   -0.702745   -0.594715   -0.405663   \n",
       "78764  -19.157836  -16.574124  -12.928118   -8.534906   -3.898626    0.521593   \n",
       "78765    2.790219    2.529147    2.295082    2.007003    1.619896    1.133762   \n",
       "\n",
       "           BVP13      BVP130      BVP131      BVP132      BVP133      BVP134  \\\n",
       "0      26.992817   14.667661   12.147132    9.811596    7.892294    6.435474   \n",
       "1       8.192907  -18.307336  -15.994923  -13.173781  -10.468259   -8.317715   \n",
       "2       8.724762  -14.746221  -10.514507   -5.265331    0.284458    5.464261   \n",
       "3     -13.474394   12.054635   11.892766   12.447745   13.835193   15.708246   \n",
       "4     -75.539535  225.883382  225.883382  223.478473  216.217499  202.158034   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "78761  -8.543908    2.736204    2.808224    3.069296    3.492413    4.014557   \n",
       "78762  -4.330746    7.075401    6.832334    6.499242    5.977098    5.229892   \n",
       "78763   6.742309   -0.261623   -0.225613   -0.288630   -0.414665   -0.522695   \n",
       "78764   3.177326    4.356651    7.408493    9.713129   11.396593   12.584921   \n",
       "78765   6.958369    0.620620    0.152491   -0.243618   -0.594715   -0.999827   \n",
       "\n",
       "           BVP135      BVP136      BVP137      BVP138     BVP139      BVP14  \\\n",
       "0        5.394888    4.493048    3.568083    2.550622   1.648781  32.288241   \n",
       "1       -6.907144   -6.190296   -5.982179   -6.051551  -6.190296   9.927216   \n",
       "2        9.695975   12.887104   15.153268   16.818204  18.205651   8.146659   \n",
       "3       17.581300   18.945623   19.523726   19.361858  18.598762 -16.457406   \n",
       "4      180.953215  154.337352  126.172172  101.336866  84.363762 -38.934051   \n",
       "...           ...         ...         ...         ...        ...        ...   \n",
       "78761    4.509693    4.815778    4.815778    4.428671   3.699470  -8.940017   \n",
       "78762    4.248621    3.087301    1.799946    0.440571  -0.972819  -2.647281   \n",
       "78763   -0.594715   -0.684740   -0.855787   -1.161871  -1.630001   6.544255   \n",
       "78764   13.386142   13.782252   13.719234   13.143075  12.161804   2.583162   \n",
       "78765   -1.512968   -2.116135   -2.701296   -3.133415  -3.331470   6.688294   \n",
       "\n",
       "          BVP140     BVP141     BVP142      BVP143      BVP144      BVP145  \\\n",
       "0       1.116926   1.209423   1.949394    3.244345    4.839910    6.458598   \n",
       "1      -6.375289  -6.652779  -7.022764   -7.485247   -8.017102   -8.456460   \n",
       "2      19.384982  20.379319  21.073042   21.419904   21.373656   20.911174   \n",
       "3      17.442556  15.939488  14.274551   12.563366   11.175919   10.274078   \n",
       "4      78.258994  83.531293  98.492600  119.743667  143.353394  165.714419   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "78761   2.763211   1.754933   0.827677    0.035459   -0.630725   -1.233891   \n",
       "78762  -2.485237  -4.177703  -6.104235   -8.282836  -10.614479  -12.874103   \n",
       "78763  -2.206160  -2.818328  -3.412492   -3.943639   -4.420771   -4.888900   \n",
       "78764  10.946469   9.812156   9.001933    8.605823    8.560811    8.650836   \n",
       "78765  -3.349475  -3.322468  -3.403490   -3.673564   -4.132691   -4.690845   \n",
       "\n",
       "           BVP146      BVP147      BVP148      BVP149      BVP15      BVP150  \\\n",
       "0        7.961666    9.187244   10.042837   10.435947  35.201880   10.412823   \n",
       "1       -8.710825   -8.780198   -8.687701   -8.618329  10.990926   -8.664577   \n",
       "2       20.078705   18.876251   17.396307   15.846991   7.661052   14.459544   \n",
       "3        9.904092    9.719099    9.279741    8.169783 -19.741031    6.227357   \n",
       "4      183.820606  195.428915  198.828160  192.792765  -2.652306  176.860246   \n",
       "...           ...         ...         ...         ...        ...         ...   \n",
       "78761   -1.837058   -2.476234   -3.187430   -3.934636  -9.354132   -4.708850   \n",
       "78762  -14.737617  -15.808912  -15.736893  -14.341508  -1.602993  -11.622757   \n",
       "78763   -5.348026   -5.807153   -6.239272   -6.608374   6.184155   -6.905456   \n",
       "78764    8.632831    8.362756    7.831610    7.129416   2.070020    6.373207   \n",
       "78765   -5.221992   -5.699123   -6.113237   -6.536354   6.409217   -7.004483   \n",
       "\n",
       "           BVP151      BVP152     BVP153     BVP154     BVP155     BVP156  \\\n",
       "0       10.158457   10.019713  10.297202  11.152795  12.424621  13.765820   \n",
       "1       -8.872694   -9.173308  -9.497045  -9.797659 -10.190769 -10.722624   \n",
       "2       13.488331   12.979600  12.863980  12.863980  12.678987  12.054635   \n",
       "3        3.614331    0.631320  -2.444189  -5.427200  -8.178971 -10.653251   \n",
       "4      151.562457  118.911198  82.005101  44.451529   9.603478 -20.642872   \n",
       "...           ...         ...        ...        ...        ...        ...   \n",
       "78761   -5.456056   -6.140245  -6.725406  -7.238548  -7.733685  -8.291839   \n",
       "78762   -7.886727   -3.601544   0.719648   4.608721   7.768592  10.082231   \n",
       "78763   -7.166528   -7.481615  -7.913734  -8.480891  -9.066052  -9.480166   \n",
       "78764    5.598994    4.788770   3.843510   2.700194   1.385831  -0.000551   \n",
       "78765   -7.562637   -8.192811  -8.904007  -9.660216 -10.425427 -11.136623   \n",
       "\n",
       "          BVP157     BVP158     BVP159      BVP16     BVP160      BVP161  \\\n",
       "0      14.783282  15.268888  15.153268  36.173093  14.552040   13.580827   \n",
       "1     -11.393223 -11.994451 -12.086947  11.337788 -11.231355   -9.196432   \n",
       "2      10.967802   9.441610   7.637928   7.337315   5.626130    3.475586   \n",
       "3     -12.803795 -14.723097 -16.642399 -23.163401 -18.862314  -21.660333   \n",
       "4     -45.825039 -66.729245 -84.465446  30.553932 -99.681117 -112.145018   \n",
       "...          ...        ...        ...        ...        ...         ...   \n",
       "78761  -8.958022  -9.714231 -10.416425  -9.741238 -10.839541  -10.767522   \n",
       "78762  11.513626  12.071780  11.810708  -1.107857  10.847442    9.353030   \n",
       "78763  -9.471164  -8.867997  -7.607650   5.761039  -5.843163   -3.817604   \n",
       "78764  -1.287906  -2.368204  -3.214438   1.511866  -3.961644   -4.798875   \n",
       "78765 -11.712782 -12.036872 -11.955849   6.121138 -11.316673  -10.011313   \n",
       "\n",
       "           BVP162      BVP163      BVP17      BVP18       BVP19         EDA10  \\\n",
       "0       12.332125   10.852181  36.057473  35.941852   36.589328  -2002.240176   \n",
       "1       -6.074676   -2.444189  11.175919  10.782809   10.528443  10508.161649   \n",
       "2        1.209423   -1.172362   7.221694   7.244818    7.360439  -7230.053448   \n",
       "3      -25.152075  -29.036928 -26.447026 -29.198797  -31.118099  -6515.801698   \n",
       "4     -121.024681 -125.603257  59.343463  83.392549  102.585569   8881.240525   \n",
       "...           ...         ...        ...        ...         ...           ...   \n",
       "78761  -10.074330   -8.714955 -10.047323 -10.110340   -9.768246  -6148.589357   \n",
       "78762    7.633555    5.995103  -1.017832  -1.215886   -1.557981  -2792.550625   \n",
       "78763   -1.783043    0.017454   5.328919   4.941813    4.617723  -4042.749331   \n",
       "78764   -5.879173   -7.274558   0.782665  -0.216611   -1.485961  -6962.850037   \n",
       "78765   -8.021764   -5.465059   5.806051   5.427947    4.986825  -4024.027079   \n",
       "\n",
       "              EDA11         EDA12         EDA13        TEMP10        TEMP11  \\\n",
       "0      -2015.464571   -944.071437   -904.398252   5090.486004   5090.486004   \n",
       "1      10349.427549  10124.571474  10018.755634  -5248.854417  -5248.854417   \n",
       "2      -7296.185763  -7243.277843  -7203.604658 -17885.826043 -17885.826043   \n",
       "3      -6515.801698  -6515.801698  -6542.250488 -16737.010440 -16737.010440   \n",
       "4       9013.505155   9979.072109  10217.162919  10604.800896  10604.800896   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "78761  -6083.072448  -6101.794701  -6111.152169  19000.821193  19000.821193   \n",
       "78762  -2783.193157  -2858.067534  -2876.789787  23376.495391  23376.495391   \n",
       "78763  -4070.821735  -4033.384547  -4070.821735 -61219.872439 -61219.872439   \n",
       "78764  -7019.002162  -6990.929757  -7019.002162  23793.226267  23793.226267   \n",
       "78765  -3967.874954  -4024.027079  -3995.947358 -54968.909299 -54968.909299   \n",
       "\n",
       "             TEMP12        TEMP13  id  stress  Cluster  \n",
       "0       4860.722884   4860.722884  11     1.0        2  \n",
       "1      -5248.854417  -5248.854417  11     1.0        2  \n",
       "2     -17885.826043 -17885.826043  11     0.0        2  \n",
       "3     -16737.010440 -16737.010440  11     0.0        2  \n",
       "4      10834.564016  10834.564016  11     0.0        2  \n",
       "...             ...           ...  ..     ...      ...  \n",
       "78761  19000.821193  19000.821193   7     0.0        2  \n",
       "78762  23376.495391  23376.495391   7     0.0        2  \n",
       "78763 -61219.872439 -61219.872439   7     0.0        2  \n",
       "78764  24209.957143  24209.957143   7     0.0        2  \n",
       "78765 -54968.909299 -54968.909299   7     0.0        2  \n",
       "\n",
       "[78766 rows x 171 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop 'dataset' to run pycaret tests based on \"Cluster\".\n",
    "\n",
    "wesad_grouped_all = wesad_grouped_all.drop('dataset', axis = 1)\n",
    "wesad_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6da72364",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_participants = wesad_grouped_all[\"Cluster\"].unique()\n",
    "wesad_group = wesad_grouped_all.groupby('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88a9c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression    0.8976  0.9639  0.8879  0.7553  0.8161  0.7458   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.7506     3.739  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947  0.9910  0.9879   \n",
       "lr      Logistic Regression    0.8976  0.9639  0.8879  0.7553  0.8161  0.7458   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.9879     0.614  \n",
       "lr   0.7506     3.739  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947  0.9910  0.9879   \n",
       "lr      Logistic Regression    0.8976  0.9639  0.8879  0.7553  0.8161  0.7458   \n",
       "nb              Naive Bayes    0.7906  0.8059  0.7033  0.5746  0.6321  0.4880   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.9879     0.614  \n",
       "lr   0.7506     3.739  \n",
       "nb   0.4931     0.172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947  0.9910   \n",
       "dt   Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825  0.9838   \n",
       "lr        Logistic Regression    0.8976  0.9639  0.8879  0.7553  0.8161   \n",
       "nb                Naive Bayes    0.7906  0.8059  0.7033  0.5746  0.6321   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.9879  0.9879     0.614  \n",
       "dt   0.9782  0.9782     0.334  \n",
       "lr   0.7458  0.7506     3.739  \n",
       "nb   0.4880  0.4931     0.172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947  0.9910   \n",
       "dt   Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825  0.9838   \n",
       "lr        Logistic Regression    0.8976  0.9639  0.8879  0.7553  0.8161   \n",
       "svm       SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076  0.8049   \n",
       "nb                Naive Bayes    0.7906  0.8059  0.7033  0.5746  0.6321   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.9879  0.9879     0.614  \n",
       "dt   0.9782  0.9782     0.334  \n",
       "lr   0.7458  0.7506     3.739  \n",
       "svm  0.7232  0.7407     0.446  \n",
       "nb   0.4880  0.4931     0.172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947  0.9910   \n",
       "dt     Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825  0.9838   \n",
       "ridge          Ridge Classifier    0.9086  0.0000  0.9267  0.7657  0.8385   \n",
       "lr          Logistic Regression    0.8976  0.9639  0.8879  0.7553  0.8161   \n",
       "svm         SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076  0.8049   \n",
       "nb                  Naive Bayes    0.7906  0.8059  0.7033  0.5746  0.6321   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.9879  0.9879     0.614  \n",
       "dt     0.9782  0.9782     0.334  \n",
       "ridge  0.7755  0.7825     0.167  \n",
       "lr     0.7458  0.7506     3.739  \n",
       "svm    0.7232  0.7407     0.446  \n",
       "nb     0.4880  0.4931     0.172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947  0.9910   \n",
       "rf     Random Forest Classifier    0.9948  0.9997  0.9926  0.9870  0.9898   \n",
       "dt     Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825  0.9838   \n",
       "ridge          Ridge Classifier    0.9086  0.0000  0.9267  0.7657  0.8385   \n",
       "lr          Logistic Regression    0.8976  0.9639  0.8879  0.7553  0.8161   \n",
       "svm         SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076  0.8049   \n",
       "nb                  Naive Bayes    0.7906  0.8059  0.7033  0.5746  0.6321   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.9879  0.9879     0.614  \n",
       "rf     0.9863  0.9863     1.392  \n",
       "dt     0.9782  0.9782     0.334  \n",
       "ridge  0.7755  0.7825     0.167  \n",
       "lr     0.7458  0.7506     3.739  \n",
       "svm    0.7232  0.7407     0.446  \n",
       "nb     0.4880  0.4931     0.172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947   \n",
       "rf            Random Forest Classifier    0.9948  0.9997  0.9926  0.9870   \n",
       "dt            Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825   \n",
       "ridge                 Ridge Classifier    0.9086  0.0000  0.9267  0.7657   \n",
       "lr                 Logistic Regression    0.8976  0.9639  0.8879  0.7553   \n",
       "svm                SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076   \n",
       "nb                         Naive Bayes    0.7906  0.8059  0.7033  0.5746   \n",
       "qda    Quadratic Discriminant Analysis    0.7779  0.9331  0.2648  0.6683   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.9910  0.9879  0.9879     0.614  \n",
       "rf     0.9898  0.9863  0.9863     1.392  \n",
       "dt     0.9838  0.9782  0.9782     0.334  \n",
       "ridge  0.8385  0.7755  0.7825     0.167  \n",
       "lr     0.8161  0.7458  0.7506     3.739  \n",
       "svm    0.8049  0.7232  0.7407     0.446  \n",
       "nb     0.6321  0.4880  0.4931     0.172  \n",
       "qda    0.3789  0.2732  0.3169     0.582  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>1.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947   \n",
       "rf            Random Forest Classifier    0.9948  0.9997  0.9926  0.9870   \n",
       "dt            Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825   \n",
       "ada               Ada Boost Classifier    0.9736  0.9970  0.9702  0.9298   \n",
       "ridge                 Ridge Classifier    0.9086  0.0000  0.9267  0.7657   \n",
       "lr                 Logistic Regression    0.8976  0.9639  0.8879  0.7553   \n",
       "svm                SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076   \n",
       "nb                         Naive Bayes    0.7906  0.8059  0.7033  0.5746   \n",
       "qda    Quadratic Discriminant Analysis    0.7779  0.9331  0.2648  0.6683   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.9910  0.9879  0.9879     0.614  \n",
       "rf     0.9898  0.9863  0.9863     1.392  \n",
       "dt     0.9838  0.9782  0.9782     0.334  \n",
       "ada    0.9495  0.9316  0.9321     1.869  \n",
       "ridge  0.8385  0.7755  0.7825     0.167  \n",
       "lr     0.8161  0.7458  0.7506     3.739  \n",
       "svm    0.8049  0.7232  0.7407     0.446  \n",
       "nb     0.6321  0.4880  0.4931     0.172  \n",
       "qda    0.3789  0.2732  0.3169     0.582  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>7.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>1.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947   \n",
       "rf            Random Forest Classifier    0.9948  0.9997  0.9926  0.9870   \n",
       "gbc       Gradient Boosting Classifier    0.9927  0.9993  0.9913  0.9802   \n",
       "dt            Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825   \n",
       "ada               Ada Boost Classifier    0.9736  0.9970  0.9702  0.9298   \n",
       "ridge                 Ridge Classifier    0.9086  0.0000  0.9267  0.7657   \n",
       "lr                 Logistic Regression    0.8976  0.9639  0.8879  0.7553   \n",
       "svm                SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076   \n",
       "nb                         Naive Bayes    0.7906  0.8059  0.7033  0.5746   \n",
       "qda    Quadratic Discriminant Analysis    0.7779  0.9331  0.2648  0.6683   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.9910  0.9879  0.9879     0.614  \n",
       "rf     0.9898  0.9863  0.9863     1.392  \n",
       "gbc    0.9857  0.9808  0.9808     7.972  \n",
       "dt     0.9838  0.9782  0.9782     0.334  \n",
       "ada    0.9495  0.9316  0.9321     1.869  \n",
       "ridge  0.8385  0.7755  0.7825     0.167  \n",
       "lr     0.8161  0.7458  0.7506     3.739  \n",
       "svm    0.8049  0.7232  0.7407     0.446  \n",
       "nb     0.6321  0.4880  0.4931     0.172  \n",
       "qda    0.3789  0.2732  0.3169     0.582  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>7.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>1.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>0.9672</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947   \n",
       "rf            Random Forest Classifier    0.9948  0.9997  0.9926  0.9870   \n",
       "gbc       Gradient Boosting Classifier    0.9927  0.9993  0.9913  0.9802   \n",
       "dt            Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825   \n",
       "ada               Ada Boost Classifier    0.9736  0.9970  0.9702  0.9298   \n",
       "ridge                 Ridge Classifier    0.9086  0.0000  0.9267  0.7657   \n",
       "lda       Linear Discriminant Analysis    0.9083  0.9672  0.9261  0.7653   \n",
       "lr                 Logistic Regression    0.8976  0.9639  0.8879  0.7553   \n",
       "svm                SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076   \n",
       "nb                         Naive Bayes    0.7906  0.8059  0.7033  0.5746   \n",
       "qda    Quadratic Discriminant Analysis    0.7779  0.9331  0.2648  0.6683   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.9910  0.9879  0.9879     0.614  \n",
       "rf     0.9898  0.9863  0.9863     1.392  \n",
       "gbc    0.9857  0.9808  0.9808     7.972  \n",
       "dt     0.9838  0.9782  0.9782     0.334  \n",
       "ada    0.9495  0.9316  0.9321     1.869  \n",
       "ridge  0.8385  0.7755  0.7825     0.167  \n",
       "lda    0.8380  0.7749  0.7818     0.742  \n",
       "lr     0.8161  0.7458  0.7506     3.739  \n",
       "svm    0.8049  0.7232  0.7407     0.446  \n",
       "nb     0.6321  0.4880  0.4931     0.172  \n",
       "qda    0.3789  0.2732  0.3169     0.582  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>7.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>1.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>0.9672</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947   \n",
       "rf            Random Forest Classifier    0.9948  0.9997  0.9926  0.9870   \n",
       "et              Extra Trees Classifier    0.9940  0.9998  0.9930  0.9837   \n",
       "gbc       Gradient Boosting Classifier    0.9927  0.9993  0.9913  0.9802   \n",
       "dt            Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825   \n",
       "ada               Ada Boost Classifier    0.9736  0.9970  0.9702  0.9298   \n",
       "ridge                 Ridge Classifier    0.9086  0.0000  0.9267  0.7657   \n",
       "lda       Linear Discriminant Analysis    0.9083  0.9672  0.9261  0.7653   \n",
       "lr                 Logistic Regression    0.8976  0.9639  0.8879  0.7553   \n",
       "svm                SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076   \n",
       "nb                         Naive Bayes    0.7906  0.8059  0.7033  0.5746   \n",
       "qda    Quadratic Discriminant Analysis    0.7779  0.9331  0.2648  0.6683   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.9910  0.9879  0.9879     0.614  \n",
       "rf     0.9898  0.9863  0.9863     1.392  \n",
       "et     0.9883  0.9843  0.9843     0.714  \n",
       "gbc    0.9857  0.9808  0.9808     7.972  \n",
       "dt     0.9838  0.9782  0.9782     0.334  \n",
       "ada    0.9495  0.9316  0.9321     1.869  \n",
       "ridge  0.8385  0.7755  0.7825     0.167  \n",
       "lda    0.8380  0.7749  0.7818     0.742  \n",
       "lr     0.8161  0.7458  0.7506     3.739  \n",
       "svm    0.8049  0.7232  0.7407     0.446  \n",
       "nb     0.6321  0.4880  0.4931     0.172  \n",
       "qda    0.3789  0.2732  0.3169     0.582  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>7.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>1.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>0.9672</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9975  0.9999  0.9954  0.9947   \n",
       "knn                K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947   \n",
       "rf               Random Forest Classifier    0.9948  0.9997  0.9926  0.9870   \n",
       "et                 Extra Trees Classifier    0.9940  0.9998  0.9930  0.9837   \n",
       "gbc          Gradient Boosting Classifier    0.9927  0.9993  0.9913  0.9802   \n",
       "dt               Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825   \n",
       "ada                  Ada Boost Classifier    0.9736  0.9970  0.9702  0.9298   \n",
       "ridge                    Ridge Classifier    0.9086  0.0000  0.9267  0.7657   \n",
       "lda          Linear Discriminant Analysis    0.9083  0.9672  0.9261  0.7653   \n",
       "lr                    Logistic Regression    0.8976  0.9639  0.8879  0.7553   \n",
       "svm                   SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076   \n",
       "nb                            Naive Bayes    0.7906  0.8059  0.7033  0.5746   \n",
       "qda       Quadratic Discriminant Analysis    0.7779  0.9331  0.2648  0.6683   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9950  0.9933  0.9933     0.763  \n",
       "knn       0.9910  0.9879  0.9879     0.614  \n",
       "rf        0.9898  0.9863  0.9863     1.392  \n",
       "et        0.9883  0.9843  0.9843     0.714  \n",
       "gbc       0.9857  0.9808  0.9808     7.972  \n",
       "dt        0.9838  0.9782  0.9782     0.334  \n",
       "ada       0.9495  0.9316  0.9321     1.869  \n",
       "ridge     0.8385  0.7755  0.7825     0.167  \n",
       "lda       0.8380  0.7749  0.7818     0.742  \n",
       "lr        0.8161  0.7458  0.7506     3.739  \n",
       "svm       0.8049  0.7232  0.7407     0.446  \n",
       "nb        0.6321  0.4880  0.4931     0.172  \n",
       "qda       0.3789  0.2732  0.3169     0.582  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>7.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>1.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>0.9672</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.7441</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9975  0.9999  0.9954  0.9947   \n",
       "knn                K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947   \n",
       "rf               Random Forest Classifier    0.9948  0.9997  0.9926  0.9870   \n",
       "et                 Extra Trees Classifier    0.9940  0.9998  0.9930  0.9837   \n",
       "gbc          Gradient Boosting Classifier    0.9927  0.9993  0.9913  0.9802   \n",
       "dt               Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825   \n",
       "ada                  Ada Boost Classifier    0.9736  0.9970  0.9702  0.9298   \n",
       "ridge                    Ridge Classifier    0.9086  0.0000  0.9267  0.7657   \n",
       "lda          Linear Discriminant Analysis    0.9083  0.9672  0.9261  0.7653   \n",
       "lr                    Logistic Regression    0.8976  0.9639  0.8879  0.7553   \n",
       "svm                   SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076   \n",
       "nb                            Naive Bayes    0.7906  0.8059  0.7033  0.5746   \n",
       "qda       Quadratic Discriminant Analysis    0.7779  0.9331  0.2648  0.6683   \n",
       "dummy                    Dummy Classifier    0.7441  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9950  0.9933  0.9933     0.763  \n",
       "knn       0.9910  0.9879  0.9879     0.614  \n",
       "rf        0.9898  0.9863  0.9863     1.392  \n",
       "et        0.9883  0.9843  0.9843     0.714  \n",
       "gbc       0.9857  0.9808  0.9808     7.972  \n",
       "dt        0.9838  0.9782  0.9782     0.334  \n",
       "ada       0.9495  0.9316  0.9321     1.869  \n",
       "ridge     0.8385  0.7755  0.7825     0.167  \n",
       "lda       0.8380  0.7749  0.7818     0.742  \n",
       "lr        0.8161  0.7458  0.7506     3.739  \n",
       "svm       0.8049  0.7232  0.7407     0.446  \n",
       "nb        0.6321  0.4880  0.4931     0.172  \n",
       "qda       0.3789  0.2732  0.3169     0.582  \n",
       "dummy     0.0000  0.0000  0.0000     0.139  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>7.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>1.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>0.9672</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>3.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.8059</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.6321</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.7441</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9975  0.9999  0.9954  0.9947   \n",
       "knn                K Neighbors Classifier    0.9954  0.9962  0.9873  0.9947   \n",
       "rf               Random Forest Classifier    0.9948  0.9997  0.9926  0.9870   \n",
       "et                 Extra Trees Classifier    0.9940  0.9998  0.9930  0.9837   \n",
       "gbc          Gradient Boosting Classifier    0.9927  0.9993  0.9913  0.9802   \n",
       "dt               Decision Tree Classifier    0.9917  0.9895  0.9850  0.9825   \n",
       "ada                  Ada Boost Classifier    0.9736  0.9970  0.9702  0.9298   \n",
       "ridge                    Ridge Classifier    0.9086  0.0000  0.9267  0.7657   \n",
       "lda          Linear Discriminant Analysis    0.9083  0.9672  0.9261  0.7653   \n",
       "lr                    Logistic Regression    0.8976  0.9639  0.8879  0.7553   \n",
       "svm                   SVM - Linear Kernel    0.8818  0.0000  0.9403  0.7076   \n",
       "nb                            Naive Bayes    0.7906  0.8059  0.7033  0.5746   \n",
       "qda       Quadratic Discriminant Analysis    0.7779  0.9331  0.2648  0.6683   \n",
       "dummy                    Dummy Classifier    0.7441  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9950  0.9933  0.9933     0.763  \n",
       "knn       0.9910  0.9879  0.9879     0.614  \n",
       "rf        0.9898  0.9863  0.9863     1.392  \n",
       "et        0.9883  0.9843  0.9843     0.714  \n",
       "gbc       0.9857  0.9808  0.9808     7.972  \n",
       "dt        0.9838  0.9782  0.9782     0.334  \n",
       "ada       0.9495  0.9316  0.9321     1.869  \n",
       "ridge     0.8385  0.7755  0.7825     0.167  \n",
       "lda       0.8380  0.7749  0.7818     0.742  \n",
       "lr        0.8161  0.7458  0.7506     3.739  \n",
       "svm       0.8049  0.7232  0.7407     0.446  \n",
       "nb        0.6321  0.4880  0.4931     0.172  \n",
       "qda       0.3789  0.2732  0.3169     0.582  \n",
       "dummy     0.0000  0.0000  0.0000     0.139  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=2142, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "Participant:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression    0.9061  0.9515  0.8506  0.7877  0.8177  0.7546   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.7559     2.566  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732  0.9791  0.9721   \n",
       "lr      Logistic Regression    0.9061  0.9515  0.8506  0.7877  0.8177  0.7546   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.9722     0.290  \n",
       "lr   0.7559     2.566  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732  0.9791  0.9721   \n",
       "lr      Logistic Regression    0.9061  0.9515  0.8506  0.7877  0.8177  0.7546   \n",
       "nb              Naive Bayes    0.7907  0.7167  0.5142  0.5883  0.5486  0.4132   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.9722     0.290  \n",
       "lr   0.7559     2.566  \n",
       "nb   0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804  0.9806   \n",
       "knn    K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732  0.9791   \n",
       "lr        Logistic Regression    0.9061  0.9515  0.8506  0.7877  0.8177   \n",
       "nb                Naive Bayes    0.7907  0.7167  0.5142  0.5883  0.5486   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.9742  0.9742     0.313  \n",
       "knn  0.9721  0.9722     0.290  \n",
       "lr   0.7546  0.7559     2.566  \n",
       "nb   0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804  0.9806   \n",
       "knn    K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732  0.9791   \n",
       "lr        Logistic Regression    0.9061  0.9515  0.8506  0.7877  0.8177   \n",
       "svm       SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929  0.7926   \n",
       "nb                Naive Bayes    0.7907  0.7167  0.5142  0.5883  0.5486   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.9742  0.9742     0.313  \n",
       "knn  0.9721  0.9722     0.290  \n",
       "lr   0.7546  0.7559     2.566  \n",
       "svm  0.7088  0.7240     0.326  \n",
       "nb   0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt     Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804  0.9806   \n",
       "knn      K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732  0.9791   \n",
       "lr          Logistic Regression    0.9061  0.9515  0.8506  0.7877  0.8177   \n",
       "svm         SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929  0.7926   \n",
       "ridge          Ridge Classifier    0.8111  0.0000  0.8126  0.5869  0.6808   \n",
       "nb                  Naive Bayes    0.7907  0.7167  0.5142  0.5883  0.5486   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "dt     0.9742  0.9742     0.313  \n",
       "knn    0.9721  0.9722     0.290  \n",
       "lr     0.7546  0.7559     2.566  \n",
       "svm    0.7088  0.7240     0.326  \n",
       "ridge  0.5518  0.5673     0.134  \n",
       "nb     0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "rf     Random Forest Classifier    0.9925  0.9997  0.9862  0.9836  0.9849   \n",
       "dt     Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804  0.9806   \n",
       "knn      K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732  0.9791   \n",
       "lr          Logistic Regression    0.9061  0.9515  0.8506  0.7877  0.8177   \n",
       "svm         SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929  0.7926   \n",
       "ridge          Ridge Classifier    0.8111  0.0000  0.8126  0.5869  0.6808   \n",
       "nb                  Naive Bayes    0.7907  0.7167  0.5142  0.5883  0.5486   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "rf     0.9799  0.9799     1.411  \n",
       "dt     0.9742  0.9742     0.313  \n",
       "knn    0.9721  0.9722     0.290  \n",
       "lr     0.7546  0.7559     2.566  \n",
       "svm    0.7088  0.7240     0.326  \n",
       "ridge  0.5518  0.5673     0.134  \n",
       "nb     0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.9925  0.9997  0.9862  0.9836   \n",
       "dt            Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804   \n",
       "knn             K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732   \n",
       "lr                 Logistic Regression    0.9061  0.9515  0.8506  0.7877   \n",
       "svm                SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929   \n",
       "ridge                 Ridge Classifier    0.8111  0.0000  0.8126  0.5869   \n",
       "qda    Quadratic Discriminant Analysis    0.8513  0.9364  0.5525  0.7833   \n",
       "nb                         Naive Bayes    0.7907  0.7167  0.5142  0.5883   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.9849  0.9799  0.9799     1.411  \n",
       "dt     0.9806  0.9742  0.9742     0.313  \n",
       "knn    0.9791  0.9721  0.9722     0.290  \n",
       "lr     0.8177  0.7546  0.7559     2.566  \n",
       "svm    0.7926  0.7088  0.7240     0.326  \n",
       "ridge  0.6808  0.5518  0.5673     0.134  \n",
       "qda    0.6478  0.5571  0.5709     0.435  \n",
       "nb     0.5486  0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>1.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.9925  0.9997  0.9862  0.9836   \n",
       "dt            Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804   \n",
       "knn             K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732   \n",
       "ada               Ada Boost Classifier    0.9792  0.9970  0.9757  0.9426   \n",
       "lr                 Logistic Regression    0.9061  0.9515  0.8506  0.7877   \n",
       "svm                SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929   \n",
       "ridge                 Ridge Classifier    0.8111  0.0000  0.8126  0.5869   \n",
       "qda    Quadratic Discriminant Analysis    0.8513  0.9364  0.5525  0.7833   \n",
       "nb                         Naive Bayes    0.7907  0.7167  0.5142  0.5883   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.9849  0.9799  0.9799     1.411  \n",
       "dt     0.9806  0.9742  0.9742     0.313  \n",
       "knn    0.9791  0.9721  0.9722     0.290  \n",
       "ada    0.9588  0.9450  0.9453     1.675  \n",
       "lr     0.8177  0.7546  0.7559     2.566  \n",
       "svm    0.7926  0.7088  0.7240     0.326  \n",
       "ridge  0.6808  0.5518  0.5673     0.134  \n",
       "qda    0.6478  0.5571  0.5709     0.435  \n",
       "nb     0.5486  0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>7.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>1.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.9925  0.9997  0.9862  0.9836   \n",
       "dt            Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804   \n",
       "gbc       Gradient Boosting Classifier    0.9897  0.9994  0.9888  0.9701   \n",
       "knn             K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732   \n",
       "ada               Ada Boost Classifier    0.9792  0.9970  0.9757  0.9426   \n",
       "lr                 Logistic Regression    0.9061  0.9515  0.8506  0.7877   \n",
       "svm                SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929   \n",
       "ridge                 Ridge Classifier    0.8111  0.0000  0.8126  0.5869   \n",
       "qda    Quadratic Discriminant Analysis    0.8513  0.9364  0.5525  0.7833   \n",
       "nb                         Naive Bayes    0.7907  0.7167  0.5142  0.5883   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.9849  0.9799  0.9799     1.411  \n",
       "dt     0.9806  0.9742  0.9742     0.313  \n",
       "gbc    0.9794  0.9725  0.9726     7.111  \n",
       "knn    0.9791  0.9721  0.9722     0.290  \n",
       "ada    0.9588  0.9450  0.9453     1.675  \n",
       "lr     0.8177  0.7546  0.7559     2.566  \n",
       "svm    0.7926  0.7088  0.7240     0.326  \n",
       "ridge  0.6808  0.5518  0.5673     0.134  \n",
       "qda    0.6478  0.5571  0.5709     0.435  \n",
       "nb     0.5486  0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>7.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>1.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8122</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.9925  0.9997  0.9862  0.9836   \n",
       "dt            Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804   \n",
       "gbc       Gradient Boosting Classifier    0.9897  0.9994  0.9888  0.9701   \n",
       "knn             K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732   \n",
       "ada               Ada Boost Classifier    0.9792  0.9970  0.9757  0.9426   \n",
       "lr                 Logistic Regression    0.9061  0.9515  0.8506  0.7877   \n",
       "svm                SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929   \n",
       "ridge                 Ridge Classifier    0.8111  0.0000  0.8126  0.5869   \n",
       "lda       Linear Discriminant Analysis    0.8108  0.8968  0.8122  0.5864   \n",
       "qda    Quadratic Discriminant Analysis    0.8513  0.9364  0.5525  0.7833   \n",
       "nb                         Naive Bayes    0.7907  0.7167  0.5142  0.5883   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.9849  0.9799  0.9799     1.411  \n",
       "dt     0.9806  0.9742  0.9742     0.313  \n",
       "gbc    0.9794  0.9725  0.9726     7.111  \n",
       "knn    0.9791  0.9721  0.9722     0.290  \n",
       "ada    0.9588  0.9450  0.9453     1.675  \n",
       "lr     0.8177  0.7546  0.7559     2.566  \n",
       "svm    0.7926  0.7088  0.7240     0.326  \n",
       "ridge  0.6808  0.5518  0.5673     0.134  \n",
       "lda    0.6802  0.5511  0.5666     0.533  \n",
       "qda    0.6478  0.5571  0.5709     0.435  \n",
       "nb     0.5486  0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>7.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>1.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8122</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.9925  0.9997  0.9862  0.9836   \n",
       "et              Extra Trees Classifier    0.9916  0.9995  0.9824  0.9837   \n",
       "dt            Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804   \n",
       "gbc       Gradient Boosting Classifier    0.9897  0.9994  0.9888  0.9701   \n",
       "knn             K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732   \n",
       "ada               Ada Boost Classifier    0.9792  0.9970  0.9757  0.9426   \n",
       "lr                 Logistic Regression    0.9061  0.9515  0.8506  0.7877   \n",
       "svm                SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929   \n",
       "ridge                 Ridge Classifier    0.8111  0.0000  0.8126  0.5869   \n",
       "lda       Linear Discriminant Analysis    0.8108  0.8968  0.8122  0.5864   \n",
       "qda    Quadratic Discriminant Analysis    0.8513  0.9364  0.5525  0.7833   \n",
       "nb                         Naive Bayes    0.7907  0.7167  0.5142  0.5883   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.9849  0.9799  0.9799     1.411  \n",
       "et     0.9830  0.9775  0.9775     0.652  \n",
       "dt     0.9806  0.9742  0.9742     0.313  \n",
       "gbc    0.9794  0.9725  0.9726     7.111  \n",
       "knn    0.9791  0.9721  0.9722     0.290  \n",
       "ada    0.9588  0.9450  0.9453     1.675  \n",
       "lr     0.8177  0.7546  0.7559     2.566  \n",
       "svm    0.7926  0.7088  0.7240     0.326  \n",
       "ridge  0.6808  0.5518  0.5673     0.134  \n",
       "lda    0.6802  0.5511  0.5666     0.533  \n",
       "qda    0.6478  0.5571  0.5709     0.435  \n",
       "nb     0.5486  0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>7.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>1.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8122</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9969  0.9999  0.9938  0.9936   \n",
       "rf               Random Forest Classifier    0.9925  0.9997  0.9862  0.9836   \n",
       "et                 Extra Trees Classifier    0.9916  0.9995  0.9824  0.9837   \n",
       "dt               Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804   \n",
       "gbc          Gradient Boosting Classifier    0.9897  0.9994  0.9888  0.9701   \n",
       "knn                K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732   \n",
       "ada                  Ada Boost Classifier    0.9792  0.9970  0.9757  0.9426   \n",
       "lr                    Logistic Regression    0.9061  0.9515  0.8506  0.7877   \n",
       "svm                   SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929   \n",
       "ridge                    Ridge Classifier    0.8111  0.0000  0.8126  0.5869   \n",
       "lda          Linear Discriminant Analysis    0.8108  0.8968  0.8122  0.5864   \n",
       "qda       Quadratic Discriminant Analysis    0.8513  0.9364  0.5525  0.7833   \n",
       "nb                            Naive Bayes    0.7907  0.7167  0.5142  0.5883   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9936  0.9916  0.9916     0.596  \n",
       "rf        0.9849  0.9799  0.9799     1.411  \n",
       "et        0.9830  0.9775  0.9775     0.652  \n",
       "dt        0.9806  0.9742  0.9742     0.313  \n",
       "gbc       0.9794  0.9725  0.9726     7.111  \n",
       "knn       0.9791  0.9721  0.9722     0.290  \n",
       "ada       0.9588  0.9450  0.9453     1.675  \n",
       "lr        0.8177  0.7546  0.7559     2.566  \n",
       "svm       0.7926  0.7088  0.7240     0.326  \n",
       "ridge     0.6808  0.5518  0.5673     0.134  \n",
       "lda       0.6802  0.5511  0.5666     0.533  \n",
       "qda       0.6478  0.5571  0.5709     0.435  \n",
       "nb        0.5486  0.4132  0.4148     0.155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>7.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>1.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8122</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.7525</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9969  0.9999  0.9938  0.9936   \n",
       "rf               Random Forest Classifier    0.9925  0.9997  0.9862  0.9836   \n",
       "et                 Extra Trees Classifier    0.9916  0.9995  0.9824  0.9837   \n",
       "dt               Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804   \n",
       "gbc          Gradient Boosting Classifier    0.9897  0.9994  0.9888  0.9701   \n",
       "knn                K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732   \n",
       "ada                  Ada Boost Classifier    0.9792  0.9970  0.9757  0.9426   \n",
       "lr                    Logistic Regression    0.9061  0.9515  0.8506  0.7877   \n",
       "svm                   SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929   \n",
       "ridge                    Ridge Classifier    0.8111  0.0000  0.8126  0.5869   \n",
       "lda          Linear Discriminant Analysis    0.8108  0.8968  0.8122  0.5864   \n",
       "qda       Quadratic Discriminant Analysis    0.8513  0.9364  0.5525  0.7833   \n",
       "nb                            Naive Bayes    0.7907  0.7167  0.5142  0.5883   \n",
       "dummy                    Dummy Classifier    0.7525  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9936  0.9916  0.9916     0.596  \n",
       "rf        0.9849  0.9799  0.9799     1.411  \n",
       "et        0.9830  0.9775  0.9775     0.652  \n",
       "dt        0.9806  0.9742  0.9742     0.313  \n",
       "gbc       0.9794  0.9725  0.9726     7.111  \n",
       "knn       0.9791  0.9721  0.9722     0.290  \n",
       "ada       0.9588  0.9450  0.9453     1.675  \n",
       "lr        0.8177  0.7546  0.7559     2.566  \n",
       "svm       0.7926  0.7088  0.7240     0.326  \n",
       "ridge     0.6808  0.5518  0.5673     0.134  \n",
       "lda       0.6802  0.5511  0.5666     0.533  \n",
       "qda       0.6478  0.5571  0.5709     0.435  \n",
       "nb        0.5486  0.4132  0.4148     0.155  \n",
       "dummy     0.0000  0.0000  0.0000     0.104  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>7.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>1.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7546</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>2.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8122</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.5511</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.5709</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.5142</td>\n",
       "      <td>0.5883</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.7525</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9969  0.9999  0.9938  0.9936   \n",
       "rf               Random Forest Classifier    0.9925  0.9997  0.9862  0.9836   \n",
       "et                 Extra Trees Classifier    0.9916  0.9995  0.9824  0.9837   \n",
       "dt               Decision Tree Classifier    0.9904  0.9872  0.9808  0.9804   \n",
       "gbc          Gradient Boosting Classifier    0.9897  0.9994  0.9888  0.9701   \n",
       "knn                K Neighbors Classifier    0.9896  0.9942  0.9851  0.9732   \n",
       "ada                  Ada Boost Classifier    0.9792  0.9970  0.9757  0.9426   \n",
       "lr                    Logistic Regression    0.9061  0.9515  0.8506  0.7877   \n",
       "svm                   SVM - Linear Kernel    0.8770  0.0000  0.9281  0.6929   \n",
       "ridge                    Ridge Classifier    0.8111  0.0000  0.8126  0.5869   \n",
       "lda          Linear Discriminant Analysis    0.8108  0.8968  0.8122  0.5864   \n",
       "qda       Quadratic Discriminant Analysis    0.8513  0.9364  0.5525  0.7833   \n",
       "nb                            Naive Bayes    0.7907  0.7167  0.5142  0.5883   \n",
       "dummy                    Dummy Classifier    0.7525  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9936  0.9916  0.9916     0.596  \n",
       "rf        0.9849  0.9799  0.9799     1.411  \n",
       "et        0.9830  0.9775  0.9775     0.652  \n",
       "dt        0.9806  0.9742  0.9742     0.313  \n",
       "gbc       0.9794  0.9725  0.9726     7.111  \n",
       "knn       0.9791  0.9721  0.9722     0.290  \n",
       "ada       0.9588  0.9450  0.9453     1.675  \n",
       "lr        0.8177  0.7546  0.7559     2.566  \n",
       "svm       0.7926  0.7088  0.7240     0.326  \n",
       "ridge     0.6808  0.5518  0.5673     0.134  \n",
       "lda       0.6802  0.5511  0.5666     0.533  \n",
       "qda       0.6478  0.5571  0.5709     0.435  \n",
       "nb        0.5486  0.4132  0.4148     0.155  \n",
       "dummy     0.0000  0.0000  0.0000     0.104  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=6528, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "Participant:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression    0.8013  0.9252  0.8165  0.5864  0.6825  0.5435   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.5593     2.949  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944  0.9880  0.9838   \n",
       "lr      Logistic Regression    0.8013  0.9252  0.8165  0.5864  0.6825  0.5435   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.9839     0.320  \n",
       "lr   0.5593     2.949  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944  0.9880  0.9838   \n",
       "lr      Logistic Regression    0.8013  0.9252  0.8165  0.5864  0.6825  0.5435   \n",
       "nb              Naive Bayes    0.5407  0.7512  0.7892  0.3383  0.4735  0.1691   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.9839     0.320  \n",
       "lr   0.5593     2.949  \n",
       "nb   0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943  0.9948   \n",
       "knn    K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944  0.9880   \n",
       "lr        Logistic Regression    0.8013  0.9252  0.8165  0.5864  0.6825   \n",
       "nb                Naive Bayes    0.5407  0.7512  0.7892  0.3383  0.4735   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.9930  0.9930     0.325  \n",
       "knn  0.9838  0.9839     0.320  \n",
       "lr   0.5435  0.5593     2.949  \n",
       "nb   0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943  0.9948   \n",
       "knn    K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944  0.9880   \n",
       "lr        Logistic Regression    0.8013  0.9252  0.8165  0.5864  0.6825   \n",
       "svm       SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348  0.6410   \n",
       "nb                Naive Bayes    0.5407  0.7512  0.7892  0.3383  0.4735   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.9930  0.9930     0.325  \n",
       "knn  0.9838  0.9839     0.320  \n",
       "lr   0.5435  0.5593     2.949  \n",
       "svm  0.4760  0.4977     0.523  \n",
       "nb   0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt     Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943  0.9948   \n",
       "knn      K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944  0.9880   \n",
       "ridge          Ridge Classifier    0.8524  0.0000  0.8132  0.6830  0.7424   \n",
       "lr          Logistic Regression    0.8013  0.9252  0.8165  0.5864  0.6825   \n",
       "svm         SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348  0.6410   \n",
       "nb                  Naive Bayes    0.5407  0.7512  0.7892  0.3383  0.4735   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "dt     0.9930  0.9930     0.325  \n",
       "knn    0.9838  0.9839     0.320  \n",
       "ridge  0.6400  0.6448     0.163  \n",
       "lr     0.5435  0.5593     2.949  \n",
       "svm    0.4760  0.4977     0.523  \n",
       "nb     0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt     Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943  0.9948   \n",
       "rf     Random Forest Classifier    0.9960  0.9999  0.9965  0.9884  0.9924   \n",
       "knn      K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944  0.9880   \n",
       "ridge          Ridge Classifier    0.8524  0.0000  0.8132  0.6830  0.7424   \n",
       "lr          Logistic Regression    0.8013  0.9252  0.8165  0.5864  0.6825   \n",
       "svm         SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348  0.6410   \n",
       "nb                  Naive Bayes    0.5407  0.7512  0.7892  0.3383  0.4735   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "dt     0.9930  0.9930     0.325  \n",
       "rf     0.9897  0.9897     1.433  \n",
       "knn    0.9838  0.9839     0.320  \n",
       "ridge  0.6400  0.6448     0.163  \n",
       "lr     0.5435  0.5593     2.949  \n",
       "svm    0.4760  0.4977     0.523  \n",
       "nb     0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943   \n",
       "rf            Random Forest Classifier    0.9960  0.9999  0.9965  0.9884   \n",
       "knn             K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944   \n",
       "ridge                 Ridge Classifier    0.8524  0.0000  0.8132  0.6830   \n",
       "lr                 Logistic Regression    0.8013  0.9252  0.8165  0.5864   \n",
       "svm                SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348   \n",
       "qda    Quadratic Discriminant Analysis    0.8385  0.9436  0.5506  0.7667   \n",
       "nb                         Naive Bayes    0.5407  0.7512  0.7892  0.3383   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.9948  0.9930  0.9930     0.325  \n",
       "rf     0.9924  0.9897  0.9897     1.433  \n",
       "knn    0.9880  0.9838  0.9839     0.320  \n",
       "ridge  0.7424  0.6400  0.6448     0.163  \n",
       "lr     0.6825  0.5435  0.5593     2.949  \n",
       "svm    0.6410  0.4760  0.4977     0.523  \n",
       "qda    0.6409  0.5403  0.5527     0.410  \n",
       "nb     0.4735  0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943   \n",
       "rf            Random Forest Classifier    0.9960  0.9999  0.9965  0.9884   \n",
       "knn             K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944   \n",
       "ada               Ada Boost Classifier    0.9907  0.9992  0.9845  0.9801   \n",
       "ridge                 Ridge Classifier    0.8524  0.0000  0.8132  0.6830   \n",
       "lr                 Logistic Regression    0.8013  0.9252  0.8165  0.5864   \n",
       "svm                SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348   \n",
       "qda    Quadratic Discriminant Analysis    0.8385  0.9436  0.5506  0.7667   \n",
       "nb                         Naive Bayes    0.5407  0.7512  0.7892  0.3383   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.9948  0.9930  0.9930     0.325  \n",
       "rf     0.9924  0.9897  0.9897     1.433  \n",
       "knn    0.9880  0.9838  0.9839     0.320  \n",
       "ada    0.9823  0.9760  0.9760     1.964  \n",
       "ridge  0.7424  0.6400  0.6448     0.163  \n",
       "lr     0.6825  0.5435  0.5593     2.949  \n",
       "svm    0.6410  0.4760  0.4977     0.523  \n",
       "qda    0.6409  0.5403  0.5527     0.410  \n",
       "nb     0.4735  0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>8.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "gbc       Gradient Boosting Classifier    0.9991  1.0000  0.9988  0.9977   \n",
       "dt            Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943   \n",
       "rf            Random Forest Classifier    0.9960  0.9999  0.9965  0.9884   \n",
       "knn             K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944   \n",
       "ada               Ada Boost Classifier    0.9907  0.9992  0.9845  0.9801   \n",
       "ridge                 Ridge Classifier    0.8524  0.0000  0.8132  0.6830   \n",
       "lr                 Logistic Regression    0.8013  0.9252  0.8165  0.5864   \n",
       "svm                SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348   \n",
       "qda    Quadratic Discriminant Analysis    0.8385  0.9436  0.5506  0.7667   \n",
       "nb                         Naive Bayes    0.5407  0.7512  0.7892  0.3383   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "gbc    0.9982  0.9976  0.9976     8.808  \n",
       "dt     0.9948  0.9930  0.9930     0.325  \n",
       "rf     0.9924  0.9897  0.9897     1.433  \n",
       "knn    0.9880  0.9838  0.9839     0.320  \n",
       "ada    0.9823  0.9760  0.9760     1.964  \n",
       "ridge  0.7424  0.6400  0.6448     0.163  \n",
       "lr     0.6825  0.5435  0.5593     2.949  \n",
       "svm    0.6410  0.4760  0.4977     0.523  \n",
       "qda    0.6409  0.5403  0.5527     0.410  \n",
       "nb     0.4735  0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>8.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.6827</td>\n",
       "      <td>0.7418</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "gbc       Gradient Boosting Classifier    0.9991  1.0000  0.9988  0.9977   \n",
       "dt            Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943   \n",
       "rf            Random Forest Classifier    0.9960  0.9999  0.9965  0.9884   \n",
       "knn             K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944   \n",
       "ada               Ada Boost Classifier    0.9907  0.9992  0.9845  0.9801   \n",
       "ridge                 Ridge Classifier    0.8524  0.0000  0.8132  0.6830   \n",
       "lda       Linear Discriminant Analysis    0.8521  0.9306  0.8121  0.6827   \n",
       "lr                 Logistic Regression    0.8013  0.9252  0.8165  0.5864   \n",
       "svm                SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348   \n",
       "qda    Quadratic Discriminant Analysis    0.8385  0.9436  0.5506  0.7667   \n",
       "nb                         Naive Bayes    0.5407  0.7512  0.7892  0.3383   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "gbc    0.9982  0.9976  0.9976     8.808  \n",
       "dt     0.9948  0.9930  0.9930     0.325  \n",
       "rf     0.9924  0.9897  0.9897     1.433  \n",
       "knn    0.9880  0.9838  0.9839     0.320  \n",
       "ada    0.9823  0.9760  0.9760     1.964  \n",
       "ridge  0.7424  0.6400  0.6448     0.163  \n",
       "lda    0.7418  0.6393  0.6440     0.580  \n",
       "lr     0.6825  0.5435  0.5593     2.949  \n",
       "svm    0.6410  0.4760  0.4977     0.523  \n",
       "qda    0.6409  0.5403  0.5527     0.410  \n",
       "nb     0.4735  0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>8.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.6827</td>\n",
       "      <td>0.7418</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "gbc       Gradient Boosting Classifier    0.9991  1.0000  0.9988  0.9977   \n",
       "dt            Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943   \n",
       "rf            Random Forest Classifier    0.9960  0.9999  0.9965  0.9884   \n",
       "knn             K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944   \n",
       "et              Extra Trees Classifier    0.9921  0.9997  0.9905  0.9796   \n",
       "ada               Ada Boost Classifier    0.9907  0.9992  0.9845  0.9801   \n",
       "ridge                 Ridge Classifier    0.8524  0.0000  0.8132  0.6830   \n",
       "lda       Linear Discriminant Analysis    0.8521  0.9306  0.8121  0.6827   \n",
       "lr                 Logistic Regression    0.8013  0.9252  0.8165  0.5864   \n",
       "svm                SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348   \n",
       "qda    Quadratic Discriminant Analysis    0.8385  0.9436  0.5506  0.7667   \n",
       "nb                         Naive Bayes    0.5407  0.7512  0.7892  0.3383   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "gbc    0.9982  0.9976  0.9976     8.808  \n",
       "dt     0.9948  0.9930  0.9930     0.325  \n",
       "rf     0.9924  0.9897  0.9897     1.433  \n",
       "knn    0.9880  0.9838  0.9839     0.320  \n",
       "et     0.9850  0.9797  0.9797     0.762  \n",
       "ada    0.9823  0.9760  0.9760     1.964  \n",
       "ridge  0.7424  0.6400  0.6448     0.163  \n",
       "lda    0.7418  0.6393  0.6440     0.580  \n",
       "lr     0.6825  0.5435  0.5593     2.949  \n",
       "svm    0.6410  0.4760  0.4977     0.523  \n",
       "qda    0.6409  0.5403  0.5527     0.410  \n",
       "nb     0.4735  0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>8.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.6827</td>\n",
       "      <td>0.7418</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9997  1.0000  0.9998  0.9992   \n",
       "gbc          Gradient Boosting Classifier    0.9991  1.0000  0.9988  0.9977   \n",
       "dt               Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943   \n",
       "rf               Random Forest Classifier    0.9960  0.9999  0.9965  0.9884   \n",
       "knn                K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944   \n",
       "et                 Extra Trees Classifier    0.9921  0.9997  0.9905  0.9796   \n",
       "ada                  Ada Boost Classifier    0.9907  0.9992  0.9845  0.9801   \n",
       "ridge                    Ridge Classifier    0.8524  0.0000  0.8132  0.6830   \n",
       "lda          Linear Discriminant Analysis    0.8521  0.9306  0.8121  0.6827   \n",
       "lr                    Logistic Regression    0.8013  0.9252  0.8165  0.5864   \n",
       "svm                   SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348   \n",
       "qda       Quadratic Discriminant Analysis    0.8385  0.9436  0.5506  0.7667   \n",
       "nb                            Naive Bayes    0.5407  0.7512  0.7892  0.3383   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9995  0.9993  0.9993     0.723  \n",
       "gbc       0.9982  0.9976  0.9976     8.808  \n",
       "dt        0.9948  0.9930  0.9930     0.325  \n",
       "rf        0.9924  0.9897  0.9897     1.433  \n",
       "knn       0.9880  0.9838  0.9839     0.320  \n",
       "et        0.9850  0.9797  0.9797     0.762  \n",
       "ada       0.9823  0.9760  0.9760     1.964  \n",
       "ridge     0.7424  0.6400  0.6448     0.163  \n",
       "lda       0.7418  0.6393  0.6440     0.580  \n",
       "lr        0.6825  0.5435  0.5593     2.949  \n",
       "svm       0.6410  0.4760  0.4977     0.523  \n",
       "qda       0.6409  0.5403  0.5527     0.410  \n",
       "nb        0.4735  0.1691  0.2182     0.165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>8.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.6827</td>\n",
       "      <td>0.7418</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.7383</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9997  1.0000  0.9998  0.9992   \n",
       "gbc          Gradient Boosting Classifier    0.9991  1.0000  0.9988  0.9977   \n",
       "dt               Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943   \n",
       "rf               Random Forest Classifier    0.9960  0.9999  0.9965  0.9884   \n",
       "knn                K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944   \n",
       "et                 Extra Trees Classifier    0.9921  0.9997  0.9905  0.9796   \n",
       "ada                  Ada Boost Classifier    0.9907  0.9992  0.9845  0.9801   \n",
       "ridge                    Ridge Classifier    0.8524  0.0000  0.8132  0.6830   \n",
       "lda          Linear Discriminant Analysis    0.8521  0.9306  0.8121  0.6827   \n",
       "lr                    Logistic Regression    0.8013  0.9252  0.8165  0.5864   \n",
       "svm                   SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348   \n",
       "qda       Quadratic Discriminant Analysis    0.8385  0.9436  0.5506  0.7667   \n",
       "nb                            Naive Bayes    0.5407  0.7512  0.7892  0.3383   \n",
       "dummy                    Dummy Classifier    0.7383  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9995  0.9993  0.9993     0.723  \n",
       "gbc       0.9982  0.9976  0.9976     8.808  \n",
       "dt        0.9948  0.9930  0.9930     0.325  \n",
       "rf        0.9924  0.9897  0.9897     1.433  \n",
       "knn       0.9880  0.9838  0.9839     0.320  \n",
       "et        0.9850  0.9797  0.9797     0.762  \n",
       "ada       0.9823  0.9760  0.9760     1.964  \n",
       "ridge     0.7424  0.6400  0.6448     0.163  \n",
       "lda       0.7418  0.6393  0.6440     0.580  \n",
       "lr        0.6825  0.5435  0.5593     2.949  \n",
       "svm       0.6410  0.4760  0.4977     0.523  \n",
       "qda       0.6409  0.5403  0.5527     0.410  \n",
       "nb        0.4735  0.1691  0.2182     0.165  \n",
       "dummy     0.0000  0.0000  0.0000     0.119  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>8.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.6827</td>\n",
       "      <td>0.7418</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.5864</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>2.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4977</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.7383</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9997  1.0000  0.9998  0.9992   \n",
       "gbc          Gradient Boosting Classifier    0.9991  1.0000  0.9988  0.9977   \n",
       "dt               Decision Tree Classifier    0.9973  0.9967  0.9955  0.9943   \n",
       "rf               Random Forest Classifier    0.9960  0.9999  0.9965  0.9884   \n",
       "knn                K Neighbors Classifier    0.9938  0.9946  0.9818  0.9944   \n",
       "et                 Extra Trees Classifier    0.9921  0.9997  0.9905  0.9796   \n",
       "ada                  Ada Boost Classifier    0.9907  0.9992  0.9845  0.9801   \n",
       "ridge                    Ridge Classifier    0.8524  0.0000  0.8132  0.6830   \n",
       "lda          Linear Discriminant Analysis    0.8521  0.9306  0.8121  0.6827   \n",
       "lr                    Logistic Regression    0.8013  0.9252  0.8165  0.5864   \n",
       "svm                   SVM - Linear Kernel    0.7639  0.0000  0.8014  0.5348   \n",
       "qda       Quadratic Discriminant Analysis    0.8385  0.9436  0.5506  0.7667   \n",
       "nb                            Naive Bayes    0.5407  0.7512  0.7892  0.3383   \n",
       "dummy                    Dummy Classifier    0.7383  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.9995  0.9993  0.9993     0.723  \n",
       "gbc       0.9982  0.9976  0.9976     8.808  \n",
       "dt        0.9948  0.9930  0.9930     0.325  \n",
       "rf        0.9924  0.9897  0.9897     1.433  \n",
       "knn       0.9880  0.9838  0.9839     0.320  \n",
       "et        0.9850  0.9797  0.9797     0.762  \n",
       "ada       0.9823  0.9760  0.9760     1.964  \n",
       "ridge     0.7424  0.6400  0.6448     0.163  \n",
       "lda       0.7418  0.6393  0.6440     0.580  \n",
       "lr        0.6825  0.5435  0.5593     2.949  \n",
       "svm       0.6410  0.4760  0.4977     0.523  \n",
       "qda       0.6409  0.5403  0.5527     0.410  \n",
       "nb        0.4735  0.1691  0.2182     0.165  \n",
       "dummy     0.0000  0.0000  0.0000     0.119  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=760, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1scores = []\n",
    "\n",
    "for participant in unique_participants:\n",
    "    print(\"Participant: \",participant)\n",
    "    part_df = wesad_group.get_group(participant)\n",
    "    grid = setup(data=part_df, target='stress', fix_imbalance = True, html=False, silent=True, verbose=False) #fix_imbalance = True,\n",
    "    best = compare_models(sort=\"F1\")\n",
    "    accuracies.append(pull()['Accuracy'][0])\n",
    "    precision.append(pull()['Prec.'][0])\n",
    "    recall.append(pull()['Recall'][0])\n",
    "    f1scores.append(pull()['F1'][0])\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f49de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = statistics.mean(accuracies)\n",
    "mean_prec = statistics.mean(precision)\n",
    "mean_rec = statistics.mean(recall)\n",
    "mean_f1 = statistics.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9542dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy WESAD - Cluster All Features:  0.9980333333333333\n",
      "Mean Precision WESAD- Cluster All Features:  0.9958333333333333\n",
      "Mean Recall WESAD- Cluster All Features:  0.9963333333333333\n",
      "Mean F1-score WESAD- Cluster All Features:  0.9960333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy WESAD - Cluster All Features: \", mean_acc)\n",
    "print(\"Mean Precision WESAD- Cluster All Features: \", mean_prec)\n",
    "print(\"Mean Recall WESAD- Cluster All Features: \", mean_rec)\n",
    "print(\"Mean F1-score WESAD- Cluster All Features: \", mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2f828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e6bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpyth8",
   "language": "python",
   "name": "env_python8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
