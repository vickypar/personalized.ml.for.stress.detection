{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2048b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc8cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from os import listdir\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3bf008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(data, participants): \n",
    "    \n",
    "    cost = []\n",
    "    silhouette = []\n",
    "\n",
    "    for i in range(2, participants):\n",
    "        kmeans = KMeans(n_clusters = i, max_iter = 500, random_state = 0)\n",
    "        kmeans.fit_predict(data)\n",
    "        \n",
    "        # Calculate Silhoutte Score\n",
    "        score = silhouette_score(data, kmeans.labels_, metric='euclidean')\n",
    "        silhouette.append(score)\n",
    "    \n",
    "        # Calculates squared error for the clustered points\n",
    "        cost.append(kmeans.inertia_)    \n",
    "        \n",
    "    # Plot the cost against K values\n",
    "    plt.plot(range(2, participants), cost, color ='g', linewidth ='3')\n",
    "    plt.xlabel(\"Value of K\")\n",
    "    plt.ylabel(\"Squared Error (Cost)\")\n",
    "    plt.show() # clear the plot\n",
    "    \n",
    "    # Plot the Silhouette Score against K values\n",
    "    plt.plot(range(2, participants), silhouette, color ='b', linewidth ='3')\n",
    "    plt.xlabel(\"Value of K\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.show() # clear the plot\n",
    "    # the point of the elbow is the\n",
    "    # most optimal value for choosing k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28f4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette(data, clusters):\n",
    "    kmeans = KMeans(n_clusters = clusters, max_iter = 500, random_state = 0)\n",
    "    kmeans.fit_predict(data)\n",
    "    \n",
    "    #Create SilhouetteVisualizer instance with KMeans instance\n",
    "    visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')#ax[q-1][mod])#, ax = ax[x%3][y%2]\n",
    "    \n",
    "    #Fit the visualizer\n",
    "    visualizer.fit(data)\n",
    "    #fig, ax = plt.subplots(3,2, figsize = (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af3d32",
   "metadata": {},
   "source": [
    "# SWELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04bd718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "swell = pd.read_csv(\"Final_CSVs/swell_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f033646",
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_extra = pd.read_excel('Personality_Files/swell_person.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4922fe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>RMSSD</th>\n",
       "      <th>SCL</th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>stress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>0.093757</td>\n",
       "      <td>119.071484</td>\n",
       "      <td>PP4</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>138.735573</td>\n",
       "      <td>PP19</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>PP22</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>120.251942</td>\n",
       "      <td>PP3</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>0.064568</td>\n",
       "      <td>561.332213</td>\n",
       "      <td>PP21</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>158.138912</td>\n",
       "      <td>PP24</td>\n",
       "      <td>Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>PP22</td>\n",
       "      <td>Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>93.893556</td>\n",
       "      <td>PP4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>PP23</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>77</td>\n",
       "      <td>0.025147</td>\n",
       "      <td>495.018099</td>\n",
       "      <td>PP12</td>\n",
       "      <td>Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3140 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HR       RMSSD         SCL    id dataset  stress\n",
       "0      58    0.093757  119.071484   PP4   Train       0\n",
       "1     999  999.000000  138.735573  PP19   Train       0\n",
       "2     999  999.000000  999.000000  PP22   Train       1\n",
       "3     999  999.000000  120.251942   PP3   Train       1\n",
       "4      70    0.064568  561.332213  PP21   Train       0\n",
       "...   ...         ...         ...   ...     ...     ...\n",
       "3135  999  999.000000  158.138912  PP24    Test       1\n",
       "3136  999  999.000000  999.000000  PP22    Test       1\n",
       "3137  999  999.000000   93.893556   PP4    Test       0\n",
       "3138  999  999.000000  999.000000  PP23    Test       0\n",
       "3139   77    0.025147  495.018099  PP12    Test       1\n",
       "\n",
       "[3140 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b85f86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PP</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Dominant hand</th>\n",
       "      <th>Glasses</th>\n",
       "      <th>smoke</th>\n",
       "      <th>coffee</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>physical</th>\n",
       "      <th>stress</th>\n",
       "      <th>heart disease</th>\n",
       "      <th>medicine</th>\n",
       "      <th>Internal control index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP1</td>\n",
       "      <td>27</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP2</td>\n",
       "      <td>25</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP3</td>\n",
       "      <td>24</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP4</td>\n",
       "      <td>24</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP5</td>\n",
       "      <td>24</td>\n",
       "      <td>f</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PP6</td>\n",
       "      <td>24</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PP7</td>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PP8</td>\n",
       "      <td>27</td>\n",
       "      <td>m</td>\n",
       "      <td>MSc Electrical Engineering</td>\n",
       "      <td>right</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PP9</td>\n",
       "      <td>28</td>\n",
       "      <td>m</td>\n",
       "      <td>PhD informatics</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PP10</td>\n",
       "      <td>25</td>\n",
       "      <td>m</td>\n",
       "      <td>Information Science</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PP11</td>\n",
       "      <td>21</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PP12</td>\n",
       "      <td>24</td>\n",
       "      <td>f</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PP13</td>\n",
       "      <td>26</td>\n",
       "      <td>m</td>\n",
       "      <td>student (Phd?)</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PP14</td>\n",
       "      <td>26</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>left</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PP15</td>\n",
       "      <td>24</td>\n",
       "      <td>m</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PP16</td>\n",
       "      <td>27</td>\n",
       "      <td>f</td>\n",
       "      <td>student</td>\n",
       "      <td>right</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PP17</td>\n",
       "      <td>38</td>\n",
       "      <td>f</td>\n",
       "      <td>PhD candidate</td>\n",
       "      <td>right</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PP18</td>\n",
       "      <td>24</td>\n",
       "      <td>m</td>\n",
       "      <td>Master MKE</td>\n",
       "      <td>left</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PP19</td>\n",
       "      <td>23</td>\n",
       "      <td>f</td>\n",
       "      <td>Icelandic</td>\n",
       "      <td>right</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PP20</td>\n",
       "      <td>21</td>\n",
       "      <td>f</td>\n",
       "      <td>Technische bestuurskunde</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PP21</td>\n",
       "      <td>22</td>\n",
       "      <td>f</td>\n",
       "      <td>Photography</td>\n",
       "      <td>right</td>\n",
       "      <td>yes</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PP22</td>\n",
       "      <td>26</td>\n",
       "      <td>m</td>\n",
       "      <td>Physics</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PP23</td>\n",
       "      <td>25</td>\n",
       "      <td>f</td>\n",
       "      <td>Computer Engineering</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PP24</td>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>MSc Electrical Engineering</td>\n",
       "      <td>right</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PP25</td>\n",
       "      <td>26</td>\n",
       "      <td>m</td>\n",
       "      <td>MSc Technische Informatica</td>\n",
       "      <td>right</td>\n",
       "      <td>yes</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PP  Age Gender                  Occupation Dominant hand Glasses  smoke  \\\n",
       "0    PP1   27      m                     student         right      no      6   \n",
       "1    PP2   25      m                     student         right      no      6   \n",
       "2    PP3   24      m                     student         right      no      6   \n",
       "3    PP4   24      m                     student         right      no      6   \n",
       "4    PP5   24      f                     student         right      no      6   \n",
       "5    PP6   24      m                     student         right      no      6   \n",
       "6    PP7   22      m                     student         right      no      6   \n",
       "7    PP8   27      m  MSc Electrical Engineering         right     yes      6   \n",
       "8    PP9   28      m             PhD informatics         right      no      6   \n",
       "9   PP10   25      m         Information Science         right      no      6   \n",
       "10  PP11   21      m                     student         right     yes      6   \n",
       "11  PP12   24      f                     student         right      no      6   \n",
       "12  PP13   26      m              student (Phd?)         right      no      6   \n",
       "13  PP14   26      m                     student          left     yes      6   \n",
       "14  PP15   24      m                     student         right      no      6   \n",
       "15  PP16   27      f                     student         right     yes      6   \n",
       "16  PP17   38      f               PhD candidate         right     yes      6   \n",
       "17  PP18   24      m                  Master MKE          left      no      6   \n",
       "18  PP19   23      f                   Icelandic         right     yes      6   \n",
       "19  PP20   21      f    Technische bestuurskunde         right      no      6   \n",
       "20  PP21   22      f                 Photography         right     yes      4   \n",
       "21  PP22   26      m                     Physics         right      no      6   \n",
       "22  PP23   25      f        Computer Engineering         right      no      6   \n",
       "23  PP24   22      m  MSc Electrical Engineering         right      no      6   \n",
       "24  PP25   26      m  MSc Technische Informatica         right     yes      6   \n",
       "\n",
       "    coffee  alcohol  physical  stress heart disease medicine  \\\n",
       "0        6        6         6       6            no       no   \n",
       "1        6        6         4       5            no       no   \n",
       "2        6        6         6       6            no       no   \n",
       "3        6        6         2       6            no       no   \n",
       "4        6        6         6       6            no       no   \n",
       "5        6        6         6       6            no       no   \n",
       "6        6        6         6       6            no       no   \n",
       "7        6        6         3       6            no       no   \n",
       "8        6        6         2       6            no       no   \n",
       "9        6        6         6       6            no       no   \n",
       "10       6        6         6       6            no       no   \n",
       "11       6        6         6       6            no       no   \n",
       "12       6        6         6       6            no       no   \n",
       "13       6        6         2       6            no       no   \n",
       "14       6        6         1       6            no       no   \n",
       "15       6        6         2       1            no       no   \n",
       "16       6        6         6       4            no       no   \n",
       "17       6        6         6       6            no       no   \n",
       "18       6        6         6       6            no       no   \n",
       "19       6        6         6       6            no       no   \n",
       "20       5        5         5       4            no       no   \n",
       "21       6        6         6       6            no       no   \n",
       "22       6        6         2       4            no       no   \n",
       "23       6        6         2       2            no       no   \n",
       "24       6        6         2       6            no       no   \n",
       "\n",
       "    Internal control index  \n",
       "0                     2.93  \n",
       "1                     4.25  \n",
       "2                     3.61  \n",
       "3                     3.61  \n",
       "4                     3.71  \n",
       "5                     3.86  \n",
       "6                     3.64  \n",
       "7                     3.57  \n",
       "8                     3.25  \n",
       "9                     3.39  \n",
       "10                    3.71  \n",
       "11                    3.89  \n",
       "12                    4.21  \n",
       "13                    3.50  \n",
       "14                    4.25  \n",
       "15                    3.61  \n",
       "16                    3.46  \n",
       "17                    3.61  \n",
       "18                    3.61  \n",
       "19                    3.57  \n",
       "20                    3.39  \n",
       "21                    3.61  \n",
       "22                    3.89  \n",
       "23                    3.82  \n",
       "24                    3.79  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swell_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15833e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary features\n",
    "\n",
    "swell_extra.drop(['heart disease'], axis=1, inplace=True)\n",
    "swell_extra.drop(['medicine'], axis=1, inplace=True)\n",
    "swell_extra.drop(['Glasses'], axis=1, inplace=True)\n",
    "swell_extra.drop(['smoke'], axis=1, inplace=True)\n",
    "swell_extra.drop(['alcohol'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45fadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all Master students in one category\n",
    "\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"MSc Electrical Engineering\",\"MSc\", swell_extra['Occupation'])\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"Master MKE\", \"MSc\", swell_extra['Occupation'])\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"MSc Technische Informatica\", \"MSc\", swell_extra['Occupation'])\n",
    "\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"Computer Engineering\", \"student\", swell_extra['Occupation'])\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"Physics\", \"student\", swell_extra['Occupation'])\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"Information Science\", \"student\", swell_extra['Occupation'])\n",
    "\n",
    "\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"Icelandic\", \"other\", swell_extra['Occupation'])\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"Photography\", \"other\", swell_extra['Occupation'])\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"Technische bestuurskunde\", \"other\", swell_extra['Occupation'])\n",
    "\n",
    "# Group all PhD students in one category\n",
    "\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"PhD informatics\", \"PhD\", swell_extra['Occupation'])\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"student (Phd?)\", \"PhD\", swell_extra['Occupation'])\n",
    "swell_extra['Occupation'] = np.where(swell_extra['Occupation']==\"PhD candidate\", \"PhD\", swell_extra['Occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c7e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding categorical features\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "swell_extra['Occupation'] = le.fit_transform(swell_extra['Occupation'])\n",
    "swell_extra['Dominant hand'] = le.fit_transform(swell_extra['Dominant hand'])\n",
    "swell_extra['Gender'] = le.fit_transform(swell_extra['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26fcc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PP</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Dominant hand</th>\n",
       "      <th>coffee</th>\n",
       "      <th>physical</th>\n",
       "      <th>stress</th>\n",
       "      <th>Internal control index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP1</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP5</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PP6</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PP7</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PP8</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PP9</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PP10</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PP11</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PP12</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PP13</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PP14</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PP15</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PP16</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PP17</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PP18</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PP19</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PP20</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PP21</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PP22</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PP23</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PP24</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PP25</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PP  Age  Gender  Occupation  Dominant hand  coffee  physical  stress  \\\n",
       "0    PP1   27       1           3              1       6         6       6   \n",
       "1    PP2   25       1           3              1       6         4       5   \n",
       "2    PP3   24       1           3              1       6         6       6   \n",
       "3    PP4   24       1           3              1       6         2       6   \n",
       "4    PP5   24       0           3              1       6         6       6   \n",
       "5    PP6   24       1           3              1       6         6       6   \n",
       "6    PP7   22       1           3              1       6         6       6   \n",
       "7    PP8   27       1           0              1       6         3       6   \n",
       "8    PP9   28       1           1              1       6         2       6   \n",
       "9   PP10   25       1           3              1       6         6       6   \n",
       "10  PP11   21       1           3              1       6         6       6   \n",
       "11  PP12   24       0           3              1       6         6       6   \n",
       "12  PP13   26       1           1              1       6         6       6   \n",
       "13  PP14   26       1           3              0       6         2       6   \n",
       "14  PP15   24       1           3              1       6         1       6   \n",
       "15  PP16   27       0           3              1       6         2       1   \n",
       "16  PP17   38       0           1              1       6         6       4   \n",
       "17  PP18   24       1           0              0       6         6       6   \n",
       "18  PP19   23       0           2              1       6         6       6   \n",
       "19  PP20   21       0           2              1       6         6       6   \n",
       "20  PP21   22       0           2              1       5         5       4   \n",
       "21  PP22   26       1           3              1       6         6       6   \n",
       "22  PP23   25       0           3              1       6         2       4   \n",
       "23  PP24   22       1           0              1       6         2       2   \n",
       "24  PP25   26       1           0              1       6         2       6   \n",
       "\n",
       "    Internal control index  \n",
       "0                     2.93  \n",
       "1                     4.25  \n",
       "2                     3.61  \n",
       "3                     3.61  \n",
       "4                     3.71  \n",
       "5                     3.86  \n",
       "6                     3.64  \n",
       "7                     3.57  \n",
       "8                     3.25  \n",
       "9                     3.39  \n",
       "10                    3.71  \n",
       "11                    3.89  \n",
       "12                    4.21  \n",
       "13                    3.50  \n",
       "14                    4.25  \n",
       "15                    3.61  \n",
       "16                    3.46  \n",
       "17                    3.61  \n",
       "18                    3.61  \n",
       "19                    3.57  \n",
       "20                    3.39  \n",
       "21                    3.61  \n",
       "22                    3.89  \n",
       "23                    3.82  \n",
       "24                    3.79  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swell_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfee8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "#swell_extra[['Age', 'Occupation', 'smoke', 'physical', 'stress', 'Internal control index']] = scaler.fit_transform(swell_extra[['Age', 'Occupation', 'smoke',  'physical', 'stress', 'Internal control index']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d740ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "swell_extra[[\"Age\", \"Gender\", \"Occupation\", \"Dominant hand\", \"coffee\", \"physical\", \"stress\", \"Internal control index\"]] = scaler.fit_transform(swell_extra[[\"Age\", \"Gender\", \"Occupation\", \"Dominant hand\", \"coffee\", \"physical\", \"stress\", \"Internal control index\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c3e57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Dominant hand</th>\n",
       "      <th>coffee</th>\n",
       "      <th>physical</th>\n",
       "      <th>stress</th>\n",
       "      <th>Internal control index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP1</td>\n",
       "      <td>0.615457</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-2.531632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-0.231695</td>\n",
       "      <td>-0.272103</td>\n",
       "      <td>1.986695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP3</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP4</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP5</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.138288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PP6</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.651734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PP7</td>\n",
       "      <td>-0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.101320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PP8</td>\n",
       "      <td>0.615457</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-0.758273</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.340928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PP9</td>\n",
       "      <td>0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-1.436280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PP10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.957064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PP11</td>\n",
       "      <td>-1.230915</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.138288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PP12</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.754424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PP13</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>1.849776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PP14</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>-3.391165</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.580537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PP15</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.811430</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>1.986695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PP16</td>\n",
       "      <td>0.615457</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-3.295474</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PP17</td>\n",
       "      <td>4.000473</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>-0.717456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PP18</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>-3.391165</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PP19</td>\n",
       "      <td>-0.615457</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PP20</td>\n",
       "      <td>-1.230915</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.340928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PP21</td>\n",
       "      <td>-0.923186</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>-4.898979</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>-0.957064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PP22</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PP23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>0.754424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PP24</td>\n",
       "      <td>-0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-2.539631</td>\n",
       "      <td>0.514815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PP25</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.412126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       Age    Gender  Occupation  Dominant hand    coffee  physical  \\\n",
       "0    PP1  0.615457  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "1    PP2  0.000000  0.685994    0.727171       0.294884  0.204124 -0.231695   \n",
       "2    PP3 -0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "3    PP4 -0.307729  0.685994    0.727171       0.294884  0.204124 -1.284851   \n",
       "4    PP5 -0.307729 -1.457738    0.727171       0.294884  0.204124  0.821462   \n",
       "5    PP6 -0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "6    PP7 -0.923186  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "7    PP8  0.615457  0.685994   -1.869867       0.294884  0.204124 -0.758273   \n",
       "8    PP9  0.923186  0.685994   -1.004188       0.294884  0.204124 -1.284851   \n",
       "9   PP10  0.000000  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "10  PP11 -1.230915  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "11  PP12 -0.307729 -1.457738    0.727171       0.294884  0.204124  0.821462   \n",
       "12  PP13  0.307729  0.685994   -1.004188       0.294884  0.204124  0.821462   \n",
       "13  PP14  0.307729  0.685994    0.727171      -3.391165  0.204124 -1.284851   \n",
       "14  PP15 -0.307729  0.685994    0.727171       0.294884  0.204124 -1.811430   \n",
       "15  PP16  0.615457 -1.457738    0.727171       0.294884  0.204124 -1.284851   \n",
       "16  PP17  4.000473 -1.457738   -1.004188       0.294884  0.204124  0.821462   \n",
       "17  PP18 -0.307729  0.685994   -1.869867      -3.391165  0.204124  0.821462   \n",
       "18  PP19 -0.615457 -1.457738   -0.138509       0.294884  0.204124  0.821462   \n",
       "19  PP20 -1.230915 -1.457738   -0.138509       0.294884  0.204124  0.821462   \n",
       "20  PP21 -0.923186 -1.457738   -0.138509       0.294884 -4.898979  0.294884   \n",
       "21  PP22  0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "22  PP23  0.000000 -1.457738    0.727171       0.294884  0.204124 -1.284851   \n",
       "23  PP24 -0.923186  0.685994   -1.869867       0.294884  0.204124 -1.284851   \n",
       "24  PP25  0.307729  0.685994   -1.869867       0.294884  0.204124 -1.284851   \n",
       "\n",
       "      stress  Internal control index  \n",
       "0   0.483739               -2.531632  \n",
       "1  -0.272103                1.986695  \n",
       "2   0.483739               -0.204009  \n",
       "3   0.483739               -0.204009  \n",
       "4   0.483739                0.138288  \n",
       "5   0.483739                0.651734  \n",
       "6   0.483739               -0.101320  \n",
       "7   0.483739               -0.340928  \n",
       "8   0.483739               -1.436280  \n",
       "9   0.483739               -0.957064  \n",
       "10  0.483739                0.138288  \n",
       "11  0.483739                0.754424  \n",
       "12  0.483739                1.849776  \n",
       "13  0.483739               -0.580537  \n",
       "14  0.483739                1.986695  \n",
       "15 -3.295474               -0.204009  \n",
       "16 -1.027946               -0.717456  \n",
       "17  0.483739               -0.204009  \n",
       "18  0.483739               -0.204009  \n",
       "19  0.483739               -0.340928  \n",
       "20 -1.027946               -0.957064  \n",
       "21  0.483739               -0.204009  \n",
       "22 -1.027946                0.754424  \n",
       "23 -2.539631                0.514815  \n",
       "24  0.483739                0.412126  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swell_extra = swell_extra.rename(columns={\"PP\":\"id\"})\n",
    "swell_extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d3d6d",
   "metadata": {},
   "source": [
    "## Single-Attribute-Splitting (Personality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdbac669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Internal control index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP1</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-2.531632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP2</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>1.986695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP3</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP4</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP5</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.138288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PP6</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.651734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PP7</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-0.101320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PP8</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-0.340928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PP9</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.436280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PP10</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-0.957064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PP11</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.138288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PP12</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.754424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PP13</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>1.849776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PP14</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-0.580537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PP15</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>1.986695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PP16</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PP17</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.717456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PP18</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PP19</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PP20</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.340928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PP21</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.957064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PP22</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-0.204009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PP23</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.754424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PP24</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.514815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PP25</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.412126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    Gender  Internal control index\n",
       "0    PP1  0.685994               -2.531632\n",
       "1    PP2  0.685994                1.986695\n",
       "2    PP3  0.685994               -0.204009\n",
       "3    PP4  0.685994               -0.204009\n",
       "4    PP5 -1.457738                0.138288\n",
       "5    PP6  0.685994                0.651734\n",
       "6    PP7  0.685994               -0.101320\n",
       "7    PP8  0.685994               -0.340928\n",
       "8    PP9  0.685994               -1.436280\n",
       "9   PP10  0.685994               -0.957064\n",
       "10  PP11  0.685994                0.138288\n",
       "11  PP12 -1.457738                0.754424\n",
       "12  PP13  0.685994                1.849776\n",
       "13  PP14  0.685994               -0.580537\n",
       "14  PP15  0.685994                1.986695\n",
       "15  PP16 -1.457738               -0.204009\n",
       "16  PP17 -1.457738               -0.717456\n",
       "17  PP18  0.685994               -0.204009\n",
       "18  PP19 -1.457738               -0.204009\n",
       "19  PP20 -1.457738               -0.340928\n",
       "20  PP21 -1.457738               -0.957064\n",
       "21  PP22  0.685994               -0.204009\n",
       "22  PP23 -1.457738                0.754424\n",
       "23  PP24  0.685994                0.514815\n",
       "24  PP25  0.685994                0.412126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to cluster the users based on gender and personality, we keep only \"Gender\" and \"Internal control index\" \n",
    "# columns\n",
    "\n",
    "swell_personality = swell_extra[['id', \"Gender\", \"Internal control index\"]]\n",
    "swell_personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3584fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep user IDs in a separate datarame\n",
    "\n",
    "ids = swell_personality['id']\n",
    "swell_personality.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd43d2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering(swell_personality, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8c55552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "silhouette(swell_personality, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acf2cb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster\n",
       "1          7\n",
       "2          4\n",
       "4          4\n",
       "0          3\n",
       "5          2\n",
       "6          2\n",
       "7          2\n",
       "3          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 8, max_iter = 500, random_state = 0)\n",
    "y = kmeans.fit_predict(swell_personality)\n",
    "y = pd.DataFrame(y, columns=[\"Cluster\"])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7a00a",
   "metadata": {},
   "source": [
    "####  Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecf49bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x28dd5d4f0a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAHTCAYAAAAXoMEJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6KElEQVR4nO3de3wU9b3/8ffsZjfZJIRbLoRbQJA7JhAIiICtl5aC9VZvrRXp6Snn1yqe05saoaJVawXRnlaspWprxYJVFG/1XtojKrUGSQQBSTAQbiGBhJBkk93szu8PJCWSK2xmdpLX8/HgQTPzjft++Gk2b2dmZwzTNE0BAAAAFnLZHQAAAADdDyUUAAAAlqOEAgAAwHKUUAAAAFiOEgoAAADLUUIBAABgOUooAAAALEcJBQAAgOVi7A7QXh999JFM05TH47E7CgAAAJoRDAZlGIYmTJjQ5lrHHAk1TVNOfLiTaZoKBAKOzN4dMS9nYV7OwrychXk5TzTMrCN9zTFHQo8fAR0/frzNSTqmtrZWW7du1fDhwxUfH293HLSBeTkL83IW5uUszMt5omFmH3/8cbvXOuZIKAAAALoOSigAAAAsRwkFAACA5SihAAAAsBwlFAAAAJajhAIAAMBylFAAAABYjhIKAAAAy1FCAQAAYDlKKAAAACxHCQUAAIDlKKEA4DDBhrDCYdPuGABwWmLsDgAAaFttfYMe+0eRPtx5SBU1AbldLp2Rmqgrcgbp7DNT7I4HAB1GCQWAKFddF9T/PJmnzXuONNm+v9Kvj0sqNXf6UH17+lCb0gHAqeF0PABEufte+uSkAnpclT+ole99puKyaotTAcDpoYQCQBQ7UhtQQUllq2sqa4J6cv1n1gQCgAihhAJAFMv77LBKj9S1uW7P4VoL0gBA5FBCASCK8Rl4AF0VJRQAotjEIX2UkhTb5rr0Xj4L0gBA5FBCASCK9U7watzAXq2uSfLF6NvnDLEkDwBECiUUAKLczXNGa2R6UrP7EmLdumpKhob3a34/AEQr7hMKAFGud2KsfnP9JP32rU+Vv7tCFTUBxbhcGpqaqIsmDNBXxqfbHREAOowSCgAOkOTz6Javj1UobOqoP6hYj0s+L2/hAJyLdzAgSu2r8KuyNqDkHrFKTYqzOw6ihNtlqFeC1+4YAHDaKKFAlHl7ywGt+WC3dhw4quq6BiX5PBrZP0nXnjNEU4Yl2x0PAICIoIQCUeTZD3bp938r0hF/sHHbEX9QHxQd0s6D1frR7FE6b0w/GxMCABAZfDoeiBI19Q3683u7mhTQE5Ufrdfjfy9SQyhscTIAACKPEgpEidXvF2tfhb/VNTsPVuv1gv0WJQIAoPNQQoEosbu8ps01YVPasrey88MAANDJKKFAlDBcRrvWuQ1+bAEAzsdvMyBKTB2erJg2fiLjvW599SxuTA4AcD5KKBAlLhyXrmFpPVpdM6p/ksYN6mVNIAAAOhElFIgSbpehRZeO05CUhGb3j0zvocWXj7c4FQAAnYP7hAJR5Mx+Sfrdf+ToiXc+U8HuCtUGQuoRF6OJQ/vo2+cMVUIsP7IAgK6B32hAlOkZ79VNXx1pdwwAADoVp+MBAABgOUooAAAALEcJBQAAgOUooQAAALAcJRQAAACWo4QCAADAcpRQAAAAWI4SCgAAAMtRQgEAAGA5SigAAAAsRwkFAACA5SihAAAAsBwlFAAAAJaLsTsAAABoWUMorL/m79N7n5YpGAqrp8+ra87O0Ij0JLujAaeFEgoAQJTac7hGi54p0Kf7qxQ2/739/7Yf1LmjUrXwknFyuQz7AgKngdPxAABEoWBDWIv+UqBt+5oWUEmqrmvQq/n79NCb2+0JB0QAJRQAgCj0wsY92r6/qsX9YVNav71MdYGQhamAyKGEAgAQhTYUlstsY83uQ7V6a8sBS/IAkUYJBQAgCgWC4XatO1xd38lJgM4R8RJaWlqqm266STk5OZoxY4buvfde1dcf+wEpKSnRvHnzlJWVpdmzZ2v9+vWRfnkAALqEJF/bnx32uA2d2a+HBWmAyItoCTVNUzfddJP8fr+eeuopPfjgg1q3bp1+9atfyTRN3XDDDUpOTtaaNWt0ySWX6MYbb9S+ffsiGQEAgC7h0kmDFOdxt7pmeFoPTR2ebFEiILIieoumnTt3atOmTXr33XeVnHzsh+Kmm27Sfffdp5kzZ6qkpESrV69WfHy8hg0bpvfff19r1qzRggULIhkDAADHyx7aR1OG99U/th5sdn9Pn0dXTR0sw+AWTXCmiB4JTUlJ0aOPPtpYQI+rrq5Wfn6+xowZo/j4+Mbt2dnZ2rRpUyQjAADQJRiGoXuuzNTFEweoX8+4xu0xbkMj03vov2eN1NcyB9iYEDg9ET0SmpSUpBkzZjR+HQ6HtXLlSk2dOlVlZWVKTU1tsr5v3746cKD9n+ozTVO1tbURy2sFv9/f5G9EN+blLMzLWZjXqfmfC8/Q0RmD9GpBqar8DRrRL0HTRyTLZRid+juReTlPNMzMNM12H53v1CcmLV26VJ988omeffZZ/fGPf5TX622y3+v1KhAItPufFwwGtXXr1kjHtERxcbHdEdABzMtZmJezMK9TM76HpB6Swn5t31Zu2esyL+exe2Zf7Hst6bQSunTpUj3xxBN68MEHNWLECMXGxqqysrLJmkAgoLi4uOb/Ac3weDwaPnx4hJN2Lr/fr+LiYg0ZMkQ+n8/uOGgD83IW5uUszMtZmJfzRMPMCgsL2722U0roXXfdpVWrVmnp0qX66le/KklKS0s7KVh5eflJp+hbYxhGk2tKncTn8zk2e3fEvJyFeTkL83IW5uU8ds6sIx+Ui/h9Qh966CGtXr1aDzzwgObMmdO4PTMzU1u2bFFdXV3jtry8PGVmZkY6AgAAAKJcREtoUVGRHn74YX3ve99Tdna2ysrKGv/k5OQoPT1dubm52rFjh1asWKGCggJdccUVkYwAAAAAB4jo6fi3335boVBIv/3tb/Xb3/62yb7t27fr4Ycf1sKFC3X55ZcrIyNDy5cvV//+/SMZAQAAAA4Q0RI6f/58zZ8/v8X9GRkZWrlyZSRfEgAAAA4U8WtCAQAAgLZQQgEAAGA5SigAAAAsRwkFAACA5SihAAAAsBwlFAAAAJajhAIAAMBylFAAAABYjhIKAAAAy1FCAQAAYDlKKAAAACxHCQUAAIDlKKEAAACwHCUUAAAAlqOEAgAAwHKUUAAAAFiOEgoAAADLUUIBAABgOUooAAAALEcJBQAAgOUooQAAALAcJRQAAACWo4QCAADAcpRQAAAAWI4SCgAAAMtRQgEAAGA5SigAAAAsRwkFAACA5WLsDgAAsEdpbame3f4XfVb1mRrCDeoZ21PT+8/QeRnny2247Y4HoIujhOIU5Ut6WtJhHfu/0VmSrpWUYGcoAO30/r739OjHK1TmL2uyPf/gJr2//13lTlkkj8tjUzoA3QGn49FBpqQ7JC2Q9IakDyVtkLRC0lxJG21LBqB9Kusr9djHj55UQCUprLA+LP1Qvy/4nQ3JAHQnlFB00HJJr0qqbWbfLkn3SKqyNBGAjnn207/ooL+01TX5Zfmqa6izKBGA7ogSig5okPR/kkKtrNkl6U/WxAFwSnZWftbmmv01+7SxNM+CNAC6K0ooOuBfktr+5SVt7uwgAE5DuNX/kPy3+lB9JycB0J1RQtEBR3TsmtC2BDo7CIDT0Du2d5treniTNKrPKAvSAOiuKKHogJGSerRjXa9OzgHgdMwZepHi3HGtrhnec5jSE/tblAhAd0QJRQcM1bEi2hqPpK9bkAXRYbukX0q6S9KjkqrtjYN2GZcyXjMGzlSM0fxd+tIT+us/xv2nxakAdDfcJxQdNF9SiaSWPlk7TdKXLEsDuxyVtFBSgZoWzxclXSyJAhPtbsy6SSm+VG3Y/752VRUrZIbUO7a3zux9puaO+Y4GJw22OyKALo4Sig6aKOlOHbsv6Db9+1ZNAyXlSLpZkmFPNFgkLOknkpr75PQ+SU9I8unYwwsQrQzD0DWjvqmrRl6t4iOfyd/gV//EAeod1/b1ogAQCZRQnIJJn//ZqmNHwhIlfVlSvJ2hYJk3deyJWS3xS3pZ0tXiLSb6uQyXzug1zO4YALohfkPgNIz+/A+6lzd17J6xrdkpab24NAMA0BI+mASgg5p7WtYXhSTt7uwgAAAHo4QC6CBfO9a4dOw6YQAAmkcJBdBBF6jtK3nOkDTTgiwAAKeihALooK9IGt/K/jhJs8Ul5wCA1lBCAXSQW9IySVN18h0R0iVdJ2mu1aEAAA7DoQoApyBJ0kOSNuvYDeoDOlZAv6X2PdoVANDdUUIBnIZxn/8BAKBjOB0PAAAAy1FCAQAAYDlKKAAAACxHCQUAAIDlKKEAAACwHJ+OBwAAjtHQ0KCCggLt2bNH9fX1crvdSklJ0cSJE5WQkGB3PHQAJRQAADhCfX29Xn31VR08eLDJ9vLycu3du1czZ85U//79bUqHjuJ0PAAAcIR169adVECPq6qq0rvvvquGhgaLU+FUUUIBAEDUq6qqarGAHldRUaEtW7ZYlAinixIKAACi3qeffqq6uro215WWllqQBpFACQUAAFEvFAq1a51pmp2cBJFCCQUAAFFvwIABcrvdba5LTEy0IA0igRIKAACi3oABA9S3b99W18THxysrK8uaQDhtlFAAABD1DMNQTk5Oi0c6PR6PxowZw71CHYQSCgAAHKF///46//zzNXjwYPl8PknHyme/fv109tlna+LEiTYnREdws3oAAOAYaWlpmjVrlvx+v6qrqxUbG6ukpCS7Y+EUUEIBAIDj+Hy+xqOhcCbLT8fX19frtttu06RJkzR9+nQ9/vjjVkcAAACAzSw/ErpkyRJt3rxZTzzxhPbt26dbbrlF/fv316xZs6yOAgAAAJtYWkJra2v1zDPP6Pe//73Gjh2rsWPHaseOHXrqqacooQAAAN2Ipafjt23bpoaGBk2YMKFxW3Z2tvLz8xUOh62MAgAAABtZeiS0rKxMvXv3ltfrbdyWnJys+vp6VVZWqk+fPq1+v2maqq2t7eyYEeX3+5v8jejGvJyFeTkL83IW5uU80TAz0zRlGEa71lpaQv1+f5MCKqnx60Ag0Ob3B4NBbd26tVOydbbi4mK7I6ADmJezMC9nYV7Owrycx+6ZfbHrtcTSEhobG3tS2Tz+dVxcXJvf7/F4NHz48E7J1ln8fr+Ki4s1ZMgQbiXhAMzLWZiXszAvZ2FezhMNMyssLGz3WktLaFpamioqKtTQ0KCYmGMvXVZWpri4uHbdaNYwDMXHx3d2zE7h8/kcm707Yl7OwrychXk5C/NyHjtn1t5T8ZLFH0waPXq0YmJitGnTpsZteXl5Gj9+vFwuniAKAADQXVja/Hw+ny699FLdcccdKigo0FtvvaXHH39cc+fOtTIGAAAAbGb5zepzc3N1xx136Prrr1diYqIWLFigr3zlK1bHAAAAgI0sL6E+n0/33Xef7rvvPqtfGgAAAFGCCzEBAABgOUooAAAALEcJBQAAgOUooQAAALAcJRQAAACWo4QCAADAcpRQAAAAWI4SCgAAAMtRQgEAAGA5SigAAAAsZ/ljOwEAiEahgwdV98abMsNhxX7pXHkGD7Y7EtClUUIBAN1auKJCFT+9WYG8jQofPChJcvXtK0/mWer1y3sVM2CAzQmBronT8QCAbitcXa3yb1+nuldfayygkhQ+dEj1f1unQ9ddr4bSUhsTAl0XJRQA0G1V/epXCm7Kb3F/w/btOnrvfRYmAroPSigAoNuqX/9em2sCGzfKDAQsSAN0L5RQAEC3ZIZCCh8+3Oa68KFDCldUWJAI6F4ooQCA7snlkuH1tr0uNlaGz9f5eYBuhhIKAOiWDMOQZ9SoNtd5Ro6UKynJgkRA90IJBQB0W4n/9T25kpNb3G8kJSn+m9dYmAjoPiihAIBuK3byZCXd/BO5+vU7aZ+R3Fc9fvB9xV/8dRuSAV0fN6sHAHRrCddeq7jzz1fVb5aroahIMk3FDBqkxBt/IM+QIXbHA7osSigAoNtz9+un3vfcZXcMoFvhdDwAAAAsRwkFgAgwTdPuCADgKJyOB4BTVN9Qp2c+/Ys2lW1SZV2lYlwxykjK0MXDL9XYvmPtjgcAUY0SCgCnoDZYqzvfX6ythz9psn1fzV59cmiLvjnqWs0+Y45N6QAg+nE6HgBOwcP5D51UQI87Ejiip7ev0oGaAxanAgDnoIQCQAdVB45q66HmC+hxFfUVWvPpMxYlAgDnoYQCQAdtPLhRZf6yNtftrd5jQRoAcCauCQUsZJqmwtvKFdpaJoVNqYdXnmmDZfSItTsaOiBshtu1js/LA0DLKKGARcKVdQo+94nM/Uel0L/rSf2Wg3KP7yfPhcNsTIeOGJ98lnrH9lZFfUWr61J8KRYlAgDn4XQ8YAGzIazgM1tk7qlqUkAlSdVBhf61R8H1u+wJhw7r6+urM3uPaHVNoidRl515uUWJAMB5KKGABRry9h07AtriAlPhzQdlhtt3mhf2+37mDcpIGtLsvjh3nGYPnaOhPc+wNhQAOAin420WKChQ8NNP5U5NVey0aTJiGElXZBYeantNWY3ChYflHpFsQSKcrr6+vrpr2j16YssftK1iqyrrKuVxezSoxyB9edD5uiDjQrsjAkBUo/HYxP/66zr6298p+MknUk2N5HYrZsQI+b5+kXrctECGYdgdERFkNrTjCKcpmVX1nR8GEdMrrpf+O/uHCoaCqqyvVKzbq6TYnnbHAgBHoITaoPbll3Vk0c8ULiv/98ZQSA1bt+poUZHChw+r15132JYPkWfExrT9SWm3ISM53oo4iDCP26OUeD6EBAAdwTWhFjPDYR39zfKmBfREgYBqn3tewV18SKUrcY1NbfOnzUhLlCujlyV5AACwGyXUYnWvva6G7dtbXWMePqzqh39rUSJYwT02VcbgXi0viItRzOQBXIYBAOg2KKEWC+TlScFgm+vCB9t+Ggucw3AZ8l4zTq5RyVK8p+m+lHjFnD9U7sx+NqUDAMB6XBNqMSMxsX0LPYymqzG8MfJeNU7hCr9CeftkBsNypSXIndlPhpv/HgQAdC80HYslfOubqnlypcKlpS0vcrkUd+651oWCpVy9fXJdwNORAADdG4dfLOZOS1PslCmtrvGMGa34q660KBEAAID1KKE26PXA/fLOmC55PCftixk1Sr3/939lNLMPAACgq+B0vA1cPp+Sn1op/wsvqvaFFxU+ckRGXKxip01T4nf/Q66EBLsjAgAAdCpKqE0Mt1vxl1+m+MsvszsKAACA5TgdDwAAAMtRQgEAAGA5TscDAAB0MYHao9r20h90uOhjhcNhxfdO1aivf0e9Bo+wO1ojSigAAEAXUlrwrjavWqbqA7uabN+b9zcN/dLlmjj3FpuSNcXpeAAAgC4iWHVIBU/+8qQCKkmBmioVvrFa217+o/XBmkEJBQAA6CIOb3hRteX7WtwfCtZp13t/lWmaFqZqHiUUAACgi6gr/azNNZW7P1XVnkIL0rSOEgoAANBFmKGGNteEg/UK1FZbkKZ1lFAAAIAuIsaX2OaauF7JSkwbaEGa1lFCAQAAuoiE4ZMkGa2u6XPGOPl6pVgTqBWUUAAAgC6iV9Z5Sh6d3eL+hNRBGn/1TRYmahklFAAAoIsw3DGa8t+/Usb0ixTfN71xuyc+USljJuuc/1mmPkNG25jw37hZPQAAQBcSE+vTtAVLVX+0Qrvff12hQJ1Sx05Wn6Fj7Y7WBCUUAACgC4rt0VtnfuUau2O0iNPxAAAAsBwlFAAAAJajhAIAAMBylFAAAABYjhIKAAAAy0W0hFZVVWnhwoWaNm2apk6dqltvvVVVVVWN+ysqKrRgwQJNmDBB5513nl544YVIvjwAAAAcIqIldPHixdq2bZtWrFihxx57TEVFRVq0aFHj/tzcXB09elRPP/20vv/972vRokUqKCiIZAQAAAA4QMTuE1pbW6vXX39dq1at0rhx4yRJt912m6699lrV19ertLRU69at09tvv62BAwdqxIgR2rRpk/785z/rrLPOilQMAAAAOEDEjoS6XC498sgjGj266aOgQqGQampqlJ+fr/T0dA0cOLBxX3Z2tj766KNIRQAAAIBDROxIaFxcnGbOnNlk25/+9CeNHDlSffr0UVlZmVJTU5vs79u3r0pLS9v9GqZpqra2NiJ5reL3+5v8jejGvJyFeTkL83IW5uU80TAz0zRlGEa71naohNbV1bVYGlNSUhQfH9/49cqVK/Xqq6/q0UcflXTsX4jX623yPV6vV4FAoN2vHwwGtXXr1o5EjhrFxcV2R0AHMC9nYV7OwrychXk5j90z+2Lfa0mHSmh+fr7mzp3b7L7ly5frggsukCQ99dRTuvvuu5Wbm6vp06dLkmJjY08qnIFAQHFxce1+fY/Ho+HDh3cksu38fr+Ki4s1ZMgQ+Xw+u+OgDczLWZiXszAvZ2FezhMNMyssLGz32g6V0ClTpmj79u2trnnssce0ZMkS3Xzzzbr++usbt6elpam8vLzJ2vLycqWkpLT79Q3DaHK01Ul8Pp9js3dHzMtZmJezMC9nYV7OY+fM2nsqXorwLZqef/55LVmyRLm5ufrud7/bZF9WVpb27t2rAwcONG7Ly8tTVlZWJCMAAIDT0NDQoFAoZHcMdAMR+2BSZWWlfv7zn+uyyy7TnDlzVFZW1rivT58+GjRokKZPn66f/vSnWrhwoT7++GO9/PLLWrlyZaQiAACAUxAOh7Vp0ybt3r1b1dXVMgxDiYmJ8nq9GjVqlN3x0EVFrIS+++67qq2t1fPPP6/nn3++yb7j9wZdsmSJFi5cqKuuukopKSn6xS9+wT1CAQCwUTgc1htvvKHdu3c32V5TUyNJ+uc//6kvf/nLHTrNCrRHxEronDlzNGfOnFbX9O3bV4888kikXhIAAJymDz/88KQCeqKdO3c2PmQGiKSIXhMKAACcwzRNlZSUtLomHA5rx44dFiVCd0IJBQCgm6qpqdHRo0fbXNeeNUBHUUIBAABgOUooAADdVHx8vHr06NHmuvasATqKEgoAQDflcrk0cODANtc47WmFcAZKKAAA3djkyZNbLaJDhgzhk/HoFBG7RRMAAHAel8ulWbNmaePGjSopKWm8P2hiYqJiY2N19tlnc49QdApKKAAA3ZzL5dKkSZOUnZ2tYDAowzAUDAa1detWCig6DafjAQCAJMkwDHm9Xnk8HrujoBvgSCgAICK276vSSx/tVUMorIzkBF02eZDiPG67YwGIUpRQAMBpOVIb0O3PFmjLniOqrm9o3P58XomumjJYV+Rk2JgOQLSihAIATlmwIayf/HmjPi45ctK+3eW1+t3bhYr3xmh21gAb0gGIZlwTCgA4ZS9u3KPNzRTQ447WNWjNv0pkmqaFqQA4ASUUAHDK1m8vU1v1cseBo9qyp+WiCqB7ooQCAE5ZzQnXgLYk0BBWyeEaC9IAcBJKKADglPm8bX/63eM2lN4r3oI0AJyEEgoAOGVThye3uWZYWg9lDu7V+WEAOAqfjgcQlUzT1L92HtJr+ftV3xBS74RYffucIerXy2d3NJzg8smD9MbH+7V1X1Wz++O9bl00YQBP3QFwEkoogKhTWVOvhX8p0Oa9laoPhhu3/23Lfn15TD/9ZM5oSk2UiPW4dd81E/SzZ/O1bX9Vk3ml94rTJdmDdEXOYBsTAohWlFAAUSUcNnXL6k3K31150r7DNUG9kFeihNgY/eDCEdaHQ7NSe8bpkf/I0QdFh/Tm5v1qCJtK7+XTt84eoh4+Hv8IoHmUUABR5W+fHNCWvS3fzqchLP19a6n+40vDLEyFthiGoSnDkzWlHdeIAoDEB5MARJm3t5SqIdT6nSd3H6rVawX7LEoEAOgMlFAAUaUuEGrXutLKuk5OAgDoTJRQAFElIa7t+066DGlISqIFaQAAnYUSCiCqXDJxkGI9rb81DU1N1Plj0yxKBADoDJRQAFFl0hl9NGlonxb3x3vd+vqEAYpx8/YFAE7GuziAqGIYhn5xVZYuGNdPfRK8TfYN7huv75x7hq45e4g94QAAEcMtmgBEnViPW3dfmanSI349+8Fu+QMhDewTr0snDVKcp+1rRgEA0Y8Sii5tz6EaPfVesQ5VB+R1G5oxKlUXjEuX28XTdpwgradPN1w40u4YAIBOQAlFl2Sappb9dZve2rxflbXBxu1/31qqpzfs1l1XnKUBfeJtTAgAQPfGNaHoklasK9QLeSVNCqh07Gk7n+w9ooV/yVewIdzCdwMAgM5GCUWXE2wI6++flCrYylN3tu+v0gt5eyxMBQAATkQJRZfzzvaDKi6raXWNKem9HWXWBAIAACehhKLLOVhVp9afPH5MPafjAQCwDSUUXc6wtER523Ej8x5xfC4PAAC7UELR5Uwa2lfD01p/rrgnxtDsrAEWJQIAAF9ECUWXYxiGrpw6WD19nhbXZA/pqxkjUyxMBQAATsT5SHRJX8scoGDI1F827NLOg9UKf36RaN/EWE0c2lsLLxknw+CG9QAA2IUSii7r4okDNTuzv974eL8+3X9U8bEx+sbkQerbI9buaAAAdHuUUHRpMW6XZmcN0Owsu5MAAIATcU0oAAAALEcJBQAAgOUooQAAALAcJRQAAACWo4QCAADAcpRQAAAAWI4SCgAAAMtRQgEAAGA5SigAAAAsxxOTviAcCqvonV3a+d4uBWqD8sTFaMiUQRpx3hlyuensAAAAkUAJPUF9TUCv/+LvOrC1TGbIbNy+O2+vtr1VqK/edq58PX02JgQAAOgaOLR3greXrdf+zQebFFBJMsNS6bZyvbV0vU3JAAAAuhZK6OfKiytUuq2s1TUHd5Rr35ZSixIBAAB0XZTQz217Y4cCNcFW1zTUhfTp33ZalAgAAKDrooR+LhQMt2tduKF96wAAANAySujneqb3aNe6xJT4Tk4CAADQ9VFCPzd29kglpSe2uiYxOV5nXTLGokQAAABdFyX0c564GI3+6pny+Jq/a1VMbIxGnDdMcT1iLU4GAADQ9XCf0BNkXTZWntgYbX2zUBW7KxVuMGW4DfUe1FMjvnSGMi/jKCgAAEAkUEK/YOzskRoza4RKPtqnI/uOqkdaggZnD+BpSQAAABFECW2G4TI0OHuAlG13EgAAgK6Jw3sAAACwHCUUAAAAlqOEAgAAwHKUUAAAAFiu00ronXfeqeuuu67JtpKSEs2bN09ZWVmaPXu21q9f31kvDwAAYAvTNFUXDCkcNu2OEtU65dPxGzdu1KpVqzR58uTGbaZp6oYbbtCIESO0Zs0avfXWW7rxxhv117/+Vf379++MGAAAAJapqK7XinWFKthdqaq6oLxul4an9dC15wzRWYN72x0v6kS8hAYCAd1+++3Kyspqsn3Dhg0qKSnR6tWrFR8fr2HDhun999/XmjVrtGDBgkjHAAAAsMyBSr9+/NRGFR2sbrJ9b4Vfm/dU6sYLR+hrWQNsShedIn46fsWKFRo5cqTOOeecJtvz8/M1ZswYxcfHN27Lzs7Wpk2bIh0BAADAUr94cctJBfS4Q9UBrVhXqCp/0OJU0S2iR0KLioq0atUqvfDCC1q1alWTfWVlZUpNTW2yrW/fvjpw4EC7//mmaaq2tjYiWa3i9/ub/I3oxrychXk5C/NyFubVfrsO1Wrr3iOtrtlfWac/rPtU3/vS0E7LEQ0zM01ThmG0a22HSmhdXZ1KS0ub3ZeSkqLbb79dCxYsUHJy8kn7/X6/vF5vk21er1eBQKDdrx8MBrV169aORI4axcXFdkdABzAvZ2FezsK8nIV5te2v22t0tK6hzXWbi0u1dWtdp+exe2Zf7Hst6VAJzc/P19y5c5vd9+Mf/1ihUEhXX311s/tjY2NVWVnZZFsgEFBcXFy7X9/j8Wj48OHtXh8N/H6/iouLNWTIEPl8PrvjoA3My1mYl7MwL2dhXu33weFd0vbdba5LTEzU6NGjOy1HNMyssLCw3Ws7VEKnTJmi7du3N7vvuuuu0+bNmzVx4kRJx45ahkIhTZgwQa+88orS0tJOClZeXn7SKfrWGIbR5JpSJ/H5fI7N3h0xL2dhXs7CvJyFebXtwvEDtObDfapu42jo0LSelvy7tHNm7T0VL0XwmtD7779fdXX/PsT85JNPKj8/X/fff79SU1OVmZmpFStWqK6urvHoZ15enrKzsyMVAQCAqGCapvwNfhmGIV8MRxG7uuH9kjQqPUkffna4xTVpSXGaO73zrgd1ooiV0LS0tCZf9+zZU3FxccrIyJAk5eTkKD09Xbm5ufrBD36gdevWqaCgQPfee2+kIgAAYKtQOKRnPn1aH5Z+qLLagzIMQ/3i+2lq/2m6ZNilHTpKBGe55etj9NNVH6m4rOakfb0TvPrOuWeoV0L7rpXsLjrlZvXNcbvdevjhh7Vw4UJdfvnlysjI0PLly7lRPQCgSwiFQ/rFP+/Wh6X/kql/PynncN1hba/YrqLKQv0o+ycU0S5qUN8E/WbuJD3y9g5t3lOpo/6gvDFuDU/roSunDNaU4Sd/aLu767QS2twN6DMyMrRy5crOekkAAGyzevufTyqgx4XMkNbvfUej+4zR7DPm2JAOVkhJitPPLhuvhlBYNfUNivO4Fetx2x0ranXas+MBAOguTNNUXmleswX0uJAZ0vp971iYCnaJcbvUM95LAW0DJRQAgNNUFahSWe3BNtcdrClV2AxbkAiIfpRQAAAsw/WgwHGUUAAATlOSN0mp8W3f9zotIU0ug1+9gEQJBQDgtBmGoUlpk2W0cqQzxojR9AEzLEwFRDdKKAAAEXDVqGs0JX1qs0U0xojRzIHnataQr9mQDIhOlt0nFACArsxtuHVLTq6e//Q5fVD6T5XVlkmGlB6frmkDztGcoRdxj1DgBJRQAAAixG24dcXIK3XFyCsVCAVkyJDH7bE7FhCVKKEAAHQCr5tHNAKtoYQCQJQLBQPaue45Ve37TN7EnhrxlW8qNqm33bEA4LRQQgEgim176Q8qWvesqvbubNxW9PYzSs88R5P/c7FcMZzqBeBMlFAAiFLbXnlCHz+7XA11NU22+w8f0M51a9Tgr9E5P3zQpnQAcHq4RRMARKFQQ0A7//bsSQX0RPvz1+tQ0WYLUwFA5FBCASAKFb/zko7sKWp1TdBfrU9fW2lRIgCILEooAEShqj1Fksw21wVrjnZ+GADoBFwTCnRDZigk/4svyv/qazL9dXIlJSlh3lzFTp5sdzR8Lq5XSrvWubzcBgiAM1FCgW6m4eBBHf7efAU35UsNDY3b/W++qbhzZ6rPw8tlePjEtd2GnX+Fdrz+lGrK9ra4xhXjVcY5cyxMBQCRw+l4oBsxTVOH5/8/BT/Ma1JAJUk1Nar766uqvOVWe8KhCW98D/WfeK5ktPw23Xf4eA2cdL6FqQAgciihQDdS99prCubnt77m/95R6PBhixKhNRPn3aYhM74ub0LPJttdnliljJ6k6T/6Nc8iB+BYnI4HuhH/Sy9LgUCra8L796vmiT8p6Yf/Y00otMjlcuvsG36pI3sKte2VJxSorpTbG6eMcy5S/wkzKaAAHI0SCnQjZn19u9aFj1R2bhB0SM+BwzXlv+6yOwYARBSn44FuxNWnT9uLDEOeESM6PwwAoFujhALdSOL878no1avVNTEjzlT8FVdYEwgA0G1RQoFuxHPmmYqb9RXJ7W52v5GYqPhrr5XBvScBAJ2Ma0KBbqb30qVyJfZQ3VtvKVS869hGt1sxI0Yo4dpvKfE782zNBwDoHiihQDdjuFzqdecdCt/8U9U+86xCpaXyjBwp30VzZMTwlgAAsAa/cYBuypWQoMR519sdAwDQTXFNKAAAACxHCQUAAIDlKKEAAACwHCUUAAAAlqOEAgAAwHKUUAAAAFiOEgoAAADLUUIBAABgOUooAAAALEcJBQAAgOUooQAAALAcJRQAAACWo4QCAADAcpRQAAAAWI4SCgAAAMtRQgEAAGA5SigAAAAsRwkFAACA5SihAAAAsBwlFAAAAJajhAIAAMBylFAAAABYjhIKAAAAy1FCAQAAYDlKKAAAACwXY3cAdBVhSW9Lel1SraQ4SedLmiXJbWMuAAAQjSihiIBqST+RtElSwwnb35X0vKRlknpaHwsAAEQtTscjAhZJ+lBNC6gkhXSsmN5mdSAAABDlKKE4TYWS8ttY87GOlVEAAIBjKKE4Tc9JOtrGmlpJL1mQBQAAOAUlFKcp0M51wU5NAQAAnIUSitM0sJ3r0jo1BQAAcBZKKE7TVZIGtLGmn6RrLcgCAACcghKK0xQv6dLP/25OnKTZknpZlAcAADgB9wlFBHxHkk/Si5KKdOzWTC5JQyV9TdI825IBAIDoRAlFhFwj6UpJ70jarWOn6M8V/xcDAADNoSEggtySvmR3CAAA4AARvSbUNE39+te/1rRp05STk6Of/exnqq+vb9xfUlKiefPmKSsrS7Nnz9b69esj+fIAAABwiIiW0N///vf685//rGXLlunRRx/Vhg0b9NBDD0k6VlBvuOEGJScna82aNbrkkkt04403at++fZGMAAAAAAeI2On4UCikP/zhD7rlllt09tlnS5IWLFigtWvXSpI2bNigkpISrV69WvHx8Ro2bJjef/99rVmzRgsWLIhUDAAAADhAxErojh07VFFRoQsuuKBx28UXX6yLL75YkpSfn68xY8YoPv7ft/LJzs7Wpk2bIhUBAAAADhGx0/F79uxRz549tXHjRl166aU699xzdc899ygQOPZYx7KyMqWmpjb5nr59++rAgQORigAAAACH6NCR0Lq6OpWWlja77+jRo6qrq9OyZcuUm5urcDisxYsXKxwO62c/+5n8fr+8Xm+T7/F6vY0ltT1M01RtbW1HItvO7/c3+RvRjXk5C/NyFublLMzLeaJhZqZpyjCMdq3tUAnNz8/X3Llzm933wAMPqK6uTosWLVJOTo4k6dZbb9WPfvQjLVy4ULGxsaqsrGzyPYFAQHFxce1+/WAwqK1bt3YkctQoLi62OwI6gHk5C/NyFublLMzLeeye2RcPOrakQyV0ypQp2r59e7P7PvjgA0nSGWec0bht6NChqq+v1+HDh5WWlqbCwsIm31NeXn7SKfrWeDweDR8+vCORbef3+1VcXKwhQ4bI5/PZHQdtYF7OwrychXk5S5vzCoUllyG186gXOl80/Ix9seu1JmIfTBozZow8Ho+2bdum6dOnS5KKioqUkJCgXr16KTMzUytWrFBdXV3j0c+8vDxlZ2e3+zUMw2jywSYn8fl8js3eHTEvZ2FezsK8nOXEeZn1DWr4R7HCxZUyawKS2yVXaoLck/vLPayvzUlxnJ0/Y+09FS9F8INJiYmJuuqqq3TXXXdp06ZN+uijj3T//ffryiuvVExMjHJycpSenq7c3Fzt2LFDK1asUEFBga644opIRQAAAJ3E9AcVeDJfoQ17ZB6olo4GpMo6hT89pOBzW9WwocTuiHCYiN6s/tZbb9XMmTM1f/58zZ8/XzNmzNCPf/xjSZLb7dbDDz+ssrIyXX755XrxxRe1fPly9e/fP5IRAABAJwi+/KnMfUeb3+lvUMO7uxWu4ENMaL+IPjve6/Vq4cKFWrhwYbP7MzIytHLlyki+JAAA6GRmTUDhPUdaX1QTVMP6XfJ+fZQ1oeB4ET0SCgAAup5Q0eFjp9/bYB7mSCjajxIKAAAAy1FCAQBAq9xDe0uJbd/70ejV/nt/A5RQAADQKqNHrFz9e7S+yBcj9zmDrQmELoESCgAA2hRz0QgZaYnN74x1yz11oNzJCdaGgqNF9NPxAACga3Ilxso7N1PBt3fKLDkisyYouQ25UhLknpAu99j2PwERkCihAACgnQyfR96LRsoMh6W6BinGLcPrtjsWHIoSCgAAOsRwuaT4tj+oBLSGa0IBAABgOUooAAAALEcJBQAAgOUooQAAALAcJRQAAACW49PxAIBTVC/pfUnVksZKGmpvHACOQgkFAHRQSNL/SnpX0q7Pt/WQNErS9yWdZVMuAE5CCQUAdIApaaGktz//38cdlfQvSXsk/ULSeOujAXAUrgkFAHTAu5LeUdMCeqL9kn5rXRwAjkUJBQB0wFoduxa0NVslfdb5UQA4GiUUANABFe1Yc1THiigAtIwSCgDoAE871hiSkjo7CACHo4QCADpgTDvWDJGU08k5ADgdJRQA0AHXSxrUyn6XpGmSvNbEAeBYlFAAQAf0lJSr5otorKQLJN1kaSIAzsR9QgEAHZQj6Y+SnpS0WVKDpN6SLpN0to5dEwoAraOEAoiIsBlWyAzJ42rPB1fgfD0l3Wh3CAAORgkFcFo27Htfrxe/ppKjJQoppN6xvZWVmqWrR35Tse5Yu+MBAKIUJRTAKVu9bZXWFj6n2obaxm2H/OUqrNyhrYc+0eKzf664mDgbEwIAohUfTAJwSnZU7NCLRWubFNATbTm0Rb8r4PGNAIDmUUIBnJK1hc+pOljd6ppPDm2Rv8FvUSIAgJNQQgGckgM1B9pcs79mvworCi1IAwBwGkoogE4VVsjuCACAKEQJBXBKUuNT21yTFt9Pw3udaUEaAIDTUEIBnJKvn3Gx4mPiW10zqs8oJXgSLEoEAHASSiiAUzImeaxmDZmtOHfzt2Aa0Xuk/l/mDyxOBQBwCu4TCuCUzRv3HQ1KGqR1u/+mkuoShcMh9Y7rrbHJ43Td6OsV72n9SCkAoPuihAI4LecPvkDnD75A/ga/gqGAEryJchtuu2MBAKIcJRRARPhifPLF+OyOAQBwCK4JBQAAgOUooQAAALAcJRQAAACWo4QCAADAcpRQAAAAWI4SCgAAAMtRQgEAAGA5SigAAAAsRwkFAACA5SihAAAAsBwlFAAAAJYzTNM07Q7RHhs3bpRpmvJ6vXZH6RDTNBUMBuXxeGQYht1x0Abm5SzMy1mYl7MwL+eJhpkFAgEZhqGJEye2uTbGgjwR4dQfAMMwHFecuzPm5SzMy1mYl7MwL+eJhpkZhtHuzuaYI6EAAADoOrgmFAAAAJajhAIAAMBylFAAAABYjhIKAAAAy1FCAQAAYDlKKAAAACxHCQUAAIDlKKEAAACwHCXUAnfeeaeuu+66JttKSko0b948ZWVlafbs2Vq/fr1N6SBJVVVVWrhwoaZNm6apU6fq1ltvVVVVVeP+iooKLViwQBMmTNB5552nF154wca0kKT6+nrddtttmjRpkqZPn67HH3/c7kg4QWlpqW666Sbl5ORoxowZuvfee1VfXy+J979oN3/+fN16662NX3/yySe68sorlZmZqW984xvavHmzjekgHXs05p133qnJkydr2rRpeuCBB3T82UNOmhcltJNt3LhRq1atarLNNE3dcMMNSk5O1po1a3TJJZfoxhtv1L59+2xKicWLF2vbtm1asWKFHnvsMRUVFWnRokWN+3Nzc3X06FE9/fTT+v73v69FixapoKDAxsRYsmSJNm/erCeeeEKLFy/WQw89pNdee83uWNCx97ibbrpJfr9fTz31lB588EGtW7dOv/rVr3j/i3KvvPKK/vGPfzR+XVtbq/nz52vSpEl67rnnNGHCBP3Xf/2XamtrbUyJu+++W++9954ee+wxLVu2TH/5y1/09NNPO29eJjpNfX29OWfOHPPqq682v/3tbzduf++998ysrCyzpqamcdv1119v/vrXv7YjZrdXU1Njjh492ty0aVPjto0bN5qjR4826+rqzF27dpkjRowwS0pKGvffdttt5i233GJHXJjHZjZ+/Hhzw4YNjduWL1/e5OcM9iksLDRHjBhhlpWVNW576aWXzOnTp/P+F8UqKirMmTNnmt/4xjca39+eeeYZ87zzzjPD4bBpmqYZDofNCy+80FyzZo2dUbu1iooKc8yYMeY///nPxm2/+93vzFtvvdVx8+JIaCdasWKFRo4cqXPOOafJ9vz8fI0ZM0bx8fGN27Kzs7Vp0yaLE0KSXC6XHnnkEY0ePbrJ9lAopJqaGuXn5ys9PV0DBw5s3Jedna2PPvrI6qj43LZt29TQ0KAJEyY0bsvOzlZ+fr7C4bCNySBJKSkpevTRR5WcnNxke3V1Ne9/Uey+++7TJZdcouHDhzduy8/PV3Z2tgzDkCQZhqGJEycyLxvl5eUpMTFROTk5jdvmz5+ve++913HzooR2kqKiIq1atUq5ubkn7SsrK1NqamqTbX379tWBAwesiocTxMXFaebMmfJ6vY3b/vSnP2nkyJHq06dPi/MqLS21Oio+V1ZWpt69ezeZWXJysurr61VZWWlfMEiSkpKSNGPGjMavw+GwVq5cqalTp/L+F6Xef/99ffjhh/rBD37QZDvzij4lJSUaMGCA1q5dq1mzZun888/X8uXLFQ6HHTevGLsDOFVdXV2LJSQlJUW33367FixYcNKRAEny+/1NfnlKktfrVSAQ6JSsaHteJx6VWblypV599VU9+uijkphXNGppJpKYSxRaunSpPvnkEz377LP64x//yM9TlKmvr9fixYt1++23Ky4ursk+3v+iT21trXbt2qXVq1fr3nvvVVlZmW6//Xb5fD7HzYsSeory8/M1d+7cZvf9+Mc/VigU0tVXX93s/tjY2JOO1gQCgZN++BE5rc1r+fLluuCCCyRJTz31lO6++27l5uZq+vTpko7N64s/wMzLXi3NRBJziTJLly7VE088oQcffFAjRozg/S8KPfTQQxo3blyTo9fH8f4XfWJiYlRdXa1ly5ZpwIABkqR9+/Zp1apVysjIcNS8KKGnaMqUKdq+fXuz+6677jpt3rxZEydOlCQFg0GFQiFNmDBBr7zyitLS0lRYWNjke8rLy086hI7IaW1exz322GNasmSJbr75Zl1//fWN29PS0lReXt5kbXl5uVJSUjolK9qWlpamiooKNTQ0KCbm2NtYWVmZ4uLilJSUZHM6HHfXXXdp1apVWrp0qb761a9KEu9/UeiVV15ReXl54zXWx0vM66+/rosuuqjZ9z/mZZ+UlBTFxsY2FlBJGjp0qPbv36+cnBxHzYtrQjvB/fffr1deeUVr167V2rVrdc0112jcuHFau3atUlNTlZmZqS1btqiurq7xe/Ly8pSZmWlj6u7t+eef15IlS5Sbm6vvfve7TfZlZWVp7969Ta6pycvLU1ZWlsUpcdzo0aMVExPT5GL7vLw8jR8/Xi4Xb2vR4KGHHtLq1av1wAMPaM6cOY3bef+LPk8++aReeumlxt9Z5513ns477zytXbtWmZmZ+uijjxrvQWmapjZu3Mi8bJSZman6+np99tlnjdt27typAQMGOG5evFt3grS0NGVkZDT+6dmzp+Li4pSRkaGYmBjl5OQoPT1dubm52rFjh1asWKGCggJdccUVdkfvliorK/Xzn/9cl112mebMmaOysrLGP6FQSIMGDdL06dP105/+VNu2bdMzzzyjl19+Wddee63d0bstn8+nSy+9VHfccYcKCgr01ltv6fHHH2/xkgtYq6ioSA8//LC+973vKTs7u8nPFO9/0WfAgAFNfmclJCQoISFBGRkZmjVrlqqqqnTPPfeosLBQ99xzj/x+v772ta/ZHbvbOuOMM/SlL31Jubm52rZtm9555x2tWLFC3/zmNx03L8M8XpfRaX7zm9/ogw8+0JNPPtm4bdeuXVq4cKHy8/OVkZGh2267TdOmTbMxZff1yiuv6Ec/+lGz+95++20NHDhQhw4d0sKFC/Xee+8pJSVFP/zhD3XRRRdZnBQn8vv9uuOOO/TGG28oMTFR3/3udzVv3jy7Y0HHbk+3bNmyZvdt376d978od/xpSb/85S8lSQUFBVq8eLGKioo0cuRI3XnnnRozZoydEbu9o0eP6q677tKbb74pn8+nb33rW7rhhhtkGIaj5kUJBQAAgOU4HQ8AAADLUUIBAABgOUooAAAALEcJBQAAgOUooQAAALAcJRQAAACWo4QCAADAcpRQAAAAWI4SCgAAAMtRQgEAAGA5SigAAAAsRwkFAACA5f4/+pqb5jrYk2AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "kmeans = KMeans(n_clusters = 8, max_iter = 500, random_state = 0)\n",
    "model = kmeans.fit(swell_personality)\n",
    "tsne = TSNE().fit_transform(swell_personality)\n",
    "plt.scatter(x = tsne[:, 0], y = tsne[:, 1], c=model.labels_, cmap='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "957c792b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PP6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PP7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PP8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PP9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PP10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PP11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PP12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PP13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PP14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PP15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PP16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PP17</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PP18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PP19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PP20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PP21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PP22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PP23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PP24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PP25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  Cluster\n",
       "0    PP1        3\n",
       "1    PP2        0\n",
       "2    PP3        1\n",
       "3    PP4        1\n",
       "4    PP5        2\n",
       "5    PP6        4\n",
       "6    PP7        1\n",
       "7    PP8        1\n",
       "8    PP9        6\n",
       "9   PP10        6\n",
       "10  PP11        4\n",
       "11  PP12        5\n",
       "12  PP13        0\n",
       "13  PP14        1\n",
       "14  PP15        0\n",
       "15  PP16        2\n",
       "16  PP17        7\n",
       "17  PP18        1\n",
       "18  PP19        2\n",
       "19  PP20        2\n",
       "20  PP21        7\n",
       "21  PP22        1\n",
       "22  PP23        5\n",
       "23  PP24        4\n",
       "24  PP25        4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = pd.concat([ids, y], axis=1)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a68a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_grouped_all = swell.join(clusters.set_index('id'), on='id')\n",
    "swell_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_grouped_all.to_csv(\"Final_CSVs/swell_clusters_personality.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'dataset' to run pycaret tests based on \"Cluster\".\n",
    "\n",
    "swell_grouped_all = swell_grouped_all.drop('dataset', axis = 1)\n",
    "swell_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_participants = swell_grouped_all[\"Cluster\"].unique()\n",
    "all_group = swell_grouped_all.groupby('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1scores = []\n",
    "\n",
    "for participant in unique_participants:\n",
    "    print(\"Participant: \",participant)\n",
    "    part_df = all_group.get_group(participant)\n",
    "    grid = setup(data=part_df, target='stress', html=False, silent=True, verbose=False) #fix_imbalance = True,\n",
    "    best = compare_models()\n",
    "    accuracies.append(pull()['Accuracy'][0])\n",
    "    precision.append(pull()['Prec.'][0])\n",
    "    recall.append(pull()['Recall'][0])\n",
    "    f1scores.append(pull()['F1'][0])\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = statistics.mean(accuracies)\n",
    "mean_prec = statistics.mean(precision)\n",
    "mean_rec = statistics.mean(recall)\n",
    "mean_f1 = statistics.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Accuracy SWELL- Cluster Personality: \", mean_acc)\n",
    "print(\"Mean Precision SWELL- Cluster Personality: \", mean_prec)\n",
    "print(\"Mean Recall SWELL- Cluster Personality: \", mean_rec)\n",
    "print(\"Mean F1-score SWELL- Cluster Personality: \", mean_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2ec6f",
   "metadata": {},
   "source": [
    "## Multi-Attribute-Splitting (All features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e3d42d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>HR</th>\n",
       "      <th>RMSSD</th>\n",
       "      <th>SCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP4</td>\n",
       "      <td>58</td>\n",
       "      <td>0.093757</td>\n",
       "      <td>119.071484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP19</td>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>138.735573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP22</td>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP3</td>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>120.251942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP21</td>\n",
       "      <td>70</td>\n",
       "      <td>0.064568</td>\n",
       "      <td>561.332213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>PP24</td>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>158.138912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>PP22</td>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>PP4</td>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>93.893556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>PP23</td>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>PP12</td>\n",
       "      <td>77</td>\n",
       "      <td>0.025147</td>\n",
       "      <td>495.018099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3140 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   HR       RMSSD         SCL\n",
       "0      PP4   58    0.093757  119.071484\n",
       "1     PP19  999  999.000000  138.735573\n",
       "2     PP22  999  999.000000  999.000000\n",
       "3      PP3  999  999.000000  120.251942\n",
       "4     PP21   70    0.064568  561.332213\n",
       "...    ...  ...         ...         ...\n",
       "3135  PP24  999  999.000000  158.138912\n",
       "3136  PP22  999  999.000000  999.000000\n",
       "3137   PP4  999  999.000000   93.893556\n",
       "3138  PP23  999  999.000000  999.000000\n",
       "3139  PP12   77    0.025147  495.018099\n",
       "\n",
       "[3140 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swell_all_features = swell[['id', 'HR', 'RMSSD', 'SCL']]\n",
    "swell_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "720ea302",
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_mean = swell_all_features.groupby('id', as_index = False, group_keys = True).mean()\n",
    "swell_mean = swell_mean.rename(columns={\"HR\": \"HR_mean\", \"RMSSD\": \"RMSSD_mean\", \"SCL\": \"SCL_mean\"})\n",
    "    \n",
    "swell_min = swell_all_features.groupby('id', as_index = False, group_keys = True).min()\n",
    "swell_min = swell_min.rename(columns={\"HR\": \"HR_min\", \"RMSSD\": \"RMSSD_min\", \"SCL\": \"SCL_min\"})\n",
    "  \n",
    "swell_std = swell_all_features.groupby('id', as_index = False, group_keys = True).std()\n",
    "swell_std = swell_std.rename(columns={\"HR\": \"HR_std\", \"RMSSD\": \"RMSSD_std\", \"SCL\": \"SCL_std\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05261243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "swell_mean[['HR_mean', 'RMSSD_mean', 'SCL_mean']] = scaler.fit_transform(swell_mean[['HR_mean', 'RMSSD_mean', 'SCL_mean']])\n",
    "swell_min[['HR_min', 'RMSSD_min', 'SCL_min']] = scaler.fit_transform(swell_min[['HR_min', 'RMSSD_min', 'SCL_min']])\n",
    "swell_std[['HR_std', 'RMSSD_std', 'SCL_std']] = scaler.fit_transform(swell_std[['HR_std', 'RMSSD_std', 'SCL_std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19cba762",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Dominant hand</th>\n",
       "      <th>coffee</th>\n",
       "      <th>physical</th>\n",
       "      <th>stress</th>\n",
       "      <th>Internal control index</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>RMSSD_mean</th>\n",
       "      <th>SCL_mean</th>\n",
       "      <th>HR_min</th>\n",
       "      <th>RMSSD_min</th>\n",
       "      <th>SCL_min</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>RMSSD_std</th>\n",
       "      <th>SCL_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP1</td>\n",
       "      <td>0.615457</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-2.531632</td>\n",
       "      <td>-0.900394</td>\n",
       "      <td>-0.876707</td>\n",
       "      <td>-1.222782</td>\n",
       "      <td>-0.319298</td>\n",
       "      <td>-0.294909</td>\n",
       "      <td>-1.004249</td>\n",
       "      <td>0.465365</td>\n",
       "      <td>0.439151</td>\n",
       "      <td>-0.949413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-0.231695</td>\n",
       "      <td>-0.272103</td>\n",
       "      <td>1.986695</td>\n",
       "      <td>-1.646229</td>\n",
       "      <td>-1.646932</td>\n",
       "      <td>-1.230384</td>\n",
       "      <td>-0.295646</td>\n",
       "      <td>-0.294888</td>\n",
       "      <td>-0.971799</td>\n",
       "      <td>-1.217241</td>\n",
       "      <td>-1.219896</td>\n",
       "      <td>-1.822954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP3</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>0.884367</td>\n",
       "      <td>0.867364</td>\n",
       "      <td>-0.548374</td>\n",
       "      <td>-0.264111</td>\n",
       "      <td>-0.294924</td>\n",
       "      <td>-0.312573</td>\n",
       "      <td>0.186334</td>\n",
       "      <td>0.249442</td>\n",
       "      <td>0.335381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP4</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>1.243265</td>\n",
       "      <td>1.247300</td>\n",
       "      <td>0.375704</td>\n",
       "      <td>-0.319298</td>\n",
       "      <td>-0.294784</td>\n",
       "      <td>-0.823730</td>\n",
       "      <td>-0.424820</td>\n",
       "      <td>-0.457861</td>\n",
       "      <td>1.405055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP5</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.138288</td>\n",
       "      <td>0.122998</td>\n",
       "      <td>0.126018</td>\n",
       "      <td>-0.862919</td>\n",
       "      <td>-0.307472</td>\n",
       "      <td>-0.294853</td>\n",
       "      <td>-0.768211</td>\n",
       "      <td>0.820475</td>\n",
       "      <td>0.816977</td>\n",
       "      <td>0.149815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PP6</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.651734</td>\n",
       "      <td>-0.575141</td>\n",
       "      <td>-0.550986</td>\n",
       "      <td>-1.113142</td>\n",
       "      <td>-0.323240</td>\n",
       "      <td>-0.294865</td>\n",
       "      <td>-0.861158</td>\n",
       "      <td>0.741422</td>\n",
       "      <td>0.707874</td>\n",
       "      <td>-1.239744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PP7</td>\n",
       "      <td>-0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.101320</td>\n",
       "      <td>-1.023418</td>\n",
       "      <td>-1.017587</td>\n",
       "      <td>-0.207807</td>\n",
       "      <td>-0.323240</td>\n",
       "      <td>-0.294906</td>\n",
       "      <td>0.021783</td>\n",
       "      <td>0.272457</td>\n",
       "      <td>0.269073</td>\n",
       "      <td>0.687546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PP8</td>\n",
       "      <td>0.615457</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-0.758273</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.340928</td>\n",
       "      <td>1.656421</td>\n",
       "      <td>1.654327</td>\n",
       "      <td>1.227664</td>\n",
       "      <td>3.390077</td>\n",
       "      <td>3.391165</td>\n",
       "      <td>1.674636</td>\n",
       "      <td>-2.793593</td>\n",
       "      <td>-2.803529</td>\n",
       "      <td>1.016631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PP9</td>\n",
       "      <td>0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-1.436280</td>\n",
       "      <td>0.450904</td>\n",
       "      <td>0.458573</td>\n",
       "      <td>2.375174</td>\n",
       "      <td>-0.323240</td>\n",
       "      <td>-0.294842</td>\n",
       "      <td>3.584380</td>\n",
       "      <td>0.682556</td>\n",
       "      <td>0.662209</td>\n",
       "      <td>-0.046574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PP10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.957064</td>\n",
       "      <td>0.398311</td>\n",
       "      <td>0.406084</td>\n",
       "      <td>-0.785393</td>\n",
       "      <td>-0.315356</td>\n",
       "      <td>-0.294852</td>\n",
       "      <td>-0.630076</td>\n",
       "      <td>0.715259</td>\n",
       "      <td>0.695735</td>\n",
       "      <td>0.187333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PP11</td>\n",
       "      <td>-1.230915</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.138288</td>\n",
       "      <td>1.656421</td>\n",
       "      <td>1.654327</td>\n",
       "      <td>1.399914</td>\n",
       "      <td>3.390077</td>\n",
       "      <td>3.391165</td>\n",
       "      <td>-0.612576</td>\n",
       "      <td>-2.793593</td>\n",
       "      <td>-2.803529</td>\n",
       "      <td>1.630332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PP12</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.754424</td>\n",
       "      <td>-0.673160</td>\n",
       "      <td>-0.688286</td>\n",
       "      <td>0.543763</td>\n",
       "      <td>-0.271995</td>\n",
       "      <td>-0.294909</td>\n",
       "      <td>0.019899</td>\n",
       "      <td>0.587485</td>\n",
       "      <td>0.613917</td>\n",
       "      <td>0.087347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PP13</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>1.849776</td>\n",
       "      <td>-0.015454</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>-0.446554</td>\n",
       "      <td>-0.323240</td>\n",
       "      <td>-0.294896</td>\n",
       "      <td>-0.324033</td>\n",
       "      <td>0.869512</td>\n",
       "      <td>0.839041</td>\n",
       "      <td>0.197979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PP14</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>-3.391165</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.580537</td>\n",
       "      <td>-0.699088</td>\n",
       "      <td>-0.715220</td>\n",
       "      <td>-0.889533</td>\n",
       "      <td>-0.268053</td>\n",
       "      <td>-0.294906</td>\n",
       "      <td>-0.464343</td>\n",
       "      <td>0.564936</td>\n",
       "      <td>0.592349</td>\n",
       "      <td>-0.989584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PP15</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.811430</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>1.986695</td>\n",
       "      <td>0.466006</td>\n",
       "      <td>0.477363</td>\n",
       "      <td>-0.097784</td>\n",
       "      <td>-0.319298</td>\n",
       "      <td>-0.294875</td>\n",
       "      <td>0.807973</td>\n",
       "      <td>0.680695</td>\n",
       "      <td>0.649216</td>\n",
       "      <td>-0.306543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PP16</td>\n",
       "      <td>0.615457</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-3.295474</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>-1.548942</td>\n",
       "      <td>-1.576753</td>\n",
       "      <td>-0.942372</td>\n",
       "      <td>-0.260169</td>\n",
       "      <td>-0.294857</td>\n",
       "      <td>-1.066454</td>\n",
       "      <td>-0.959707</td>\n",
       "      <td>-0.945663</td>\n",
       "      <td>-0.779952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PP17</td>\n",
       "      <td>4.000473</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>-0.717456</td>\n",
       "      <td>-1.312910</td>\n",
       "      <td>-1.307585</td>\n",
       "      <td>-0.372496</td>\n",
       "      <td>-0.315356</td>\n",
       "      <td>-0.294917</td>\n",
       "      <td>0.941927</td>\n",
       "      <td>-0.215431</td>\n",
       "      <td>-0.218956</td>\n",
       "      <td>-1.304895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PP18</td>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>-3.391165</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>-0.993383</td>\n",
       "      <td>-1.001877</td>\n",
       "      <td>-0.507674</td>\n",
       "      <td>-0.283820</td>\n",
       "      <td>-0.294917</td>\n",
       "      <td>0.570615</td>\n",
       "      <td>0.277847</td>\n",
       "      <td>0.291028</td>\n",
       "      <td>-1.347435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PP19</td>\n",
       "      <td>-0.615457</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>0.139741</td>\n",
       "      <td>0.131338</td>\n",
       "      <td>-0.586040</td>\n",
       "      <td>-0.268053</td>\n",
       "      <td>-0.294862</td>\n",
       "      <td>-0.619700</td>\n",
       "      <td>0.791563</td>\n",
       "      <td>0.815099</td>\n",
       "      <td>0.246811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PP20</td>\n",
       "      <td>-1.230915</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.340928</td>\n",
       "      <td>1.115547</td>\n",
       "      <td>1.115816</td>\n",
       "      <td>0.493612</td>\n",
       "      <td>-0.287762</td>\n",
       "      <td>-0.294851</td>\n",
       "      <td>0.227648</td>\n",
       "      <td>-0.156073</td>\n",
       "      <td>-0.163981</td>\n",
       "      <td>1.254304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PP21</td>\n",
       "      <td>-0.923186</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>-4.898979</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>-0.957064</td>\n",
       "      <td>-1.430428</td>\n",
       "      <td>-1.425883</td>\n",
       "      <td>0.071310</td>\n",
       "      <td>-0.295646</td>\n",
       "      <td>-0.294842</td>\n",
       "      <td>0.436899</td>\n",
       "      <td>-0.487757</td>\n",
       "      <td>-0.490870</td>\n",
       "      <td>-1.322268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PP22</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>0.685103</td>\n",
       "      <td>0.689476</td>\n",
       "      <td>0.403560</td>\n",
       "      <td>-0.303530</td>\n",
       "      <td>-0.294899</td>\n",
       "      <td>0.372228</td>\n",
       "      <td>0.478256</td>\n",
       "      <td>0.463279</td>\n",
       "      <td>1.225381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PP23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>0.754424</td>\n",
       "      <td>0.231754</td>\n",
       "      <td>0.227235</td>\n",
       "      <td>2.526465</td>\n",
       "      <td>-0.287762</td>\n",
       "      <td>-0.294896</td>\n",
       "      <td>0.171827</td>\n",
       "      <td>0.769992</td>\n",
       "      <td>0.784510</td>\n",
       "      <td>-0.511894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PP24</td>\n",
       "      <td>-0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-2.539631</td>\n",
       "      <td>0.514815</td>\n",
       "      <td>0.534514</td>\n",
       "      <td>0.532129</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>-0.283820</td>\n",
       "      <td>-0.294931</td>\n",
       "      <td>0.218670</td>\n",
       "      <td>0.598568</td>\n",
       "      <td>0.606903</td>\n",
       "      <td>0.852896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PP25</td>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.412126</td>\n",
       "      <td>1.233195</td>\n",
       "      <td>1.220136</td>\n",
       "      <td>0.199072</td>\n",
       "      <td>-0.220749</td>\n",
       "      <td>-0.294947</td>\n",
       "      <td>-0.589581</td>\n",
       "      <td>-0.454509</td>\n",
       "      <td>-0.391519</td>\n",
       "      <td>1.344447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       Age    Gender  Occupation  Dominant hand    coffee  physical  \\\n",
       "0    PP1  0.615457  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "1    PP2  0.000000  0.685994    0.727171       0.294884  0.204124 -0.231695   \n",
       "2    PP3 -0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "3    PP4 -0.307729  0.685994    0.727171       0.294884  0.204124 -1.284851   \n",
       "4    PP5 -0.307729 -1.457738    0.727171       0.294884  0.204124  0.821462   \n",
       "5    PP6 -0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "6    PP7 -0.923186  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "7    PP8  0.615457  0.685994   -1.869867       0.294884  0.204124 -0.758273   \n",
       "8    PP9  0.923186  0.685994   -1.004188       0.294884  0.204124 -1.284851   \n",
       "9   PP10  0.000000  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "10  PP11 -1.230915  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "11  PP12 -0.307729 -1.457738    0.727171       0.294884  0.204124  0.821462   \n",
       "12  PP13  0.307729  0.685994   -1.004188       0.294884  0.204124  0.821462   \n",
       "13  PP14  0.307729  0.685994    0.727171      -3.391165  0.204124 -1.284851   \n",
       "14  PP15 -0.307729  0.685994    0.727171       0.294884  0.204124 -1.811430   \n",
       "15  PP16  0.615457 -1.457738    0.727171       0.294884  0.204124 -1.284851   \n",
       "16  PP17  4.000473 -1.457738   -1.004188       0.294884  0.204124  0.821462   \n",
       "17  PP18 -0.307729  0.685994   -1.869867      -3.391165  0.204124  0.821462   \n",
       "18  PP19 -0.615457 -1.457738   -0.138509       0.294884  0.204124  0.821462   \n",
       "19  PP20 -1.230915 -1.457738   -0.138509       0.294884  0.204124  0.821462   \n",
       "20  PP21 -0.923186 -1.457738   -0.138509       0.294884 -4.898979  0.294884   \n",
       "21  PP22  0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "22  PP23  0.000000 -1.457738    0.727171       0.294884  0.204124 -1.284851   \n",
       "23  PP24 -0.923186  0.685994   -1.869867       0.294884  0.204124 -1.284851   \n",
       "24  PP25  0.307729  0.685994   -1.869867       0.294884  0.204124 -1.284851   \n",
       "\n",
       "      stress  Internal control index   HR_mean  RMSSD_mean  SCL_mean  \\\n",
       "0   0.483739               -2.531632 -0.900394   -0.876707 -1.222782   \n",
       "1  -0.272103                1.986695 -1.646229   -1.646932 -1.230384   \n",
       "2   0.483739               -0.204009  0.884367    0.867364 -0.548374   \n",
       "3   0.483739               -0.204009  1.243265    1.247300  0.375704   \n",
       "4   0.483739                0.138288  0.122998    0.126018 -0.862919   \n",
       "5   0.483739                0.651734 -0.575141   -0.550986 -1.113142   \n",
       "6   0.483739               -0.101320 -1.023418   -1.017587 -0.207807   \n",
       "7   0.483739               -0.340928  1.656421    1.654327  1.227664   \n",
       "8   0.483739               -1.436280  0.450904    0.458573  2.375174   \n",
       "9   0.483739               -0.957064  0.398311    0.406084 -0.785393   \n",
       "10  0.483739                0.138288  1.656421    1.654327  1.399914   \n",
       "11  0.483739                0.754424 -0.673160   -0.688286  0.543763   \n",
       "12  0.483739                1.849776 -0.015454    0.000329 -0.446554   \n",
       "13  0.483739               -0.580537 -0.699088   -0.715220 -0.889533   \n",
       "14  0.483739                1.986695  0.466006    0.477363 -0.097784   \n",
       "15 -3.295474               -0.204009 -1.548942   -1.576753 -0.942372   \n",
       "16 -1.027946               -0.717456 -1.312910   -1.307585 -0.372496   \n",
       "17  0.483739               -0.204009 -0.993383   -1.001877 -0.507674   \n",
       "18  0.483739               -0.204009  0.139741    0.131338 -0.586040   \n",
       "19  0.483739               -0.340928  1.115547    1.115816  0.493612   \n",
       "20 -1.027946               -0.957064 -1.430428   -1.425883  0.071310   \n",
       "21  0.483739               -0.204009  0.685103    0.689476  0.403560   \n",
       "22 -1.027946                0.754424  0.231754    0.227235  2.526465   \n",
       "23 -2.539631                0.514815  0.534514    0.532129  0.197014   \n",
       "24  0.483739                0.412126  1.233195    1.220136  0.199072   \n",
       "\n",
       "      HR_min  RMSSD_min   SCL_min    HR_std  RMSSD_std   SCL_std  \n",
       "0  -0.319298  -0.294909 -1.004249  0.465365   0.439151 -0.949413  \n",
       "1  -0.295646  -0.294888 -0.971799 -1.217241  -1.219896 -1.822954  \n",
       "2  -0.264111  -0.294924 -0.312573  0.186334   0.249442  0.335381  \n",
       "3  -0.319298  -0.294784 -0.823730 -0.424820  -0.457861  1.405055  \n",
       "4  -0.307472  -0.294853 -0.768211  0.820475   0.816977  0.149815  \n",
       "5  -0.323240  -0.294865 -0.861158  0.741422   0.707874 -1.239744  \n",
       "6  -0.323240  -0.294906  0.021783  0.272457   0.269073  0.687546  \n",
       "7   3.390077   3.391165  1.674636 -2.793593  -2.803529  1.016631  \n",
       "8  -0.323240  -0.294842  3.584380  0.682556   0.662209 -0.046574  \n",
       "9  -0.315356  -0.294852 -0.630076  0.715259   0.695735  0.187333  \n",
       "10  3.390077   3.391165 -0.612576 -2.793593  -2.803529  1.630332  \n",
       "11 -0.271995  -0.294909  0.019899  0.587485   0.613917  0.087347  \n",
       "12 -0.323240  -0.294896 -0.324033  0.869512   0.839041  0.197979  \n",
       "13 -0.268053  -0.294906 -0.464343  0.564936   0.592349 -0.989584  \n",
       "14 -0.319298  -0.294875  0.807973  0.680695   0.649216 -0.306543  \n",
       "15 -0.260169  -0.294857 -1.066454 -0.959707  -0.945663 -0.779952  \n",
       "16 -0.315356  -0.294917  0.941927 -0.215431  -0.218956 -1.304895  \n",
       "17 -0.283820  -0.294917  0.570615  0.277847   0.291028 -1.347435  \n",
       "18 -0.268053  -0.294862 -0.619700  0.791563   0.815099  0.246811  \n",
       "19 -0.287762  -0.294851  0.227648 -0.156073  -0.163981  1.254304  \n",
       "20 -0.295646  -0.294842  0.436899 -0.487757  -0.490870 -1.322268  \n",
       "21 -0.303530  -0.294899  0.372228  0.478256   0.463279  1.225381  \n",
       "22 -0.287762  -0.294896  0.171827  0.769992   0.784510 -0.511894  \n",
       "23 -0.283820  -0.294931  0.218670  0.598568   0.606903  0.852896  \n",
       "24 -0.220749  -0.294947 -0.589581 -0.454509  -0.391519  1.344447  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swell_all_grouped = swell_extra.join(swell_mean.set_index('id'), on='id')\n",
    "swell_all_grouped = swell_all_grouped.join(swell_min.set_index('id'), on='id')\n",
    "swell_all_grouped = swell_all_grouped.join(swell_std.set_index('id'), on='id')\n",
    "swell_all_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a6c375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep user IDs in a separate datarame\n",
    "\n",
    "ids = swell_all_grouped['id']\n",
    "swell_all_grouped.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9237da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Dominant hand</th>\n",
       "      <th>coffee</th>\n",
       "      <th>physical</th>\n",
       "      <th>stress</th>\n",
       "      <th>Internal control index</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>RMSSD_mean</th>\n",
       "      <th>SCL_mean</th>\n",
       "      <th>HR_min</th>\n",
       "      <th>SCL_min</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>RMSSD_std</th>\n",
       "      <th>SCL_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.615457</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-2.531632</td>\n",
       "      <td>-0.900394</td>\n",
       "      <td>-0.876707</td>\n",
       "      <td>-1.222782</td>\n",
       "      <td>-0.319298</td>\n",
       "      <td>-1.004249</td>\n",
       "      <td>0.465365</td>\n",
       "      <td>0.439151</td>\n",
       "      <td>-0.949413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-0.231695</td>\n",
       "      <td>-0.272103</td>\n",
       "      <td>1.986695</td>\n",
       "      <td>-1.646229</td>\n",
       "      <td>-1.646932</td>\n",
       "      <td>-1.230384</td>\n",
       "      <td>-0.295646</td>\n",
       "      <td>-0.971799</td>\n",
       "      <td>-1.217241</td>\n",
       "      <td>-1.219896</td>\n",
       "      <td>-1.822954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>0.884367</td>\n",
       "      <td>0.867364</td>\n",
       "      <td>-0.548374</td>\n",
       "      <td>-0.264111</td>\n",
       "      <td>-0.312573</td>\n",
       "      <td>0.186334</td>\n",
       "      <td>0.249442</td>\n",
       "      <td>0.335381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>1.243265</td>\n",
       "      <td>1.247300</td>\n",
       "      <td>0.375704</td>\n",
       "      <td>-0.319298</td>\n",
       "      <td>-0.823730</td>\n",
       "      <td>-0.424820</td>\n",
       "      <td>-0.457861</td>\n",
       "      <td>1.405055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.307729</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.138288</td>\n",
       "      <td>0.122998</td>\n",
       "      <td>0.126018</td>\n",
       "      <td>-0.862919</td>\n",
       "      <td>-0.307472</td>\n",
       "      <td>-0.768211</td>\n",
       "      <td>0.820475</td>\n",
       "      <td>0.816977</td>\n",
       "      <td>0.149815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.651734</td>\n",
       "      <td>-0.575141</td>\n",
       "      <td>-0.550986</td>\n",
       "      <td>-1.113142</td>\n",
       "      <td>-0.323240</td>\n",
       "      <td>-0.861158</td>\n",
       "      <td>0.741422</td>\n",
       "      <td>0.707874</td>\n",
       "      <td>-1.239744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.101320</td>\n",
       "      <td>-1.023418</td>\n",
       "      <td>-1.017587</td>\n",
       "      <td>-0.207807</td>\n",
       "      <td>-0.323240</td>\n",
       "      <td>0.021783</td>\n",
       "      <td>0.272457</td>\n",
       "      <td>0.269073</td>\n",
       "      <td>0.687546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.615457</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-0.758273</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.340928</td>\n",
       "      <td>1.656421</td>\n",
       "      <td>1.654327</td>\n",
       "      <td>1.227664</td>\n",
       "      <td>3.390077</td>\n",
       "      <td>1.674636</td>\n",
       "      <td>-2.793593</td>\n",
       "      <td>-2.803529</td>\n",
       "      <td>1.016631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-1.436280</td>\n",
       "      <td>0.450904</td>\n",
       "      <td>0.458573</td>\n",
       "      <td>2.375174</td>\n",
       "      <td>-0.323240</td>\n",
       "      <td>3.584380</td>\n",
       "      <td>0.682556</td>\n",
       "      <td>0.662209</td>\n",
       "      <td>-0.046574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.957064</td>\n",
       "      <td>0.398311</td>\n",
       "      <td>0.406084</td>\n",
       "      <td>-0.785393</td>\n",
       "      <td>-0.315356</td>\n",
       "      <td>-0.630076</td>\n",
       "      <td>0.715259</td>\n",
       "      <td>0.695735</td>\n",
       "      <td>0.187333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.230915</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.138288</td>\n",
       "      <td>1.656421</td>\n",
       "      <td>1.654327</td>\n",
       "      <td>1.399914</td>\n",
       "      <td>3.390077</td>\n",
       "      <td>-0.612576</td>\n",
       "      <td>-2.793593</td>\n",
       "      <td>-2.803529</td>\n",
       "      <td>1.630332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.307729</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.754424</td>\n",
       "      <td>-0.673160</td>\n",
       "      <td>-0.688286</td>\n",
       "      <td>0.543763</td>\n",
       "      <td>-0.271995</td>\n",
       "      <td>0.019899</td>\n",
       "      <td>0.587485</td>\n",
       "      <td>0.613917</td>\n",
       "      <td>0.087347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>1.849776</td>\n",
       "      <td>-0.015454</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>-0.446554</td>\n",
       "      <td>-0.323240</td>\n",
       "      <td>-0.324033</td>\n",
       "      <td>0.869512</td>\n",
       "      <td>0.839041</td>\n",
       "      <td>0.197979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>-3.391165</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.580537</td>\n",
       "      <td>-0.699088</td>\n",
       "      <td>-0.715220</td>\n",
       "      <td>-0.889533</td>\n",
       "      <td>-0.268053</td>\n",
       "      <td>-0.464343</td>\n",
       "      <td>0.564936</td>\n",
       "      <td>0.592349</td>\n",
       "      <td>-0.989584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.811430</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>1.986695</td>\n",
       "      <td>0.466006</td>\n",
       "      <td>0.477363</td>\n",
       "      <td>-0.097784</td>\n",
       "      <td>-0.319298</td>\n",
       "      <td>0.807973</td>\n",
       "      <td>0.680695</td>\n",
       "      <td>0.649216</td>\n",
       "      <td>-0.306543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615457</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-3.295474</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>-1.548942</td>\n",
       "      <td>-1.576753</td>\n",
       "      <td>-0.942372</td>\n",
       "      <td>-0.260169</td>\n",
       "      <td>-1.066454</td>\n",
       "      <td>-0.959707</td>\n",
       "      <td>-0.945663</td>\n",
       "      <td>-0.779952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.000473</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-1.004188</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>-0.717456</td>\n",
       "      <td>-1.312910</td>\n",
       "      <td>-1.307585</td>\n",
       "      <td>-0.372496</td>\n",
       "      <td>-0.315356</td>\n",
       "      <td>0.941927</td>\n",
       "      <td>-0.215431</td>\n",
       "      <td>-0.218956</td>\n",
       "      <td>-1.304895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>-3.391165</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>-0.993383</td>\n",
       "      <td>-1.001877</td>\n",
       "      <td>-0.507674</td>\n",
       "      <td>-0.283820</td>\n",
       "      <td>0.570615</td>\n",
       "      <td>0.277847</td>\n",
       "      <td>0.291028</td>\n",
       "      <td>-1.347435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.615457</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>0.139741</td>\n",
       "      <td>0.131338</td>\n",
       "      <td>-0.586040</td>\n",
       "      <td>-0.268053</td>\n",
       "      <td>-0.619700</td>\n",
       "      <td>0.791563</td>\n",
       "      <td>0.815099</td>\n",
       "      <td>0.246811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.230915</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.340928</td>\n",
       "      <td>1.115547</td>\n",
       "      <td>1.115816</td>\n",
       "      <td>0.493612</td>\n",
       "      <td>-0.287762</td>\n",
       "      <td>0.227648</td>\n",
       "      <td>-0.156073</td>\n",
       "      <td>-0.163981</td>\n",
       "      <td>1.254304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.923186</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>-0.138509</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>-4.898979</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>-0.957064</td>\n",
       "      <td>-1.430428</td>\n",
       "      <td>-1.425883</td>\n",
       "      <td>0.071310</td>\n",
       "      <td>-0.295646</td>\n",
       "      <td>0.436899</td>\n",
       "      <td>-0.487757</td>\n",
       "      <td>-0.490870</td>\n",
       "      <td>-1.322268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.821462</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>0.685103</td>\n",
       "      <td>0.689476</td>\n",
       "      <td>0.403560</td>\n",
       "      <td>-0.303530</td>\n",
       "      <td>0.372228</td>\n",
       "      <td>0.478256</td>\n",
       "      <td>0.463279</td>\n",
       "      <td>1.225381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.457738</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-1.027946</td>\n",
       "      <td>0.754424</td>\n",
       "      <td>0.231754</td>\n",
       "      <td>0.227235</td>\n",
       "      <td>2.526465</td>\n",
       "      <td>-0.287762</td>\n",
       "      <td>0.171827</td>\n",
       "      <td>0.769992</td>\n",
       "      <td>0.784510</td>\n",
       "      <td>-0.511894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.923186</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>-2.539631</td>\n",
       "      <td>0.514815</td>\n",
       "      <td>0.534514</td>\n",
       "      <td>0.532129</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>-0.283820</td>\n",
       "      <td>0.218670</td>\n",
       "      <td>0.598568</td>\n",
       "      <td>0.606903</td>\n",
       "      <td>0.852896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.307729</td>\n",
       "      <td>0.685994</td>\n",
       "      <td>-1.869867</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>-1.284851</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.412126</td>\n",
       "      <td>1.233195</td>\n",
       "      <td>1.220136</td>\n",
       "      <td>0.199072</td>\n",
       "      <td>-0.220749</td>\n",
       "      <td>-0.589581</td>\n",
       "      <td>-0.454509</td>\n",
       "      <td>-0.391519</td>\n",
       "      <td>1.344447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age    Gender  Occupation  Dominant hand    coffee  physical  \\\n",
       "0   0.615457  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "1   0.000000  0.685994    0.727171       0.294884  0.204124 -0.231695   \n",
       "2  -0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "3  -0.307729  0.685994    0.727171       0.294884  0.204124 -1.284851   \n",
       "4  -0.307729 -1.457738    0.727171       0.294884  0.204124  0.821462   \n",
       "5  -0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "6  -0.923186  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "7   0.615457  0.685994   -1.869867       0.294884  0.204124 -0.758273   \n",
       "8   0.923186  0.685994   -1.004188       0.294884  0.204124 -1.284851   \n",
       "9   0.000000  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "10 -1.230915  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "11 -0.307729 -1.457738    0.727171       0.294884  0.204124  0.821462   \n",
       "12  0.307729  0.685994   -1.004188       0.294884  0.204124  0.821462   \n",
       "13  0.307729  0.685994    0.727171      -3.391165  0.204124 -1.284851   \n",
       "14 -0.307729  0.685994    0.727171       0.294884  0.204124 -1.811430   \n",
       "15  0.615457 -1.457738    0.727171       0.294884  0.204124 -1.284851   \n",
       "16  4.000473 -1.457738   -1.004188       0.294884  0.204124  0.821462   \n",
       "17 -0.307729  0.685994   -1.869867      -3.391165  0.204124  0.821462   \n",
       "18 -0.615457 -1.457738   -0.138509       0.294884  0.204124  0.821462   \n",
       "19 -1.230915 -1.457738   -0.138509       0.294884  0.204124  0.821462   \n",
       "20 -0.923186 -1.457738   -0.138509       0.294884 -4.898979  0.294884   \n",
       "21  0.307729  0.685994    0.727171       0.294884  0.204124  0.821462   \n",
       "22  0.000000 -1.457738    0.727171       0.294884  0.204124 -1.284851   \n",
       "23 -0.923186  0.685994   -1.869867       0.294884  0.204124 -1.284851   \n",
       "24  0.307729  0.685994   -1.869867       0.294884  0.204124 -1.284851   \n",
       "\n",
       "      stress  Internal control index   HR_mean  RMSSD_mean  SCL_mean  \\\n",
       "0   0.483739               -2.531632 -0.900394   -0.876707 -1.222782   \n",
       "1  -0.272103                1.986695 -1.646229   -1.646932 -1.230384   \n",
       "2   0.483739               -0.204009  0.884367    0.867364 -0.548374   \n",
       "3   0.483739               -0.204009  1.243265    1.247300  0.375704   \n",
       "4   0.483739                0.138288  0.122998    0.126018 -0.862919   \n",
       "5   0.483739                0.651734 -0.575141   -0.550986 -1.113142   \n",
       "6   0.483739               -0.101320 -1.023418   -1.017587 -0.207807   \n",
       "7   0.483739               -0.340928  1.656421    1.654327  1.227664   \n",
       "8   0.483739               -1.436280  0.450904    0.458573  2.375174   \n",
       "9   0.483739               -0.957064  0.398311    0.406084 -0.785393   \n",
       "10  0.483739                0.138288  1.656421    1.654327  1.399914   \n",
       "11  0.483739                0.754424 -0.673160   -0.688286  0.543763   \n",
       "12  0.483739                1.849776 -0.015454    0.000329 -0.446554   \n",
       "13  0.483739               -0.580537 -0.699088   -0.715220 -0.889533   \n",
       "14  0.483739                1.986695  0.466006    0.477363 -0.097784   \n",
       "15 -3.295474               -0.204009 -1.548942   -1.576753 -0.942372   \n",
       "16 -1.027946               -0.717456 -1.312910   -1.307585 -0.372496   \n",
       "17  0.483739               -0.204009 -0.993383   -1.001877 -0.507674   \n",
       "18  0.483739               -0.204009  0.139741    0.131338 -0.586040   \n",
       "19  0.483739               -0.340928  1.115547    1.115816  0.493612   \n",
       "20 -1.027946               -0.957064 -1.430428   -1.425883  0.071310   \n",
       "21  0.483739               -0.204009  0.685103    0.689476  0.403560   \n",
       "22 -1.027946                0.754424  0.231754    0.227235  2.526465   \n",
       "23 -2.539631                0.514815  0.534514    0.532129  0.197014   \n",
       "24  0.483739                0.412126  1.233195    1.220136  0.199072   \n",
       "\n",
       "      HR_min   SCL_min    HR_std  RMSSD_std   SCL_std  \n",
       "0  -0.319298 -1.004249  0.465365   0.439151 -0.949413  \n",
       "1  -0.295646 -0.971799 -1.217241  -1.219896 -1.822954  \n",
       "2  -0.264111 -0.312573  0.186334   0.249442  0.335381  \n",
       "3  -0.319298 -0.823730 -0.424820  -0.457861  1.405055  \n",
       "4  -0.307472 -0.768211  0.820475   0.816977  0.149815  \n",
       "5  -0.323240 -0.861158  0.741422   0.707874 -1.239744  \n",
       "6  -0.323240  0.021783  0.272457   0.269073  0.687546  \n",
       "7   3.390077  1.674636 -2.793593  -2.803529  1.016631  \n",
       "8  -0.323240  3.584380  0.682556   0.662209 -0.046574  \n",
       "9  -0.315356 -0.630076  0.715259   0.695735  0.187333  \n",
       "10  3.390077 -0.612576 -2.793593  -2.803529  1.630332  \n",
       "11 -0.271995  0.019899  0.587485   0.613917  0.087347  \n",
       "12 -0.323240 -0.324033  0.869512   0.839041  0.197979  \n",
       "13 -0.268053 -0.464343  0.564936   0.592349 -0.989584  \n",
       "14 -0.319298  0.807973  0.680695   0.649216 -0.306543  \n",
       "15 -0.260169 -1.066454 -0.959707  -0.945663 -0.779952  \n",
       "16 -0.315356  0.941927 -0.215431  -0.218956 -1.304895  \n",
       "17 -0.283820  0.570615  0.277847   0.291028 -1.347435  \n",
       "18 -0.268053 -0.619700  0.791563   0.815099  0.246811  \n",
       "19 -0.287762  0.227648 -0.156073  -0.163981  1.254304  \n",
       "20 -0.295646  0.436899 -0.487757  -0.490870 -1.322268  \n",
       "21 -0.303530  0.372228  0.478256   0.463279  1.225381  \n",
       "22 -0.287762  0.171827  0.769992   0.784510 -0.511894  \n",
       "23 -0.283820  0.218670  0.598568   0.606903  0.852896  \n",
       "24 -0.220749 -0.589581 -0.454509  -0.391519  1.344447  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swell_all_grouped.drop(['RMSSD_min'], axis=1, inplace=True)\n",
    "swell_all_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0346528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering(swell_all_grouped, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette(swell_all_grouped, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be855ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 11, max_iter = 500, random_state = 0)\n",
    "y = kmeans.fit_predict(swell_all_grouped)\n",
    "y = pd.DataFrame(y, columns=[\"Cluster\"])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6652ed",
   "metadata": {},
   "source": [
    "####  Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ad9cfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x28ddcdf20d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "kmeans = KMeans(n_clusters = 8, max_iter = 500, random_state = 0)\n",
    "model = kmeans.fit(swell_all_grouped)\n",
    "tsne = TSNE().fit_transform(swell_all_grouped)\n",
    "plt.scatter(x = tsne[:, 0], y = tsne[:, 1], c=model.labels_, cmap='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ffe04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters = pd.concat([ids, y], axis=1)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_grouped_all = swell.join(clusters.set_index('id'), on='id')\n",
    "swell_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99439a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "swell_grouped_all.to_csv(\"Final_CSVs/swell_clusters_all11.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93374ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'dataset' to run pycaret tests based on \"Cluster\".\n",
    "\n",
    "swell_grouped_all = swell_grouped_all.drop('dataset', axis = 1)\n",
    "swell_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_participants = swell_grouped_all[\"Cluster\"].unique()\n",
    "all_group = swell_grouped_all.groupby('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c74895",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1scores = []\n",
    "\n",
    "for participant in unique_participants:\n",
    "    print(\"Participant: \",participant)\n",
    "    part_df = all_group.get_group(participant)\n",
    "    grid = setup(data=part_df, target='stress', html=False, silent=True, verbose=False) #fix_imbalance = True,\n",
    "    best = compare_models()\n",
    "    accuracies.append(pull()['Accuracy'][0])\n",
    "    precision.append(pull()['Prec.'][0])\n",
    "    recall.append(pull()['Recall'][0])\n",
    "    f1scores.append(pull()['F1'][0])\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bda6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = statistics.mean(accuracies)\n",
    "mean_prec = statistics.mean(precision)\n",
    "mean_rec = statistics.mean(recall)\n",
    "mean_f1 = statistics.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e768234",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Accuracy SWELL- Cluster All Features: \",mean_acc)\n",
    "print(\"Mean Precision SWELL- Cluster All Features: \",mean_f1)\n",
    "print(\"Mean Recall SWELL- Cluster All Features: \",mean_rec)\n",
    "print(\"Mean F1-score SWELL- Cluster All Features: \",mean_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a9378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a1ae2ed",
   "metadata": {},
   "source": [
    "# Lifesnaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4879c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesnaps = pd.read_csv(\"Final_CSVs/lifesnaps_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cd0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lifesnaps_breq = pd.read_csv('../scored_surveys/breq.csv')\n",
    "lifesnaps_personality = pd.read_csv('../scored_surveys/personality.csv')\n",
    "#lifesnaps_ttm = pd.read_csv('../scored_surveys/ttm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1180c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesnaps_personality = lifesnaps_personality.drop([\"Unnamed: 0\", \"submitdate\"], axis = 1)\n",
    "lifesnaps_personality = lifesnaps_personality.drop(\"type\", axis = 1)\n",
    "lifesnaps_personality.rename(columns={\"user_id\": \"id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874c87d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>stability</th>\n",
       "      <th>intellect</th>\n",
       "      <th>gender</th>\n",
       "      <th>ipip_extraversion_category</th>\n",
       "      <th>ipip_agreeableness_category</th>\n",
       "      <th>ipip_conscientiousness_category</th>\n",
       "      <th>ipip_stability_category</th>\n",
       "      <th>ipip_intellect_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>621e2e8e67b776a24055b564</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>621e2eaf67b776a2406b14ac</td>\n",
       "      <td>32.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621e2ed667b776a24085d8d1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>621e2f3967b776a240c654db</td>\n",
       "      <td>25.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>621e2f6167b776a240e082a9</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>621e2f7a67b776a240f14425</td>\n",
       "      <td>29.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>621e2f9167b776a240011ccb</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>621e2fb367b776a24015accd</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>621e2fce67b776a240279baa</td>\n",
       "      <td>34.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>621e2ff067b776a2403eb737</td>\n",
       "      <td>33.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>621e301367b776a24057738e</td>\n",
       "      <td>26.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>621e301e67b776a240608a72</td>\n",
       "      <td>35.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>621e30c867b776a240d4aa6c</td>\n",
       "      <td>34.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>621e30e267b776a240e5bf90</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>621e30e467b776a240e817c7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>621e30f467b776a240f22944</td>\n",
       "      <td>28.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>621e310d67b776a24003096d</td>\n",
       "      <td>43.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>621e312a67b776a240164d59</td>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>621e314867b776a24029ebf9</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>621e323667b776a240f19134</td>\n",
       "      <td>39.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>621e324e67b776a2400191cb</td>\n",
       "      <td>32.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>621e328667b776a240281372</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>621e329067b776a2402ffad2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>621e32d067b776a2405b7d54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>621e331067b776a24085dd3f</td>\n",
       "      <td>19.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>621e332267b776a24092a584</td>\n",
       "      <td>22.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>621e333567b776a240a0c217</td>\n",
       "      <td>30.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>621e333967b776a240a3cd06</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>621e335a67b776a240bb12ff</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>621e337667b776a240ce78ab</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>621e339967b776a240e502de</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>621e33b067b776a240f39e56</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>621e33cf67b776a240087de9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>621e33ed67b776a2401cf5f7</td>\n",
       "      <td>39.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>621e341067b776a24037b105</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>621e346f67b776a24081744f</td>\n",
       "      <td>35.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>621e34db67b776a240c9c2be</td>\n",
       "      <td>28.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>621e34ec67b776a240d60873</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>621e34f767b776a240de4e1a</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>621e356967b776a24027bd9f</td>\n",
       "      <td>26.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>621e362467b776a2404ad513</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>621e366567b776a24076a727</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>621e367e67b776a24087d75d</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>621e36bb67b776a240b40d64</td>\n",
       "      <td>41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>621e36c267b776a240ba2756</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>621e36dd67b776a240ce9a45</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>621e36f967b776a240e5e7c9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>621e375b67b776a240290cdc</td>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  extraversion  agreeableness  conscientiousness  \\\n",
       "0   621e2e8e67b776a24055b564          21.0           33.0               45.0   \n",
       "1   621e2eaf67b776a2406b14ac          32.0           45.0               30.0   \n",
       "2   621e2ed667b776a24085d8d1          40.0           43.0               22.0   \n",
       "3   621e2f3967b776a240c654db          25.0           34.0               30.0   \n",
       "4   621e2f6167b776a240e082a9          41.0           41.0               30.0   \n",
       "5   621e2f7a67b776a240f14425          29.0           38.0               43.0   \n",
       "6   621e2f9167b776a240011ccb          21.0           45.0               31.0   \n",
       "7   621e2fb367b776a24015accd          37.0           36.0               29.0   \n",
       "8   621e2fce67b776a240279baa          34.0           42.0               35.0   \n",
       "9   621e2ff067b776a2403eb737          33.0           44.0               23.0   \n",
       "10  621e301367b776a24057738e          26.0           36.0               31.0   \n",
       "11  621e301e67b776a240608a72          35.0           44.0               40.0   \n",
       "12  621e30c867b776a240d4aa6c          34.0           40.0               26.0   \n",
       "13  621e30e267b776a240e5bf90          38.0           42.0               27.0   \n",
       "14  621e30e467b776a240e817c7          34.0           20.0               22.0   \n",
       "15  621e30f467b776a240f22944          28.0           40.0               34.0   \n",
       "16  621e310d67b776a24003096d          43.0           48.0               31.0   \n",
       "17  621e312a67b776a240164d59          25.0           33.0               40.0   \n",
       "18  621e314867b776a24029ebf9          20.0           40.0               39.0   \n",
       "19  621e323667b776a240f19134          39.0           44.0               32.0   \n",
       "20  621e324e67b776a2400191cb          32.0           39.0               41.0   \n",
       "21  621e328667b776a240281372          37.0           39.0               39.0   \n",
       "22  621e329067b776a2402ffad2          29.0           47.0               39.0   \n",
       "23  621e32af67b776a24045b4cf          18.0           36.0               46.0   \n",
       "24  621e32d067b776a2405b7d54          21.0           36.0               27.0   \n",
       "25  621e32d967b776a240627414          34.0           34.0               39.0   \n",
       "26  621e331067b776a24085dd3f          19.0           40.0               41.0   \n",
       "27  621e332267b776a24092a584          22.0           32.0               26.0   \n",
       "28  621e333567b776a240a0c217          30.0           41.0               36.0   \n",
       "29  621e333967b776a240a3cd06          22.0           37.0               39.0   \n",
       "30  621e335a67b776a240bb12ff          21.0           29.0               39.0   \n",
       "31  621e337667b776a240ce78ab          22.0           30.0               24.0   \n",
       "32  621e339967b776a240e502de          35.0           45.0               29.0   \n",
       "33  621e33b067b776a240f39e56          45.0           49.0               41.0   \n",
       "34  621e33cf67b776a240087de9          16.0           31.0               41.0   \n",
       "35  621e33ed67b776a2401cf5f7          39.0           48.0               35.0   \n",
       "36  621e341067b776a24037b105          19.0           30.0               25.0   \n",
       "37  621e346f67b776a24081744f          35.0           44.0               21.0   \n",
       "38  621e34db67b776a240c9c2be          28.0           45.0               35.0   \n",
       "39  621e34ec67b776a240d60873          31.0           29.0               25.0   \n",
       "40  621e34f767b776a240de4e1a          40.0           42.0               43.0   \n",
       "41  621e356967b776a24027bd9f          26.0           38.0               41.0   \n",
       "42  621e362467b776a2404ad513          19.0           36.0               28.0   \n",
       "43  621e366567b776a24076a727          30.0           36.0               41.0   \n",
       "44  621e367e67b776a24087d75d          13.0           26.0               28.0   \n",
       "45  621e36bb67b776a240b40d64          41.0           29.0               33.0   \n",
       "46  621e36c267b776a240ba2756          39.0           36.0               23.0   \n",
       "47  621e36dd67b776a240ce9a45          33.0           34.0               36.0   \n",
       "48  621e36f967b776a240e5e7c9          42.0           42.0               38.0   \n",
       "49  621e375b67b776a240290cdc          34.0           46.0               33.0   \n",
       "\n",
       "    stability  intellect  gender ipip_extraversion_category  \\\n",
       "0        42.0       40.0    MALE                        LOW   \n",
       "1        18.0       41.0  FEMALE                    AVERAGE   \n",
       "2        28.0       34.0  FEMALE                       HIGH   \n",
       "3        39.0       37.0    MALE                    AVERAGE   \n",
       "4        34.0       30.0  FEMALE                       HIGH   \n",
       "5        39.0       35.0    MALE                    AVERAGE   \n",
       "6        18.0       39.0  FEMALE                        LOW   \n",
       "7        27.0       41.0    MALE                       HIGH   \n",
       "8        27.0       35.0    MALE                       HIGH   \n",
       "9        11.0       30.0  FEMALE                    AVERAGE   \n",
       "10       39.0       36.0    MALE                    AVERAGE   \n",
       "11       19.0       35.0  FEMALE                    AVERAGE   \n",
       "12       24.0       39.0  FEMALE                    AVERAGE   \n",
       "13       33.0       35.0    MALE                       HIGH   \n",
       "14       13.0       27.0    MALE                       HIGH   \n",
       "15       29.0       36.0    MALE                    AVERAGE   \n",
       "16       30.0       41.0  FEMALE                       HIGH   \n",
       "17       30.0       33.0    MALE                    AVERAGE   \n",
       "18       42.0       35.0    MALE                        LOW   \n",
       "19       33.0       38.0    MALE                       HIGH   \n",
       "20       46.0       37.0    MALE                    AVERAGE   \n",
       "21       27.0       30.0    MALE                       HIGH   \n",
       "22       31.0       40.0    MALE                    AVERAGE   \n",
       "23       18.0       43.0    MALE                        LOW   \n",
       "24       33.0       33.0  FEMALE                        LOW   \n",
       "25       33.0       46.0    MALE                       HIGH   \n",
       "26       25.0       35.0  FEMALE                        LOW   \n",
       "27       31.0       46.0    MALE                        LOW   \n",
       "28       24.0       34.0    MALE                    AVERAGE   \n",
       "29       24.0       48.0    MALE                        LOW   \n",
       "30       31.0       28.0  FEMALE                        LOW   \n",
       "31       24.0       46.0    MALE                        LOW   \n",
       "32       44.0       35.0  FEMALE                    AVERAGE   \n",
       "33       36.0       45.0    MALE                       HIGH   \n",
       "34       37.0       28.0    MALE                        LOW   \n",
       "35       42.0       44.0    MALE                       HIGH   \n",
       "36       21.0       30.0    MALE                        LOW   \n",
       "37       33.0       35.0  FEMALE                    AVERAGE   \n",
       "38       36.0       41.0  FEMALE                    AVERAGE   \n",
       "39       30.0       30.0    MALE                    AVERAGE   \n",
       "40       32.0       37.0  FEMALE                       HIGH   \n",
       "41       25.0       39.0  FEMALE                        LOW   \n",
       "42       35.0       35.0  FEMALE                        LOW   \n",
       "43       33.0       37.0    MALE                    AVERAGE   \n",
       "44       28.0       34.0    MALE                        LOW   \n",
       "45       17.0       30.0  FEMALE                       HIGH   \n",
       "46       40.0       28.0    MALE                       HIGH   \n",
       "47       32.0       38.0    MALE                       HIGH   \n",
       "48       31.0       41.0  FEMALE                       HIGH   \n",
       "49       18.0       40.0  FEMALE                    AVERAGE   \n",
       "\n",
       "   ipip_agreeableness_category ipip_conscientiousness_category  \\\n",
       "0                          LOW                            HIGH   \n",
       "1                         HIGH                         AVERAGE   \n",
       "2                      AVERAGE                             LOW   \n",
       "3                      AVERAGE                             LOW   \n",
       "4                      AVERAGE                         AVERAGE   \n",
       "5                      AVERAGE                            HIGH   \n",
       "6                         HIGH                         AVERAGE   \n",
       "7                      AVERAGE                             LOW   \n",
       "8                         HIGH                         AVERAGE   \n",
       "9                         HIGH                             LOW   \n",
       "10                     AVERAGE                         AVERAGE   \n",
       "11                        HIGH                            HIGH   \n",
       "12                     AVERAGE                             LOW   \n",
       "13                        HIGH                             LOW   \n",
       "14                         LOW                             LOW   \n",
       "15                        HIGH                         AVERAGE   \n",
       "16                        HIGH                         AVERAGE   \n",
       "17                         LOW                            HIGH   \n",
       "18                        HIGH                            HIGH   \n",
       "19                        HIGH                         AVERAGE   \n",
       "20                     AVERAGE                            HIGH   \n",
       "21                     AVERAGE                            HIGH   \n",
       "22                        HIGH                            HIGH   \n",
       "23                     AVERAGE                            HIGH   \n",
       "24                         LOW                             LOW   \n",
       "25                     AVERAGE                            HIGH   \n",
       "26                     AVERAGE                            HIGH   \n",
       "27                         LOW                             LOW   \n",
       "28                        HIGH                         AVERAGE   \n",
       "29                     AVERAGE                            HIGH   \n",
       "30                         LOW                            HIGH   \n",
       "31                         LOW                             LOW   \n",
       "32                        HIGH                         AVERAGE   \n",
       "33                        HIGH                            HIGH   \n",
       "34                         LOW                            HIGH   \n",
       "35                        HIGH                         AVERAGE   \n",
       "36                         LOW                             LOW   \n",
       "37                        HIGH                             LOW   \n",
       "38                        HIGH                         AVERAGE   \n",
       "39                         LOW                             LOW   \n",
       "40                     AVERAGE                            HIGH   \n",
       "41                         LOW                            HIGH   \n",
       "42                         LOW                             LOW   \n",
       "43                     AVERAGE                            HIGH   \n",
       "44                         LOW                             LOW   \n",
       "45                         LOW                         AVERAGE   \n",
       "46                     AVERAGE                             LOW   \n",
       "47                     AVERAGE                         AVERAGE   \n",
       "48                     AVERAGE                            HIGH   \n",
       "49                        HIGH                         AVERAGE   \n",
       "\n",
       "   ipip_stability_category ipip_intellect_category  \n",
       "0                     HIGH                 AVERAGE  \n",
       "1                      LOW                    HIGH  \n",
       "2                  AVERAGE                 AVERAGE  \n",
       "3                     HIGH                 AVERAGE  \n",
       "4                     HIGH                     LOW  \n",
       "5                     HIGH                 AVERAGE  \n",
       "6                      LOW                    HIGH  \n",
       "7                      LOW                    HIGH  \n",
       "8                      LOW                 AVERAGE  \n",
       "9                      LOW                     LOW  \n",
       "10                    HIGH                 AVERAGE  \n",
       "11                     LOW                 AVERAGE  \n",
       "12                 AVERAGE                    HIGH  \n",
       "13                 AVERAGE                 AVERAGE  \n",
       "14                     LOW                     LOW  \n",
       "15                 AVERAGE                 AVERAGE  \n",
       "16                 AVERAGE                    HIGH  \n",
       "17                 AVERAGE                     LOW  \n",
       "18                    HIGH                 AVERAGE  \n",
       "19                 AVERAGE                 AVERAGE  \n",
       "20                    HIGH                 AVERAGE  \n",
       "21                     LOW                     LOW  \n",
       "22                 AVERAGE                 AVERAGE  \n",
       "23                     LOW                    HIGH  \n",
       "24                    HIGH                     LOW  \n",
       "25                 AVERAGE                    HIGH  \n",
       "26                 AVERAGE                 AVERAGE  \n",
       "27                 AVERAGE                    HIGH  \n",
       "28                     LOW                     LOW  \n",
       "29                     LOW                    HIGH  \n",
       "30                 AVERAGE                     LOW  \n",
       "31                     LOW                    HIGH  \n",
       "32                    HIGH                 AVERAGE  \n",
       "33                    HIGH                    HIGH  \n",
       "34                    HIGH                     LOW  \n",
       "35                    HIGH                    HIGH  \n",
       "36                     LOW                     LOW  \n",
       "37                    HIGH                 AVERAGE  \n",
       "38                    HIGH                    HIGH  \n",
       "39                 AVERAGE                     LOW  \n",
       "40                    HIGH                 AVERAGE  \n",
       "41                 AVERAGE                    HIGH  \n",
       "42                    HIGH                 AVERAGE  \n",
       "43                 AVERAGE                 AVERAGE  \n",
       "44                 AVERAGE                     LOW  \n",
       "45                     LOW                     LOW  \n",
       "46                    HIGH                     LOW  \n",
       "47                 AVERAGE                 AVERAGE  \n",
       "48                 AVERAGE                    HIGH  \n",
       "49                     LOW                    HIGH  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03cfa5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "lifesnaps_personality['gender'] = le.fit_transform(lifesnaps_personality['gender'])\n",
    "lifesnaps_personality['ipip_extraversion_category'] = le.fit_transform(lifesnaps_personality['ipip_extraversion_category'])\n",
    "lifesnaps_personality['ipip_agreeableness_category'] = le.fit_transform(lifesnaps_personality['ipip_agreeableness_category'])\n",
    "lifesnaps_personality['ipip_conscientiousness_category'] = le.fit_transform(lifesnaps_personality['ipip_conscientiousness_category'])\n",
    "lifesnaps_personality['ipip_stability_category'] = le.fit_transform(lifesnaps_personality['ipip_stability_category'])\n",
    "lifesnaps_personality['ipip_intellect_category'] = le.fit_transform(lifesnaps_personality['ipip_intellect_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9577b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "lifesnaps_personality[[\"extraversion\", \"agreeableness\", \"conscientiousness\", \"stability\", \"intellect\", \"gender\"]] = scaler.fit_transform(lifesnaps_personality[[\"extraversion\", \"agreeableness\", \"conscientiousness\", \"stability\", \"intellect\", \"gender\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6390d",
   "metadata": {},
   "source": [
    "## Single-Attribute-Splitting (Personality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1a9a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = lifesnaps_personality[\"id\"]\n",
    "lifesnaps_personality = lifesnaps_personality.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d720bbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering(lifesnaps_personality, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9062124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "silhouette(lifesnaps_personality, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fffc8f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster\n",
       "3          10\n",
       "5           9\n",
       "1           8\n",
       "2           6\n",
       "0           4\n",
       "4           4\n",
       "7           3\n",
       "6           2\n",
       "8           2\n",
       "9           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 10, max_iter = 500, random_state = 0)\n",
    "y = kmeans.fit_predict(lifesnaps_personality)\n",
    "y = pd.DataFrame(y, columns=[\"Cluster\"])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00006922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=sklearn.decomposition.PCA(n_components = 2).fit_transform(lifesnaps_personality)\n",
    "#plt.scatter(x[:,0], x[:,1], c=model.labels_, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc230f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e185f80",
   "metadata": {},
   "source": [
    "####  Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "944518c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x14a11383df0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "kmeans = KMeans(n_clusters = 10, max_iter = 500, random_state = 0)\n",
    "model = kmeans.fit(lifesnaps_personality)\n",
    "tsne = TSNE().fit_transform(lifesnaps_personality)\n",
    "#cmap = ListedColormap(['r', 'g', 'b'])\n",
    "plt.scatter(x = tsne[:, 0], y = tsne[:, 1], c=model.labels_, cmap='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61a7d6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1b62ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.concat([pd.DataFrame(tsne, columns=['x','y']),pd.DataFrame(model.labels_, columns=['c'])], axis=1)\n",
    "mark = [\"o\", \"*\", \"P\", \"X\", \"s\", \"D\", \"^\", \"v\", \"<\", \">\"]\n",
    "sns.scatterplot(data=test, x=\"x\", y=\"y\",palette='Set1', hue=\"c\", style='c', legend=False, markers = mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32c86070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>621e2e8e67b776a24055b564</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>621e2eaf67b776a2406b14ac</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621e2ed667b776a24085d8d1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>621e2f3967b776a240c654db</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>621e2f6167b776a240e082a9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>621e2f7a67b776a240f14425</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>621e2f9167b776a240011ccb</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>621e2fb367b776a24015accd</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>621e2fce67b776a240279baa</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>621e2ff067b776a2403eb737</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>621e301367b776a24057738e</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>621e301e67b776a240608a72</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>621e30c867b776a240d4aa6c</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>621e30e267b776a240e5bf90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>621e30e467b776a240e817c7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>621e30f467b776a240f22944</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>621e310d67b776a24003096d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>621e312a67b776a240164d59</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>621e314867b776a24029ebf9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>621e323667b776a240f19134</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>621e324e67b776a2400191cb</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>621e328667b776a240281372</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>621e329067b776a2402ffad2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>621e32d067b776a2405b7d54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>621e331067b776a24085dd3f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>621e332267b776a24092a584</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>621e333567b776a240a0c217</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>621e333967b776a240a3cd06</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>621e335a67b776a240bb12ff</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>621e337667b776a240ce78ab</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>621e339967b776a240e502de</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>621e33b067b776a240f39e56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>621e33cf67b776a240087de9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>621e33ed67b776a2401cf5f7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>621e341067b776a24037b105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>621e346f67b776a24081744f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>621e34db67b776a240c9c2be</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>621e34ec67b776a240d60873</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>621e34f767b776a240de4e1a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>621e356967b776a24027bd9f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>621e362467b776a2404ad513</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>621e366567b776a24076a727</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>621e367e67b776a24087d75d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>621e36bb67b776a240b40d64</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>621e36c267b776a240ba2756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>621e36dd67b776a240ce9a45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>621e36f967b776a240e5e7c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>621e375b67b776a240290cdc</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  Cluster\n",
       "0   621e2e8e67b776a24055b564        8\n",
       "1   621e2eaf67b776a2406b14ac        5\n",
       "2   621e2ed667b776a24085d8d1        2\n",
       "3   621e2f3967b776a240c654db        3\n",
       "4   621e2f6167b776a240e082a9        1\n",
       "5   621e2f7a67b776a240f14425        3\n",
       "6   621e2f9167b776a240011ccb        5\n",
       "7   621e2fb367b776a24015accd        2\n",
       "8   621e2fce67b776a240279baa        5\n",
       "9   621e2ff067b776a2403eb737        5\n",
       "10  621e301367b776a24057738e        3\n",
       "11  621e301e67b776a240608a72        5\n",
       "12  621e30c867b776a240d4aa6c        2\n",
       "13  621e30e267b776a240e5bf90        2\n",
       "14  621e30e467b776a240e817c7        7\n",
       "15  621e30f467b776a240f22944        3\n",
       "16  621e310d67b776a24003096d        1\n",
       "17  621e312a67b776a240164d59        7\n",
       "18  621e314867b776a24029ebf9        8\n",
       "19  621e323667b776a240f19134        3\n",
       "20  621e324e67b776a2400191cb        3\n",
       "21  621e328667b776a240281372        5\n",
       "22  621e329067b776a2402ffad2        3\n",
       "23  621e32af67b776a24045b4cf        9\n",
       "24  621e32d067b776a2405b7d54        0\n",
       "25  621e32d967b776a240627414        3\n",
       "26  621e331067b776a24085dd3f        4\n",
       "27  621e332267b776a24092a584        6\n",
       "28  621e333567b776a240a0c217        5\n",
       "29  621e333967b776a240a3cd06        9\n",
       "30  621e335a67b776a240bb12ff        4\n",
       "31  621e337667b776a240ce78ab        6\n",
       "32  621e339967b776a240e502de        1\n",
       "33  621e33b067b776a240f39e56        1\n",
       "34  621e33cf67b776a240087de9        0\n",
       "35  621e33ed67b776a2401cf5f7        1\n",
       "36  621e341067b776a24037b105        0\n",
       "37  621e346f67b776a24081744f        2\n",
       "38  621e34db67b776a240c9c2be        1\n",
       "39  621e34ec67b776a240d60873        7\n",
       "40  621e34f767b776a240de4e1a        1\n",
       "41  621e356967b776a24027bd9f        4\n",
       "42  621e362467b776a2404ad513        4\n",
       "43  621e366567b776a24076a727        3\n",
       "44  621e367e67b776a24087d75d        0\n",
       "45  621e36bb67b776a240b40d64        5\n",
       "46  621e36c267b776a240ba2756        2\n",
       "47  621e36dd67b776a240ce9a45        3\n",
       "48  621e36f967b776a240e5e7c9        1\n",
       "49  621e375b67b776a240290cdc        5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = pd.concat([ids, y], axis=1)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66a43469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>stability</th>\n",
       "      <th>intellect</th>\n",
       "      <th>gender</th>\n",
       "      <th>ipip_extraversion_category</th>\n",
       "      <th>ipip_agreeableness_category</th>\n",
       "      <th>ipip_conscientiousness_category</th>\n",
       "      <th>ipip_stability_category</th>\n",
       "      <th>ipip_intellect_category</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>28.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>43.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>39.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>22.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>39.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>35.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>28.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>26.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LOW</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>AVERAGE</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    extraversion  agreeableness  conscientiousness  stability  intellect  \\\n",
       "0           21.0           33.0               45.0       42.0       40.0   \n",
       "1           32.0           45.0               30.0       18.0       41.0   \n",
       "2           40.0           43.0               22.0       28.0       34.0   \n",
       "3           25.0           34.0               30.0       39.0       37.0   \n",
       "4           41.0           41.0               30.0       34.0       30.0   \n",
       "5           29.0           38.0               43.0       39.0       35.0   \n",
       "6           21.0           45.0               31.0       18.0       39.0   \n",
       "7           37.0           36.0               29.0       27.0       41.0   \n",
       "8           34.0           42.0               35.0       27.0       35.0   \n",
       "9           33.0           44.0               23.0       11.0       30.0   \n",
       "10          26.0           36.0               31.0       39.0       36.0   \n",
       "11          35.0           44.0               40.0       19.0       35.0   \n",
       "12          34.0           40.0               26.0       24.0       39.0   \n",
       "13          38.0           42.0               27.0       33.0       35.0   \n",
       "14          34.0           20.0               22.0       13.0       27.0   \n",
       "15          28.0           40.0               34.0       29.0       36.0   \n",
       "16          43.0           48.0               31.0       30.0       41.0   \n",
       "17          25.0           33.0               40.0       30.0       33.0   \n",
       "18          20.0           40.0               39.0       42.0       35.0   \n",
       "19          39.0           44.0               32.0       33.0       38.0   \n",
       "20          32.0           39.0               41.0       46.0       37.0   \n",
       "21          37.0           39.0               39.0       27.0       30.0   \n",
       "22          29.0           47.0               39.0       31.0       40.0   \n",
       "23          18.0           36.0               46.0       18.0       43.0   \n",
       "24          21.0           36.0               27.0       33.0       33.0   \n",
       "25          34.0           34.0               39.0       33.0       46.0   \n",
       "26          19.0           40.0               41.0       25.0       35.0   \n",
       "27          22.0           32.0               26.0       31.0       46.0   \n",
       "28          30.0           41.0               36.0       24.0       34.0   \n",
       "29          22.0           37.0               39.0       24.0       48.0   \n",
       "30          21.0           29.0               39.0       31.0       28.0   \n",
       "31          22.0           30.0               24.0       24.0       46.0   \n",
       "32          35.0           45.0               29.0       44.0       35.0   \n",
       "33          45.0           49.0               41.0       36.0       45.0   \n",
       "34          16.0           31.0               41.0       37.0       28.0   \n",
       "35          39.0           48.0               35.0       42.0       44.0   \n",
       "36          19.0           30.0               25.0       21.0       30.0   \n",
       "37          35.0           44.0               21.0       33.0       35.0   \n",
       "38          28.0           45.0               35.0       36.0       41.0   \n",
       "39          31.0           29.0               25.0       30.0       30.0   \n",
       "40          40.0           42.0               43.0       32.0       37.0   \n",
       "41          26.0           38.0               41.0       25.0       39.0   \n",
       "42          19.0           36.0               28.0       35.0       35.0   \n",
       "43          30.0           36.0               41.0       33.0       37.0   \n",
       "44          13.0           26.0               28.0       28.0       34.0   \n",
       "45          41.0           29.0               33.0       17.0       30.0   \n",
       "46          39.0           36.0               23.0       40.0       28.0   \n",
       "47          33.0           34.0               36.0       32.0       38.0   \n",
       "48          42.0           42.0               38.0       31.0       41.0   \n",
       "49          34.0           46.0               33.0       18.0       40.0   \n",
       "\n",
       "    gender ipip_extraversion_category ipip_agreeableness_category  \\\n",
       "0     MALE                        LOW                         LOW   \n",
       "1   FEMALE                    AVERAGE                        HIGH   \n",
       "2   FEMALE                       HIGH                     AVERAGE   \n",
       "3     MALE                    AVERAGE                     AVERAGE   \n",
       "4   FEMALE                       HIGH                     AVERAGE   \n",
       "5     MALE                    AVERAGE                     AVERAGE   \n",
       "6   FEMALE                        LOW                        HIGH   \n",
       "7     MALE                       HIGH                     AVERAGE   \n",
       "8     MALE                       HIGH                        HIGH   \n",
       "9   FEMALE                    AVERAGE                        HIGH   \n",
       "10    MALE                    AVERAGE                     AVERAGE   \n",
       "11  FEMALE                    AVERAGE                        HIGH   \n",
       "12  FEMALE                    AVERAGE                     AVERAGE   \n",
       "13    MALE                       HIGH                        HIGH   \n",
       "14    MALE                       HIGH                         LOW   \n",
       "15    MALE                    AVERAGE                        HIGH   \n",
       "16  FEMALE                       HIGH                        HIGH   \n",
       "17    MALE                    AVERAGE                         LOW   \n",
       "18    MALE                        LOW                        HIGH   \n",
       "19    MALE                       HIGH                        HIGH   \n",
       "20    MALE                    AVERAGE                     AVERAGE   \n",
       "21    MALE                       HIGH                     AVERAGE   \n",
       "22    MALE                    AVERAGE                        HIGH   \n",
       "23    MALE                        LOW                     AVERAGE   \n",
       "24  FEMALE                        LOW                         LOW   \n",
       "25    MALE                       HIGH                     AVERAGE   \n",
       "26  FEMALE                        LOW                     AVERAGE   \n",
       "27    MALE                        LOW                         LOW   \n",
       "28    MALE                    AVERAGE                        HIGH   \n",
       "29    MALE                        LOW                     AVERAGE   \n",
       "30  FEMALE                        LOW                         LOW   \n",
       "31    MALE                        LOW                         LOW   \n",
       "32  FEMALE                    AVERAGE                        HIGH   \n",
       "33    MALE                       HIGH                        HIGH   \n",
       "34    MALE                        LOW                         LOW   \n",
       "35    MALE                       HIGH                        HIGH   \n",
       "36    MALE                        LOW                         LOW   \n",
       "37  FEMALE                    AVERAGE                        HIGH   \n",
       "38  FEMALE                    AVERAGE                        HIGH   \n",
       "39    MALE                    AVERAGE                         LOW   \n",
       "40  FEMALE                       HIGH                     AVERAGE   \n",
       "41  FEMALE                        LOW                         LOW   \n",
       "42  FEMALE                        LOW                         LOW   \n",
       "43    MALE                    AVERAGE                     AVERAGE   \n",
       "44    MALE                        LOW                         LOW   \n",
       "45  FEMALE                       HIGH                         LOW   \n",
       "46    MALE                       HIGH                     AVERAGE   \n",
       "47    MALE                       HIGH                     AVERAGE   \n",
       "48  FEMALE                       HIGH                     AVERAGE   \n",
       "49  FEMALE                    AVERAGE                        HIGH   \n",
       "\n",
       "   ipip_conscientiousness_category ipip_stability_category  \\\n",
       "0                             HIGH                    HIGH   \n",
       "1                          AVERAGE                     LOW   \n",
       "2                              LOW                 AVERAGE   \n",
       "3                              LOW                    HIGH   \n",
       "4                          AVERAGE                    HIGH   \n",
       "5                             HIGH                    HIGH   \n",
       "6                          AVERAGE                     LOW   \n",
       "7                              LOW                     LOW   \n",
       "8                          AVERAGE                     LOW   \n",
       "9                              LOW                     LOW   \n",
       "10                         AVERAGE                    HIGH   \n",
       "11                            HIGH                     LOW   \n",
       "12                             LOW                 AVERAGE   \n",
       "13                             LOW                 AVERAGE   \n",
       "14                             LOW                     LOW   \n",
       "15                         AVERAGE                 AVERAGE   \n",
       "16                         AVERAGE                 AVERAGE   \n",
       "17                            HIGH                 AVERAGE   \n",
       "18                            HIGH                    HIGH   \n",
       "19                         AVERAGE                 AVERAGE   \n",
       "20                            HIGH                    HIGH   \n",
       "21                            HIGH                     LOW   \n",
       "22                            HIGH                 AVERAGE   \n",
       "23                            HIGH                     LOW   \n",
       "24                             LOW                    HIGH   \n",
       "25                            HIGH                 AVERAGE   \n",
       "26                            HIGH                 AVERAGE   \n",
       "27                             LOW                 AVERAGE   \n",
       "28                         AVERAGE                     LOW   \n",
       "29                            HIGH                     LOW   \n",
       "30                            HIGH                 AVERAGE   \n",
       "31                             LOW                     LOW   \n",
       "32                         AVERAGE                    HIGH   \n",
       "33                            HIGH                    HIGH   \n",
       "34                            HIGH                    HIGH   \n",
       "35                         AVERAGE                    HIGH   \n",
       "36                             LOW                     LOW   \n",
       "37                             LOW                    HIGH   \n",
       "38                         AVERAGE                    HIGH   \n",
       "39                             LOW                 AVERAGE   \n",
       "40                            HIGH                    HIGH   \n",
       "41                            HIGH                 AVERAGE   \n",
       "42                             LOW                    HIGH   \n",
       "43                            HIGH                 AVERAGE   \n",
       "44                             LOW                 AVERAGE   \n",
       "45                         AVERAGE                     LOW   \n",
       "46                             LOW                    HIGH   \n",
       "47                         AVERAGE                 AVERAGE   \n",
       "48                            HIGH                 AVERAGE   \n",
       "49                         AVERAGE                     LOW   \n",
       "\n",
       "   ipip_intellect_category  Cluster  \n",
       "0                  AVERAGE        8  \n",
       "1                     HIGH        5  \n",
       "2                  AVERAGE        2  \n",
       "3                  AVERAGE        3  \n",
       "4                      LOW        1  \n",
       "5                  AVERAGE        3  \n",
       "6                     HIGH        5  \n",
       "7                     HIGH        2  \n",
       "8                  AVERAGE        5  \n",
       "9                      LOW        5  \n",
       "10                 AVERAGE        3  \n",
       "11                 AVERAGE        5  \n",
       "12                    HIGH        2  \n",
       "13                 AVERAGE        2  \n",
       "14                     LOW        7  \n",
       "15                 AVERAGE        3  \n",
       "16                    HIGH        1  \n",
       "17                     LOW        7  \n",
       "18                 AVERAGE        8  \n",
       "19                 AVERAGE        3  \n",
       "20                 AVERAGE        3  \n",
       "21                     LOW        5  \n",
       "22                 AVERAGE        3  \n",
       "23                    HIGH        9  \n",
       "24                     LOW        0  \n",
       "25                    HIGH        3  \n",
       "26                 AVERAGE        4  \n",
       "27                    HIGH        6  \n",
       "28                     LOW        5  \n",
       "29                    HIGH        9  \n",
       "30                     LOW        4  \n",
       "31                    HIGH        6  \n",
       "32                 AVERAGE        1  \n",
       "33                    HIGH        1  \n",
       "34                     LOW        0  \n",
       "35                    HIGH        1  \n",
       "36                     LOW        0  \n",
       "37                 AVERAGE        2  \n",
       "38                    HIGH        1  \n",
       "39                     LOW        7  \n",
       "40                 AVERAGE        1  \n",
       "41                    HIGH        4  \n",
       "42                 AVERAGE        4  \n",
       "43                 AVERAGE        3  \n",
       "44                     LOW        0  \n",
       "45                     LOW        5  \n",
       "46                     LOW        2  \n",
       "47                 AVERAGE        3  \n",
       "48                    HIGH        1  \n",
       "49                    HIGH        5  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics = pd.merge(lifesnaps_personality, clusters, on = \"id\")\n",
    "lifesnaps_statistics = lifesnaps_statistics.drop(\"id\", axis = 1)\n",
    "#lifesnaps_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4930eda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>stability</th>\n",
       "      <th>intellect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>31.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39.125000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>35.250000</td>\n",
       "      <td>35.625000</td>\n",
       "      <td>39.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>37.166667</td>\n",
       "      <td>40.166667</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>30.833333</td>\n",
       "      <td>35.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>35.400000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>37.250000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>34.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>19.888889</td>\n",
       "      <td>34.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>27.333333</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>37.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>45.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cluster  extraversion  agreeableness  conscientiousness  stability  \\\n",
       "0        0     17.250000      30.750000          30.250000  29.750000   \n",
       "1        1     39.125000      45.000000          35.250000  35.625000   \n",
       "2        2     37.166667      40.166667          24.666667  30.833333   \n",
       "3        3     30.500000      38.200000          36.600000  35.400000   \n",
       "4        4     21.250000      35.750000          37.250000  29.000000   \n",
       "5        5     33.000000      41.666667          33.333333  19.888889   \n",
       "6        6     22.000000      31.000000          25.000000  27.500000   \n",
       "7        7     30.000000      27.333333          29.000000  24.333333   \n",
       "8        8     20.500000      36.500000          42.000000  42.000000   \n",
       "9        9     20.000000      36.500000          42.500000  21.000000   \n",
       "\n",
       "   intellect  \n",
       "0  31.250000  \n",
       "1  39.250000  \n",
       "2  35.333333  \n",
       "3  38.000000  \n",
       "4  34.250000  \n",
       "5  34.888889  \n",
       "6  46.000000  \n",
       "7  30.000000  \n",
       "8  37.500000  \n",
       "9  45.500000  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_mean = lifesnaps_statistics.groupby('Cluster', as_index = False, group_keys = True).mean()\n",
    "lifesnaps_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a72176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extraversion         30.14\n",
       "agreeableness        38.28\n",
       "conscientiousness    33.44\n",
       "stability            29.84\n",
       "intellect            36.60\n",
       "Cluster               3.56\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "accb0de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>stability</th>\n",
       "      <th>intellect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.112988</td>\n",
       "      <td>7.274384</td>\n",
       "      <td>6.898067</td>\n",
       "      <td>2.753785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.383507</td>\n",
       "      <td>3.116775</td>\n",
       "      <td>5.147815</td>\n",
       "      <td>5.069164</td>\n",
       "      <td>4.978525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.316607</td>\n",
       "      <td>3.488075</td>\n",
       "      <td>3.141125</td>\n",
       "      <td>5.706721</td>\n",
       "      <td>4.501851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.143268</td>\n",
       "      <td>4.442222</td>\n",
       "      <td>4.647580</td>\n",
       "      <td>5.168279</td>\n",
       "      <td>3.126944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.304038</td>\n",
       "      <td>4.787136</td>\n",
       "      <td>6.238322</td>\n",
       "      <td>4.898979</td>\n",
       "      <td>4.573474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>5.244044</td>\n",
       "      <td>5.123475</td>\n",
       "      <td>5.206833</td>\n",
       "      <td>4.371626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>6.658328</td>\n",
       "      <td>9.643651</td>\n",
       "      <td>9.814955</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.535534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>3.535534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cluster  extraversion  agreeableness  conscientiousness  stability  \\\n",
       "0        0      3.500000       4.112988           7.274384   6.898067   \n",
       "1        1      5.383507       3.116775           5.147815   5.069164   \n",
       "2        2      2.316607       3.488075           3.141125   5.706721   \n",
       "3        3      4.143268       4.442222           4.647580   5.168279   \n",
       "4        4      3.304038       4.787136           6.238322   4.898979   \n",
       "5        5      5.477226       5.244044           5.123475   5.206833   \n",
       "6        6      0.000000       1.414214           1.414214   4.949747   \n",
       "7        7      4.582576       6.658328           9.643651   9.814955   \n",
       "8        8      0.707107       4.949747           4.242641   0.000000   \n",
       "9        9      2.828427       0.707107           4.949747   4.242641   \n",
       "\n",
       "   intellect  \n",
       "0   2.753785  \n",
       "1   4.978525  \n",
       "2   4.501851  \n",
       "3   3.126944  \n",
       "4   4.573474  \n",
       "5   4.371626  \n",
       "6   0.000000  \n",
       "7   3.000000  \n",
       "8   3.535534  \n",
       "9   3.535534  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_std = lifesnaps_statistics.groupby('Cluster', as_index = False, group_keys = True).std()\n",
    "lifesnaps_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1bc0fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extraversion         8.164158\n",
       "agreeableness        6.353659\n",
       "conscientiousness    6.916883\n",
       "stability            8.182510\n",
       "intellect            5.268311\n",
       "Cluster              2.417285\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f6db449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gender</th>\n",
       "      <th>FEMALE</th>\n",
       "      <th>MALE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "gender   FEMALE  MALE\n",
       "Cluster              \n",
       "0             1     3\n",
       "1             6     2\n",
       "2             3     3\n",
       "3             0    10\n",
       "4             4     0\n",
       "5             6     3\n",
       "6             0     2\n",
       "7             0     3\n",
       "8             0     2\n",
       "9             0     2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics.groupby('Cluster')['gender'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4235b28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ipip_extraversion_category</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ipip_extraversion_category  AVERAGE  HIGH  LOW\n",
       "Cluster                                       \n",
       "0                                 0     0    4\n",
       "1                                 2     6    0\n",
       "2                                 2     4    0\n",
       "3                                 7     3    0\n",
       "4                                 0     0    4\n",
       "5                                 5     3    1\n",
       "6                                 0     0    2\n",
       "7                                 2     1    0\n",
       "8                                 0     0    2\n",
       "9                                 0     0    2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics.groupby('Cluster')['ipip_extraversion_category'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1271bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ipip_agreeableness_category</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ipip_agreeableness_category  AVERAGE  HIGH  LOW\n",
       "Cluster                                        \n",
       "0                                  0     0    4\n",
       "1                                  3     5    0\n",
       "2                                  4     2    0\n",
       "3                                  7     3    0\n",
       "4                                  1     0    3\n",
       "5                                  1     7    1\n",
       "6                                  0     0    2\n",
       "7                                  0     0    3\n",
       "8                                  0     1    1\n",
       "9                                  2     0    0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics.groupby('Cluster')['ipip_agreeableness_category'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44ca7068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ipip_conscientiousness_category</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ipip_conscientiousness_category  AVERAGE  HIGH  LOW\n",
       "Cluster                                            \n",
       "0                                      0     1    3\n",
       "1                                      5     3    0\n",
       "2                                      0     0    6\n",
       "3                                      4     5    1\n",
       "4                                      0     3    1\n",
       "5                                      6     2    1\n",
       "6                                      0     0    2\n",
       "7                                      0     1    2\n",
       "8                                      0     2    0\n",
       "9                                      0     2    0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics.groupby('Cluster')['ipip_conscientiousness_category'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f73a1565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ipip_stability_category</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ipip_stability_category  AVERAGE  HIGH  LOW\n",
       "Cluster                                    \n",
       "0                              1     2    1\n",
       "1                              2     6    0\n",
       "2                              3     2    1\n",
       "3                              6     4    0\n",
       "4                              3     1    0\n",
       "5                              0     0    9\n",
       "6                              1     0    1\n",
       "7                              2     0    1\n",
       "8                              0     2    0\n",
       "9                              0     0    2"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics.groupby('Cluster')['ipip_stability_category'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9fe48265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ipip_intellect_category</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ipip_intellect_category  AVERAGE  HIGH  LOW\n",
       "Cluster                                    \n",
       "0                              0     0    4\n",
       "1                              2     5    1\n",
       "2                              3     2    1\n",
       "3                              9     1    0\n",
       "4                              2     1    1\n",
       "5                              2     3    4\n",
       "6                              0     2    0\n",
       "7                              0     0    3\n",
       "8                              2     0    0\n",
       "9                              0     2    0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_statistics.groupby('Cluster')['ipip_intellect_category'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesnaps_grouped_all = pd.merge(lifesnaps, clusters, on = \"id\")\n",
    "lifesnaps_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989271d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesnaps_grouped_all.to_csv(\"Final_CSVs/lifesnaps_clusters_personality10.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ff422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'dataset' to run pycaret tests based on \"Cluster\".\n",
    "\n",
    "lifesnaps_grouped_all = lifesnaps_grouped_all.drop('dataset', axis = 1)\n",
    "lifesnaps_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_participants = lifesnaps_grouped_all[\"Cluster\"].unique()\n",
    "personality_group = lifesnaps_grouped_all.groupby('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3203e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesnaps_grouped_all[[\"id\", \"Cluster\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1scores = []\n",
    "\n",
    "for participant in unique_participants:\n",
    "    print(\"Participant: \",participant)\n",
    "    part_df = personality_group.get_group(participant)\n",
    "    grid = setup(data=part_df, target='stress', fix_imbalance = True, html=False, silent=True, verbose=False) #fix_imbalance = True,\n",
    "    best = compare_models(sort=\"F1\")\n",
    "    accuracies.append(pull()['Accuracy'][0])\n",
    "    precision.append(pull()['Prec.'][0])\n",
    "    recall.append(pull()['Recall'][0])\n",
    "    f1scores.append(pull()['F1'][0])\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = statistics.mean(accuracies)\n",
    "mean_prec = statistics.mean(precision)\n",
    "mean_rec = statistics.mean(recall)\n",
    "mean_f1 = statistics.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Accuracy Lifesnaps - Cluster Personality: \", mean_acc)\n",
    "print(\"Mean Precision Lifesnaps- Cluster Personality: \", mean_prec)\n",
    "print(\"Mean Recall Lifesnaps- Cluster Personality: \", mean_rec)\n",
    "print(\"Mean F1-score Lifesnaps- Cluster Personality: \", mean_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735dcb6",
   "metadata": {},
   "source": [
    "## Multi-Attribute-Splittng (All Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2619e668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTERTAINMENT</th>\n",
       "      <th>GYM</th>\n",
       "      <th>HOME</th>\n",
       "      <th>HOME1OFFICE</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>OUTDOORS</th>\n",
       "      <th>TRANSIT</th>\n",
       "      <th>WORK/SCHOOL</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bpm</th>\n",
       "      <th>calories</th>\n",
       "      <th>daily1temperature1variation</th>\n",
       "      <th>day</th>\n",
       "      <th>day1cos</th>\n",
       "      <th>day1sin</th>\n",
       "      <th>distance</th>\n",
       "      <th>filteredDemographicVO2Max</th>\n",
       "      <th>full1sleep1breathing1rate</th>\n",
       "      <th>gender</th>\n",
       "      <th>id</th>\n",
       "      <th>lightly1active1minutes</th>\n",
       "      <th>max1goal</th>\n",
       "      <th>min1goal</th>\n",
       "      <th>mindfulness1session</th>\n",
       "      <th>minutesAfterWakeup</th>\n",
       "      <th>minutesAsleep</th>\n",
       "      <th>minutesAwake</th>\n",
       "      <th>minutesToFallAsleep</th>\n",
       "      <th>minutes1below1default1zone11</th>\n",
       "      <th>minutes1in1default1zone11</th>\n",
       "      <th>minutes1in1default1zone12</th>\n",
       "      <th>minutes1in1default1zone13</th>\n",
       "      <th>moderately1active1minutes</th>\n",
       "      <th>month</th>\n",
       "      <th>month1cos</th>\n",
       "      <th>month1sin</th>\n",
       "      <th>nightly1temperature</th>\n",
       "      <th>nremhr</th>\n",
       "      <th>resting1hr</th>\n",
       "      <th>rmssd</th>\n",
       "      <th>scl1avg</th>\n",
       "      <th>sedentary1minutes</th>\n",
       "      <th>sleep1deep1ratio</th>\n",
       "      <th>sleep1duration</th>\n",
       "      <th>sleep1efficiency</th>\n",
       "      <th>sleep1light1ratio</th>\n",
       "      <th>sleep1rem1ratio</th>\n",
       "      <th>sleep1wake1ratio</th>\n",
       "      <th>spo2</th>\n",
       "      <th>step1goal</th>\n",
       "      <th>step1goal1label</th>\n",
       "      <th>steps</th>\n",
       "      <th>very1active1minutes</th>\n",
       "      <th>week</th>\n",
       "      <th>week1cos</th>\n",
       "      <th>week1sin</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekday1cos</th>\n",
       "      <th>weekday1sin</th>\n",
       "      <th>year</th>\n",
       "      <th>dataset</th>\n",
       "      <th>stress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>91.659123</td>\n",
       "      <td>3241.52</td>\n",
       "      <td>-0.460087</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>9275.7</td>\n",
       "      <td>48.07785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e329067b776a2402ffad2</td>\n",
       "      <td>376.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.809017</td>\n",
       "      <td>-5.877853e-01</td>\n",
       "      <td>33.513581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.461346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649.0</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>21360000.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.689119</td>\n",
       "      <td>1.179104</td>\n",
       "      <td>1.378378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12974.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.988831</td>\n",
       "      <td>-1.490423e-01</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.758758</td>\n",
       "      <td>0.651372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>621e2f1b67b776a240b3d87c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.309017</td>\n",
       "      <td>-9.510565e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.360605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90.021444</td>\n",
       "      <td>2523.09</td>\n",
       "      <td>-1.986618</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>6187.5</td>\n",
       "      <td>49.72347</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>621e339967b776a240e502de</td>\n",
       "      <td>256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>9.510565e-01</td>\n",
       "      <td>35.075916</td>\n",
       "      <td>51.707</td>\n",
       "      <td>57.433536</td>\n",
       "      <td>47.293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>488.0</td>\n",
       "      <td>1.297297</td>\n",
       "      <td>30780000.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.970954</td>\n",
       "      <td>1.364583</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>94.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17479.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.365341</td>\n",
       "      <td>9.308737e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1800.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.820763</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e332267b776a24092a584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.809017</td>\n",
       "      <td>-5.877853e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2318.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.820763</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.309017</td>\n",
       "      <td>-9.510565e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.733052</td>\n",
       "      <td>-6.801727e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7405</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.571340</td>\n",
       "      <td>1429.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.994869</td>\n",
       "      <td>0.101168</td>\n",
       "      <td>1723.5</td>\n",
       "      <td>36.39159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>621e346f67b776a24081744f</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.809017</td>\n",
       "      <td>5.877853e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.955573</td>\n",
       "      <td>2.947552e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>2022</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7406</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>0.485302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e34ec67b776a240d60873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.309017</td>\n",
       "      <td>-9.510565e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7407</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1713.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.151428</td>\n",
       "      <td>-0.988468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e309267b776a240ae1cdb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>9.510565e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51</td>\n",
       "      <td>0.222521</td>\n",
       "      <td>9.749279e-01</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7408</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2318.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>0.485302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>-9.510565e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.074730</td>\n",
       "      <td>-9.972038e-01</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7409</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>75.570213</td>\n",
       "      <td>1759.72</td>\n",
       "      <td>-2.080849</td>\n",
       "      <td>27</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>-0.724793</td>\n",
       "      <td>6701.5</td>\n",
       "      <td>45.77456</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>621e360b67b776a24039709f</td>\n",
       "      <td>206.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>35.236221</td>\n",
       "      <td>65.201</td>\n",
       "      <td>65.773235</td>\n",
       "      <td>41.219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>585.0</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>36120000.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.426471</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.403226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9987.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7410 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENTERTAINMENT  GYM  HOME  HOME1OFFICE  OTHER  OUTDOORS  TRANSIT  \\\n",
       "0               0.0  0.0   1.0          0.0    0.0       0.0      0.0   \n",
       "1               NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "2               NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "3               NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "4               NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "...             ...  ...   ...          ...    ...       ...      ...   \n",
       "7405            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "7406            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "7407            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "7408            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "7409            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "\n",
       "      WORK/SCHOOL  age   bmi        bpm  calories  \\\n",
       "0             1.0  1.0  23.0  91.659123   3241.52   \n",
       "1             NaN  1.0  22.0        NaN       NaN   \n",
       "2             NaN  0.0  20.0  90.021444   2523.09   \n",
       "3             NaN  0.0  21.0        NaN   1800.00   \n",
       "4             NaN  1.0  30.0        NaN   2318.40   \n",
       "...           ...  ...   ...        ...       ...   \n",
       "7405          NaN  NaN   NaN  82.571340   1429.29   \n",
       "7406          NaN  0.0  24.0        NaN   1612.80   \n",
       "7407          NaN  0.0  18.0        NaN   1713.60   \n",
       "7408          NaN  1.0  30.0        NaN   2318.40   \n",
       "7409          NaN  1.0  18.0  75.570213   1759.72   \n",
       "\n",
       "      daily1temperature1variation  day   day1cos   day1sin  distance  \\\n",
       "0                       -0.460087    4  0.688967  0.724793    9275.7   \n",
       "1                             NaN   12 -0.758758  0.651372       NaN   \n",
       "2                       -1.986618   18 -0.874347 -0.485302    6187.5   \n",
       "3                             NaN    3  0.820763  0.571268       NaN   \n",
       "4                             NaN    3  0.820763  0.571268       NaN   \n",
       "...                           ...  ...       ...       ...       ...   \n",
       "7405                          NaN   15 -0.994869  0.101168    1723.5   \n",
       "7406                          NaN   13 -0.874347  0.485302       NaN   \n",
       "7407                          NaN   24  0.151428 -0.988468       NaN   \n",
       "7408                          NaN   13 -0.874347  0.485302       NaN   \n",
       "7409                    -2.080849   27  0.688967 -0.724793    6701.5   \n",
       "\n",
       "      filteredDemographicVO2Max  full1sleep1breathing1rate  gender  \\\n",
       "0                      48.07785                        NaN     0.0   \n",
       "1                           NaN                        NaN     1.0   \n",
       "2                      49.72347                       11.6     1.0   \n",
       "3                           NaN                        NaN     0.0   \n",
       "4                           NaN                        NaN     0.0   \n",
       "...                         ...                        ...     ...   \n",
       "7405                   36.39159                        NaN     NaN   \n",
       "7406                        NaN                        NaN     0.0   \n",
       "7407                        NaN                        NaN     0.0   \n",
       "7408                        NaN                        NaN     0.0   \n",
       "7409                   45.77456                       15.6     1.0   \n",
       "\n",
       "                            id  lightly1active1minutes  max1goal  min1goal  \\\n",
       "0     621e329067b776a2402ffad2                   376.0   10000.0    8000.0   \n",
       "1     621e2f1b67b776a240b3d87c                     NaN       NaN       NaN   \n",
       "2     621e339967b776a240e502de                   256.0       NaN       NaN   \n",
       "3     621e332267b776a24092a584                     0.0       NaN       NaN   \n",
       "4     621e32d967b776a240627414                     0.0       NaN       NaN   \n",
       "...                        ...                     ...       ...       ...   \n",
       "7405  621e346f67b776a24081744f                    30.0       NaN       NaN   \n",
       "7406  621e34ec67b776a240d60873                     0.0       NaN       NaN   \n",
       "7407  621e309267b776a240ae1cdb                     0.0       NaN       NaN   \n",
       "7408  621e32d967b776a240627414                     0.0       NaN       NaN   \n",
       "7409  621e360b67b776a24039709f                   206.0       NaN       NaN   \n",
       "\n",
       "     mindfulness1session  minutesAfterWakeup  minutesAsleep  minutesAwake  \\\n",
       "0                  False                 0.0          305.0          51.0   \n",
       "1                    NaN                 NaN            NaN           NaN   \n",
       "2                  False                 0.0          461.0          52.0   \n",
       "3                  False                 NaN            NaN           NaN   \n",
       "4                  False                 NaN            NaN           NaN   \n",
       "...                  ...                 ...            ...           ...   \n",
       "7405               False                 NaN            NaN           NaN   \n",
       "7406               False                 NaN            NaN           NaN   \n",
       "7407               False                 NaN            NaN           NaN   \n",
       "7408               False                 NaN            NaN           NaN   \n",
       "7409               False                 0.0          515.0          87.0   \n",
       "\n",
       "      minutesToFallAsleep  minutes1below1default1zone11  \\\n",
       "0                     0.0                         616.0   \n",
       "1                     NaN                           NaN   \n",
       "2                     0.0                        1209.0   \n",
       "3                     NaN                           NaN   \n",
       "4                     NaN                           NaN   \n",
       "...                   ...                           ...   \n",
       "7405                  NaN                         168.0   \n",
       "7406                  NaN                           NaN   \n",
       "7407                  NaN                           NaN   \n",
       "7408                  NaN                           NaN   \n",
       "7409                  0.0                        1320.0   \n",
       "\n",
       "      minutes1in1default1zone11  minutes1in1default1zone12  \\\n",
       "0                         672.0                        0.0   \n",
       "1                           NaN                        NaN   \n",
       "2                         153.0                       62.0   \n",
       "3                           NaN                        NaN   \n",
       "4                           NaN                        NaN   \n",
       "...                         ...                        ...   \n",
       "7405                       29.0                        0.0   \n",
       "7406                        NaN                        NaN   \n",
       "7407                        NaN                        NaN   \n",
       "7408                        NaN                        NaN   \n",
       "7409                      113.0                        7.0   \n",
       "\n",
       "      minutes1in1default1zone13  moderately1active1minutes  month  month1cos  \\\n",
       "0                           0.0                       39.0      6  -0.809017   \n",
       "1                           NaN                        NaN      7  -0.309017   \n",
       "2                           5.0                       74.0     12   0.309017   \n",
       "3                           NaN                        0.0      9   0.809017   \n",
       "4                           NaN                        0.0      7  -0.309017   \n",
       "...                         ...                        ...    ...        ...   \n",
       "7405                        0.0                        1.0      1   0.809017   \n",
       "7406                        NaN                        0.0      7  -0.309017   \n",
       "7407                        NaN                        0.0     12   0.309017   \n",
       "7408                        NaN                        0.0      8   0.309017   \n",
       "7409                        0.0                       35.0      5  -1.000000   \n",
       "\n",
       "         month1sin  nightly1temperature  nremhr  resting1hr   rmssd   scl1avg  \\\n",
       "0    -5.877853e-01            33.513581     NaN   75.461346     NaN       NaN   \n",
       "1    -9.510565e-01                  NaN     NaN         NaN     NaN  6.360605   \n",
       "2     9.510565e-01            35.075916  51.707   57.433536  47.293       NaN   \n",
       "3    -5.877853e-01                  NaN     NaN         NaN     NaN       NaN   \n",
       "4    -9.510565e-01                  NaN     NaN         NaN     NaN       NaN   \n",
       "...            ...                  ...     ...         ...     ...       ...   \n",
       "7405  5.877853e-01                  NaN     NaN         NaN     NaN       NaN   \n",
       "7406 -9.510565e-01                  NaN     NaN         NaN     NaN       NaN   \n",
       "7407  9.510565e-01                  NaN     NaN         NaN     NaN       NaN   \n",
       "7408 -9.510565e-01                  NaN     NaN         NaN     NaN       NaN   \n",
       "7409  1.224647e-16            35.236221  65.201   65.773235  41.219       NaN   \n",
       "\n",
       "      sedentary1minutes  sleep1deep1ratio  sleep1duration  sleep1efficiency  \\\n",
       "0                 649.0          1.550000      21360000.0              95.0   \n",
       "1                   NaN               NaN             NaN               NaN   \n",
       "2                 488.0          1.297297      30780000.0              98.0   \n",
       "3                1440.0               NaN             NaN               NaN   \n",
       "4                1440.0               NaN             NaN               NaN   \n",
       "...                 ...               ...             ...               ...   \n",
       "7405             1399.0               NaN             NaN               NaN   \n",
       "7406             1440.0               NaN             NaN               NaN   \n",
       "7407             1440.0               NaN             NaN               NaN   \n",
       "7408             1440.0               NaN             NaN               NaN   \n",
       "7409              585.0          0.918605      36120000.0              89.0   \n",
       "\n",
       "      sleep1light1ratio  sleep1rem1ratio  sleep1wake1ratio  spo2  step1goal  \\\n",
       "0              0.689119         1.179104          1.378378   NaN     9999.0   \n",
       "1                   NaN              NaN               NaN   NaN        NaN   \n",
       "2              0.970954         1.364583          0.732394  94.9        NaN   \n",
       "3                   NaN              NaN               NaN   NaN        NaN   \n",
       "4                   NaN              NaN               NaN   NaN        NaN   \n",
       "...                 ...              ...               ...   ...        ...   \n",
       "7405                NaN              NaN               NaN   NaN        NaN   \n",
       "7406                NaN              NaN               NaN   NaN        NaN   \n",
       "7407                NaN              NaN               NaN   NaN        NaN   \n",
       "7408                NaN              NaN               NaN   NaN        NaN   \n",
       "7409           1.426471         0.960000          1.403226   NaN        NaN   \n",
       "\n",
       "      step1goal1label    steps  very1active1minutes  week  week1cos  \\\n",
       "0                 4.0  12974.0                 20.0    22 -0.988831   \n",
       "1                 NaN      NaN                  NaN    28 -0.500000   \n",
       "2                 NaN  17479.0                109.0    50  0.365341   \n",
       "3                 NaN      NaN                  0.0    35  0.500000   \n",
       "4                 NaN      NaN                  0.0    26 -0.733052   \n",
       "...               ...      ...                  ...   ...       ...   \n",
       "7405              NaN   2595.0                 10.0     2  0.955573   \n",
       "7406              NaN      NaN                  0.0    28 -0.500000   \n",
       "7407              NaN      NaN                  0.0    51  0.222521   \n",
       "7408              NaN      NaN                  0.0    32  0.074730   \n",
       "7409              NaN   9987.0                 12.0    21 -1.000000   \n",
       "\n",
       "          week1sin  weekday  weekday1cos  weekday1sin  year dataset  stress  \n",
       "0    -1.490423e-01        4    -0.900969    -0.433884  2021   Train       1  \n",
       "1    -8.660254e-01        0     1.000000     0.000000  2021   Train       0  \n",
       "2     9.308737e-01        5    -0.222521    -0.974928  2021   Train       0  \n",
       "3    -8.660254e-01        4    -0.900969    -0.433884  2021   Train       0  \n",
       "4    -6.801727e-01        5    -0.222521    -0.974928  2021   Train       0  \n",
       "...            ...      ...          ...          ...   ...     ...     ...  \n",
       "7405  2.947552e-01        5    -0.222521    -0.974928  2022    Test       0  \n",
       "7406 -8.660254e-01        1     0.623490     0.781831  2021    Test       0  \n",
       "7407  9.749279e-01        4    -0.900969    -0.433884  2021    Test       0  \n",
       "7408 -9.972038e-01        4    -0.900969    -0.433884  2021    Test       0  \n",
       "7409  1.224647e-16        3    -0.900969     0.433884  2021    Test       0  \n",
       "\n",
       "[7410 rows x 63 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54afb404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bpm</th>\n",
       "      <th>calories</th>\n",
       "      <th>daily1temperature1variation</th>\n",
       "      <th>distance</th>\n",
       "      <th>filteredDemographicVO2Max</th>\n",
       "      <th>full1sleep1breathing1rate</th>\n",
       "      <th>gender</th>\n",
       "      <th>id</th>\n",
       "      <th>lightly1active1minutes</th>\n",
       "      <th>max1goal</th>\n",
       "      <th>min1goal</th>\n",
       "      <th>mindfulness1session</th>\n",
       "      <th>minutesAfterWakeup</th>\n",
       "      <th>minutesAsleep</th>\n",
       "      <th>minutesAwake</th>\n",
       "      <th>minutesToFallAsleep</th>\n",
       "      <th>minutes1below1default1zone11</th>\n",
       "      <th>minutes1in1default1zone11</th>\n",
       "      <th>minutes1in1default1zone12</th>\n",
       "      <th>minutes1in1default1zone13</th>\n",
       "      <th>moderately1active1minutes</th>\n",
       "      <th>nightly1temperature</th>\n",
       "      <th>nremhr</th>\n",
       "      <th>resting1hr</th>\n",
       "      <th>rmssd</th>\n",
       "      <th>scl1avg</th>\n",
       "      <th>sedentary1minutes</th>\n",
       "      <th>sleep1deep1ratio</th>\n",
       "      <th>sleep1duration</th>\n",
       "      <th>sleep1efficiency</th>\n",
       "      <th>sleep1light1ratio</th>\n",
       "      <th>sleep1rem1ratio</th>\n",
       "      <th>sleep1wake1ratio</th>\n",
       "      <th>spo2</th>\n",
       "      <th>step1goal</th>\n",
       "      <th>step1goal1label</th>\n",
       "      <th>steps</th>\n",
       "      <th>very1active1minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>91.659123</td>\n",
       "      <td>3241.52</td>\n",
       "      <td>-0.460087</td>\n",
       "      <td>9275.7</td>\n",
       "      <td>48.07785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e329067b776a2402ffad2</td>\n",
       "      <td>376.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>33.513581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.461346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649.0</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>21360000.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.689119</td>\n",
       "      <td>1.179104</td>\n",
       "      <td>1.378378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12974.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>621e2f1b67b776a240b3d87c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.360605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90.021444</td>\n",
       "      <td>2523.09</td>\n",
       "      <td>-1.986618</td>\n",
       "      <td>6187.5</td>\n",
       "      <td>49.72347</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>621e339967b776a240e502de</td>\n",
       "      <td>256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>35.075916</td>\n",
       "      <td>51.707</td>\n",
       "      <td>57.433536</td>\n",
       "      <td>47.293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>488.0</td>\n",
       "      <td>1.297297</td>\n",
       "      <td>30780000.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.970954</td>\n",
       "      <td>1.364583</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>94.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17479.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1800.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e332267b776a24092a584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2318.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7405</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.571340</td>\n",
       "      <td>1429.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1723.5</td>\n",
       "      <td>36.39159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>621e346f67b776a24081744f</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7406</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e34ec67b776a240d60873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7407</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1713.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e309267b776a240ae1cdb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7408</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2318.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7409</th>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>75.570213</td>\n",
       "      <td>1759.72</td>\n",
       "      <td>-2.080849</td>\n",
       "      <td>6701.5</td>\n",
       "      <td>45.77456</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>621e360b67b776a24039709f</td>\n",
       "      <td>206.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.236221</td>\n",
       "      <td>65.201</td>\n",
       "      <td>65.773235</td>\n",
       "      <td>41.219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>585.0</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>36120000.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.426471</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.403226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9987.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7410 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age   bmi        bpm  calories  daily1temperature1variation  distance  \\\n",
       "0     1.0  23.0  91.659123   3241.52                    -0.460087    9275.7   \n",
       "1     1.0  22.0        NaN       NaN                          NaN       NaN   \n",
       "2     0.0  20.0  90.021444   2523.09                    -1.986618    6187.5   \n",
       "3     0.0  21.0        NaN   1800.00                          NaN       NaN   \n",
       "4     1.0  30.0        NaN   2318.40                          NaN       NaN   \n",
       "...   ...   ...        ...       ...                          ...       ...   \n",
       "7405  NaN   NaN  82.571340   1429.29                          NaN    1723.5   \n",
       "7406  0.0  24.0        NaN   1612.80                          NaN       NaN   \n",
       "7407  0.0  18.0        NaN   1713.60                          NaN       NaN   \n",
       "7408  1.0  30.0        NaN   2318.40                          NaN       NaN   \n",
       "7409  1.0  18.0  75.570213   1759.72                    -2.080849    6701.5   \n",
       "\n",
       "      filteredDemographicVO2Max  full1sleep1breathing1rate  gender  \\\n",
       "0                      48.07785                        NaN     0.0   \n",
       "1                           NaN                        NaN     1.0   \n",
       "2                      49.72347                       11.6     1.0   \n",
       "3                           NaN                        NaN     0.0   \n",
       "4                           NaN                        NaN     0.0   \n",
       "...                         ...                        ...     ...   \n",
       "7405                   36.39159                        NaN     NaN   \n",
       "7406                        NaN                        NaN     0.0   \n",
       "7407                        NaN                        NaN     0.0   \n",
       "7408                        NaN                        NaN     0.0   \n",
       "7409                   45.77456                       15.6     1.0   \n",
       "\n",
       "                            id  lightly1active1minutes  max1goal  min1goal  \\\n",
       "0     621e329067b776a2402ffad2                   376.0   10000.0    8000.0   \n",
       "1     621e2f1b67b776a240b3d87c                     NaN       NaN       NaN   \n",
       "2     621e339967b776a240e502de                   256.0       NaN       NaN   \n",
       "3     621e332267b776a24092a584                     0.0       NaN       NaN   \n",
       "4     621e32d967b776a240627414                     0.0       NaN       NaN   \n",
       "...                        ...                     ...       ...       ...   \n",
       "7405  621e346f67b776a24081744f                    30.0       NaN       NaN   \n",
       "7406  621e34ec67b776a240d60873                     0.0       NaN       NaN   \n",
       "7407  621e309267b776a240ae1cdb                     0.0       NaN       NaN   \n",
       "7408  621e32d967b776a240627414                     0.0       NaN       NaN   \n",
       "7409  621e360b67b776a24039709f                   206.0       NaN       NaN   \n",
       "\n",
       "     mindfulness1session  minutesAfterWakeup  minutesAsleep  minutesAwake  \\\n",
       "0                  False                 0.0          305.0          51.0   \n",
       "1                    NaN                 NaN            NaN           NaN   \n",
       "2                  False                 0.0          461.0          52.0   \n",
       "3                  False                 NaN            NaN           NaN   \n",
       "4                  False                 NaN            NaN           NaN   \n",
       "...                  ...                 ...            ...           ...   \n",
       "7405               False                 NaN            NaN           NaN   \n",
       "7406               False                 NaN            NaN           NaN   \n",
       "7407               False                 NaN            NaN           NaN   \n",
       "7408               False                 NaN            NaN           NaN   \n",
       "7409               False                 0.0          515.0          87.0   \n",
       "\n",
       "      minutesToFallAsleep  minutes1below1default1zone11  \\\n",
       "0                     0.0                         616.0   \n",
       "1                     NaN                           NaN   \n",
       "2                     0.0                        1209.0   \n",
       "3                     NaN                           NaN   \n",
       "4                     NaN                           NaN   \n",
       "...                   ...                           ...   \n",
       "7405                  NaN                         168.0   \n",
       "7406                  NaN                           NaN   \n",
       "7407                  NaN                           NaN   \n",
       "7408                  NaN                           NaN   \n",
       "7409                  0.0                        1320.0   \n",
       "\n",
       "      minutes1in1default1zone11  minutes1in1default1zone12  \\\n",
       "0                         672.0                        0.0   \n",
       "1                           NaN                        NaN   \n",
       "2                         153.0                       62.0   \n",
       "3                           NaN                        NaN   \n",
       "4                           NaN                        NaN   \n",
       "...                         ...                        ...   \n",
       "7405                       29.0                        0.0   \n",
       "7406                        NaN                        NaN   \n",
       "7407                        NaN                        NaN   \n",
       "7408                        NaN                        NaN   \n",
       "7409                      113.0                        7.0   \n",
       "\n",
       "      minutes1in1default1zone13  moderately1active1minutes  \\\n",
       "0                           0.0                       39.0   \n",
       "1                           NaN                        NaN   \n",
       "2                           5.0                       74.0   \n",
       "3                           NaN                        0.0   \n",
       "4                           NaN                        0.0   \n",
       "...                         ...                        ...   \n",
       "7405                        0.0                        1.0   \n",
       "7406                        NaN                        0.0   \n",
       "7407                        NaN                        0.0   \n",
       "7408                        NaN                        0.0   \n",
       "7409                        0.0                       35.0   \n",
       "\n",
       "      nightly1temperature  nremhr  resting1hr   rmssd   scl1avg  \\\n",
       "0               33.513581     NaN   75.461346     NaN       NaN   \n",
       "1                     NaN     NaN         NaN     NaN  6.360605   \n",
       "2               35.075916  51.707   57.433536  47.293       NaN   \n",
       "3                     NaN     NaN         NaN     NaN       NaN   \n",
       "4                     NaN     NaN         NaN     NaN       NaN   \n",
       "...                   ...     ...         ...     ...       ...   \n",
       "7405                  NaN     NaN         NaN     NaN       NaN   \n",
       "7406                  NaN     NaN         NaN     NaN       NaN   \n",
       "7407                  NaN     NaN         NaN     NaN       NaN   \n",
       "7408                  NaN     NaN         NaN     NaN       NaN   \n",
       "7409            35.236221  65.201   65.773235  41.219       NaN   \n",
       "\n",
       "      sedentary1minutes  sleep1deep1ratio  sleep1duration  sleep1efficiency  \\\n",
       "0                 649.0          1.550000      21360000.0              95.0   \n",
       "1                   NaN               NaN             NaN               NaN   \n",
       "2                 488.0          1.297297      30780000.0              98.0   \n",
       "3                1440.0               NaN             NaN               NaN   \n",
       "4                1440.0               NaN             NaN               NaN   \n",
       "...                 ...               ...             ...               ...   \n",
       "7405             1399.0               NaN             NaN               NaN   \n",
       "7406             1440.0               NaN             NaN               NaN   \n",
       "7407             1440.0               NaN             NaN               NaN   \n",
       "7408             1440.0               NaN             NaN               NaN   \n",
       "7409              585.0          0.918605      36120000.0              89.0   \n",
       "\n",
       "      sleep1light1ratio  sleep1rem1ratio  sleep1wake1ratio  spo2  step1goal  \\\n",
       "0              0.689119         1.179104          1.378378   NaN     9999.0   \n",
       "1                   NaN              NaN               NaN   NaN        NaN   \n",
       "2              0.970954         1.364583          0.732394  94.9        NaN   \n",
       "3                   NaN              NaN               NaN   NaN        NaN   \n",
       "4                   NaN              NaN               NaN   NaN        NaN   \n",
       "...                 ...              ...               ...   ...        ...   \n",
       "7405                NaN              NaN               NaN   NaN        NaN   \n",
       "7406                NaN              NaN               NaN   NaN        NaN   \n",
       "7407                NaN              NaN               NaN   NaN        NaN   \n",
       "7408                NaN              NaN               NaN   NaN        NaN   \n",
       "7409           1.426471         0.960000          1.403226   NaN        NaN   \n",
       "\n",
       "      step1goal1label    steps  very1active1minutes  \n",
       "0                 4.0  12974.0                 20.0  \n",
       "1                 NaN      NaN                  NaN  \n",
       "2                 NaN  17479.0                109.0  \n",
       "3                 NaN      NaN                  0.0  \n",
       "4                 NaN      NaN                  0.0  \n",
       "...               ...      ...                  ...  \n",
       "7405              NaN   2595.0                 10.0  \n",
       "7406              NaN      NaN                  0.0  \n",
       "7407              NaN      NaN                  0.0  \n",
       "7408              NaN      NaN                  0.0  \n",
       "7409              NaN   9987.0                 12.0  \n",
       "\n",
       "[7410 rows x 40 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_all_features = lifesnaps.loc[:, ~lifesnaps.columns.isin(['stress', 'dataset', 'ENTERTAINMENT', 'GYM', 'HOME', 'HOME1OFFICE', 'OTHER', 'OUTDOORS',\n",
    "       'TRANSIT', 'WORK/SCHOOL','month', 'month1cos', 'month1sin', 'week', 'week1cos', 'week1sin', 'weekday', 'weekday1cos', 'weekday1sin', 'year', 'day', 'day1cos', 'day1sin'])]\n",
    "lifesnaps_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad6cd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'bmi', 'bpm', 'calories', 'daily1temperature1variation',\n",
       "       'distance', 'filteredDemographicVO2Max', 'full1sleep1breathing1rate',\n",
       "       'gender', 'id', 'lightly1active1minutes', 'max1goal', 'min1goal',\n",
       "       'mindfulness1session', 'minutesAfterWakeup', 'minutesAsleep',\n",
       "       'minutesAwake', 'minutesToFallAsleep', 'minutes1below1default1zone11',\n",
       "       'minutes1in1default1zone11', 'minutes1in1default1zone12',\n",
       "       'minutes1in1default1zone13', 'moderately1active1minutes',\n",
       "       'nightly1temperature', 'nremhr', 'resting1hr', 'rmssd', 'scl1avg',\n",
       "       'sedentary1minutes', 'sleep1deep1ratio', 'sleep1duration',\n",
       "       'sleep1efficiency', 'sleep1light1ratio', 'sleep1rem1ratio',\n",
       "       'sleep1wake1ratio', 'spo2', 'step1goal', 'step1goal1label', 'steps',\n",
       "       'very1active1minutes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_all_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d99ba2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age_std</th>\n",
       "      <th>bmi_std</th>\n",
       "      <th>bpm_std</th>\n",
       "      <th>calories_std</th>\n",
       "      <th>daily1temperature1variation_std</th>\n",
       "      <th>distance_std</th>\n",
       "      <th>filteredDemographicVO2Max_std</th>\n",
       "      <th>full1sleep1breathing1rate_std</th>\n",
       "      <th>gender_std</th>\n",
       "      <th>lightly1active1minutes_std</th>\n",
       "      <th>max1goal_std</th>\n",
       "      <th>min1goal_std</th>\n",
       "      <th>minutesAfterWakeup_std</th>\n",
       "      <th>minutesAsleep_std</th>\n",
       "      <th>minutesAwake_std</th>\n",
       "      <th>minutesToFallAsleep_std</th>\n",
       "      <th>minutes1below1default1zone11_std</th>\n",
       "      <th>minutes1in1default1zone11_std</th>\n",
       "      <th>minutes1in1default1zone12_std</th>\n",
       "      <th>minutes1in1default1zone13_std</th>\n",
       "      <th>moderately1active1minutes_std</th>\n",
       "      <th>nightly1temperature_std</th>\n",
       "      <th>nremhr_std</th>\n",
       "      <th>resting1hr_std</th>\n",
       "      <th>rmssd_std</th>\n",
       "      <th>scl1avg_std</th>\n",
       "      <th>sedentary1minutes_std</th>\n",
       "      <th>sleep1deep1ratio_std</th>\n",
       "      <th>sleep1duration_std</th>\n",
       "      <th>sleep1efficiency_std</th>\n",
       "      <th>sleep1light1ratio_std</th>\n",
       "      <th>sleep1rem1ratio_std</th>\n",
       "      <th>sleep1wake1ratio_std</th>\n",
       "      <th>spo2_std</th>\n",
       "      <th>step1goal_std</th>\n",
       "      <th>step1goal1label_std</th>\n",
       "      <th>steps_std</th>\n",
       "      <th>very1active1minutes_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>621e2e8e67b776a24055b564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.115235</td>\n",
       "      <td>268.393641</td>\n",
       "      <td>0.570898</td>\n",
       "      <td>2495.171517</td>\n",
       "      <td>0.593936</td>\n",
       "      <td>0.569264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.619691</td>\n",
       "      <td>2708.012802</td>\n",
       "      <td>2337.602931</td>\n",
       "      <td>1.096940</td>\n",
       "      <td>53.017232</td>\n",
       "      <td>15.108025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.517602</td>\n",
       "      <td>59.979071</td>\n",
       "      <td>1.119463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.083772</td>\n",
       "      <td>0.224603</td>\n",
       "      <td>2.766234</td>\n",
       "      <td>1.936963</td>\n",
       "      <td>9.402428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202.624254</td>\n",
       "      <td>0.236235</td>\n",
       "      <td>3.470355e+06</td>\n",
       "      <td>2.005752</td>\n",
       "      <td>0.159124</td>\n",
       "      <td>0.250086</td>\n",
       "      <td>0.188958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2708.012802</td>\n",
       "      <td>0.847319</td>\n",
       "      <td>3384.097986</td>\n",
       "      <td>16.297671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>621e2eaf67b776a2406b14ac</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.885394</td>\n",
       "      <td>361.985428</td>\n",
       "      <td>0.593731</td>\n",
       "      <td>4162.391371</td>\n",
       "      <td>1.380626</td>\n",
       "      <td>0.536698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.807592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.723198</td>\n",
       "      <td>89.888392</td>\n",
       "      <td>17.318701</td>\n",
       "      <td>0.987878</td>\n",
       "      <td>182.344647</td>\n",
       "      <td>101.605571</td>\n",
       "      <td>24.489053</td>\n",
       "      <td>1.675411</td>\n",
       "      <td>26.665031</td>\n",
       "      <td>1.041019</td>\n",
       "      <td>8.843768</td>\n",
       "      <td>3.109986</td>\n",
       "      <td>6.103989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185.741548</td>\n",
       "      <td>0.391454</td>\n",
       "      <td>5.942277e+06</td>\n",
       "      <td>2.696577</td>\n",
       "      <td>0.282058</td>\n",
       "      <td>0.361365</td>\n",
       "      <td>0.329824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5973.028379</td>\n",
       "      <td>22.312539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621e2ed667b776a24085d8d1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.278665</td>\n",
       "      <td>191.883956</td>\n",
       "      <td>0.644815</td>\n",
       "      <td>1739.393218</td>\n",
       "      <td>0.797480</td>\n",
       "      <td>5.797068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.521424</td>\n",
       "      <td>3228.366225</td>\n",
       "      <td>2651.114688</td>\n",
       "      <td>1.644982</td>\n",
       "      <td>77.658628</td>\n",
       "      <td>18.598939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.490681</td>\n",
       "      <td>126.533886</td>\n",
       "      <td>5.329953</td>\n",
       "      <td>0.752587</td>\n",
       "      <td>11.704070</td>\n",
       "      <td>0.449799</td>\n",
       "      <td>5.600547</td>\n",
       "      <td>3.476818</td>\n",
       "      <td>7.926298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319.013454</td>\n",
       "      <td>0.337037</td>\n",
       "      <td>5.434833e+06</td>\n",
       "      <td>3.405877</td>\n",
       "      <td>0.230605</td>\n",
       "      <td>0.411375</td>\n",
       "      <td>0.348230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3228.233061</td>\n",
       "      <td>1.015038</td>\n",
       "      <td>2616.982198</td>\n",
       "      <td>5.371150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>621e2ef567b776a24099f889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.439423</td>\n",
       "      <td>421.581261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3374.706124</td>\n",
       "      <td>0.414427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.705161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202.514689</td>\n",
       "      <td>91.656307</td>\n",
       "      <td>40.681447</td>\n",
       "      <td>6.633728</td>\n",
       "      <td>6.349365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.890932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.104642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4842.354770</td>\n",
       "      <td>13.890850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>621e2efa67b776a2409dd1c3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.611945</td>\n",
       "      <td>444.840078</td>\n",
       "      <td>0.723875</td>\n",
       "      <td>3454.517701</td>\n",
       "      <td>1.348074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.259973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.517075</td>\n",
       "      <td>89.881128</td>\n",
       "      <td>19.035900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>394.129769</td>\n",
       "      <td>117.944882</td>\n",
       "      <td>19.157025</td>\n",
       "      <td>0.904858</td>\n",
       "      <td>20.105649</td>\n",
       "      <td>0.477403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.587303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>378.120638</td>\n",
       "      <td>0.317896</td>\n",
       "      <td>6.156437e+06</td>\n",
       "      <td>2.541268</td>\n",
       "      <td>0.200754</td>\n",
       "      <td>0.321502</td>\n",
       "      <td>0.256729</td>\n",
       "      <td>0.972034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4858.421862</td>\n",
       "      <td>16.780661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>621e2f1b67b776a240b3d87c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.206342</td>\n",
       "      <td>229.874475</td>\n",
       "      <td>0.607866</td>\n",
       "      <td>1884.230068</td>\n",
       "      <td>1.713408</td>\n",
       "      <td>0.822889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.115416</td>\n",
       "      <td>1811.501714</td>\n",
       "      <td>1392.286443</td>\n",
       "      <td>1.145165</td>\n",
       "      <td>100.981543</td>\n",
       "      <td>25.669177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>241.523923</td>\n",
       "      <td>201.320773</td>\n",
       "      <td>6.014706</td>\n",
       "      <td>0.875933</td>\n",
       "      <td>5.250616</td>\n",
       "      <td>0.634215</td>\n",
       "      <td>4.638591</td>\n",
       "      <td>3.716837</td>\n",
       "      <td>4.858914</td>\n",
       "      <td>2.571259</td>\n",
       "      <td>198.164000</td>\n",
       "      <td>0.383739</td>\n",
       "      <td>7.308553e+06</td>\n",
       "      <td>3.653093</td>\n",
       "      <td>0.253809</td>\n",
       "      <td>0.305544</td>\n",
       "      <td>0.336127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1811.066049</td>\n",
       "      <td>0.603834</td>\n",
       "      <td>3047.406559</td>\n",
       "      <td>2.512481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>621e2f3967b776a240c654db</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.494502</td>\n",
       "      <td>485.469253</td>\n",
       "      <td>0.831227</td>\n",
       "      <td>2747.269423</td>\n",
       "      <td>0.987326</td>\n",
       "      <td>5.759812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.626832</td>\n",
       "      <td>2784.452557</td>\n",
       "      <td>2514.567359</td>\n",
       "      <td>0.939891</td>\n",
       "      <td>163.351774</td>\n",
       "      <td>27.663504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>279.134521</td>\n",
       "      <td>104.715502</td>\n",
       "      <td>51.812301</td>\n",
       "      <td>3.948086</td>\n",
       "      <td>20.399270</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>21.888513</td>\n",
       "      <td>2.367473</td>\n",
       "      <td>6.443923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.350714</td>\n",
       "      <td>0.294823</td>\n",
       "      <td>1.018628e+07</td>\n",
       "      <td>1.948843</td>\n",
       "      <td>0.273882</td>\n",
       "      <td>0.331098</td>\n",
       "      <td>0.315731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2784.052800</td>\n",
       "      <td>0.965215</td>\n",
       "      <td>3988.212150</td>\n",
       "      <td>47.909084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>621e2f5767b776a240d8f9d6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.394377</td>\n",
       "      <td>539.547830</td>\n",
       "      <td>0.756005</td>\n",
       "      <td>4491.329470</td>\n",
       "      <td>2.133406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.911893</td>\n",
       "      <td>2129.076568</td>\n",
       "      <td>851.630627</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>113.431492</td>\n",
       "      <td>24.709043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>344.160072</td>\n",
       "      <td>119.878349</td>\n",
       "      <td>18.117268</td>\n",
       "      <td>0.464095</td>\n",
       "      <td>30.699280</td>\n",
       "      <td>0.534537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.340050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>386.910575</td>\n",
       "      <td>0.326771</td>\n",
       "      <td>8.032977e+06</td>\n",
       "      <td>2.625631</td>\n",
       "      <td>0.216569</td>\n",
       "      <td>0.340377</td>\n",
       "      <td>0.315533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2129.076568</td>\n",
       "      <td>0.425815</td>\n",
       "      <td>6635.420494</td>\n",
       "      <td>18.324390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>621e2f6167b776a240e082a9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.867758</td>\n",
       "      <td>364.175994</td>\n",
       "      <td>0.793513</td>\n",
       "      <td>2869.464415</td>\n",
       "      <td>0.529432</td>\n",
       "      <td>0.830599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.848650</td>\n",
       "      <td>2945.486263</td>\n",
       "      <td>2379.922608</td>\n",
       "      <td>1.659682</td>\n",
       "      <td>75.091944</td>\n",
       "      <td>18.665233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>415.203720</td>\n",
       "      <td>61.069562</td>\n",
       "      <td>29.070513</td>\n",
       "      <td>0.871007</td>\n",
       "      <td>20.386749</td>\n",
       "      <td>1.220035</td>\n",
       "      <td>4.805904</td>\n",
       "      <td>2.001050</td>\n",
       "      <td>5.847453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.675749</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>5.002301e+06</td>\n",
       "      <td>3.129043</td>\n",
       "      <td>0.245131</td>\n",
       "      <td>0.307935</td>\n",
       "      <td>0.383327</td>\n",
       "      <td>0.922177</td>\n",
       "      <td>2945.486263</td>\n",
       "      <td>0.895752</td>\n",
       "      <td>4182.653506</td>\n",
       "      <td>10.160671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>621e2f7a67b776a240f14425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.283408</td>\n",
       "      <td>831.618631</td>\n",
       "      <td>0.651416</td>\n",
       "      <td>4742.986715</td>\n",
       "      <td>0.199581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.198107</td>\n",
       "      <td>1587.094176</td>\n",
       "      <td>1122.322888</td>\n",
       "      <td>2.340562</td>\n",
       "      <td>67.221969</td>\n",
       "      <td>9.190375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>346.977593</td>\n",
       "      <td>79.129365</td>\n",
       "      <td>37.271939</td>\n",
       "      <td>0.717552</td>\n",
       "      <td>36.950046</td>\n",
       "      <td>0.623608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.413380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>323.163492</td>\n",
       "      <td>0.249814</td>\n",
       "      <td>4.303500e+06</td>\n",
       "      <td>2.796042</td>\n",
       "      <td>0.189773</td>\n",
       "      <td>0.358719</td>\n",
       "      <td>0.186923</td>\n",
       "      <td>0.522015</td>\n",
       "      <td>1586.622610</td>\n",
       "      <td>0.529031</td>\n",
       "      <td>7279.751311</td>\n",
       "      <td>61.211257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>621e2f9167b776a240011ccb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.838724</td>\n",
       "      <td>178.765850</td>\n",
       "      <td>0.613612</td>\n",
       "      <td>1615.812527</td>\n",
       "      <td>0.492603</td>\n",
       "      <td>0.576618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.376826</td>\n",
       "      <td>1251.766723</td>\n",
       "      <td>1396.957536</td>\n",
       "      <td>1.658982</td>\n",
       "      <td>90.329939</td>\n",
       "      <td>19.331824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>161.955855</td>\n",
       "      <td>116.005118</td>\n",
       "      <td>14.457583</td>\n",
       "      <td>3.579678</td>\n",
       "      <td>12.920582</td>\n",
       "      <td>0.917710</td>\n",
       "      <td>3.583879</td>\n",
       "      <td>2.201841</td>\n",
       "      <td>5.294612</td>\n",
       "      <td>3.350611</td>\n",
       "      <td>128.138044</td>\n",
       "      <td>0.425925</td>\n",
       "      <td>5.894189e+06</td>\n",
       "      <td>1.997353</td>\n",
       "      <td>0.233573</td>\n",
       "      <td>0.326609</td>\n",
       "      <td>0.397631</td>\n",
       "      <td>0.631326</td>\n",
       "      <td>1251.766723</td>\n",
       "      <td>0.465653</td>\n",
       "      <td>2333.171809</td>\n",
       "      <td>10.425672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>621e2fb367b776a24015accd</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.548097</td>\n",
       "      <td>442.612334</td>\n",
       "      <td>0.895167</td>\n",
       "      <td>2882.059970</td>\n",
       "      <td>0.337607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.095268</td>\n",
       "      <td>3318.135781</td>\n",
       "      <td>2981.248834</td>\n",
       "      <td>0.939739</td>\n",
       "      <td>71.873806</td>\n",
       "      <td>13.165072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.224367</td>\n",
       "      <td>85.880613</td>\n",
       "      <td>34.531166</td>\n",
       "      <td>3.202821</td>\n",
       "      <td>20.198880</td>\n",
       "      <td>0.860951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.180213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>290.602172</td>\n",
       "      <td>0.395425</td>\n",
       "      <td>4.815377e+06</td>\n",
       "      <td>2.190917</td>\n",
       "      <td>0.219879</td>\n",
       "      <td>0.232598</td>\n",
       "      <td>0.284822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3318.135781</td>\n",
       "      <td>0.839949</td>\n",
       "      <td>3886.928564</td>\n",
       "      <td>29.052969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>621e2fce67b776a240279baa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.154261</td>\n",
       "      <td>650.783162</td>\n",
       "      <td>0.502053</td>\n",
       "      <td>3688.292605</td>\n",
       "      <td>0.665279</td>\n",
       "      <td>0.835135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.041861</td>\n",
       "      <td>2087.615080</td>\n",
       "      <td>2123.885424</td>\n",
       "      <td>2.902816</td>\n",
       "      <td>77.886528</td>\n",
       "      <td>17.893749</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>216.209936</td>\n",
       "      <td>189.034983</td>\n",
       "      <td>23.286413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.085774</td>\n",
       "      <td>0.367319</td>\n",
       "      <td>10.415561</td>\n",
       "      <td>3.309934</td>\n",
       "      <td>6.308012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.400492</td>\n",
       "      <td>0.293537</td>\n",
       "      <td>5.290011e+06</td>\n",
       "      <td>2.360177</td>\n",
       "      <td>0.236132</td>\n",
       "      <td>0.256352</td>\n",
       "      <td>0.350925</td>\n",
       "      <td>0.885685</td>\n",
       "      <td>2087.615080</td>\n",
       "      <td>0.726310</td>\n",
       "      <td>5114.149336</td>\n",
       "      <td>36.125025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>621e2ff067b776a2403eb737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.753444</td>\n",
       "      <td>373.494296</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>1893.468125</td>\n",
       "      <td>0.370978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.014633</td>\n",
       "      <td>1125.634799</td>\n",
       "      <td>1033.544776</td>\n",
       "      <td>0.869444</td>\n",
       "      <td>76.497278</td>\n",
       "      <td>15.056053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.992134</td>\n",
       "      <td>62.252720</td>\n",
       "      <td>8.295894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.973142</td>\n",
       "      <td>0.547051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.806857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259.379382</td>\n",
       "      <td>0.287375</td>\n",
       "      <td>5.137727e+06</td>\n",
       "      <td>2.343400</td>\n",
       "      <td>0.191372</td>\n",
       "      <td>0.232251</td>\n",
       "      <td>0.253002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1125.538099</td>\n",
       "      <td>0.375212</td>\n",
       "      <td>2881.375764</td>\n",
       "      <td>12.542395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>621e300767b776a2404dc717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.224496</td>\n",
       "      <td>858.935510</td>\n",
       "      <td>0.866752</td>\n",
       "      <td>4697.812939</td>\n",
       "      <td>0.391154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.428953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.716863</td>\n",
       "      <td>95.590346</td>\n",
       "      <td>26.144470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>347.670132</td>\n",
       "      <td>34.610914</td>\n",
       "      <td>6.097886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.566682</td>\n",
       "      <td>0.641080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.354457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350.428856</td>\n",
       "      <td>0.459548</td>\n",
       "      <td>5.961189e+06</td>\n",
       "      <td>2.441701</td>\n",
       "      <td>0.224824</td>\n",
       "      <td>0.411004</td>\n",
       "      <td>0.462126</td>\n",
       "      <td>0.861772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6406.290001</td>\n",
       "      <td>23.201760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>621e301367b776a24057738e</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.542366</td>\n",
       "      <td>437.075187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3525.117138</td>\n",
       "      <td>0.272389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.517661</td>\n",
       "      <td>2880.972058</td>\n",
       "      <td>3033.150178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218.626171</td>\n",
       "      <td>73.069235</td>\n",
       "      <td>26.356118</td>\n",
       "      <td>7.881998</td>\n",
       "      <td>13.834481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.106011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.792490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2880.677038</td>\n",
       "      <td>1.095445</td>\n",
       "      <td>4988.049322</td>\n",
       "      <td>22.802198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>621e301e67b776a240608a72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.457537</td>\n",
       "      <td>998.220801</td>\n",
       "      <td>0.824319</td>\n",
       "      <td>3894.279294</td>\n",
       "      <td>1.043161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.745773</td>\n",
       "      <td>1460.593487</td>\n",
       "      <td>912.870929</td>\n",
       "      <td>1.305898</td>\n",
       "      <td>96.344289</td>\n",
       "      <td>20.712876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>460.900916</td>\n",
       "      <td>92.868182</td>\n",
       "      <td>27.493769</td>\n",
       "      <td>10.055930</td>\n",
       "      <td>18.115085</td>\n",
       "      <td>1.133724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.868683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282.419250</td>\n",
       "      <td>0.235329</td>\n",
       "      <td>6.745169e+06</td>\n",
       "      <td>2.992613</td>\n",
       "      <td>0.204134</td>\n",
       "      <td>0.486749</td>\n",
       "      <td>0.255118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1460.410912</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>5688.378667</td>\n",
       "      <td>10.286994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>621e309267b776a240ae1cdb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.097517</td>\n",
       "      <td>618.523493</td>\n",
       "      <td>0.384837</td>\n",
       "      <td>2847.395913</td>\n",
       "      <td>0.721162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.952895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.064121</td>\n",
       "      <td>51.906095</td>\n",
       "      <td>7.904764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>414.981920</td>\n",
       "      <td>100.078103</td>\n",
       "      <td>13.635152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.035690</td>\n",
       "      <td>0.434717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.958411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.189105</td>\n",
       "      <td>0.241820</td>\n",
       "      <td>3.390207e+06</td>\n",
       "      <td>2.114377</td>\n",
       "      <td>0.186102</td>\n",
       "      <td>0.224791</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3612.821071</td>\n",
       "      <td>22.083272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>621e309b67b776a240b532b0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.417155</td>\n",
       "      <td>793.519343</td>\n",
       "      <td>0.816167</td>\n",
       "      <td>6807.277819</td>\n",
       "      <td>0.610893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.756290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.695649</td>\n",
       "      <td>82.615409</td>\n",
       "      <td>22.146525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>421.529253</td>\n",
       "      <td>88.461447</td>\n",
       "      <td>11.869467</td>\n",
       "      <td>21.816951</td>\n",
       "      <td>26.242993</td>\n",
       "      <td>1.053230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.033931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>325.649215</td>\n",
       "      <td>0.315798</td>\n",
       "      <td>5.757670e+06</td>\n",
       "      <td>3.065611</td>\n",
       "      <td>0.243835</td>\n",
       "      <td>0.315722</td>\n",
       "      <td>0.507952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8041.328888</td>\n",
       "      <td>44.072083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>621e30b267b776a240c5e13f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.691032</td>\n",
       "      <td>447.173574</td>\n",
       "      <td>0.709569</td>\n",
       "      <td>2642.691534</td>\n",
       "      <td>0.308310</td>\n",
       "      <td>4.605268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.396457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.987842</td>\n",
       "      <td>99.949893</td>\n",
       "      <td>19.658190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>247.727226</td>\n",
       "      <td>101.549570</td>\n",
       "      <td>8.476688</td>\n",
       "      <td>0.129099</td>\n",
       "      <td>16.022619</td>\n",
       "      <td>0.530312</td>\n",
       "      <td>17.079217</td>\n",
       "      <td>1.806892</td>\n",
       "      <td>16.279125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>276.939991</td>\n",
       "      <td>0.292857</td>\n",
       "      <td>6.462285e+06</td>\n",
       "      <td>1.959523</td>\n",
       "      <td>0.324306</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.450614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3600.804498</td>\n",
       "      <td>12.788239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>621e30c867b776a240d4aa6c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.354255</td>\n",
       "      <td>258.699086</td>\n",
       "      <td>0.624105</td>\n",
       "      <td>2715.697331</td>\n",
       "      <td>0.660949</td>\n",
       "      <td>2.630871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.366329</td>\n",
       "      <td>1869.200148</td>\n",
       "      <td>1789.287676</td>\n",
       "      <td>1.015370</td>\n",
       "      <td>73.404375</td>\n",
       "      <td>17.655049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>304.584665</td>\n",
       "      <td>50.570310</td>\n",
       "      <td>15.488554</td>\n",
       "      <td>0.465679</td>\n",
       "      <td>18.189800</td>\n",
       "      <td>0.518626</td>\n",
       "      <td>5.178568</td>\n",
       "      <td>2.352260</td>\n",
       "      <td>8.606000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317.322980</td>\n",
       "      <td>0.345933</td>\n",
       "      <td>4.987277e+06</td>\n",
       "      <td>2.070640</td>\n",
       "      <td>0.279084</td>\n",
       "      <td>0.277748</td>\n",
       "      <td>0.464833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1869.200148</td>\n",
       "      <td>0.638381</td>\n",
       "      <td>3950.160473</td>\n",
       "      <td>15.943215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>621e30e267b776a240e5bf90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.335792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.605481</td>\n",
       "      <td>282.842712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.505553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.037628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282.842712</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.656552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>621e30e467b776a240e817c7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.459930</td>\n",
       "      <td>1306.632137</td>\n",
       "      <td>1.527309</td>\n",
       "      <td>3532.329056</td>\n",
       "      <td>0.386383</td>\n",
       "      <td>6.605695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.899269</td>\n",
       "      <td>1780.005914</td>\n",
       "      <td>1823.819012</td>\n",
       "      <td>1.950243</td>\n",
       "      <td>146.114618</td>\n",
       "      <td>36.977471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>274.986683</td>\n",
       "      <td>91.247331</td>\n",
       "      <td>65.559019</td>\n",
       "      <td>3.616679</td>\n",
       "      <td>16.212999</td>\n",
       "      <td>2.439351</td>\n",
       "      <td>15.278501</td>\n",
       "      <td>2.814818</td>\n",
       "      <td>11.506192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>330.038870</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>1.031088e+07</td>\n",
       "      <td>4.398145</td>\n",
       "      <td>0.227219</td>\n",
       "      <td>0.408014</td>\n",
       "      <td>0.257759</td>\n",
       "      <td>0.958177</td>\n",
       "      <td>1779.896522</td>\n",
       "      <td>0.638666</td>\n",
       "      <td>4973.972533</td>\n",
       "      <td>80.676036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>621e30f467b776a240f22944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.864180</td>\n",
       "      <td>587.728822</td>\n",
       "      <td>0.551962</td>\n",
       "      <td>5209.616733</td>\n",
       "      <td>0.400693</td>\n",
       "      <td>0.303315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.039900</td>\n",
       "      <td>4532.693032</td>\n",
       "      <td>4035.606825</td>\n",
       "      <td>1.887993</td>\n",
       "      <td>46.785648</td>\n",
       "      <td>13.937203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.994740</td>\n",
       "      <td>72.586330</td>\n",
       "      <td>3.130187</td>\n",
       "      <td>0.855699</td>\n",
       "      <td>26.951860</td>\n",
       "      <td>0.366418</td>\n",
       "      <td>2.811258</td>\n",
       "      <td>1.364450</td>\n",
       "      <td>6.744896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320.336680</td>\n",
       "      <td>0.572307</td>\n",
       "      <td>3.101226e+06</td>\n",
       "      <td>1.889043</td>\n",
       "      <td>0.167967</td>\n",
       "      <td>0.380207</td>\n",
       "      <td>0.237194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4532.584082</td>\n",
       "      <td>1.280306</td>\n",
       "      <td>7268.244429</td>\n",
       "      <td>29.316851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>621e310d67b776a24003096d</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.311126</td>\n",
       "      <td>319.741161</td>\n",
       "      <td>0.591500</td>\n",
       "      <td>2515.269609</td>\n",
       "      <td>0.819949</td>\n",
       "      <td>0.638058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.959205</td>\n",
       "      <td>1708.658010</td>\n",
       "      <td>1428.371066</td>\n",
       "      <td>1.589199</td>\n",
       "      <td>98.023119</td>\n",
       "      <td>19.569588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>392.397360</td>\n",
       "      <td>116.257735</td>\n",
       "      <td>10.246390</td>\n",
       "      <td>0.625875</td>\n",
       "      <td>16.717565</td>\n",
       "      <td>0.440212</td>\n",
       "      <td>9.521616</td>\n",
       "      <td>2.553710</td>\n",
       "      <td>6.090532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>307.330277</td>\n",
       "      <td>0.291070</td>\n",
       "      <td>6.604433e+06</td>\n",
       "      <td>2.581989</td>\n",
       "      <td>0.225380</td>\n",
       "      <td>0.288965</td>\n",
       "      <td>0.342315</td>\n",
       "      <td>1.026023</td>\n",
       "      <td>1708.358262</td>\n",
       "      <td>0.569553</td>\n",
       "      <td>3728.635394</td>\n",
       "      <td>6.463344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>621e312a67b776a240164d59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.823333</td>\n",
       "      <td>694.697704</td>\n",
       "      <td>0.785540</td>\n",
       "      <td>3634.578628</td>\n",
       "      <td>0.765350</td>\n",
       "      <td>0.866418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.960025</td>\n",
       "      <td>1519.109051</td>\n",
       "      <td>1012.739367</td>\n",
       "      <td>1.759782</td>\n",
       "      <td>95.756708</td>\n",
       "      <td>20.474262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>276.752596</td>\n",
       "      <td>87.799653</td>\n",
       "      <td>4.027673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.326636</td>\n",
       "      <td>0.423492</td>\n",
       "      <td>5.843888</td>\n",
       "      <td>2.990870</td>\n",
       "      <td>8.447702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>374.201434</td>\n",
       "      <td>0.253395</td>\n",
       "      <td>6.801330e+06</td>\n",
       "      <td>2.592838</td>\n",
       "      <td>0.210069</td>\n",
       "      <td>0.445040</td>\n",
       "      <td>0.254890</td>\n",
       "      <td>0.633071</td>\n",
       "      <td>1518.602681</td>\n",
       "      <td>0.506370</td>\n",
       "      <td>4764.450265</td>\n",
       "      <td>34.244538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>621e314867b776a24029ebf9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.034591</td>\n",
       "      <td>568.527928</td>\n",
       "      <td>0.832059</td>\n",
       "      <td>4029.130721</td>\n",
       "      <td>0.291852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.513157</td>\n",
       "      <td>2772.473568</td>\n",
       "      <td>2512.079081</td>\n",
       "      <td>2.578176</td>\n",
       "      <td>89.855716</td>\n",
       "      <td>18.288327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>406.695482</td>\n",
       "      <td>31.029329</td>\n",
       "      <td>7.536466</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>19.281771</td>\n",
       "      <td>0.609296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.583371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.293247</td>\n",
       "      <td>0.261498</td>\n",
       "      <td>6.145704e+06</td>\n",
       "      <td>2.076278</td>\n",
       "      <td>0.185324</td>\n",
       "      <td>0.300291</td>\n",
       "      <td>0.271969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2772.304027</td>\n",
       "      <td>0.926040</td>\n",
       "      <td>5406.145666</td>\n",
       "      <td>30.770835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>621e320b67b776a240d36a07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.812559</td>\n",
       "      <td>606.158831</td>\n",
       "      <td>0.585386</td>\n",
       "      <td>3736.194826</td>\n",
       "      <td>0.411686</td>\n",
       "      <td>1.157345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.969921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.394313</td>\n",
       "      <td>85.749369</td>\n",
       "      <td>18.670328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>382.914261</td>\n",
       "      <td>79.610170</td>\n",
       "      <td>5.092782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.146043</td>\n",
       "      <td>0.406407</td>\n",
       "      <td>3.096445</td>\n",
       "      <td>2.098141</td>\n",
       "      <td>10.950537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.818895</td>\n",
       "      <td>0.224629</td>\n",
       "      <td>5.908784e+06</td>\n",
       "      <td>3.334545</td>\n",
       "      <td>0.190009</td>\n",
       "      <td>0.336890</td>\n",
       "      <td>0.370585</td>\n",
       "      <td>0.624111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4927.911594</td>\n",
       "      <td>32.536969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>621e322367b776a240e44e9b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.263333</td>\n",
       "      <td>923.720727</td>\n",
       "      <td>0.560433</td>\n",
       "      <td>4824.819166</td>\n",
       "      <td>0.852744</td>\n",
       "      <td>0.829691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.752862</td>\n",
       "      <td>10620.848335</td>\n",
       "      <td>8241.258810</td>\n",
       "      <td>2.367414</td>\n",
       "      <td>97.626916</td>\n",
       "      <td>18.575200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>341.655077</td>\n",
       "      <td>148.720445</td>\n",
       "      <td>12.866571</td>\n",
       "      <td>0.334087</td>\n",
       "      <td>26.014053</td>\n",
       "      <td>0.660987</td>\n",
       "      <td>7.849796</td>\n",
       "      <td>3.361619</td>\n",
       "      <td>12.234927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>401.867018</td>\n",
       "      <td>0.264040</td>\n",
       "      <td>6.711642e+06</td>\n",
       "      <td>2.854746</td>\n",
       "      <td>0.206815</td>\n",
       "      <td>0.320125</td>\n",
       "      <td>0.306234</td>\n",
       "      <td>1.161009</td>\n",
       "      <td>10384.047758</td>\n",
       "      <td>3.097866</td>\n",
       "      <td>6663.338097</td>\n",
       "      <td>43.639983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>621e323667b776a240f19134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.746812</td>\n",
       "      <td>394.329912</td>\n",
       "      <td>0.700069</td>\n",
       "      <td>1932.322522</td>\n",
       "      <td>1.137081</td>\n",
       "      <td>0.650998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.255077</td>\n",
       "      <td>944.465203</td>\n",
       "      <td>1181.757896</td>\n",
       "      <td>1.561905</td>\n",
       "      <td>75.261435</td>\n",
       "      <td>23.290665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>265.136651</td>\n",
       "      <td>55.452000</td>\n",
       "      <td>4.247052</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8.196850</td>\n",
       "      <td>0.596824</td>\n",
       "      <td>2.948840</td>\n",
       "      <td>2.356937</td>\n",
       "      <td>6.738554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>316.920712</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>5.208532e+06</td>\n",
       "      <td>2.707804</td>\n",
       "      <td>0.207883</td>\n",
       "      <td>0.295539</td>\n",
       "      <td>0.334711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>944.465203</td>\n",
       "      <td>0.393919</td>\n",
       "      <td>2800.821773</td>\n",
       "      <td>9.527729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>621e324e67b776a2400191cb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.289063</td>\n",
       "      <td>395.629615</td>\n",
       "      <td>0.358968</td>\n",
       "      <td>1840.205451</td>\n",
       "      <td>0.408398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.869732</td>\n",
       "      <td>1784.236754</td>\n",
       "      <td>1463.824305</td>\n",
       "      <td>1.881008</td>\n",
       "      <td>94.426647</td>\n",
       "      <td>19.306979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>187.791496</td>\n",
       "      <td>111.147846</td>\n",
       "      <td>9.169413</td>\n",
       "      <td>2.082738</td>\n",
       "      <td>28.021104</td>\n",
       "      <td>0.507779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.165922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>270.242305</td>\n",
       "      <td>0.384675</td>\n",
       "      <td>6.597532e+06</td>\n",
       "      <td>3.861831</td>\n",
       "      <td>0.330973</td>\n",
       "      <td>0.402203</td>\n",
       "      <td>0.351125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1784.236754</td>\n",
       "      <td>0.555726</td>\n",
       "      <td>2516.283262</td>\n",
       "      <td>23.199168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>621e326767b776a24012e179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.150367</td>\n",
       "      <td>294.723219</td>\n",
       "      <td>0.643569</td>\n",
       "      <td>3137.534246</td>\n",
       "      <td>0.725879</td>\n",
       "      <td>1.882794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.513193</td>\n",
       "      <td>3620.020983</td>\n",
       "      <td>3475.989579</td>\n",
       "      <td>1.749976</td>\n",
       "      <td>51.404872</td>\n",
       "      <td>13.470574</td>\n",
       "      <td>0.610847</td>\n",
       "      <td>130.614334</td>\n",
       "      <td>48.342719</td>\n",
       "      <td>7.792567</td>\n",
       "      <td>3.398729</td>\n",
       "      <td>37.864456</td>\n",
       "      <td>0.410469</td>\n",
       "      <td>3.021691</td>\n",
       "      <td>1.827606</td>\n",
       "      <td>24.156976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>266.475339</td>\n",
       "      <td>0.384988</td>\n",
       "      <td>3.371451e+06</td>\n",
       "      <td>2.250182</td>\n",
       "      <td>0.192929</td>\n",
       "      <td>0.448492</td>\n",
       "      <td>0.245942</td>\n",
       "      <td>0.855960</td>\n",
       "      <td>3620.020983</td>\n",
       "      <td>0.724004</td>\n",
       "      <td>4609.375148</td>\n",
       "      <td>25.290314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>621e328667b776a240281372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.462322</td>\n",
       "      <td>433.778208</td>\n",
       "      <td>0.315369</td>\n",
       "      <td>2368.441636</td>\n",
       "      <td>0.108469</td>\n",
       "      <td>0.806639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.380064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.602082</td>\n",
       "      <td>75.978067</td>\n",
       "      <td>13.731958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>131.847157</td>\n",
       "      <td>83.800738</td>\n",
       "      <td>31.247400</td>\n",
       "      <td>11.156735</td>\n",
       "      <td>9.066117</td>\n",
       "      <td>0.568247</td>\n",
       "      <td>4.924928</td>\n",
       "      <td>1.399144</td>\n",
       "      <td>28.604556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.246861</td>\n",
       "      <td>0.510148</td>\n",
       "      <td>5.122292e+06</td>\n",
       "      <td>2.228602</td>\n",
       "      <td>0.144650</td>\n",
       "      <td>0.293130</td>\n",
       "      <td>0.240134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3033.072352</td>\n",
       "      <td>12.868278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>621e329067b776a2402ffad2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.437519</td>\n",
       "      <td>344.623444</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>2123.200622</td>\n",
       "      <td>0.265054</td>\n",
       "      <td>0.780944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.082997</td>\n",
       "      <td>2092.493984</td>\n",
       "      <td>1106.095081</td>\n",
       "      <td>1.973319</td>\n",
       "      <td>90.743649</td>\n",
       "      <td>15.688271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>150.533387</td>\n",
       "      <td>136.527091</td>\n",
       "      <td>14.986767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.104903</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>3.189904</td>\n",
       "      <td>1.653434</td>\n",
       "      <td>4.786963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.925750</td>\n",
       "      <td>0.381323</td>\n",
       "      <td>6.228270e+06</td>\n",
       "      <td>3.035311</td>\n",
       "      <td>0.287139</td>\n",
       "      <td>0.350371</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2092.493984</td>\n",
       "      <td>0.418499</td>\n",
       "      <td>2938.643253</td>\n",
       "      <td>31.261539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.821743</td>\n",
       "      <td>254.878544</td>\n",
       "      <td>0.427655</td>\n",
       "      <td>2216.573742</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>0.568304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.844673</td>\n",
       "      <td>1558.512672</td>\n",
       "      <td>1632.658498</td>\n",
       "      <td>1.725227</td>\n",
       "      <td>49.455722</td>\n",
       "      <td>11.614966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>170.002724</td>\n",
       "      <td>49.258189</td>\n",
       "      <td>1.605163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.530968</td>\n",
       "      <td>0.441477</td>\n",
       "      <td>3.525828</td>\n",
       "      <td>2.042879</td>\n",
       "      <td>5.663418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.140764</td>\n",
       "      <td>0.217351</td>\n",
       "      <td>3.174924e+06</td>\n",
       "      <td>3.173354</td>\n",
       "      <td>0.128951</td>\n",
       "      <td>0.336865</td>\n",
       "      <td>0.168858</td>\n",
       "      <td>1.116423</td>\n",
       "      <td>1558.512672</td>\n",
       "      <td>0.544219</td>\n",
       "      <td>3140.205333</td>\n",
       "      <td>17.940905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>621e32d067b776a2405b7d54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.754418</td>\n",
       "      <td>476.472701</td>\n",
       "      <td>0.663381</td>\n",
       "      <td>3886.612503</td>\n",
       "      <td>0.438958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.982173</td>\n",
       "      <td>4868.679534</td>\n",
       "      <td>3596.715674</td>\n",
       "      <td>1.847096</td>\n",
       "      <td>108.424441</td>\n",
       "      <td>27.412904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>417.653309</td>\n",
       "      <td>81.241916</td>\n",
       "      <td>10.903536</td>\n",
       "      <td>0.296145</td>\n",
       "      <td>9.475015</td>\n",
       "      <td>0.386215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.255779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>313.561378</td>\n",
       "      <td>0.193162</td>\n",
       "      <td>7.720290e+06</td>\n",
       "      <td>2.652536</td>\n",
       "      <td>0.114852</td>\n",
       "      <td>0.299446</td>\n",
       "      <td>0.437070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4868.234721</td>\n",
       "      <td>1.829058</td>\n",
       "      <td>5234.089263</td>\n",
       "      <td>10.376768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.038625</td>\n",
       "      <td>758.289950</td>\n",
       "      <td>1.314977</td>\n",
       "      <td>5775.772830</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>4.496898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.708789</td>\n",
       "      <td>4404.054828</td>\n",
       "      <td>3654.721501</td>\n",
       "      <td>2.546624</td>\n",
       "      <td>137.256121</td>\n",
       "      <td>19.157244</td>\n",
       "      <td>1.940285</td>\n",
       "      <td>394.661125</td>\n",
       "      <td>108.994506</td>\n",
       "      <td>15.852139</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>15.216793</td>\n",
       "      <td>0.638745</td>\n",
       "      <td>1.412799</td>\n",
       "      <td>2.103172</td>\n",
       "      <td>3.572303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>226.106159</td>\n",
       "      <td>0.241650</td>\n",
       "      <td>9.103800e+06</td>\n",
       "      <td>2.778595</td>\n",
       "      <td>0.180978</td>\n",
       "      <td>0.296380</td>\n",
       "      <td>0.205972</td>\n",
       "      <td>0.862278</td>\n",
       "      <td>4403.904925</td>\n",
       "      <td>1.364685</td>\n",
       "      <td>7788.436038</td>\n",
       "      <td>33.855310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>621e32e667b776a2406d2f1c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.650352</td>\n",
       "      <td>372.124261</td>\n",
       "      <td>0.496201</td>\n",
       "      <td>2452.466760</td>\n",
       "      <td>0.895622</td>\n",
       "      <td>0.841080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.050900</td>\n",
       "      <td>3535.533906</td>\n",
       "      <td>1414.213562</td>\n",
       "      <td>2.767770</td>\n",
       "      <td>82.375415</td>\n",
       "      <td>21.199534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>279.601267</td>\n",
       "      <td>197.545469</td>\n",
       "      <td>33.297145</td>\n",
       "      <td>25.333140</td>\n",
       "      <td>23.791228</td>\n",
       "      <td>0.549028</td>\n",
       "      <td>5.496603</td>\n",
       "      <td>3.305073</td>\n",
       "      <td>5.752806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184.120278</td>\n",
       "      <td>0.291762</td>\n",
       "      <td>5.957200e+06</td>\n",
       "      <td>3.075090</td>\n",
       "      <td>0.302695</td>\n",
       "      <td>0.362848</td>\n",
       "      <td>0.384633</td>\n",
       "      <td>0.860320</td>\n",
       "      <td>3535.533906</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>3404.256445</td>\n",
       "      <td>22.724241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>621e331067b776a24085dd3f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.709254</td>\n",
       "      <td>379.444839</td>\n",
       "      <td>0.582873</td>\n",
       "      <td>1447.993622</td>\n",
       "      <td>0.555822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.242220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408680</td>\n",
       "      <td>103.467235</td>\n",
       "      <td>21.430797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>308.771573</td>\n",
       "      <td>132.053127</td>\n",
       "      <td>28.519937</td>\n",
       "      <td>4.043483</td>\n",
       "      <td>26.865965</td>\n",
       "      <td>0.393754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.088075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>335.910004</td>\n",
       "      <td>0.376717</td>\n",
       "      <td>7.134875e+06</td>\n",
       "      <td>2.554273</td>\n",
       "      <td>0.250677</td>\n",
       "      <td>0.375603</td>\n",
       "      <td>0.404467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2097.256189</td>\n",
       "      <td>13.461343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>621e332267b776a24092a584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.345370</td>\n",
       "      <td>706.247053</td>\n",
       "      <td>0.543419</td>\n",
       "      <td>6587.221005</td>\n",
       "      <td>0.171099</td>\n",
       "      <td>1.027433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.069370</td>\n",
       "      <td>2672.612419</td>\n",
       "      <td>1995.530721</td>\n",
       "      <td>5.954990</td>\n",
       "      <td>194.437773</td>\n",
       "      <td>24.434753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>200.193595</td>\n",
       "      <td>136.003151</td>\n",
       "      <td>2.469020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.583993</td>\n",
       "      <td>0.441337</td>\n",
       "      <td>4.006307</td>\n",
       "      <td>1.932482</td>\n",
       "      <td>32.821757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342.039627</td>\n",
       "      <td>0.345635</td>\n",
       "      <td>1.282971e+07</td>\n",
       "      <td>2.148089</td>\n",
       "      <td>0.427907</td>\n",
       "      <td>0.420928</td>\n",
       "      <td>0.308466</td>\n",
       "      <td>1.210214</td>\n",
       "      <td>2672.612419</td>\n",
       "      <td>0.534522</td>\n",
       "      <td>8328.303750</td>\n",
       "      <td>36.294362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>621e333567b776a240a0c217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.517850</td>\n",
       "      <td>293.563087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2931.093071</td>\n",
       "      <td>0.242340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.297461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.879415</td>\n",
       "      <td>63.941938</td>\n",
       "      <td>10.472185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.654286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.661560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4168.204387</td>\n",
       "      <td>1.130853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>621e333967b776a240a3cd06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.978838</td>\n",
       "      <td>564.948789</td>\n",
       "      <td>0.865269</td>\n",
       "      <td>3287.330653</td>\n",
       "      <td>0.998054</td>\n",
       "      <td>6.715698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.961691</td>\n",
       "      <td>2886.751346</td>\n",
       "      <td>1154.700538</td>\n",
       "      <td>1.903406</td>\n",
       "      <td>97.476805</td>\n",
       "      <td>27.003471</td>\n",
       "      <td>13.723561</td>\n",
       "      <td>379.290695</td>\n",
       "      <td>111.247495</td>\n",
       "      <td>8.196036</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>21.126643</td>\n",
       "      <td>0.714850</td>\n",
       "      <td>18.582127</td>\n",
       "      <td>3.091735</td>\n",
       "      <td>19.885873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>263.850381</td>\n",
       "      <td>0.340447</td>\n",
       "      <td>6.428511e+06</td>\n",
       "      <td>4.437847</td>\n",
       "      <td>0.267359</td>\n",
       "      <td>0.564392</td>\n",
       "      <td>0.284244</td>\n",
       "      <td>1.068885</td>\n",
       "      <td>2886.751346</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>4739.000034</td>\n",
       "      <td>17.982538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>621e335a67b776a240bb12ff</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.293807</td>\n",
       "      <td>285.384726</td>\n",
       "      <td>0.550272</td>\n",
       "      <td>2718.555712</td>\n",
       "      <td>1.073530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.965168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.880588</td>\n",
       "      <td>73.689434</td>\n",
       "      <td>18.019191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>200.044323</td>\n",
       "      <td>107.870066</td>\n",
       "      <td>21.354337</td>\n",
       "      <td>0.954411</td>\n",
       "      <td>21.210755</td>\n",
       "      <td>0.551644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.572926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256.573131</td>\n",
       "      <td>0.295859</td>\n",
       "      <td>5.125944e+06</td>\n",
       "      <td>2.132695</td>\n",
       "      <td>0.188820</td>\n",
       "      <td>0.281549</td>\n",
       "      <td>0.284335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3997.571658</td>\n",
       "      <td>21.889189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>621e337667b776a240ce78ab</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.006323</td>\n",
       "      <td>408.040474</td>\n",
       "      <td>0.629650</td>\n",
       "      <td>2776.802692</td>\n",
       "      <td>0.316337</td>\n",
       "      <td>0.832718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.018499</td>\n",
       "      <td>2626.970503</td>\n",
       "      <td>2543.900268</td>\n",
       "      <td>0.919199</td>\n",
       "      <td>58.673190</td>\n",
       "      <td>19.823643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>201.725416</td>\n",
       "      <td>110.935148</td>\n",
       "      <td>5.511562</td>\n",
       "      <td>0.442174</td>\n",
       "      <td>35.008249</td>\n",
       "      <td>0.482630</td>\n",
       "      <td>3.960673</td>\n",
       "      <td>1.803039</td>\n",
       "      <td>6.501855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>343.816789</td>\n",
       "      <td>0.265672</td>\n",
       "      <td>4.054830e+06</td>\n",
       "      <td>1.732007</td>\n",
       "      <td>0.161932</td>\n",
       "      <td>0.312433</td>\n",
       "      <td>0.262818</td>\n",
       "      <td>1.120074</td>\n",
       "      <td>2626.970503</td>\n",
       "      <td>0.889878</td>\n",
       "      <td>3674.386789</td>\n",
       "      <td>18.935621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>621e339967b776a240e502de</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.180612</td>\n",
       "      <td>333.288547</td>\n",
       "      <td>0.624139</td>\n",
       "      <td>4290.627861</td>\n",
       "      <td>1.254990</td>\n",
       "      <td>1.712996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.904043</td>\n",
       "      <td>3934.211735</td>\n",
       "      <td>2991.746889</td>\n",
       "      <td>1.391533</td>\n",
       "      <td>72.830492</td>\n",
       "      <td>17.906793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>167.456899</td>\n",
       "      <td>41.111679</td>\n",
       "      <td>18.379690</td>\n",
       "      <td>3.788868</td>\n",
       "      <td>23.809769</td>\n",
       "      <td>0.454720</td>\n",
       "      <td>7.857323</td>\n",
       "      <td>2.492545</td>\n",
       "      <td>5.610983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227.633314</td>\n",
       "      <td>0.277844</td>\n",
       "      <td>3.120838e+06</td>\n",
       "      <td>2.288239</td>\n",
       "      <td>0.204888</td>\n",
       "      <td>0.295220</td>\n",
       "      <td>0.251707</td>\n",
       "      <td>0.965013</td>\n",
       "      <td>3934.211735</td>\n",
       "      <td>1.138729</td>\n",
       "      <td>6027.766506</td>\n",
       "      <td>36.378002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>621e33b067b776a240f39e56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.659364</td>\n",
       "      <td>434.958310</td>\n",
       "      <td>0.437581</td>\n",
       "      <td>2767.098252</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>0.560133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.572016</td>\n",
       "      <td>3252.153553</td>\n",
       "      <td>2080.878335</td>\n",
       "      <td>0.551539</td>\n",
       "      <td>69.926843</td>\n",
       "      <td>14.331214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.587107</td>\n",
       "      <td>151.255156</td>\n",
       "      <td>7.031348</td>\n",
       "      <td>1.262051</td>\n",
       "      <td>26.699195</td>\n",
       "      <td>0.369531</td>\n",
       "      <td>4.758690</td>\n",
       "      <td>2.420177</td>\n",
       "      <td>10.345600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>230.460407</td>\n",
       "      <td>0.323046</td>\n",
       "      <td>4.936802e+06</td>\n",
       "      <td>2.192645</td>\n",
       "      <td>0.231883</td>\n",
       "      <td>0.269799</td>\n",
       "      <td>0.278632</td>\n",
       "      <td>0.975783</td>\n",
       "      <td>3252.153553</td>\n",
       "      <td>0.719365</td>\n",
       "      <td>3657.485378</td>\n",
       "      <td>10.375136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>621e33cf67b776a240087de9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.611652</td>\n",
       "      <td>552.845079</td>\n",
       "      <td>0.508310</td>\n",
       "      <td>3111.735145</td>\n",
       "      <td>0.407413</td>\n",
       "      <td>2.774618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.619443</td>\n",
       "      <td>5196.831614</td>\n",
       "      <td>4135.641354</td>\n",
       "      <td>1.605172</td>\n",
       "      <td>57.378377</td>\n",
       "      <td>22.283044</td>\n",
       "      <td>7.427439</td>\n",
       "      <td>202.718048</td>\n",
       "      <td>148.699650</td>\n",
       "      <td>10.646546</td>\n",
       "      <td>2.131366</td>\n",
       "      <td>29.699917</td>\n",
       "      <td>0.490166</td>\n",
       "      <td>10.241256</td>\n",
       "      <td>2.839953</td>\n",
       "      <td>10.153877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.472160</td>\n",
       "      <td>0.414980</td>\n",
       "      <td>4.087359e+06</td>\n",
       "      <td>3.582591</td>\n",
       "      <td>0.188179</td>\n",
       "      <td>0.860539</td>\n",
       "      <td>0.276776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5196.552720</td>\n",
       "      <td>1.578036</td>\n",
       "      <td>4304.092034</td>\n",
       "      <td>30.222773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>621e33ed67b776a2401cf5f7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.331066</td>\n",
       "      <td>497.470614</td>\n",
       "      <td>0.429344</td>\n",
       "      <td>3069.016444</td>\n",
       "      <td>0.410085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.033796</td>\n",
       "      <td>1463.850109</td>\n",
       "      <td>975.900073</td>\n",
       "      <td>0.614689</td>\n",
       "      <td>86.303977</td>\n",
       "      <td>16.351021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.483036</td>\n",
       "      <td>49.209858</td>\n",
       "      <td>8.395586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.941953</td>\n",
       "      <td>0.283890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.501492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315.231054</td>\n",
       "      <td>0.299536</td>\n",
       "      <td>5.879226e+06</td>\n",
       "      <td>1.810423</td>\n",
       "      <td>0.272358</td>\n",
       "      <td>0.482072</td>\n",
       "      <td>0.277131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1463.362159</td>\n",
       "      <td>0.487950</td>\n",
       "      <td>4119.898438</td>\n",
       "      <td>28.489012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>621e340467b776a2402d7982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.856454</td>\n",
       "      <td>507.667999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2829.757782</td>\n",
       "      <td>1.195321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.718089</td>\n",
       "      <td>1892.620031</td>\n",
       "      <td>1595.297322</td>\n",
       "      <td>4.618802</td>\n",
       "      <td>82.963848</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>258.620255</td>\n",
       "      <td>107.752316</td>\n",
       "      <td>25.211528</td>\n",
       "      <td>2.697755</td>\n",
       "      <td>12.821401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.001404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.525722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.683204e+06</td>\n",
       "      <td>4.041452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1892.620031</td>\n",
       "      <td>1.014833</td>\n",
       "      <td>3556.487375</td>\n",
       "      <td>18.016302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>621e341067b776a24037b105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.209001</td>\n",
       "      <td>420.832714</td>\n",
       "      <td>0.448376</td>\n",
       "      <td>2285.546941</td>\n",
       "      <td>0.381021</td>\n",
       "      <td>4.941968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.748410</td>\n",
       "      <td>603.022689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932768</td>\n",
       "      <td>79.476650</td>\n",
       "      <td>14.994717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.380008</td>\n",
       "      <td>122.433314</td>\n",
       "      <td>2.269361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.981334</td>\n",
       "      <td>0.470098</td>\n",
       "      <td>5.219525</td>\n",
       "      <td>2.006655</td>\n",
       "      <td>5.382929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.166138</td>\n",
       "      <td>0.256256</td>\n",
       "      <td>5.446189e+06</td>\n",
       "      <td>2.634999</td>\n",
       "      <td>0.202219</td>\n",
       "      <td>0.350570</td>\n",
       "      <td>0.246181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>603.022689</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>3353.684376</td>\n",
       "      <td>11.310567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>621e342e67b776a2404ce460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.963159</td>\n",
       "      <td>639.934028</td>\n",
       "      <td>0.733043</td>\n",
       "      <td>3943.450061</td>\n",
       "      <td>2.530828</td>\n",
       "      <td>0.928743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.797144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.958928</td>\n",
       "      <td>121.584867</td>\n",
       "      <td>28.538631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>405.573686</td>\n",
       "      <td>76.069417</td>\n",
       "      <td>18.703976</td>\n",
       "      <td>0.392339</td>\n",
       "      <td>16.269481</td>\n",
       "      <td>0.660551</td>\n",
       "      <td>4.227562</td>\n",
       "      <td>3.986295</td>\n",
       "      <td>15.229551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224.610374</td>\n",
       "      <td>0.460087</td>\n",
       "      <td>8.594049e+06</td>\n",
       "      <td>3.312321</td>\n",
       "      <td>0.255508</td>\n",
       "      <td>0.440427</td>\n",
       "      <td>0.369723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5455.568963</td>\n",
       "      <td>32.292920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>621e345267b776a240691064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.813389</td>\n",
       "      <td>432.862978</td>\n",
       "      <td>0.792715</td>\n",
       "      <td>2706.564597</td>\n",
       "      <td>0.795618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.765629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.852926</td>\n",
       "      <td>194.284699</td>\n",
       "      <td>31.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>293.472936</td>\n",
       "      <td>138.445164</td>\n",
       "      <td>30.473512</td>\n",
       "      <td>0.590937</td>\n",
       "      <td>17.976293</td>\n",
       "      <td>1.706760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.144593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>233.067773</td>\n",
       "      <td>0.250561</td>\n",
       "      <td>1.338846e+07</td>\n",
       "      <td>3.025815</td>\n",
       "      <td>0.188036</td>\n",
       "      <td>0.232163</td>\n",
       "      <td>0.416426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3827.558259</td>\n",
       "      <td>9.300272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>621e345c67b776a2407146a8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.618648</td>\n",
       "      <td>299.782763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2567.115223</td>\n",
       "      <td>0.739161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.942943</td>\n",
       "      <td>1095.445115</td>\n",
       "      <td>1643.167673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>174.646974</td>\n",
       "      <td>116.862691</td>\n",
       "      <td>25.455408</td>\n",
       "      <td>1.747472</td>\n",
       "      <td>12.573063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.217869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.589139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1095.445115</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>3692.964587</td>\n",
       "      <td>10.870802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>621e346f67b776a24081744f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.282943</td>\n",
       "      <td>256.298391</td>\n",
       "      <td>0.738235</td>\n",
       "      <td>2879.708500</td>\n",
       "      <td>0.950313</td>\n",
       "      <td>4.593337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.168951</td>\n",
       "      <td>1054.092553</td>\n",
       "      <td>1581.138830</td>\n",
       "      <td>2.063596</td>\n",
       "      <td>73.811373</td>\n",
       "      <td>18.409560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>264.328432</td>\n",
       "      <td>109.811054</td>\n",
       "      <td>3.393689</td>\n",
       "      <td>0.486144</td>\n",
       "      <td>22.882605</td>\n",
       "      <td>0.506941</td>\n",
       "      <td>5.639735</td>\n",
       "      <td>3.376689</td>\n",
       "      <td>6.926666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.915737</td>\n",
       "      <td>0.313741</td>\n",
       "      <td>5.160637e+06</td>\n",
       "      <td>3.160492</td>\n",
       "      <td>0.210179</td>\n",
       "      <td>0.340331</td>\n",
       "      <td>0.295085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1054.092553</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>4344.882147</td>\n",
       "      <td>11.901518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>621e34ca67b776a240be3b69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.071728</td>\n",
       "      <td>143.721667</td>\n",
       "      <td>0.685836</td>\n",
       "      <td>1954.447955</td>\n",
       "      <td>0.700648</td>\n",
       "      <td>0.554390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.877307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.873386</td>\n",
       "      <td>101.305584</td>\n",
       "      <td>24.848286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>449.353562</td>\n",
       "      <td>139.095854</td>\n",
       "      <td>11.677866</td>\n",
       "      <td>0.420084</td>\n",
       "      <td>5.838127</td>\n",
       "      <td>0.327364</td>\n",
       "      <td>2.491380</td>\n",
       "      <td>1.828111</td>\n",
       "      <td>3.078852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>303.935906</td>\n",
       "      <td>0.295060</td>\n",
       "      <td>7.056230e+06</td>\n",
       "      <td>2.517873</td>\n",
       "      <td>0.198599</td>\n",
       "      <td>0.295372</td>\n",
       "      <td>0.302204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2879.744309</td>\n",
       "      <td>4.090605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>621e34db67b776a240c9c2be</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.961261</td>\n",
       "      <td>351.092045</td>\n",
       "      <td>0.371869</td>\n",
       "      <td>2203.571835</td>\n",
       "      <td>0.369974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.160729</td>\n",
       "      <td>864.312197</td>\n",
       "      <td>864.312197</td>\n",
       "      <td>2.702990</td>\n",
       "      <td>44.131918</td>\n",
       "      <td>11.158233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>191.262602</td>\n",
       "      <td>75.014508</td>\n",
       "      <td>7.061296</td>\n",
       "      <td>0.895779</td>\n",
       "      <td>7.054555</td>\n",
       "      <td>0.377124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.590923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271.113650</td>\n",
       "      <td>0.283817</td>\n",
       "      <td>3.002702e+06</td>\n",
       "      <td>1.993837</td>\n",
       "      <td>0.202408</td>\n",
       "      <td>0.715087</td>\n",
       "      <td>0.186931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864.312197</td>\n",
       "      <td>0.288104</td>\n",
       "      <td>2981.727421</td>\n",
       "      <td>5.190448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>621e34ec67b776a240d60873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.287028</td>\n",
       "      <td>429.629720</td>\n",
       "      <td>1.184388</td>\n",
       "      <td>4461.142415</td>\n",
       "      <td>0.164451</td>\n",
       "      <td>0.681502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.593431</td>\n",
       "      <td>5773.502692</td>\n",
       "      <td>4618.802154</td>\n",
       "      <td>2.131770</td>\n",
       "      <td>54.450793</td>\n",
       "      <td>18.615704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>411.468568</td>\n",
       "      <td>123.853774</td>\n",
       "      <td>15.565327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.622183</td>\n",
       "      <td>2.308408</td>\n",
       "      <td>4.286223</td>\n",
       "      <td>1.675323</td>\n",
       "      <td>2.618873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197.330957</td>\n",
       "      <td>0.513654</td>\n",
       "      <td>3.864655e+06</td>\n",
       "      <td>3.368151</td>\n",
       "      <td>0.223868</td>\n",
       "      <td>0.402005</td>\n",
       "      <td>0.286496</td>\n",
       "      <td>0.738241</td>\n",
       "      <td>5772.925342</td>\n",
       "      <td>2.309401</td>\n",
       "      <td>6332.922159</td>\n",
       "      <td>22.762507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>621e34f767b776a240de4e1a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.911654</td>\n",
       "      <td>274.103317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2802.343364</td>\n",
       "      <td>0.287531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.955214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>197.282792</td>\n",
       "      <td>7.778175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>182.016068</td>\n",
       "      <td>107.427392</td>\n",
       "      <td>25.527452</td>\n",
       "      <td>3.516260</td>\n",
       "      <td>13.014937</td>\n",
       "      <td>0.407550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.522136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.137028e+07</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4147.102452</td>\n",
       "      <td>9.051935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>621e34ff67b776a240e446d6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.492922</td>\n",
       "      <td>628.396278</td>\n",
       "      <td>0.561892</td>\n",
       "      <td>4474.277251</td>\n",
       "      <td>0.574657</td>\n",
       "      <td>2.967734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.803692</td>\n",
       "      <td>3592.426178</td>\n",
       "      <td>2615.598349</td>\n",
       "      <td>14.552230</td>\n",
       "      <td>97.713129</td>\n",
       "      <td>52.701269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231.257012</td>\n",
       "      <td>67.069287</td>\n",
       "      <td>19.084835</td>\n",
       "      <td>7.814084</td>\n",
       "      <td>27.122494</td>\n",
       "      <td>0.490009</td>\n",
       "      <td>11.344246</td>\n",
       "      <td>3.800464</td>\n",
       "      <td>19.880470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>381.015197</td>\n",
       "      <td>0.305796</td>\n",
       "      <td>7.686313e+06</td>\n",
       "      <td>8.727714</td>\n",
       "      <td>0.235782</td>\n",
       "      <td>0.336560</td>\n",
       "      <td>0.305961</td>\n",
       "      <td>0.680651</td>\n",
       "      <td>3592.426178</td>\n",
       "      <td>0.963228</td>\n",
       "      <td>6144.744453</td>\n",
       "      <td>42.408989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>621e351a67b776a240f6204b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.723424</td>\n",
       "      <td>489.778983</td>\n",
       "      <td>0.560825</td>\n",
       "      <td>3272.236340</td>\n",
       "      <td>0.650775</td>\n",
       "      <td>1.230991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.749910</td>\n",
       "      <td>1648.326767</td>\n",
       "      <td>1849.698259</td>\n",
       "      <td>3.191370</td>\n",
       "      <td>82.615947</td>\n",
       "      <td>22.140228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>264.557221</td>\n",
       "      <td>74.857443</td>\n",
       "      <td>19.784814</td>\n",
       "      <td>0.408937</td>\n",
       "      <td>21.085988</td>\n",
       "      <td>0.433419</td>\n",
       "      <td>8.522220</td>\n",
       "      <td>3.131163</td>\n",
       "      <td>20.934396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>293.795965</td>\n",
       "      <td>0.298019</td>\n",
       "      <td>6.000150e+06</td>\n",
       "      <td>3.746843</td>\n",
       "      <td>0.267807</td>\n",
       "      <td>0.671425</td>\n",
       "      <td>0.322591</td>\n",
       "      <td>1.243116</td>\n",
       "      <td>1648.326767</td>\n",
       "      <td>0.616566</td>\n",
       "      <td>4486.731665</td>\n",
       "      <td>30.772508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>621e356967b776a24027bd9f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.894385</td>\n",
       "      <td>342.242504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2856.032665</td>\n",
       "      <td>0.686746</td>\n",
       "      <td>0.533907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.464984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.629080</td>\n",
       "      <td>48.569700</td>\n",
       "      <td>15.653401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109.603795</td>\n",
       "      <td>79.104334</td>\n",
       "      <td>10.371045</td>\n",
       "      <td>12.600173</td>\n",
       "      <td>17.208899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.295279</td>\n",
       "      <td>3.336830</td>\n",
       "      <td>21.735361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145.528112</td>\n",
       "      <td>0.417086</td>\n",
       "      <td>3.510572e+06</td>\n",
       "      <td>2.282121</td>\n",
       "      <td>0.154441</td>\n",
       "      <td>0.234183</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3404.827583</td>\n",
       "      <td>26.661226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>621e360b67b776a24039709f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.061105</td>\n",
       "      <td>386.153263</td>\n",
       "      <td>0.855516</td>\n",
       "      <td>4128.485922</td>\n",
       "      <td>0.848097</td>\n",
       "      <td>3.058334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.650181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.244736</td>\n",
       "      <td>86.979454</td>\n",
       "      <td>20.291494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>362.687602</td>\n",
       "      <td>86.501965</td>\n",
       "      <td>6.235290</td>\n",
       "      <td>0.358944</td>\n",
       "      <td>26.155474</td>\n",
       "      <td>0.570762</td>\n",
       "      <td>14.082555</td>\n",
       "      <td>1.753078</td>\n",
       "      <td>13.292221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>377.094309</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>6.017665e+06</td>\n",
       "      <td>3.078841</td>\n",
       "      <td>0.200991</td>\n",
       "      <td>0.498824</td>\n",
       "      <td>0.296885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6061.955019</td>\n",
       "      <td>11.453121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>621e362467b776a2404ad513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.332905</td>\n",
       "      <td>339.600269</td>\n",
       "      <td>0.871593</td>\n",
       "      <td>2753.476418</td>\n",
       "      <td>0.664338</td>\n",
       "      <td>2.434995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.445832</td>\n",
       "      <td>4451.903463</td>\n",
       "      <td>3134.682531</td>\n",
       "      <td>0.237635</td>\n",
       "      <td>106.319817</td>\n",
       "      <td>23.854872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>353.271349</td>\n",
       "      <td>151.216017</td>\n",
       "      <td>13.211942</td>\n",
       "      <td>12.736626</td>\n",
       "      <td>12.409696</td>\n",
       "      <td>0.351489</td>\n",
       "      <td>11.207136</td>\n",
       "      <td>2.888025</td>\n",
       "      <td>7.033274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310.425098</td>\n",
       "      <td>0.346997</td>\n",
       "      <td>7.471484e+06</td>\n",
       "      <td>2.057602</td>\n",
       "      <td>0.241514</td>\n",
       "      <td>0.397756</td>\n",
       "      <td>0.391313</td>\n",
       "      <td>1.529520</td>\n",
       "      <td>4451.515577</td>\n",
       "      <td>1.557043</td>\n",
       "      <td>3846.316792</td>\n",
       "      <td>13.191246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>621e366567b776a24076a727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.430404</td>\n",
       "      <td>541.086360</td>\n",
       "      <td>0.372036</td>\n",
       "      <td>2825.107253</td>\n",
       "      <td>0.565026</td>\n",
       "      <td>1.847917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.161433</td>\n",
       "      <td>1358.846808</td>\n",
       "      <td>1804.035880</td>\n",
       "      <td>2.844833</td>\n",
       "      <td>60.490731</td>\n",
       "      <td>15.277903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>181.680536</td>\n",
       "      <td>50.375926</td>\n",
       "      <td>5.306457</td>\n",
       "      <td>0.983999</td>\n",
       "      <td>33.830385</td>\n",
       "      <td>0.394529</td>\n",
       "      <td>2.831545</td>\n",
       "      <td>1.854080</td>\n",
       "      <td>8.535313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>284.746986</td>\n",
       "      <td>0.231727</td>\n",
       "      <td>4.239713e+06</td>\n",
       "      <td>3.266094</td>\n",
       "      <td>0.265270</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.354105</td>\n",
       "      <td>0.542066</td>\n",
       "      <td>1358.846808</td>\n",
       "      <td>0.601345</td>\n",
       "      <td>4078.703018</td>\n",
       "      <td>25.518321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>621e367e67b776a24087d75d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.368774</td>\n",
       "      <td>493.537280</td>\n",
       "      <td>0.619367</td>\n",
       "      <td>2666.955407</td>\n",
       "      <td>0.670923</td>\n",
       "      <td>4.772355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.750603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.546493</td>\n",
       "      <td>85.759261</td>\n",
       "      <td>15.794813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>283.678374</td>\n",
       "      <td>128.232863</td>\n",
       "      <td>1.819046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.298367</td>\n",
       "      <td>0.571933</td>\n",
       "      <td>5.742360</td>\n",
       "      <td>2.811835</td>\n",
       "      <td>6.614765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>299.643831</td>\n",
       "      <td>0.299299</td>\n",
       "      <td>5.863997e+06</td>\n",
       "      <td>2.606369</td>\n",
       "      <td>0.247075</td>\n",
       "      <td>0.399194</td>\n",
       "      <td>0.288283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3910.970539</td>\n",
       "      <td>15.437449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>621e36bb67b776a240b40d64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.737098</td>\n",
       "      <td>431.723638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4222.995560</td>\n",
       "      <td>0.821766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138.991421</td>\n",
       "      <td>4369.314488</td>\n",
       "      <td>3487.640515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>189.832349</td>\n",
       "      <td>115.828143</td>\n",
       "      <td>9.695810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.267230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.105555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165.569995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.242641e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4369.314488</td>\n",
       "      <td>1.103713</td>\n",
       "      <td>6510.176877</td>\n",
       "      <td>12.985607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>621e36c267b776a240ba2756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.067612</td>\n",
       "      <td>572.812881</td>\n",
       "      <td>0.488540</td>\n",
       "      <td>3393.013595</td>\n",
       "      <td>0.457191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.220365</td>\n",
       "      <td>2758.602927</td>\n",
       "      <td>2234.839031</td>\n",
       "      <td>0.210707</td>\n",
       "      <td>77.477065</td>\n",
       "      <td>15.704526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>293.369632</td>\n",
       "      <td>115.555999</td>\n",
       "      <td>18.814207</td>\n",
       "      <td>0.125988</td>\n",
       "      <td>33.750328</td>\n",
       "      <td>0.494226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.085960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>341.865202</td>\n",
       "      <td>0.268407</td>\n",
       "      <td>5.411164e+06</td>\n",
       "      <td>2.297508</td>\n",
       "      <td>0.239305</td>\n",
       "      <td>0.303629</td>\n",
       "      <td>0.331738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2758.602927</td>\n",
       "      <td>0.841897</td>\n",
       "      <td>4245.010432</td>\n",
       "      <td>29.729212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>621e36dd67b776a240ce9a45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.715774</td>\n",
       "      <td>559.326386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3791.787320</td>\n",
       "      <td>1.157102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.288214</td>\n",
       "      <td>1602.554779</td>\n",
       "      <td>1083.624669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.077711</td>\n",
       "      <td>4.163332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.466921</td>\n",
       "      <td>67.617520</td>\n",
       "      <td>16.734212</td>\n",
       "      <td>1.202586</td>\n",
       "      <td>18.470124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.510381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.556723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.949983e+06</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1602.554779</td>\n",
       "      <td>0.426401</td>\n",
       "      <td>4911.054001</td>\n",
       "      <td>29.397413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>621e36f967b776a240e5e7c9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.304942</td>\n",
       "      <td>401.149188</td>\n",
       "      <td>0.634171</td>\n",
       "      <td>4012.881654</td>\n",
       "      <td>0.415635</td>\n",
       "      <td>0.494495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.354145</td>\n",
       "      <td>3587.565198</td>\n",
       "      <td>2472.758736</td>\n",
       "      <td>1.009050</td>\n",
       "      <td>73.388629</td>\n",
       "      <td>10.928741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>272.201284</td>\n",
       "      <td>87.783373</td>\n",
       "      <td>11.564906</td>\n",
       "      <td>1.742808</td>\n",
       "      <td>21.383593</td>\n",
       "      <td>0.489525</td>\n",
       "      <td>4.421718</td>\n",
       "      <td>2.294795</td>\n",
       "      <td>17.012828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>345.647143</td>\n",
       "      <td>0.209374</td>\n",
       "      <td>4.873251e+06</td>\n",
       "      <td>1.878103</td>\n",
       "      <td>0.164576</td>\n",
       "      <td>0.225304</td>\n",
       "      <td>0.214419</td>\n",
       "      <td>0.738650</td>\n",
       "      <td>3587.519432</td>\n",
       "      <td>0.939575</td>\n",
       "      <td>5844.593245</td>\n",
       "      <td>29.616532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>621e375367b776a24021e950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.967566</td>\n",
       "      <td>560.583292</td>\n",
       "      <td>1.701892</td>\n",
       "      <td>3050.448676</td>\n",
       "      <td>0.364927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.913895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>171.449701</td>\n",
       "      <td>15.206906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>284.813747</td>\n",
       "      <td>89.423190</td>\n",
       "      <td>17.276058</td>\n",
       "      <td>13.117149</td>\n",
       "      <td>12.232178</td>\n",
       "      <td>2.113160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.214767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>171.606947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.062868e+07</td>\n",
       "      <td>0.971825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4227.040783</td>\n",
       "      <td>19.739210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>621e375b67b776a240290cdc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.575472</td>\n",
       "      <td>405.942981</td>\n",
       "      <td>0.679779</td>\n",
       "      <td>2554.173563</td>\n",
       "      <td>0.586119</td>\n",
       "      <td>0.712772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.916056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.482892</td>\n",
       "      <td>97.714550</td>\n",
       "      <td>19.918464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>224.934369</td>\n",
       "      <td>105.613360</td>\n",
       "      <td>12.714794</td>\n",
       "      <td>0.166633</td>\n",
       "      <td>27.459140</td>\n",
       "      <td>0.855560</td>\n",
       "      <td>4.296365</td>\n",
       "      <td>2.787740</td>\n",
       "      <td>3.276017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>325.714637</td>\n",
       "      <td>0.295221</td>\n",
       "      <td>6.891690e+06</td>\n",
       "      <td>2.106268</td>\n",
       "      <td>0.198393</td>\n",
       "      <td>0.330632</td>\n",
       "      <td>0.305634</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3872.471535</td>\n",
       "      <td>21.301009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  age_std  bmi_std    bpm_std  calories_std  \\\n",
       "0   621e2e8e67b776a24055b564      0.0      0.0   3.115235    268.393641   \n",
       "1   621e2eaf67b776a2406b14ac      0.0      0.0   3.885394    361.985428   \n",
       "2   621e2ed667b776a24085d8d1      0.0      0.0   4.278665    191.883956   \n",
       "3   621e2ef567b776a24099f889      0.0      0.0  18.439423    421.581261   \n",
       "4   621e2efa67b776a2409dd1c3      0.0      0.0   6.611945    444.840078   \n",
       "5   621e2f1b67b776a240b3d87c      0.0      0.0   4.206342    229.874475   \n",
       "6   621e2f3967b776a240c654db      0.0      0.0   7.494502    485.469253   \n",
       "7   621e2f5767b776a240d8f9d6      0.0      0.0   8.394377    539.547830   \n",
       "8   621e2f6167b776a240e082a9      0.0      0.0   7.867758    364.175994   \n",
       "9   621e2f7a67b776a240f14425      0.0      0.0  12.283408    831.618631   \n",
       "10  621e2f9167b776a240011ccb      0.0      0.0   3.838724    178.765850   \n",
       "11  621e2fb367b776a24015accd      0.0      0.0   9.548097    442.612334   \n",
       "12  621e2fce67b776a240279baa      0.0      0.0   6.154261    650.783162   \n",
       "13  621e2ff067b776a2403eb737      0.0      0.0   2.753444    373.494296   \n",
       "14  621e300767b776a2404dc717      0.0      0.0   5.224496    858.935510   \n",
       "15  621e301367b776a24057738e      0.0      0.0  10.542366    437.075187   \n",
       "16  621e301e67b776a240608a72      0.0      0.0  10.457537    998.220801   \n",
       "17  621e309267b776a240ae1cdb      0.0      0.0   9.097517    618.523493   \n",
       "18  621e309b67b776a240b532b0      0.0      0.0  12.417155    793.519343   \n",
       "19  621e30b267b776a240c5e13f      0.0      0.0   4.691032    447.173574   \n",
       "20  621e30c867b776a240d4aa6c      0.0      0.0   5.354255    258.699086   \n",
       "21  621e30e267b776a240e5bf90      0.0      0.0        NaN    188.335792   \n",
       "22  621e30e467b776a240e817c7      0.0      0.0  21.459930   1306.632137   \n",
       "23  621e30f467b776a240f22944      0.0      0.0   4.864180    587.728822   \n",
       "24  621e310d67b776a24003096d      0.0      0.0   6.311126    319.741161   \n",
       "25  621e312a67b776a240164d59      0.0      0.0   5.823333    694.697704   \n",
       "26  621e314867b776a24029ebf9      0.0      0.0   9.034591    568.527928   \n",
       "27  621e320b67b776a240d36a07      0.0      0.0   5.812559    606.158831   \n",
       "28  621e322367b776a240e44e9b      0.0      0.0   6.263333    923.720727   \n",
       "29  621e323667b776a240f19134      0.0      0.0   2.746812    394.329912   \n",
       "30  621e324e67b776a2400191cb      0.0      0.0   3.289063    395.629615   \n",
       "31  621e326767b776a24012e179      0.0      0.0   5.150367    294.723219   \n",
       "32  621e328667b776a240281372      0.0      0.0   5.462322    433.778208   \n",
       "33  621e329067b776a2402ffad2      0.0      0.0   3.437519    344.623444   \n",
       "34  621e32af67b776a24045b4cf      0.0      0.0   3.821743    254.878544   \n",
       "35  621e32d067b776a2405b7d54      0.0      0.0   5.754418    476.472701   \n",
       "36  621e32d967b776a240627414      0.0      0.0   8.038625    758.289950   \n",
       "37  621e32e667b776a2406d2f1c      0.0      0.0   6.650352    372.124261   \n",
       "38  621e331067b776a24085dd3f      0.0      0.0   5.709254    379.444839   \n",
       "39  621e332267b776a24092a584      0.0      0.0   8.345370    706.247053   \n",
       "40  621e333567b776a240a0c217      0.0      0.0   4.517850    293.563087   \n",
       "41  621e333967b776a240a3cd06      0.0      0.0   7.978838    564.948789   \n",
       "42  621e335a67b776a240bb12ff      0.0      0.0   4.293807    285.384726   \n",
       "43  621e337667b776a240ce78ab      0.0      0.0   5.006323    408.040474   \n",
       "44  621e339967b776a240e502de      0.0      0.0   7.180612    333.288547   \n",
       "45  621e33b067b776a240f39e56      0.0      0.0   3.659364    434.958310   \n",
       "46  621e33cf67b776a240087de9      0.0      0.0   4.611652    552.845079   \n",
       "47  621e33ed67b776a2401cf5f7      0.0      0.0   3.331066    497.470614   \n",
       "48  621e340467b776a2402d7982      0.0      0.0   9.856454    507.667999   \n",
       "49  621e341067b776a24037b105      0.0      0.0   4.209001    420.832714   \n",
       "50  621e342e67b776a2404ce460      0.0      0.0  10.963159    639.934028   \n",
       "51  621e345267b776a240691064      0.0      0.0   6.813389    432.862978   \n",
       "52  621e345c67b776a2407146a8      0.0      0.0   7.618648    299.782763   \n",
       "53  621e346f67b776a24081744f      NaN      NaN   4.282943    256.298391   \n",
       "54  621e34ca67b776a240be3b69      0.0      0.0   9.071728    143.721667   \n",
       "55  621e34db67b776a240c9c2be      0.0      0.0   2.961261    351.092045   \n",
       "56  621e34ec67b776a240d60873      0.0      0.0  10.287028    429.629720   \n",
       "57  621e34f767b776a240de4e1a      0.0      0.0  10.911654    274.103317   \n",
       "58  621e34ff67b776a240e446d6      0.0      0.0   5.492922    628.396278   \n",
       "59  621e351a67b776a240f6204b      0.0      0.0   8.723424    489.778983   \n",
       "60  621e356967b776a24027bd9f      NaN      NaN   5.894385    342.242504   \n",
       "61  621e360b67b776a24039709f      0.0      0.0   5.061105    386.153263   \n",
       "62  621e362467b776a2404ad513      0.0      0.0   6.332905    339.600269   \n",
       "63  621e366567b776a24076a727      0.0      0.0   4.430404    541.086360   \n",
       "64  621e367e67b776a24087d75d      NaN      NaN   4.368774    493.537280   \n",
       "65  621e36bb67b776a240b40d64      0.0      0.0   6.737098    431.723638   \n",
       "66  621e36c267b776a240ba2756      0.0      0.0   6.067612    572.812881   \n",
       "67  621e36dd67b776a240ce9a45      0.0      0.0   8.715774    559.326386   \n",
       "68  621e36f967b776a240e5e7c9      0.0      0.0   6.304942    401.149188   \n",
       "69  621e375367b776a24021e950      0.0      0.0   7.967566    560.583292   \n",
       "70  621e375b67b776a240290cdc      0.0      0.0   5.575472    405.942981   \n",
       "\n",
       "    daily1temperature1variation_std  distance_std  \\\n",
       "0                          0.570898   2495.171517   \n",
       "1                          0.593731   4162.391371   \n",
       "2                          0.644815   1739.393218   \n",
       "3                               NaN   3374.706124   \n",
       "4                          0.723875   3454.517701   \n",
       "5                          0.607866   1884.230068   \n",
       "6                          0.831227   2747.269423   \n",
       "7                          0.756005   4491.329470   \n",
       "8                          0.793513   2869.464415   \n",
       "9                          0.651416   4742.986715   \n",
       "10                         0.613612   1615.812527   \n",
       "11                         0.895167   2882.059970   \n",
       "12                         0.502053   3688.292605   \n",
       "13                         0.606116   1893.468125   \n",
       "14                         0.866752   4697.812939   \n",
       "15                              NaN   3525.117138   \n",
       "16                         0.824319   3894.279294   \n",
       "17                         0.384837   2847.395913   \n",
       "18                         0.816167   6807.277819   \n",
       "19                         0.709569   2642.691534   \n",
       "20                         0.624105   2715.697331   \n",
       "21                              NaN           NaN   \n",
       "22                         1.527309   3532.329056   \n",
       "23                         0.551962   5209.616733   \n",
       "24                         0.591500   2515.269609   \n",
       "25                         0.785540   3634.578628   \n",
       "26                         0.832059   4029.130721   \n",
       "27                         0.585386   3736.194826   \n",
       "28                         0.560433   4824.819166   \n",
       "29                         0.700069   1932.322522   \n",
       "30                         0.358968   1840.205451   \n",
       "31                         0.643569   3137.534246   \n",
       "32                         0.315369   2368.441636   \n",
       "33                         0.382699   2123.200622   \n",
       "34                         0.427655   2216.573742   \n",
       "35                         0.663381   3886.612503   \n",
       "36                         1.314977   5775.772830   \n",
       "37                         0.496201   2452.466760   \n",
       "38                         0.582873   1447.993622   \n",
       "39                         0.543419   6587.221005   \n",
       "40                              NaN   2931.093071   \n",
       "41                         0.865269   3287.330653   \n",
       "42                         0.550272   2718.555712   \n",
       "43                         0.629650   2776.802692   \n",
       "44                         0.624139   4290.627861   \n",
       "45                         0.437581   2767.098252   \n",
       "46                         0.508310   3111.735145   \n",
       "47                         0.429344   3069.016444   \n",
       "48                              NaN   2829.757782   \n",
       "49                         0.448376   2285.546941   \n",
       "50                         0.733043   3943.450061   \n",
       "51                         0.792715   2706.564597   \n",
       "52                              NaN   2567.115223   \n",
       "53                         0.738235   2879.708500   \n",
       "54                         0.685836   1954.447955   \n",
       "55                         0.371869   2203.571835   \n",
       "56                         1.184388   4461.142415   \n",
       "57                              NaN   2802.343364   \n",
       "58                         0.561892   4474.277251   \n",
       "59                         0.560825   3272.236340   \n",
       "60                              NaN   2856.032665   \n",
       "61                         0.855516   4128.485922   \n",
       "62                         0.871593   2753.476418   \n",
       "63                         0.372036   2825.107253   \n",
       "64                         0.619367   2666.955407   \n",
       "65                              NaN   4222.995560   \n",
       "66                         0.488540   3393.013595   \n",
       "67                              NaN   3791.787320   \n",
       "68                         0.634171   4012.881654   \n",
       "69                         1.701892   3050.448676   \n",
       "70                         0.679779   2554.173563   \n",
       "\n",
       "    filteredDemographicVO2Max_std  full1sleep1breathing1rate_std  gender_std  \\\n",
       "0                        0.593936                       0.569264         0.0   \n",
       "1                        1.380626                       0.536698         0.0   \n",
       "2                        0.797480                       5.797068         0.0   \n",
       "3                        0.414427                            NaN         0.0   \n",
       "4                        1.348074                            NaN         0.0   \n",
       "5                        1.713408                       0.822889         0.0   \n",
       "6                        0.987326                       5.759812         0.0   \n",
       "7                        2.133406                            NaN         0.0   \n",
       "8                        0.529432                       0.830599         0.0   \n",
       "9                        0.199581                            NaN         0.0   \n",
       "10                       0.492603                       0.576618         0.0   \n",
       "11                       0.337607                            NaN         0.0   \n",
       "12                       0.665279                       0.835135         0.0   \n",
       "13                       0.370978                            NaN         0.0   \n",
       "14                       0.391154                            NaN         0.0   \n",
       "15                       0.272389                            NaN         0.0   \n",
       "16                       1.043161                            NaN         0.0   \n",
       "17                       0.721162                            NaN         0.0   \n",
       "18                       0.610893                            NaN         0.0   \n",
       "19                       0.308310                       4.605268         0.0   \n",
       "20                       0.660949                       2.630871         0.0   \n",
       "21                            NaN                            NaN         0.0   \n",
       "22                       0.386383                       6.605695         0.0   \n",
       "23                       0.400693                       0.303315         0.0   \n",
       "24                       0.819949                       0.638058         0.0   \n",
       "25                       0.765350                       0.866418         0.0   \n",
       "26                       0.291852                            NaN         0.0   \n",
       "27                       0.411686                       1.157345         0.0   \n",
       "28                       0.852744                       0.829691         0.0   \n",
       "29                       1.137081                       0.650998         0.0   \n",
       "30                       0.408398                            NaN         0.0   \n",
       "31                       0.725879                       1.882794         0.0   \n",
       "32                       0.108469                       0.806639         0.0   \n",
       "33                       0.265054                       0.780944         0.0   \n",
       "34                       0.505792                       0.568304         0.0   \n",
       "35                       0.438958                            NaN         0.0   \n",
       "36                       0.393921                       4.496898         0.0   \n",
       "37                       0.895622                       0.841080         0.0   \n",
       "38                       0.555822                            NaN         0.0   \n",
       "39                       0.171099                       1.027433         0.0   \n",
       "40                       0.242340                            NaN         0.0   \n",
       "41                       0.998054                       6.715698         0.0   \n",
       "42                       1.073530                            NaN         0.0   \n",
       "43                       0.316337                       0.832718         0.0   \n",
       "44                       1.254990                       1.712996         0.0   \n",
       "45                       0.436672                       0.560133         0.0   \n",
       "46                       0.407413                       2.774618         0.0   \n",
       "47                       0.410085                            NaN         0.0   \n",
       "48                       1.195321                            NaN         0.0   \n",
       "49                       0.381021                       4.941968         0.0   \n",
       "50                       2.530828                       0.928743         0.0   \n",
       "51                       0.795618                            NaN         0.0   \n",
       "52                       0.739161                            NaN         0.0   \n",
       "53                       0.950313                       4.593337         NaN   \n",
       "54                       0.700648                       0.554390         0.0   \n",
       "55                       0.369974                            NaN         0.0   \n",
       "56                       0.164451                       0.681502         0.0   \n",
       "57                       0.287531                            NaN         0.0   \n",
       "58                       0.574657                       2.967734         0.0   \n",
       "59                       0.650775                       1.230991         0.0   \n",
       "60                       0.686746                       0.533907         NaN   \n",
       "61                       0.848097                       3.058334         0.0   \n",
       "62                       0.664338                       2.434995         0.0   \n",
       "63                       0.565026                       1.847917         0.0   \n",
       "64                       0.670923                       4.772355         0.0   \n",
       "65                       0.821766                            NaN         0.0   \n",
       "66                       0.457191                            NaN         0.0   \n",
       "67                       1.157102                            NaN         0.0   \n",
       "68                       0.415635                       0.494495         0.0   \n",
       "69                       0.364927                            NaN         0.0   \n",
       "70                       0.586119                       0.712772         0.0   \n",
       "\n",
       "    lightly1active1minutes_std  max1goal_std  min1goal_std  \\\n",
       "0                    39.619691   2708.012802   2337.602931   \n",
       "1                    73.807592      0.000000      0.000000   \n",
       "2                    84.521424   3228.366225   2651.114688   \n",
       "3                    25.705161           NaN           NaN   \n",
       "4                   129.259973           NaN           NaN   \n",
       "5                   102.115416   1811.501714   1392.286443   \n",
       "6                    32.626832   2784.452557   2514.567359   \n",
       "7                   137.911893   2129.076568    851.630627   \n",
       "8                   118.848650   2945.486263   2379.922608   \n",
       "9                    91.198107   1587.094176   1122.322888   \n",
       "10                   62.376826   1251.766723   1396.957536   \n",
       "11                   94.095268   3318.135781   2981.248834   \n",
       "12                   64.041861   2087.615080   2123.885424   \n",
       "13                  104.014633   1125.634799   1033.544776   \n",
       "14                  154.428953           NaN           NaN   \n",
       "15                   89.517661   2880.972058   3033.150178   \n",
       "16                   92.745773   1460.593487    912.870929   \n",
       "17                  103.952895      0.000000      0.000000   \n",
       "18                  110.756290           NaN           NaN   \n",
       "19                  106.396457           NaN           NaN   \n",
       "20                  120.366329   1869.200148   1789.287676   \n",
       "21                   12.605481    282.842712      0.000000   \n",
       "22                   42.899269   1780.005914   1823.819012   \n",
       "23                  117.039900   4532.693032   4035.606825   \n",
       "24                  106.959205   1708.658010   1428.371066   \n",
       "25                   87.960025   1519.109051   1012.739367   \n",
       "26                   74.513157   2772.473568   2512.079081   \n",
       "27                   68.969921           NaN           NaN   \n",
       "28                  119.752862  10620.848335   8241.258810   \n",
       "29                  103.255077    944.465203   1181.757896   \n",
       "30                   95.869732   1784.236754   1463.824305   \n",
       "31                  106.513193   3620.020983   3475.989579   \n",
       "32                   70.380064      0.000000      0.000000   \n",
       "33                  101.082997   2092.493984   1106.095081   \n",
       "34                   60.844673   1558.512672   1632.658498   \n",
       "35                  132.982173   4868.679534   3596.715674   \n",
       "36                   70.708789   4404.054828   3654.721501   \n",
       "37                   71.050900   3535.533906   1414.213562   \n",
       "38                  127.242220      0.000000      0.000000   \n",
       "39                   82.069370   2672.612419   1995.530721   \n",
       "40                   66.297461           NaN           NaN   \n",
       "41                  107.961691   2886.751346   1154.700538   \n",
       "42                   91.965168      0.000000      0.000000   \n",
       "43                   88.018499   2626.970503   2543.900268   \n",
       "44                   67.904043   3934.211735   2991.746889   \n",
       "45                  122.572016   3252.153553   2080.878335   \n",
       "46                   91.619443   5196.831614   4135.641354   \n",
       "47                   88.033796   1463.850109    975.900073   \n",
       "48                   77.718089   1892.620031   1595.297322   \n",
       "49                  109.748410    603.022689      0.000000   \n",
       "50                   93.797144           NaN           NaN   \n",
       "51                  144.765629           NaN           NaN   \n",
       "52                  122.942943   1095.445115   1643.167673   \n",
       "53                   61.168951   1054.092553   1581.138830   \n",
       "54                   61.877307      0.000000      0.000000   \n",
       "55                  112.160729    864.312197    864.312197   \n",
       "56                   61.593431   5773.502692   4618.802154   \n",
       "57                   77.955214           NaN           NaN   \n",
       "58                  134.803692   3592.426178   2615.598349   \n",
       "59                  100.749910   1648.326767   1849.698259   \n",
       "60                   65.464984           NaN           NaN   \n",
       "61                  154.650181           NaN           NaN   \n",
       "62                   81.445832   4451.903463   3134.682531   \n",
       "63                  119.161433   1358.846808   1804.035880   \n",
       "64                  107.750603           NaN           NaN   \n",
       "65                  138.991421   4369.314488   3487.640515   \n",
       "66                  120.220365   2758.602927   2234.839031   \n",
       "67                   90.288214   1602.554779   1083.624669   \n",
       "68                  115.354145   3587.565198   2472.758736   \n",
       "69                   76.913895           NaN           NaN   \n",
       "70                  109.916056      0.000000      0.000000   \n",
       "\n",
       "    minutesAfterWakeup_std  minutesAsleep_std  minutesAwake_std  \\\n",
       "0                 1.096940          53.017232         15.108025   \n",
       "1                 2.723198          89.888392         17.318701   \n",
       "2                 1.644982          77.658628         18.598939   \n",
       "3                      NaN                NaN               NaN   \n",
       "4                 2.517075          89.881128         19.035900   \n",
       "5                 1.145165         100.981543         25.669177   \n",
       "6                 0.939891         163.351774         27.663504   \n",
       "7                 0.962500         113.431492         24.709043   \n",
       "8                 1.659682          75.091944         18.665233   \n",
       "9                 2.340562          67.221969          9.190375   \n",
       "10                1.658982          90.329939         19.331824   \n",
       "11                0.939739          71.873806         13.165072   \n",
       "12                2.902816          77.886528         17.893749   \n",
       "13                0.869444          76.497278         15.056053   \n",
       "14                1.716863          95.590346         26.144470   \n",
       "15                     NaN                NaN               NaN   \n",
       "16                1.305898          96.344289         20.712876   \n",
       "17                1.064121          51.906095          7.904764   \n",
       "18                0.695649          82.615409         22.146525   \n",
       "19                1.987842          99.949893         19.658190   \n",
       "20                1.015370          73.404375         17.655049   \n",
       "21                     NaN                NaN               NaN   \n",
       "22                1.950243         146.114618         36.977471   \n",
       "23                1.887993          46.785648         13.937203   \n",
       "24                1.589199          98.023119         19.569588   \n",
       "25                1.759782          95.756708         20.474262   \n",
       "26                2.578176          89.855716         18.288327   \n",
       "27                1.394313          85.749369         18.670328   \n",
       "28                2.367414          97.626916         18.575200   \n",
       "29                1.561905          75.261435         23.290665   \n",
       "30                1.881008          94.426647         19.306979   \n",
       "31                1.749976          51.404872         13.470574   \n",
       "32                1.602082          75.978067         13.731958   \n",
       "33                1.973319          90.743649         15.688271   \n",
       "34                1.725227          49.455722         11.614966   \n",
       "35                1.847096         108.424441         27.412904   \n",
       "36                2.546624         137.256121         19.157244   \n",
       "37                2.767770          82.375415         21.199534   \n",
       "38                0.408680         103.467235         21.430797   \n",
       "39                5.954990         194.437773         24.434753   \n",
       "40                     NaN                NaN               NaN   \n",
       "41                1.903406          97.476805         27.003471   \n",
       "42                1.880588          73.689434         18.019191   \n",
       "43                0.919199          58.673190         19.823643   \n",
       "44                1.391533          72.830492         17.906793   \n",
       "45                0.551539          69.926843         14.331214   \n",
       "46                1.605172          57.378377         22.283044   \n",
       "47                0.614689          86.303977         16.351021   \n",
       "48                4.618802          82.963848          4.358899   \n",
       "49                0.932768          79.476650         14.994717   \n",
       "50                2.958928         121.584867         28.538631   \n",
       "51                1.852926         194.284699         31.670000   \n",
       "52                     NaN                NaN               NaN   \n",
       "53                2.063596          73.811373         18.409560   \n",
       "54                2.873386         101.305584         24.848286   \n",
       "55                2.702990          44.131918         11.158233   \n",
       "56                2.131770          54.450793         18.615704   \n",
       "57                0.000000         197.282792          7.778175   \n",
       "58               14.552230          97.713129         52.701269   \n",
       "59                3.191370          82.615947         22.140228   \n",
       "60                0.629080          48.569700         15.653401   \n",
       "61                2.244736          86.979454         20.291494   \n",
       "62                0.237635         106.319817         23.854872   \n",
       "63                2.844833          60.490731         15.277903   \n",
       "64                1.546493          85.759261         15.794813   \n",
       "65                0.000000           0.707107          0.000000   \n",
       "66                0.210707          77.477065         15.704526   \n",
       "67                0.000000          45.077711          4.163332   \n",
       "68                1.009050          73.388629         10.928741   \n",
       "69                0.000000         171.449701         15.206906   \n",
       "70                3.482892          97.714550         19.918464   \n",
       "\n",
       "    minutesToFallAsleep_std  minutes1below1default1zone11_std  \\\n",
       "0                  0.000000                        120.517602   \n",
       "1                  0.987878                        182.344647   \n",
       "2                  0.000000                        255.490681   \n",
       "3                       NaN                        202.514689   \n",
       "4                  0.000000                        394.129769   \n",
       "5                  0.000000                        241.523923   \n",
       "6                  0.000000                        279.134521   \n",
       "7                  0.000000                        344.160072   \n",
       "8                  0.000000                        415.203720   \n",
       "9                  0.000000                        346.977593   \n",
       "10                 0.000000                        161.955855   \n",
       "11                 0.000000                        266.224367   \n",
       "12                 0.875000                        216.209936   \n",
       "13                 0.000000                        164.992134   \n",
       "14                 0.000000                        347.670132   \n",
       "15                      NaN                        218.626171   \n",
       "16                 0.000000                        460.900916   \n",
       "17                 0.000000                        414.981920   \n",
       "18                 0.000000                        421.529253   \n",
       "19                 0.000000                        247.727226   \n",
       "20                 0.000000                        304.584665   \n",
       "21                      NaN                               NaN   \n",
       "22                 0.000000                        274.986683   \n",
       "23                 0.000000                        220.994740   \n",
       "24                 0.000000                        392.397360   \n",
       "25                 0.000000                        276.752596   \n",
       "26                 0.000000                        406.695482   \n",
       "27                 0.000000                        382.914261   \n",
       "28                 0.000000                        341.655077   \n",
       "29                 0.000000                        265.136651   \n",
       "30                 0.000000                        187.791496   \n",
       "31                 0.610847                        130.614334   \n",
       "32                 0.000000                        131.847157   \n",
       "33                 0.000000                        150.533387   \n",
       "34                 0.000000                        170.002724   \n",
       "35                 0.000000                        417.653309   \n",
       "36                 1.940285                        394.661125   \n",
       "37                 0.000000                        279.601267   \n",
       "38                 0.000000                        308.771573   \n",
       "39                 0.000000                        200.193595   \n",
       "40                      NaN                        360.879415   \n",
       "41                13.723561                        379.290695   \n",
       "42                 0.000000                        200.044323   \n",
       "43                 0.000000                        201.725416   \n",
       "44                 0.000000                        167.456899   \n",
       "45                 0.000000                        164.587107   \n",
       "46                 7.427439                        202.718048   \n",
       "47                 0.000000                        147.483036   \n",
       "48                 0.000000                        258.620255   \n",
       "49                 0.000000                        277.380008   \n",
       "50                 0.000000                        405.573686   \n",
       "51                 0.000000                        293.472936   \n",
       "52                      NaN                        174.646974   \n",
       "53                 0.000000                        264.328432   \n",
       "54                 0.000000                        449.353562   \n",
       "55                 0.000000                        191.262602   \n",
       "56                 0.000000                        411.468568   \n",
       "57                 0.000000                        182.016068   \n",
       "58                 0.000000                        231.257012   \n",
       "59                 0.000000                        264.557221   \n",
       "60                 0.000000                        109.603795   \n",
       "61                 0.000000                        362.687602   \n",
       "62                 0.000000                        353.271349   \n",
       "63                 0.000000                        181.680536   \n",
       "64                 0.000000                        283.678374   \n",
       "65                 0.000000                        189.832349   \n",
       "66                 0.000000                        293.369632   \n",
       "67                 0.000000                        185.466921   \n",
       "68                 0.000000                        272.201284   \n",
       "69                 0.000000                        284.813747   \n",
       "70                 0.000000                        224.934369   \n",
       "\n",
       "    minutes1in1default1zone11_std  minutes1in1default1zone12_std  \\\n",
       "0                       59.979071                       1.119463   \n",
       "1                      101.605571                      24.489053   \n",
       "2                      126.533886                       5.329953   \n",
       "3                       91.656307                      40.681447   \n",
       "4                      117.944882                      19.157025   \n",
       "5                      201.320773                       6.014706   \n",
       "6                      104.715502                      51.812301   \n",
       "7                      119.878349                      18.117268   \n",
       "8                       61.069562                      29.070513   \n",
       "9                       79.129365                      37.271939   \n",
       "10                     116.005118                      14.457583   \n",
       "11                      85.880613                      34.531166   \n",
       "12                     189.034983                      23.286413   \n",
       "13                      62.252720                       8.295894   \n",
       "14                      34.610914                       6.097886   \n",
       "15                      73.069235                      26.356118   \n",
       "16                      92.868182                      27.493769   \n",
       "17                     100.078103                      13.635152   \n",
       "18                      88.461447                      11.869467   \n",
       "19                     101.549570                       8.476688   \n",
       "20                      50.570310                      15.488554   \n",
       "21                            NaN                            NaN   \n",
       "22                      91.247331                      65.559019   \n",
       "23                      72.586330                       3.130187   \n",
       "24                     116.257735                      10.246390   \n",
       "25                      87.799653                       4.027673   \n",
       "26                      31.029329                       7.536466   \n",
       "27                      79.610170                       5.092782   \n",
       "28                     148.720445                      12.866571   \n",
       "29                      55.452000                       4.247052   \n",
       "30                     111.147846                       9.169413   \n",
       "31                      48.342719                       7.792567   \n",
       "32                      83.800738                      31.247400   \n",
       "33                     136.527091                      14.986767   \n",
       "34                      49.258189                       1.605163   \n",
       "35                      81.241916                      10.903536   \n",
       "36                     108.994506                      15.852139   \n",
       "37                     197.545469                      33.297145   \n",
       "38                     132.053127                      28.519937   \n",
       "39                     136.003151                       2.469020   \n",
       "40                      63.941938                      10.472185   \n",
       "41                     111.247495                       8.196036   \n",
       "42                     107.870066                      21.354337   \n",
       "43                     110.935148                       5.511562   \n",
       "44                      41.111679                      18.379690   \n",
       "45                     151.255156                       7.031348   \n",
       "46                     148.699650                      10.646546   \n",
       "47                      49.209858                       8.395586   \n",
       "48                     107.752316                      25.211528   \n",
       "49                     122.433314                       2.269361   \n",
       "50                      76.069417                      18.703976   \n",
       "51                     138.445164                      30.473512   \n",
       "52                     116.862691                      25.455408   \n",
       "53                     109.811054                       3.393689   \n",
       "54                     139.095854                      11.677866   \n",
       "55                      75.014508                       7.061296   \n",
       "56                     123.853774                      15.565327   \n",
       "57                     107.427392                      25.527452   \n",
       "58                      67.069287                      19.084835   \n",
       "59                      74.857443                      19.784814   \n",
       "60                      79.104334                      10.371045   \n",
       "61                      86.501965                       6.235290   \n",
       "62                     151.216017                      13.211942   \n",
       "63                      50.375926                       5.306457   \n",
       "64                     128.232863                       1.819046   \n",
       "65                     115.828143                       9.695810   \n",
       "66                     115.555999                      18.814207   \n",
       "67                      67.617520                      16.734212   \n",
       "68                      87.783373                      11.564906   \n",
       "69                      89.423190                      17.276058   \n",
       "70                     105.613360                      12.714794   \n",
       "\n",
       "    minutes1in1default1zone13_std  moderately1active1minutes_std  \\\n",
       "0                        0.000000                      19.083772   \n",
       "1                        1.675411                      26.665031   \n",
       "2                        0.752587                      11.704070   \n",
       "3                        6.633728                       6.349365   \n",
       "4                        0.904858                      20.105649   \n",
       "5                        0.875933                       5.250616   \n",
       "6                        3.948086                      20.399270   \n",
       "7                        0.464095                      30.699280   \n",
       "8                        0.871007                      20.386749   \n",
       "9                        0.717552                      36.950046   \n",
       "10                       3.579678                      12.920582   \n",
       "11                       3.202821                      20.198880   \n",
       "12                       0.000000                      48.085774   \n",
       "13                       0.000000                       9.973142   \n",
       "14                       0.000000                      23.566682   \n",
       "15                       7.881998                      13.834481   \n",
       "16                      10.055930                      18.115085   \n",
       "17                       0.000000                      19.035690   \n",
       "18                      21.816951                      26.242993   \n",
       "19                       0.129099                      16.022619   \n",
       "20                       0.465679                      18.189800   \n",
       "21                            NaN                       7.505553   \n",
       "22                       3.616679                      16.212999   \n",
       "23                       0.855699                      26.951860   \n",
       "24                       0.625875                      16.717565   \n",
       "25                       0.000000                      31.326636   \n",
       "26                       0.844221                      19.281771   \n",
       "27                       0.000000                      19.146043   \n",
       "28                       0.334087                      26.014053   \n",
       "29                       0.750000                       8.196850   \n",
       "30                       2.082738                      28.021104   \n",
       "31                       3.398729                      37.864456   \n",
       "32                      11.156735                       9.066117   \n",
       "33                       0.000000                      33.104903   \n",
       "34                       0.000000                      15.530968   \n",
       "35                       0.296145                       9.475015   \n",
       "36                       2.121320                      15.216793   \n",
       "37                      25.333140                      23.791228   \n",
       "38                       4.043483                      26.865965   \n",
       "39                       0.000000                      27.583993   \n",
       "40                       0.000000                       1.654286   \n",
       "41                       0.204124                      21.126643   \n",
       "42                       0.954411                      21.210755   \n",
       "43                       0.442174                      35.008249   \n",
       "44                       3.788868                      23.809769   \n",
       "45                       1.262051                      26.699195   \n",
       "46                       2.131366                      29.699917   \n",
       "47                       0.000000                      16.941953   \n",
       "48                       2.697755                      12.821401   \n",
       "49                       0.000000                      13.981334   \n",
       "50                       0.392339                      16.269481   \n",
       "51                       0.590937                      17.976293   \n",
       "52                       1.747472                      12.573063   \n",
       "53                       0.486144                      22.882605   \n",
       "54                       0.420084                       5.838127   \n",
       "55                       0.895779                       7.054555   \n",
       "56                       0.000000                      12.622183   \n",
       "57                       3.516260                      13.014937   \n",
       "58                       7.814084                      27.122494   \n",
       "59                       0.408937                      21.085988   \n",
       "60                      12.600173                      17.208899   \n",
       "61                       0.358944                      26.155474   \n",
       "62                      12.736626                      12.409696   \n",
       "63                       0.983999                      33.830385   \n",
       "64                       0.000000                      16.298367   \n",
       "65                       0.000000                      17.267230   \n",
       "66                       0.125988                      33.750328   \n",
       "67                       1.202586                      18.470124   \n",
       "68                       1.742808                      21.383593   \n",
       "69                      13.117149                      12.232178   \n",
       "70                       0.166633                      27.459140   \n",
       "\n",
       "    nightly1temperature_std  nremhr_std  resting1hr_std  rmssd_std  \\\n",
       "0                  0.224603    2.766234        1.936963   9.402428   \n",
       "1                  1.041019    8.843768        3.109986   6.103989   \n",
       "2                  0.449799    5.600547        3.476818   7.926298   \n",
       "3                       NaN         NaN        1.890932        NaN   \n",
       "4                  0.477403         NaN        3.587303        NaN   \n",
       "5                  0.634215    4.638591        3.716837   4.858914   \n",
       "6                  0.956000   21.888513        2.367473   6.443923   \n",
       "7                  0.534537         NaN        4.340050        NaN   \n",
       "8                  1.220035    4.805904        2.001050   5.847453   \n",
       "9                  0.623608         NaN        1.413380        NaN   \n",
       "10                 0.917710    3.583879        2.201841   5.294612   \n",
       "11                 0.860951         NaN        2.180213        NaN   \n",
       "12                 0.367319   10.415561        3.309934   6.308012   \n",
       "13                 0.547051         NaN        1.806857        NaN   \n",
       "14                 0.641080         NaN        2.354457        NaN   \n",
       "15                      NaN         NaN        1.106011        NaN   \n",
       "16                 1.133724         NaN        2.868683        NaN   \n",
       "17                 0.434717         NaN        1.958411        NaN   \n",
       "18                 1.053230         NaN        2.033931        NaN   \n",
       "19                 0.530312   17.079217        1.806892  16.279125   \n",
       "20                 0.518626    5.178568        2.352260   8.606000   \n",
       "21                      NaN         NaN             NaN        NaN   \n",
       "22                 2.439351   15.278501        2.814818  11.506192   \n",
       "23                 0.366418    2.811258        1.364450   6.744896   \n",
       "24                 0.440212    9.521616        2.553710   6.090532   \n",
       "25                 0.423492    5.843888        2.990870   8.447702   \n",
       "26                 0.609296         NaN        1.583371        NaN   \n",
       "27                 0.406407    3.096445        2.098141  10.950537   \n",
       "28                 0.660987    7.849796        3.361619  12.234927   \n",
       "29                 0.596824    2.948840        2.356937   6.738554   \n",
       "30                 0.507779         NaN        2.165922        NaN   \n",
       "31                 0.410469    3.021691        1.827606  24.156976   \n",
       "32                 0.568247    4.924928        1.399144  28.604556   \n",
       "33                 0.642074    3.189904        1.653434   4.786963   \n",
       "34                 0.441477    3.525828        2.042879   5.663418   \n",
       "35                 0.386215         NaN        2.255779        NaN   \n",
       "36                 0.638745    1.412799        2.103172   3.572303   \n",
       "37                 0.549028    5.496603        3.305073   5.752806   \n",
       "38                 0.393754         NaN        2.088075        NaN   \n",
       "39                 0.441337    4.006307        1.932482  32.821757   \n",
       "40                      NaN         NaN        0.969193        NaN   \n",
       "41                 0.714850   18.582127        3.091735  19.885873   \n",
       "42                 0.551644         NaN        2.572926        NaN   \n",
       "43                 0.482630    3.960673        1.803039   6.501855   \n",
       "44                 0.454720    7.857323        2.492545   5.610983   \n",
       "45                 0.369531    4.758690        2.420177  10.345600   \n",
       "46                 0.490166   10.241256        2.839953  10.153877   \n",
       "47                 0.283890         NaN        1.501492        NaN   \n",
       "48                      NaN         NaN        4.001404        NaN   \n",
       "49                 0.470098    5.219525        2.006655   5.382929   \n",
       "50                 0.660551    4.227562        3.986295  15.229551   \n",
       "51                 1.706760         NaN        2.144593        NaN   \n",
       "52                      NaN         NaN        2.217869        NaN   \n",
       "53                 0.506941    5.639735        3.376689   6.926666   \n",
       "54                 0.327364    2.491380        1.828111   3.078852   \n",
       "55                 0.377124         NaN        1.590923        NaN   \n",
       "56                 2.308408    4.286223        1.675323   2.618873   \n",
       "57                 0.407550         NaN        0.980846        NaN   \n",
       "58                 0.490009   11.344246        3.800464  19.880470   \n",
       "59                 0.433419    8.522220        3.131163  20.934396   \n",
       "60                      NaN    5.295279        3.336830  21.735361   \n",
       "61                 0.570762   14.082555        1.753078  13.292221   \n",
       "62                 0.351489   11.207136        2.888025   7.033274   \n",
       "63                 0.394529    2.831545        1.854080   8.535313   \n",
       "64                 0.571933    5.742360        2.811835   6.614765   \n",
       "65                      NaN         NaN        2.105555        NaN   \n",
       "66                 0.494226         NaN        2.085960        NaN   \n",
       "67                      NaN         NaN        3.510381        NaN   \n",
       "68                 0.489525    4.421718        2.294795  17.012828   \n",
       "69                 2.113160         NaN        2.214767        NaN   \n",
       "70                 0.855560    4.296365        2.787740   3.276017   \n",
       "\n",
       "    scl1avg_std  sedentary1minutes_std  sleep1deep1ratio_std  \\\n",
       "0           NaN             202.624254              0.236235   \n",
       "1           NaN             185.741548              0.391454   \n",
       "2           NaN             319.013454              0.337037   \n",
       "3           NaN              95.104642                   NaN   \n",
       "4           NaN             378.120638              0.317896   \n",
       "5      2.571259             198.164000              0.383739   \n",
       "6           NaN             269.350714              0.294823   \n",
       "7           NaN             386.910575              0.326771   \n",
       "8           NaN             300.675749              0.289252   \n",
       "9           NaN             323.163492              0.249814   \n",
       "10     3.350611             128.138044              0.425925   \n",
       "11          NaN             290.602172              0.395425   \n",
       "12          NaN             118.400492              0.293537   \n",
       "13          NaN             259.379382              0.287375   \n",
       "14          NaN             350.428856              0.459548   \n",
       "15          NaN             124.792490                   NaN   \n",
       "16          NaN             282.419250              0.235329   \n",
       "17          NaN             304.189105              0.241820   \n",
       "18          NaN             325.649215              0.315798   \n",
       "19          NaN             276.939991              0.292857   \n",
       "20          NaN             317.322980              0.345933   \n",
       "21          NaN              84.037628                   NaN   \n",
       "22          NaN             330.038870              0.469100   \n",
       "23          NaN             320.336680              0.572307   \n",
       "24          NaN             307.330277              0.291070   \n",
       "25          NaN             374.201434              0.253395   \n",
       "26          NaN             301.293247              0.261498   \n",
       "27          NaN             265.818895              0.224629   \n",
       "28          NaN             401.867018              0.264040   \n",
       "29          NaN             316.920712              0.293500   \n",
       "30          NaN             270.242305              0.384675   \n",
       "31          NaN             266.475339              0.384988   \n",
       "32          NaN             177.246861              0.510148   \n",
       "33          NaN             265.925750              0.381323   \n",
       "34          NaN             147.140764              0.217351   \n",
       "35          NaN             313.561378              0.193162   \n",
       "36          NaN             226.106159              0.241650   \n",
       "37          NaN             184.120278              0.291762   \n",
       "38          NaN             335.910004              0.376717   \n",
       "39          NaN             342.039627              0.345635   \n",
       "40          NaN              76.661560                   NaN   \n",
       "41          NaN             263.850381              0.340447   \n",
       "42          NaN             256.573131              0.295859   \n",
       "43          NaN             343.816789              0.265672   \n",
       "44          NaN             227.633314              0.277844   \n",
       "45          NaN             230.460407              0.323046   \n",
       "46          NaN             211.472160              0.414980   \n",
       "47          NaN             315.231054              0.299536   \n",
       "48          NaN              92.525722                   NaN   \n",
       "49          NaN             304.166138              0.256256   \n",
       "50          NaN             224.610374              0.460087   \n",
       "51          NaN             233.067773              0.250561   \n",
       "52          NaN             132.589139                   NaN   \n",
       "53          NaN             199.915737              0.313741   \n",
       "54          NaN             303.935906              0.295060   \n",
       "55          NaN             271.113650              0.283817   \n",
       "56          NaN             197.330957              0.513654   \n",
       "57          NaN             114.522136                   NaN   \n",
       "58          NaN             381.015197              0.305796   \n",
       "59          NaN             293.795965              0.298019   \n",
       "60          NaN             145.528112              0.417086   \n",
       "61          NaN             377.094309              0.278351   \n",
       "62          NaN             310.425098              0.346997   \n",
       "63          NaN             284.746986              0.231727   \n",
       "64          NaN             299.643831              0.299299   \n",
       "65          NaN             165.569995                   NaN   \n",
       "66          NaN             341.865202              0.268407   \n",
       "67          NaN             123.556723                   NaN   \n",
       "68          NaN             345.647143              0.209374   \n",
       "69          NaN             171.606947                   NaN   \n",
       "70          NaN             325.714637              0.295221   \n",
       "\n",
       "    sleep1duration_std  sleep1efficiency_std  sleep1light1ratio_std  \\\n",
       "0         3.470355e+06              2.005752               0.159124   \n",
       "1         5.942277e+06              2.696577               0.282058   \n",
       "2         5.434833e+06              3.405877               0.230605   \n",
       "3                  NaN                   NaN                    NaN   \n",
       "4         6.156437e+06              2.541268               0.200754   \n",
       "5         7.308553e+06              3.653093               0.253809   \n",
       "6         1.018628e+07              1.948843               0.273882   \n",
       "7         8.032977e+06              2.625631               0.216569   \n",
       "8         5.002301e+06              3.129043               0.245131   \n",
       "9         4.303500e+06              2.796042               0.189773   \n",
       "10        5.894189e+06              1.997353               0.233573   \n",
       "11        4.815377e+06              2.190917               0.219879   \n",
       "12        5.290011e+06              2.360177               0.236132   \n",
       "13        5.137727e+06              2.343400               0.191372   \n",
       "14        5.961189e+06              2.441701               0.224824   \n",
       "15                 NaN                   NaN                    NaN   \n",
       "16        6.745169e+06              2.992613               0.204134   \n",
       "17        3.390207e+06              2.114377               0.186102   \n",
       "18        5.757670e+06              3.065611               0.243835   \n",
       "19        6.462285e+06              1.959523               0.324306   \n",
       "20        4.987277e+06              2.070640               0.279084   \n",
       "21                 NaN                   NaN                    NaN   \n",
       "22        1.031088e+07              4.398145               0.227219   \n",
       "23        3.101226e+06              1.889043               0.167967   \n",
       "24        6.604433e+06              2.581989               0.225380   \n",
       "25        6.801330e+06              2.592838               0.210069   \n",
       "26        6.145704e+06              2.076278               0.185324   \n",
       "27        5.908784e+06              3.334545               0.190009   \n",
       "28        6.711642e+06              2.854746               0.206815   \n",
       "29        5.208532e+06              2.707804               0.207883   \n",
       "30        6.597532e+06              3.861831               0.330973   \n",
       "31        3.371451e+06              2.250182               0.192929   \n",
       "32        5.122292e+06              2.228602               0.144650   \n",
       "33        6.228270e+06              3.035311               0.287139   \n",
       "34        3.174924e+06              3.173354               0.128951   \n",
       "35        7.720290e+06              2.652536               0.114852   \n",
       "36        9.103800e+06              2.778595               0.180978   \n",
       "37        5.957200e+06              3.075090               0.302695   \n",
       "38        7.134875e+06              2.554273               0.250677   \n",
       "39        1.282971e+07              2.148089               0.427907   \n",
       "40                 NaN                   NaN                    NaN   \n",
       "41        6.428511e+06              4.437847               0.267359   \n",
       "42        5.125944e+06              2.132695               0.188820   \n",
       "43        4.054830e+06              1.732007               0.161932   \n",
       "44        3.120838e+06              2.288239               0.204888   \n",
       "45        4.936802e+06              2.192645               0.231883   \n",
       "46        4.087359e+06              3.582591               0.188179   \n",
       "47        5.879226e+06              1.810423               0.272358   \n",
       "48        4.683204e+06              4.041452                    NaN   \n",
       "49        5.446189e+06              2.634999               0.202219   \n",
       "50        8.594049e+06              3.312321               0.255508   \n",
       "51        1.338846e+07              3.025815               0.188036   \n",
       "52                 NaN                   NaN                    NaN   \n",
       "53        5.160637e+06              3.160492               0.210179   \n",
       "54        7.056230e+06              2.517873               0.198599   \n",
       "55        3.002702e+06              1.993837               0.202408   \n",
       "56        3.864655e+06              3.368151               0.223868   \n",
       "57        1.137028e+07              4.242641                    NaN   \n",
       "58        7.686313e+06              8.727714               0.235782   \n",
       "59        6.000150e+06              3.746843               0.267807   \n",
       "60        3.510572e+06              2.282121               0.154441   \n",
       "61        6.017665e+06              3.078841               0.200991   \n",
       "62        7.471484e+06              2.057602               0.241514   \n",
       "63        4.239713e+06              3.266094               0.265270   \n",
       "64        5.863997e+06              2.606369               0.247075   \n",
       "65        4.242641e+04              0.000000                    NaN   \n",
       "66        5.411164e+06              2.297508               0.239305   \n",
       "67        2.949983e+06              1.154701                    NaN   \n",
       "68        4.873251e+06              1.878103               0.164576   \n",
       "69        1.062868e+07              0.971825                    NaN   \n",
       "70        6.891690e+06              2.106268               0.198393   \n",
       "\n",
       "    sleep1rem1ratio_std  sleep1wake1ratio_std  spo2_std  step1goal_std  \\\n",
       "0              0.250086              0.188958       NaN    2708.012802   \n",
       "1              0.361365              0.329824       NaN       0.000000   \n",
       "2              0.411375              0.348230       NaN    3228.233061   \n",
       "3                   NaN                   NaN       NaN            NaN   \n",
       "4              0.321502              0.256729  0.972034            NaN   \n",
       "5              0.305544              0.336127       NaN    1811.066049   \n",
       "6              0.331098              0.315731       NaN    2784.052800   \n",
       "7              0.340377              0.315533       NaN    2129.076568   \n",
       "8              0.307935              0.383327  0.922177    2945.486263   \n",
       "9              0.358719              0.186923  0.522015    1586.622610   \n",
       "10             0.326609              0.397631  0.631326    1251.766723   \n",
       "11             0.232598              0.284822       NaN    3318.135781   \n",
       "12             0.256352              0.350925  0.885685    2087.615080   \n",
       "13             0.232251              0.253002       NaN    1125.538099   \n",
       "14             0.411004              0.462126  0.861772            NaN   \n",
       "15                  NaN                   NaN       NaN    2880.677038   \n",
       "16             0.486749              0.255118       NaN    1460.410912   \n",
       "17             0.224791              0.183673       NaN       0.000000   \n",
       "18             0.315722              0.507952       NaN            NaN   \n",
       "19             0.495360              0.450614       NaN            NaN   \n",
       "20             0.277748              0.464833       NaN    1869.200148   \n",
       "21                  NaN                   NaN       NaN     282.842712   \n",
       "22             0.408014              0.257759  0.958177    1779.896522   \n",
       "23             0.380207              0.237194       NaN    4532.584082   \n",
       "24             0.288965              0.342315  1.026023    1708.358262   \n",
       "25             0.445040              0.254890  0.633071    1518.602681   \n",
       "26             0.300291              0.271969       NaN    2772.304027   \n",
       "27             0.336890              0.370585  0.624111            NaN   \n",
       "28             0.320125              0.306234  1.161009   10384.047758   \n",
       "29             0.295539              0.334711       NaN     944.465203   \n",
       "30             0.402203              0.351125       NaN    1784.236754   \n",
       "31             0.448492              0.245942  0.855960    3620.020983   \n",
       "32             0.293130              0.240134       NaN       0.000000   \n",
       "33             0.350371              0.333998       NaN    2092.493984   \n",
       "34             0.336865              0.168858  1.116423    1558.512672   \n",
       "35             0.299446              0.437070       NaN    4868.234721   \n",
       "36             0.296380              0.205972  0.862278    4403.904925   \n",
       "37             0.362848              0.384633  0.860320    3535.533906   \n",
       "38             0.375603              0.404467       NaN       0.000000   \n",
       "39             0.420928              0.308466  1.210214    2672.612419   \n",
       "40                  NaN                   NaN       NaN            NaN   \n",
       "41             0.564392              0.284244  1.068885    2886.751346   \n",
       "42             0.281549              0.284335       NaN       0.000000   \n",
       "43             0.312433              0.262818  1.120074    2626.970503   \n",
       "44             0.295220              0.251707  0.965013    3934.211735   \n",
       "45             0.269799              0.278632  0.975783    3252.153553   \n",
       "46             0.860539              0.276776       NaN    5196.552720   \n",
       "47             0.482072              0.277131       NaN    1463.362159   \n",
       "48                  NaN                   NaN       NaN    1892.620031   \n",
       "49             0.350570              0.246181       NaN     603.022689   \n",
       "50             0.440427              0.369723       NaN            NaN   \n",
       "51             0.232163              0.416426       NaN            NaN   \n",
       "52                  NaN                   NaN       NaN    1095.445115   \n",
       "53             0.340331              0.295085       NaN    1054.092553   \n",
       "54             0.295372              0.302204       NaN       0.000000   \n",
       "55             0.715087              0.186931       NaN     864.312197   \n",
       "56             0.402005              0.286496  0.738241    5772.925342   \n",
       "57                  NaN                   NaN       NaN            NaN   \n",
       "58             0.336560              0.305961  0.680651    3592.426178   \n",
       "59             0.671425              0.322591  1.243116    1648.326767   \n",
       "60             0.234183              0.298800       NaN            NaN   \n",
       "61             0.498824              0.296885       NaN            NaN   \n",
       "62             0.397756              0.391313  1.529520    4451.515577   \n",
       "63             0.444079              0.354105  0.542066    1358.846808   \n",
       "64             0.399194              0.288283       NaN            NaN   \n",
       "65                  NaN                   NaN       NaN    4369.314488   \n",
       "66             0.303629              0.331738       NaN    2758.602927   \n",
       "67                  NaN                   NaN       NaN    1602.554779   \n",
       "68             0.225304              0.214419  0.738650    3587.519432   \n",
       "69                  NaN                   NaN       NaN            NaN   \n",
       "70             0.330632              0.305634       NaN       0.000000   \n",
       "\n",
       "    step1goal1label_std    steps_std  very1active1minutes_std  \n",
       "0              0.847319  3384.097986                16.297671  \n",
       "1              0.000000  5973.028379                22.312539  \n",
       "2              1.015038  2616.982198                 5.371150  \n",
       "3                   NaN  4842.354770                13.890850  \n",
       "4                   NaN  4858.421862                16.780661  \n",
       "5              0.603834  3047.406559                 2.512481  \n",
       "6              0.965215  3988.212150                47.909084  \n",
       "7              0.425815  6635.420494                18.324390  \n",
       "8              0.895752  4182.653506                10.160671  \n",
       "9              0.529031  7279.751311                61.211257  \n",
       "10             0.465653  2333.171809                10.425672  \n",
       "11             0.839949  3886.928564                29.052969  \n",
       "12             0.726310  5114.149336                36.125025  \n",
       "13             0.375212  2881.375764                12.542395  \n",
       "14                  NaN  6406.290001                23.201760  \n",
       "15             1.095445  4988.049322                22.802198  \n",
       "16             0.547723  5688.378667                10.286994  \n",
       "17             0.000000  3612.821071                22.083272  \n",
       "18                  NaN  8041.328888                44.072083  \n",
       "19                  NaN  3600.804498                12.788239  \n",
       "20             0.638381  3950.160473                15.943215  \n",
       "21             0.141421          NaN                 3.656552  \n",
       "22             0.638666  4973.972533                80.676036  \n",
       "23             1.280306  7268.244429                29.316851  \n",
       "24             0.569553  3728.635394                 6.463344  \n",
       "25             0.506370  4764.450265                34.244538  \n",
       "26             0.926040  5406.145666                30.770835  \n",
       "27                  NaN  4927.911594                32.536969  \n",
       "28             3.097866  6663.338097                43.639983  \n",
       "29             0.393919  2800.821773                 9.527729  \n",
       "30             0.555726  2516.283262                23.199168  \n",
       "31             0.724004  4609.375148                25.290314  \n",
       "32             0.000000  3033.072352                12.868278  \n",
       "33             0.418499  2938.643253                31.261539  \n",
       "34             0.544219  3140.205333                17.940905  \n",
       "35             1.829058  5234.089263                10.376768  \n",
       "36             1.364685  7788.436038                33.855310  \n",
       "37             0.707107  3404.256445                22.724241  \n",
       "38             0.000000  2097.256189                13.461343  \n",
       "39             0.534522  8328.303750                36.294362  \n",
       "40                  NaN  4168.204387                 1.130853  \n",
       "41             0.577350  4739.000034                17.982538  \n",
       "42             0.000000  3997.571658                21.889189  \n",
       "43             0.889878  3674.386789                18.935621  \n",
       "44             1.138729  6027.766506                36.378002  \n",
       "45             0.719365  3657.485378                10.375136  \n",
       "46             1.578036  4304.092034                30.222773  \n",
       "47             0.487950  4119.898438                28.489012  \n",
       "48             1.014833  3556.487375                18.016302  \n",
       "49             0.301511  3353.684376                11.310567  \n",
       "50                  NaN  5455.568963                32.292920  \n",
       "51                  NaN  3827.558259                 9.300272  \n",
       "52             0.547723  3692.964587                10.870802  \n",
       "53             0.527046  4344.882147                11.901518  \n",
       "54             0.000000  2879.744309                 4.090605  \n",
       "55             0.288104  2981.727421                 5.190448  \n",
       "56             2.309401  6332.922159                22.762507  \n",
       "57                  NaN  4147.102452                 9.051935  \n",
       "58             0.963228  6144.744453                42.408989  \n",
       "59             0.616566  4486.731665                30.772508  \n",
       "60                  NaN  3404.827583                26.661226  \n",
       "61                  NaN  6061.955019                11.453121  \n",
       "62             1.557043  3846.316792                13.191246  \n",
       "63             0.601345  4078.703018                25.518321  \n",
       "64                  NaN  3910.970539                15.437449  \n",
       "65             1.103713  6510.176877                12.985607  \n",
       "66             0.841897  4245.010432                29.729212  \n",
       "67             0.426401  4911.054001                29.397413  \n",
       "68             0.939575  5844.593245                29.616532  \n",
       "69                  NaN  4227.040783                19.739210  \n",
       "70             0.000000  3872.471535                21.301009  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_mean = lifesnaps_all_features.groupby('id', as_index = False, group_keys = True).mean()\n",
    "lifesnaps_mean.columns = map(lambda x: x + '_mean', lifesnaps_mean.columns)\n",
    "lifesnaps_mean = lifesnaps_mean.rename({'id_mean': 'id'}, axis='columns')\n",
    "lifesnaps_mean\n",
    "\n",
    "lifesnaps_min = lifesnaps_all_features.groupby('id', as_index = False, group_keys = True).min()\n",
    "lifesnaps_min.columns = map(lambda x: x + '_min', lifesnaps_min.columns)\n",
    "lifesnaps_min = lifesnaps_min.rename({'id_min': 'id'}, axis='columns')\n",
    "lifesnaps_min\n",
    "\n",
    "lifesnaps_std = lifesnaps_all_features.groupby('id', as_index = False, group_keys = True).std()\n",
    "lifesnaps_std.columns = map(lambda x: x + '_std', lifesnaps_std.columns)\n",
    "lifesnaps_std = lifesnaps_std.rename({'id_std': 'id'}, axis='columns')\n",
    "lifesnaps_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29459601",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "lifesnaps_mean.loc[:, lifesnaps_mean.columns!='id'] = scaler.fit_transform(lifesnaps_mean.loc[:, lifesnaps_mean.columns!='id'])\n",
    "lifesnaps_min.loc[:, lifesnaps_min.columns!='id'] = scaler.fit_transform(lifesnaps_min.loc[:, lifesnaps_min.columns!='id'])\n",
    "lifesnaps_std.loc[:, lifesnaps_std.columns!='id'] = scaler.fit_transform(lifesnaps_std.loc[:, lifesnaps_std.columns!='id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f5edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesnaps_all_grouped = pd.merge(lifesnaps_mean, lifesnaps_personality, on='id')\n",
    "lifesnaps_all_grouped = pd.merge(lifesnaps_min, lifesnaps_all_grouped, on='id')\n",
    "lifesnaps_all_grouped = pd.merge(lifesnaps_std, lifesnaps_all_grouped, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cc51788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age_std</th>\n",
       "      <th>bmi_std</th>\n",
       "      <th>bpm_std</th>\n",
       "      <th>calories_std</th>\n",
       "      <th>daily1temperature1variation_std</th>\n",
       "      <th>distance_std</th>\n",
       "      <th>filteredDemographicVO2Max_std</th>\n",
       "      <th>full1sleep1breathing1rate_std</th>\n",
       "      <th>gender_std</th>\n",
       "      <th>lightly1active1minutes_std</th>\n",
       "      <th>max1goal_std</th>\n",
       "      <th>min1goal_std</th>\n",
       "      <th>minutesAfterWakeup_std</th>\n",
       "      <th>minutesAsleep_std</th>\n",
       "      <th>minutesAwake_std</th>\n",
       "      <th>minutesToFallAsleep_std</th>\n",
       "      <th>minutes1below1default1zone11_std</th>\n",
       "      <th>minutes1in1default1zone11_std</th>\n",
       "      <th>minutes1in1default1zone12_std</th>\n",
       "      <th>minutes1in1default1zone13_std</th>\n",
       "      <th>moderately1active1minutes_std</th>\n",
       "      <th>nightly1temperature_std</th>\n",
       "      <th>nremhr_std</th>\n",
       "      <th>resting1hr_std</th>\n",
       "      <th>rmssd_std</th>\n",
       "      <th>scl1avg_std</th>\n",
       "      <th>sedentary1minutes_std</th>\n",
       "      <th>sleep1deep1ratio_std</th>\n",
       "      <th>sleep1duration_std</th>\n",
       "      <th>sleep1efficiency_std</th>\n",
       "      <th>sleep1light1ratio_std</th>\n",
       "      <th>sleep1rem1ratio_std</th>\n",
       "      <th>sleep1wake1ratio_std</th>\n",
       "      <th>spo2_std</th>\n",
       "      <th>step1goal_std</th>\n",
       "      <th>step1goal1label_std</th>\n",
       "      <th>steps_std</th>\n",
       "      <th>very1active1minutes_std</th>\n",
       "      <th>age_min</th>\n",
       "      <th>bmi_min</th>\n",
       "      <th>bpm_min</th>\n",
       "      <th>calories_min</th>\n",
       "      <th>daily1temperature1variation_min</th>\n",
       "      <th>distance_min</th>\n",
       "      <th>filteredDemographicVO2Max_min</th>\n",
       "      <th>full1sleep1breathing1rate_min</th>\n",
       "      <th>gender_min</th>\n",
       "      <th>lightly1active1minutes_min</th>\n",
       "      <th>max1goal_min</th>\n",
       "      <th>min1goal_min</th>\n",
       "      <th>mindfulness1session_min</th>\n",
       "      <th>minutesAfterWakeup_min</th>\n",
       "      <th>minutesAsleep_min</th>\n",
       "      <th>minutesAwake_min</th>\n",
       "      <th>minutesToFallAsleep_min</th>\n",
       "      <th>minutes1below1default1zone11_min</th>\n",
       "      <th>minutes1in1default1zone11_min</th>\n",
       "      <th>minutes1in1default1zone12_min</th>\n",
       "      <th>minutes1in1default1zone13_min</th>\n",
       "      <th>moderately1active1minutes_min</th>\n",
       "      <th>nightly1temperature_min</th>\n",
       "      <th>nremhr_min</th>\n",
       "      <th>resting1hr_min</th>\n",
       "      <th>rmssd_min</th>\n",
       "      <th>scl1avg_min</th>\n",
       "      <th>sedentary1minutes_min</th>\n",
       "      <th>sleep1deep1ratio_min</th>\n",
       "      <th>sleep1duration_min</th>\n",
       "      <th>sleep1efficiency_min</th>\n",
       "      <th>sleep1light1ratio_min</th>\n",
       "      <th>sleep1rem1ratio_min</th>\n",
       "      <th>sleep1wake1ratio_min</th>\n",
       "      <th>spo2_min</th>\n",
       "      <th>step1goal_min</th>\n",
       "      <th>step1goal1label_min</th>\n",
       "      <th>steps_min</th>\n",
       "      <th>very1active1minutes_min</th>\n",
       "      <th>age_mean</th>\n",
       "      <th>bmi_mean</th>\n",
       "      <th>bpm_mean</th>\n",
       "      <th>calories_mean</th>\n",
       "      <th>daily1temperature1variation_mean</th>\n",
       "      <th>distance_mean</th>\n",
       "      <th>filteredDemographicVO2Max_mean</th>\n",
       "      <th>full1sleep1breathing1rate_mean</th>\n",
       "      <th>gender_mean</th>\n",
       "      <th>lightly1active1minutes_mean</th>\n",
       "      <th>max1goal_mean</th>\n",
       "      <th>min1goal_mean</th>\n",
       "      <th>minutesAfterWakeup_mean</th>\n",
       "      <th>minutesAsleep_mean</th>\n",
       "      <th>minutesAwake_mean</th>\n",
       "      <th>minutesToFallAsleep_mean</th>\n",
       "      <th>minutes1below1default1zone11_mean</th>\n",
       "      <th>minutes1in1default1zone11_mean</th>\n",
       "      <th>minutes1in1default1zone12_mean</th>\n",
       "      <th>minutes1in1default1zone13_mean</th>\n",
       "      <th>moderately1active1minutes_mean</th>\n",
       "      <th>nightly1temperature_mean</th>\n",
       "      <th>nremhr_mean</th>\n",
       "      <th>resting1hr_mean</th>\n",
       "      <th>rmssd_mean</th>\n",
       "      <th>scl1avg_mean</th>\n",
       "      <th>sedentary1minutes_mean</th>\n",
       "      <th>sleep1deep1ratio_mean</th>\n",
       "      <th>sleep1duration_mean</th>\n",
       "      <th>sleep1efficiency_mean</th>\n",
       "      <th>sleep1light1ratio_mean</th>\n",
       "      <th>sleep1rem1ratio_mean</th>\n",
       "      <th>sleep1wake1ratio_mean</th>\n",
       "      <th>spo2_mean</th>\n",
       "      <th>step1goal_mean</th>\n",
       "      <th>step1goal1label_mean</th>\n",
       "      <th>steps_mean</th>\n",
       "      <th>very1active1minutes_mean</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>stability</th>\n",
       "      <th>intellect</th>\n",
       "      <th>gender</th>\n",
       "      <th>ipip_extraversion_category</th>\n",
       "      <th>ipip_agreeableness_category</th>\n",
       "      <th>ipip_conscientiousness_category</th>\n",
       "      <th>ipip_stability_category</th>\n",
       "      <th>ipip_intellect_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>621e2e8e67b776a24055b564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.110783</td>\n",
       "      <td>-1.002972</td>\n",
       "      <td>-0.415743</td>\n",
       "      <td>-0.705350</td>\n",
       "      <td>-0.186683</td>\n",
       "      <td>-0.765453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.843042</td>\n",
       "      <td>0.197341</td>\n",
       "      <td>0.306925</td>\n",
       "      <td>-0.427242</td>\n",
       "      <td>-1.007991</td>\n",
       "      <td>-0.486219</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.616408</td>\n",
       "      <td>-1.087310</td>\n",
       "      <td>-1.223248</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-0.100121</td>\n",
       "      <td>-0.982470</td>\n",
       "      <td>-0.872711</td>\n",
       "      <td>-0.604984</td>\n",
       "      <td>-0.148081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.643462</td>\n",
       "      <td>-1.069866</td>\n",
       "      <td>-1.084652</td>\n",
       "      <td>-0.657884</td>\n",
       "      <td>-1.243605</td>\n",
       "      <td>-0.989510</td>\n",
       "      <td>-1.625158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201760</td>\n",
       "      <td>0.209111</td>\n",
       "      <td>-0.813912</td>\n",
       "      <td>-0.412437</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-1.487354</td>\n",
       "      <td>-0.317875</td>\n",
       "      <td>1.374707</td>\n",
       "      <td>0.186440</td>\n",
       "      <td>0.486070</td>\n",
       "      <td>2.498475</td>\n",
       "      <td>0.805308</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>1.272309</td>\n",
       "      <td>2.015456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.907459</td>\n",
       "      <td>-0.294897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000823</td>\n",
       "      <td>0.498386</td>\n",
       "      <td>-0.778488</td>\n",
       "      <td>3.892913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.379651</td>\n",
       "      <td>1.312915</td>\n",
       "      <td>1.220305</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>-0.191007</td>\n",
       "      <td>1.152834</td>\n",
       "      <td>0.086826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>0.468029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-1.487354</td>\n",
       "      <td>-1.512877</td>\n",
       "      <td>0.310669</td>\n",
       "      <td>-0.524013</td>\n",
       "      <td>0.337865</td>\n",
       "      <td>2.486255</td>\n",
       "      <td>0.224010</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.031059</td>\n",
       "      <td>0.481441</td>\n",
       "      <td>0.620486</td>\n",
       "      <td>-0.330695</td>\n",
       "      <td>1.102600</td>\n",
       "      <td>1.795480</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.297210</td>\n",
       "      <td>-0.730252</td>\n",
       "      <td>-1.027843</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>1.136107</td>\n",
       "      <td>0.578562</td>\n",
       "      <td>-0.747719</td>\n",
       "      <td>-0.933525</td>\n",
       "      <td>3.356337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.143153</td>\n",
       "      <td>0.551802</td>\n",
       "      <td>1.241448</td>\n",
       "      <td>-0.182225</td>\n",
       "      <td>0.299354</td>\n",
       "      <td>0.050381</td>\n",
       "      <td>0.084276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.482325</td>\n",
       "      <td>0.630392</td>\n",
       "      <td>0.228625</td>\n",
       "      <td>1.354252</td>\n",
       "      <td>-1.130894</td>\n",
       "      <td>-0.839454</td>\n",
       "      <td>1.688241</td>\n",
       "      <td>1.501184</td>\n",
       "      <td>0.651920</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>621e2eaf67b776a2406b14ac</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.876672</td>\n",
       "      <td>-0.540978</td>\n",
       "      <td>-0.325256</td>\n",
       "      <td>0.837708</td>\n",
       "      <td>1.633165</td>\n",
       "      <td>-0.782661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.678570</td>\n",
       "      <td>-1.291586</td>\n",
       "      <td>-1.310834</td>\n",
       "      <td>0.433363</td>\n",
       "      <td>0.026673</td>\n",
       "      <td>-0.194824</td>\n",
       "      <td>0.315262</td>\n",
       "      <td>-0.948505</td>\n",
       "      <td>0.065829</td>\n",
       "      <td>0.717323</td>\n",
       "      <td>-0.215531</td>\n",
       "      <td>0.761270</td>\n",
       "      <td>0.856707</td>\n",
       "      <td>0.439622</td>\n",
       "      <td>0.937525</td>\n",
       "      <td>-0.615973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.843950</td>\n",
       "      <td>0.875181</td>\n",
       "      <td>-0.025375</td>\n",
       "      <td>-0.016948</td>\n",
       "      <td>1.158586</td>\n",
       "      <td>-0.034989</td>\n",
       "      <td>0.279885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.302687</td>\n",
       "      <td>-1.306226</td>\n",
       "      <td>1.010380</td>\n",
       "      <td>0.024112</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-1.487354</td>\n",
       "      <td>1.002774</td>\n",
       "      <td>-1.312867</td>\n",
       "      <td>-0.253652</td>\n",
       "      <td>1.436865</td>\n",
       "      <td>-0.213089</td>\n",
       "      <td>0.775686</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>2.608968</td>\n",
       "      <td>2.348858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.909886</td>\n",
       "      <td>-0.231175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551496</td>\n",
       "      <td>0.890923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.567073</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>0.313390</td>\n",
       "      <td>-0.426218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.929092</td>\n",
       "      <td>0.540417</td>\n",
       "      <td>0.845443</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>-0.110026</td>\n",
       "      <td>-0.187861</td>\n",
       "      <td>-0.419095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.609115</td>\n",
       "      <td>2.233940</td>\n",
       "      <td>1.689095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-1.487354</td>\n",
       "      <td>0.472681</td>\n",
       "      <td>-0.103172</td>\n",
       "      <td>-0.793389</td>\n",
       "      <td>2.586309</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>-0.005455</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>2.272765</td>\n",
       "      <td>1.696434</td>\n",
       "      <td>1.464901</td>\n",
       "      <td>0.191421</td>\n",
       "      <td>0.603060</td>\n",
       "      <td>0.312919</td>\n",
       "      <td>0.082416</td>\n",
       "      <td>0.526364</td>\n",
       "      <td>1.028905</td>\n",
       "      <td>0.147250</td>\n",
       "      <td>-0.261041</td>\n",
       "      <td>1.835488</td>\n",
       "      <td>0.537027</td>\n",
       "      <td>1.052896</td>\n",
       "      <td>0.481014</td>\n",
       "      <td>-0.864938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.800374</td>\n",
       "      <td>1.393204</td>\n",
       "      <td>0.564587</td>\n",
       "      <td>-0.011070</td>\n",
       "      <td>1.350585</td>\n",
       "      <td>0.466318</td>\n",
       "      <td>0.753044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.698147</td>\n",
       "      <td>1.511501</td>\n",
       "      <td>2.768485</td>\n",
       "      <td>1.432584</td>\n",
       "      <td>0.230138</td>\n",
       "      <td>1.068396</td>\n",
       "      <td>-0.502383</td>\n",
       "      <td>-1.461679</td>\n",
       "      <td>0.843661</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621e2ed667b776a24085d8d1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.757127</td>\n",
       "      <td>-1.380645</td>\n",
       "      <td>-0.122808</td>\n",
       "      <td>-1.404843</td>\n",
       "      <td>0.284176</td>\n",
       "      <td>1.996907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.313647</td>\n",
       "      <td>0.483443</td>\n",
       "      <td>0.523893</td>\n",
       "      <td>-0.137222</td>\n",
       "      <td>-0.316514</td>\n",
       "      <td>-0.026073</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.158325</td>\n",
       "      <td>0.756393</td>\n",
       "      <td>-0.873616</td>\n",
       "      <td>-0.406402</td>\n",
       "      <td>-0.938611</td>\n",
       "      <td>-0.475160</td>\n",
       "      <td>-0.260693</td>\n",
       "      <td>1.419905</td>\n",
       "      <td>-0.357474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.738702</td>\n",
       "      <td>0.193283</td>\n",
       "      <td>-0.242827</td>\n",
       "      <td>0.641129</td>\n",
       "      <td>0.153160</td>\n",
       "      <td>0.393982</td>\n",
       "      <td>0.528802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490771</td>\n",
       "      <td>0.509059</td>\n",
       "      <td>-1.354461</td>\n",
       "      <td>-1.205466</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>0.921982</td>\n",
       "      <td>0.245818</td>\n",
       "      <td>-0.260055</td>\n",
       "      <td>-0.331386</td>\n",
       "      <td>-1.377621</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.816691</td>\n",
       "      <td>0.143263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137309</td>\n",
       "      <td>-0.029415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.570136</td>\n",
       "      <td>0.861285</td>\n",
       "      <td>0.970284</td>\n",
       "      <td>-1.102324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.174774</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>0.631237</td>\n",
       "      <td>-1.428662</td>\n",
       "      <td>-0.158589</td>\n",
       "      <td>-0.234400</td>\n",
       "      <td>-1.426636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.320172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>0.332837</td>\n",
       "      <td>-1.106954</td>\n",
       "      <td>0.079949</td>\n",
       "      <td>-1.317373</td>\n",
       "      <td>-1.371431</td>\n",
       "      <td>1.321969</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.362747</td>\n",
       "      <td>0.241324</td>\n",
       "      <td>0.210399</td>\n",
       "      <td>-0.231862</td>\n",
       "      <td>0.546269</td>\n",
       "      <td>0.556444</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.505688</td>\n",
       "      <td>-0.071672</td>\n",
       "      <td>-0.939046</td>\n",
       "      <td>-0.431078</td>\n",
       "      <td>-0.815144</td>\n",
       "      <td>0.370190</td>\n",
       "      <td>1.607685</td>\n",
       "      <td>1.405968</td>\n",
       "      <td>-0.835065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.120562</td>\n",
       "      <td>0.606062</td>\n",
       "      <td>0.556260</td>\n",
       "      <td>0.150443</td>\n",
       "      <td>0.214440</td>\n",
       "      <td>0.378261</td>\n",
       "      <td>0.552121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.242059</td>\n",
       "      <td>0.288143</td>\n",
       "      <td>-1.230658</td>\n",
       "      <td>-1.070591</td>\n",
       "      <td>1.219979</td>\n",
       "      <td>0.750421</td>\n",
       "      <td>-1.670716</td>\n",
       "      <td>-0.227153</td>\n",
       "      <td>-0.498527</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>621e2f3967b776a240c654db</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220415</td>\n",
       "      <td>0.068573</td>\n",
       "      <td>0.615941</td>\n",
       "      <td>-0.472026</td>\n",
       "      <td>0.723346</td>\n",
       "      <td>1.977221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.081226</td>\n",
       "      <td>0.239369</td>\n",
       "      <td>0.429395</td>\n",
       "      <td>-0.510351</td>\n",
       "      <td>2.088174</td>\n",
       "      <td>1.168750</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.097094</td>\n",
       "      <td>0.151980</td>\n",
       "      <td>2.986199</td>\n",
       "      <td>0.254536</td>\n",
       "      <td>0.049347</td>\n",
       "      <td>0.665182</td>\n",
       "      <td>3.256397</td>\n",
       "      <td>-0.038869</td>\n",
       "      <td>-0.567753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.148939</td>\n",
       "      <td>-0.335704</td>\n",
       "      <td>1.793282</td>\n",
       "      <td>-0.710683</td>\n",
       "      <td>0.998823</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>0.089293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.244005</td>\n",
       "      <td>0.419955</td>\n",
       "      <td>-0.388223</td>\n",
       "      <td>1.881868</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.047765</td>\n",
       "      <td>1.057597</td>\n",
       "      <td>-1.858666</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>0.695245</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.498389</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226382</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.618347</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>-0.012726</td>\n",
       "      <td>-0.267625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.877873</td>\n",
       "      <td>-0.935169</td>\n",
       "      <td>-0.645077</td>\n",
       "      <td>0.530646</td>\n",
       "      <td>-0.352513</td>\n",
       "      <td>0.150326</td>\n",
       "      <td>-0.894681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-0.295129</td>\n",
       "      <td>-0.255937</td>\n",
       "      <td>0.710365</td>\n",
       "      <td>-0.914596</td>\n",
       "      <td>0.779307</td>\n",
       "      <td>-0.417111</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.919856</td>\n",
       "      <td>-0.451207</td>\n",
       "      <td>-0.541926</td>\n",
       "      <td>-0.346858</td>\n",
       "      <td>1.044168</td>\n",
       "      <td>0.541659</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.183183</td>\n",
       "      <td>-0.202169</td>\n",
       "      <td>2.525190</td>\n",
       "      <td>0.804163</td>\n",
       "      <td>0.437305</td>\n",
       "      <td>-0.419781</td>\n",
       "      <td>-1.522436</td>\n",
       "      <td>-0.080220</td>\n",
       "      <td>-0.514731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.843292</td>\n",
       "      <td>0.048615</td>\n",
       "      <td>0.978434</td>\n",
       "      <td>0.186987</td>\n",
       "      <td>0.440661</td>\n",
       "      <td>-0.686518</td>\n",
       "      <td>0.404497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.450893</td>\n",
       "      <td>-0.402632</td>\n",
       "      <td>-0.866819</td>\n",
       "      <td>1.065344</td>\n",
       "      <td>-0.635973</td>\n",
       "      <td>-0.680467</td>\n",
       "      <td>-0.502383</td>\n",
       "      <td>1.130826</td>\n",
       "      <td>0.076696</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>621e2f6167b776a240e082a9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333876</td>\n",
       "      <td>-0.530164</td>\n",
       "      <td>0.466479</td>\n",
       "      <td>-0.358932</td>\n",
       "      <td>-0.335899</td>\n",
       "      <td>-0.627364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.855571</td>\n",
       "      <td>0.327909</td>\n",
       "      <td>0.336213</td>\n",
       "      <td>-0.129442</td>\n",
       "      <td>-0.388539</td>\n",
       "      <td>-0.017335</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>1.567018</td>\n",
       "      <td>-1.057101</td>\n",
       "      <td>1.097759</td>\n",
       "      <td>-0.381909</td>\n",
       "      <td>0.047924</td>\n",
       "      <td>1.259984</td>\n",
       "      <td>-0.432281</td>\n",
       "      <td>-0.520710</td>\n",
       "      <td>-0.652363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.520935</td>\n",
       "      <td>-0.405512</td>\n",
       "      <td>-0.428177</td>\n",
       "      <td>0.384287</td>\n",
       "      <td>0.437003</td>\n",
       "      <td>-0.493298</td>\n",
       "      <td>1.003447</td>\n",
       "      <td>0.044596</td>\n",
       "      <td>0.333690</td>\n",
       "      <td>0.295729</td>\n",
       "      <td>-0.251209</td>\n",
       "      <td>-0.857851</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>-1.011274</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>-0.644241</td>\n",
       "      <td>-0.501529</td>\n",
       "      <td>-0.851700</td>\n",
       "      <td>0.331354</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.112554</td>\n",
       "      <td>0.143263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.793497</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.770505</td>\n",
       "      <td>0.568598</td>\n",
       "      <td>-0.696740</td>\n",
       "      <td>-0.254918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370010</td>\n",
       "      <td>0.221019</td>\n",
       "      <td>0.184973</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>-1.390649</td>\n",
       "      <td>0.111041</td>\n",
       "      <td>-0.469241</td>\n",
       "      <td>0.545678</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.508967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>-0.495544</td>\n",
       "      <td>-0.377764</td>\n",
       "      <td>0.108849</td>\n",
       "      <td>-0.617785</td>\n",
       "      <td>-0.945505</td>\n",
       "      <td>-0.798003</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>0.614251</td>\n",
       "      <td>0.342487</td>\n",
       "      <td>0.304072</td>\n",
       "      <td>-0.227561</td>\n",
       "      <td>-0.096130</td>\n",
       "      <td>-0.150959</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.068961</td>\n",
       "      <td>-0.883814</td>\n",
       "      <td>0.301210</td>\n",
       "      <td>-0.357986</td>\n",
       "      <td>-0.133966</td>\n",
       "      <td>0.837568</td>\n",
       "      <td>0.079174</td>\n",
       "      <td>-0.697311</td>\n",
       "      <td>-0.995158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.155453</td>\n",
       "      <td>-0.736343</td>\n",
       "      <td>-0.110487</td>\n",
       "      <td>0.137855</td>\n",
       "      <td>-0.666586</td>\n",
       "      <td>-1.035030</td>\n",
       "      <td>0.134312</td>\n",
       "      <td>1.696286</td>\n",
       "      <td>0.343276</td>\n",
       "      <td>0.383125</td>\n",
       "      <td>-0.542573</td>\n",
       "      <td>-0.763898</td>\n",
       "      <td>1.343710</td>\n",
       "      <td>0.432446</td>\n",
       "      <td>-0.502383</td>\n",
       "      <td>0.513563</td>\n",
       "      <td>-1.265492</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>621e2f7a67b776a240f14425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.676134</td>\n",
       "      <td>1.777261</td>\n",
       "      <td>-0.096650</td>\n",
       "      <td>1.375064</td>\n",
       "      <td>-1.098944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.086233</td>\n",
       "      <td>-0.418966</td>\n",
       "      <td>-0.534120</td>\n",
       "      <td>0.230875</td>\n",
       "      <td>-0.609383</td>\n",
       "      <td>-1.266239</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.829987</td>\n",
       "      <td>-0.556808</td>\n",
       "      <td>1.778792</td>\n",
       "      <td>-0.413649</td>\n",
       "      <td>1.929864</td>\n",
       "      <td>-0.083613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.293488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.787985</td>\n",
       "      <td>-0.899702</td>\n",
       "      <td>-0.727630</td>\n",
       "      <td>0.075334</td>\n",
       "      <td>-0.644706</td>\n",
       "      <td>-0.057683</td>\n",
       "      <td>-1.652670</td>\n",
       "      <td>-1.661116</td>\n",
       "      <td>-0.421233</td>\n",
       "      <td>-0.360111</td>\n",
       "      <td>1.931164</td>\n",
       "      <td>2.847318</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-2.160409</td>\n",
       "      <td>-0.279987</td>\n",
       "      <td>-0.352786</td>\n",
       "      <td>-0.516388</td>\n",
       "      <td>1.255798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.788327</td>\n",
       "      <td>0.367927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230835</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.784169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426214</td>\n",
       "      <td>0.393449</td>\n",
       "      <td>-0.752181</td>\n",
       "      <td>-0.693922</td>\n",
       "      <td>0.578769</td>\n",
       "      <td>-0.843550</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>1.204256</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.525478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-2.003846</td>\n",
       "      <td>1.046362</td>\n",
       "      <td>0.919543</td>\n",
       "      <td>0.252637</td>\n",
       "      <td>1.168094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.326785</td>\n",
       "      <td>-1.116538</td>\n",
       "      <td>-1.296114</td>\n",
       "      <td>0.417016</td>\n",
       "      <td>0.417392</td>\n",
       "      <td>-0.369630</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.660782</td>\n",
       "      <td>-1.038450</td>\n",
       "      <td>0.644843</td>\n",
       "      <td>-0.416231</td>\n",
       "      <td>0.776002</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.012405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.188973</td>\n",
       "      <td>-0.102054</td>\n",
       "      <td>0.287764</td>\n",
       "      <td>0.168255</td>\n",
       "      <td>-0.621408</td>\n",
       "      <td>-0.475220</td>\n",
       "      <td>-0.557779</td>\n",
       "      <td>1.441073</td>\n",
       "      <td>-1.116594</td>\n",
       "      <td>-1.091775</td>\n",
       "      <td>0.724232</td>\n",
       "      <td>1.990123</td>\n",
       "      <td>-0.141052</td>\n",
       "      <td>-0.044517</td>\n",
       "      <td>1.396158</td>\n",
       "      <td>1.130826</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>621e2f9167b776a240011ccb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.890859</td>\n",
       "      <td>-1.445400</td>\n",
       "      <td>-0.246466</td>\n",
       "      <td>-1.519221</td>\n",
       "      <td>-0.421096</td>\n",
       "      <td>-0.761567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.067913</td>\n",
       "      <td>-0.603336</td>\n",
       "      <td>-0.344057</td>\n",
       "      <td>-0.129813</td>\n",
       "      <td>0.039063</td>\n",
       "      <td>0.070530</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.168760</td>\n",
       "      <td>0.464725</td>\n",
       "      <td>-0.115673</td>\n",
       "      <td>0.178337</td>\n",
       "      <td>-0.800390</td>\n",
       "      <td>0.578924</td>\n",
       "      <td>-0.696156</td>\n",
       "      <td>-0.256672</td>\n",
       "      <td>-0.730785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.528013</td>\n",
       "      <td>1.307144</td>\n",
       "      <td>-0.045982</td>\n",
       "      <td>-0.665676</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>-0.333110</td>\n",
       "      <td>1.196886</td>\n",
       "      <td>-1.195173</td>\n",
       "      <td>-0.607263</td>\n",
       "      <td>-0.473457</td>\n",
       "      <td>-1.554448</td>\n",
       "      <td>-0.838617</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-0.326458</td>\n",
       "      <td>0.693867</td>\n",
       "      <td>-0.662340</td>\n",
       "      <td>-0.263289</td>\n",
       "      <td>-0.385855</td>\n",
       "      <td>0.627576</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.125610</td>\n",
       "      <td>-0.680501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.658383</td>\n",
       "      <td>-0.100210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.109511</td>\n",
       "      <td>0.604479</td>\n",
       "      <td>0.008364</td>\n",
       "      <td>-0.353026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113915</td>\n",
       "      <td>-1.966679</td>\n",
       "      <td>-0.305917</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>-1.256569</td>\n",
       "      <td>0.395916</td>\n",
       "      <td>0.020797</td>\n",
       "      <td>0.282247</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.261308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-0.521684</td>\n",
       "      <td>-0.864867</td>\n",
       "      <td>-1.187141</td>\n",
       "      <td>-0.727421</td>\n",
       "      <td>-0.404553</td>\n",
       "      <td>-0.274580</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>1.147969</td>\n",
       "      <td>-0.044130</td>\n",
       "      <td>-0.111315</td>\n",
       "      <td>-0.199234</td>\n",
       "      <td>0.265319</td>\n",
       "      <td>-0.092711</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.313348</td>\n",
       "      <td>0.886782</td>\n",
       "      <td>-0.420660</td>\n",
       "      <td>-0.017843</td>\n",
       "      <td>-0.739886</td>\n",
       "      <td>0.238219</td>\n",
       "      <td>0.112472</td>\n",
       "      <td>0.085122</td>\n",
       "      <td>-0.926358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.070076</td>\n",
       "      <td>-0.300397</td>\n",
       "      <td>0.206204</td>\n",
       "      <td>0.603161</td>\n",
       "      <td>0.064059</td>\n",
       "      <td>0.076824</td>\n",
       "      <td>0.497716</td>\n",
       "      <td>0.025585</td>\n",
       "      <td>-0.043605</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>-0.697705</td>\n",
       "      <td>-0.975048</td>\n",
       "      <td>-1.130894</td>\n",
       "      <td>1.068396</td>\n",
       "      <td>-0.356341</td>\n",
       "      <td>-1.461679</td>\n",
       "      <td>0.460179</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>621e2fb367b776a24015accd</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.844661</td>\n",
       "      <td>-0.142981</td>\n",
       "      <td>0.869333</td>\n",
       "      <td>-0.347274</td>\n",
       "      <td>-0.779647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>0.532800</td>\n",
       "      <td>0.752366</td>\n",
       "      <td>-0.510432</td>\n",
       "      <td>-0.478845</td>\n",
       "      <td>-0.742325</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.042371</td>\n",
       "      <td>-0.369784</td>\n",
       "      <td>1.551203</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.026578</td>\n",
       "      <td>0.451060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.285114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401307</td>\n",
       "      <td>0.924946</td>\n",
       "      <td>-0.508278</td>\n",
       "      <td>-0.486091</td>\n",
       "      <td>-0.056428</td>\n",
       "      <td>-1.139519</td>\n",
       "      <td>-0.328717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.540716</td>\n",
       "      <td>0.195930</td>\n",
       "      <td>-0.459592</td>\n",
       "      <td>0.513322</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.300960</td>\n",
       "      <td>-0.257564</td>\n",
       "      <td>0.595134</td>\n",
       "      <td>-2.404135</td>\n",
       "      <td>-0.113909</td>\n",
       "      <td>0.680233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.695133</td>\n",
       "      <td>-0.755389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.686610</td>\n",
       "      <td>0.059079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.319015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.514355</td>\n",
       "      <td>-1.966679</td>\n",
       "      <td>-0.770031</td>\n",
       "      <td>0.285732</td>\n",
       "      <td>0.137483</td>\n",
       "      <td>-0.138219</td>\n",
       "      <td>0.378210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.135684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.300960</td>\n",
       "      <td>0.230269</td>\n",
       "      <td>0.894760</td>\n",
       "      <td>0.550148</td>\n",
       "      <td>-0.534365</td>\n",
       "      <td>0.592785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.080463</td>\n",
       "      <td>0.249286</td>\n",
       "      <td>0.243891</td>\n",
       "      <td>-0.346691</td>\n",
       "      <td>0.070054</td>\n",
       "      <td>-0.433776</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.808931</td>\n",
       "      <td>-0.398811</td>\n",
       "      <td>1.773844</td>\n",
       "      <td>0.234930</td>\n",
       "      <td>0.123822</td>\n",
       "      <td>0.381938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.253315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.262119</td>\n",
       "      <td>-0.305118</td>\n",
       "      <td>-0.018147</td>\n",
       "      <td>0.419259</td>\n",
       "      <td>-0.012359</td>\n",
       "      <td>-0.322383</td>\n",
       "      <td>0.544023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.250012</td>\n",
       "      <td>0.272047</td>\n",
       "      <td>-0.595418</td>\n",
       "      <td>0.342921</td>\n",
       "      <td>0.848789</td>\n",
       "      <td>-0.362492</td>\n",
       "      <td>-0.648425</td>\n",
       "      <td>-0.350606</td>\n",
       "      <td>0.843661</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>621e2fce67b776a240279baa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.186988</td>\n",
       "      <td>0.884608</td>\n",
       "      <td>-0.688576</td>\n",
       "      <td>0.398916</td>\n",
       "      <td>-0.021646</td>\n",
       "      <td>-0.624967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.011200</td>\n",
       "      <td>-0.143768</td>\n",
       "      <td>0.159020</td>\n",
       "      <td>0.528416</td>\n",
       "      <td>-0.310119</td>\n",
       "      <td>-0.119026</td>\n",
       "      <td>0.256004</td>\n",
       "      <td>-0.582666</td>\n",
       "      <td>2.487800</td>\n",
       "      <td>0.617458</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>3.195118</td>\n",
       "      <td>-0.660966</td>\n",
       "      <td>0.779022</td>\n",
       "      <td>1.200454</td>\n",
       "      <td>-0.587032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.643650</td>\n",
       "      <td>-0.351817</td>\n",
       "      <td>-0.304886</td>\n",
       "      <td>-0.329054</td>\n",
       "      <td>0.261162</td>\n",
       "      <td>-0.935762</td>\n",
       "      <td>0.565254</td>\n",
       "      <td>-0.110956</td>\n",
       "      <td>-0.142904</td>\n",
       "      <td>-0.007300</td>\n",
       "      <td>0.405170</td>\n",
       "      <td>1.026600</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>1.008707</td>\n",
       "      <td>1.784754</td>\n",
       "      <td>0.657767</td>\n",
       "      <td>-0.088619</td>\n",
       "      <td>-0.136862</td>\n",
       "      <td>0.479465</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>6.098663</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.361073</td>\n",
       "      <td>-0.156287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.970136</td>\n",
       "      <td>0.076778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.009076</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>1.547150</td>\n",
       "      <td>-0.337453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.770778</td>\n",
       "      <td>-0.009547</td>\n",
       "      <td>0.301002</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>-0.287032</td>\n",
       "      <td>0.661083</td>\n",
       "      <td>-0.608930</td>\n",
       "      <td>0.282247</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.101227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>1.464144</td>\n",
       "      <td>1.128118</td>\n",
       "      <td>1.035021</td>\n",
       "      <td>-0.637194</td>\n",
       "      <td>-0.170250</td>\n",
       "      <td>-0.197162</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>-0.279068</td>\n",
       "      <td>-0.412620</td>\n",
       "      <td>0.029757</td>\n",
       "      <td>0.407064</td>\n",
       "      <td>0.067382</td>\n",
       "      <td>0.084866</td>\n",
       "      <td>-0.070656</td>\n",
       "      <td>2.633101</td>\n",
       "      <td>0.083420</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>1.416700</td>\n",
       "      <td>0.714738</td>\n",
       "      <td>1.238441</td>\n",
       "      <td>1.799488</td>\n",
       "      <td>-0.907062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.075795</td>\n",
       "      <td>-0.197907</td>\n",
       "      <td>0.355040</td>\n",
       "      <td>0.581169</td>\n",
       "      <td>1.113514</td>\n",
       "      <td>-0.815981</td>\n",
       "      <td>0.732975</td>\n",
       "      <td>-0.013925</td>\n",
       "      <td>-0.278702</td>\n",
       "      <td>-0.228620</td>\n",
       "      <td>-0.718472</td>\n",
       "      <td>0.809819</td>\n",
       "      <td>0.477598</td>\n",
       "      <td>0.591434</td>\n",
       "      <td>0.227825</td>\n",
       "      <td>-0.350606</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>621e2ff067b776a2403eb737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.220759</td>\n",
       "      <td>-0.484167</td>\n",
       "      <td>-0.276172</td>\n",
       "      <td>-1.262243</td>\n",
       "      <td>-0.702450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.350310</td>\n",
       "      <td>-0.672687</td>\n",
       "      <td>-0.595560</td>\n",
       "      <td>-0.547631</td>\n",
       "      <td>-0.349103</td>\n",
       "      <td>-0.493070</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.135960</td>\n",
       "      <td>-1.024325</td>\n",
       "      <td>-0.627330</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-1.135281</td>\n",
       "      <td>-0.256077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.776072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030526</td>\n",
       "      <td>-0.429030</td>\n",
       "      <td>-0.370144</td>\n",
       "      <td>-0.344619</td>\n",
       "      <td>-0.613454</td>\n",
       "      <td>-1.142494</td>\n",
       "      <td>-0.759035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.677390</td>\n",
       "      <td>-0.635201</td>\n",
       "      <td>-1.168156</td>\n",
       "      <td>-0.684989</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>0.850237</td>\n",
       "      <td>-0.234992</td>\n",
       "      <td>-0.967784</td>\n",
       "      <td>0.661527</td>\n",
       "      <td>-2.206836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.343064</td>\n",
       "      <td>-0.306063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.473395</td>\n",
       "      <td>-0.330294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.027466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.267900</td>\n",
       "      <td>0.153906</td>\n",
       "      <td>-0.448721</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>0.826140</td>\n",
       "      <td>1.856076</td>\n",
       "      <td>-0.855715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>0.761631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>0.058565</td>\n",
       "      <td>0.353308</td>\n",
       "      <td>-1.785434</td>\n",
       "      <td>-0.553965</td>\n",
       "      <td>-2.303962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>1.173876</td>\n",
       "      <td>-0.048311</td>\n",
       "      <td>-0.131270</td>\n",
       "      <td>-0.370586</td>\n",
       "      <td>0.841068</td>\n",
       "      <td>0.217254</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.209194</td>\n",
       "      <td>-0.675493</td>\n",
       "      <td>-0.397494</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-0.756741</td>\n",
       "      <td>0.573332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.943785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.187702</td>\n",
       "      <td>-1.015719</td>\n",
       "      <td>0.748025</td>\n",
       "      <td>0.150443</td>\n",
       "      <td>0.821075</td>\n",
       "      <td>0.286768</td>\n",
       "      <td>-1.216821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.047784</td>\n",
       "      <td>-0.011925</td>\n",
       "      <td>-0.397821</td>\n",
       "      <td>-0.468924</td>\n",
       "      <td>0.353868</td>\n",
       "      <td>0.909409</td>\n",
       "      <td>-1.524674</td>\n",
       "      <td>-2.325848</td>\n",
       "      <td>-1.265492</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>621e301367b776a24057738e</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.146896</td>\n",
       "      <td>-0.170314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.247893</td>\n",
       "      <td>-0.930517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.143470</td>\n",
       "      <td>0.292438</td>\n",
       "      <td>0.788284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.556564</td>\n",
       "      <td>-0.724686</td>\n",
       "      <td>0.872361</td>\n",
       "      <td>1.068204</td>\n",
       "      <td>-0.696552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.697675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.567742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.297685</td>\n",
       "      <td>0.652858</td>\n",
       "      <td>0.316314</td>\n",
       "      <td>0.059651</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>1.198977</td>\n",
       "      <td>0.554551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.488736</td>\n",
       "      <td>0.748887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.798682</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.642074</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.001538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.943833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.903910</td>\n",
       "      <td>1.632757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.545678</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.496764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>1.362798</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078603</td>\n",
       "      <td>0.656447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.571661</td>\n",
       "      <td>-0.585639</td>\n",
       "      <td>-0.695024</td>\n",
       "      <td>-0.554475</td>\n",
       "      <td>-3.325221</td>\n",
       "      <td>-2.768547</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-1.690245</td>\n",
       "      <td>-0.741429</td>\n",
       "      <td>0.994963</td>\n",
       "      <td>1.318277</td>\n",
       "      <td>-0.374815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.654279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.240155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.300546</td>\n",
       "      <td>1.094499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.751222</td>\n",
       "      <td>-0.585435</td>\n",
       "      <td>-0.507039</td>\n",
       "      <td>0.027434</td>\n",
       "      <td>0.467397</td>\n",
       "      <td>-0.512243</td>\n",
       "      <td>-0.362492</td>\n",
       "      <td>-0.356341</td>\n",
       "      <td>1.130826</td>\n",
       "      <td>-0.115045</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>621e301e67b776a240608a72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.121110</td>\n",
       "      <td>2.599655</td>\n",
       "      <td>0.588563</td>\n",
       "      <td>0.589562</td>\n",
       "      <td>0.852508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.033518</td>\n",
       "      <td>-0.488519</td>\n",
       "      <td>-0.679074</td>\n",
       "      <td>-0.316663</td>\n",
       "      <td>0.207836</td>\n",
       "      <td>0.252570</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>2.060675</td>\n",
       "      <td>-0.176215</td>\n",
       "      <td>0.966829</td>\n",
       "      <td>1.517847</td>\n",
       "      <td>-0.210185</td>\n",
       "      <td>1.065549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.620215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.304132</td>\n",
       "      <td>-1.081224</td>\n",
       "      <td>0.318683</td>\n",
       "      <td>0.257709</td>\n",
       "      <td>-0.364076</td>\n",
       "      <td>1.040523</td>\n",
       "      <td>-0.730425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.491350</td>\n",
       "      <td>-0.326684</td>\n",
       "      <td>0.809801</td>\n",
       "      <td>-0.848682</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-1.190755</td>\n",
       "      <td>-0.498555</td>\n",
       "      <td>-1.312787</td>\n",
       "      <td>-0.360994</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>-0.127633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.746908</td>\n",
       "      <td>-0.680501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.820218</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.814261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.400439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388636</td>\n",
       "      <td>-0.230788</td>\n",
       "      <td>-0.689704</td>\n",
       "      <td>-0.816378</td>\n",
       "      <td>-0.164256</td>\n",
       "      <td>-1.233209</td>\n",
       "      <td>0.608543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-1.190755</td>\n",
       "      <td>0.673068</td>\n",
       "      <td>-0.165261</td>\n",
       "      <td>-0.503116</td>\n",
       "      <td>-0.271767</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-1.019590</td>\n",
       "      <td>-1.805915</td>\n",
       "      <td>-1.484996</td>\n",
       "      <td>-0.311603</td>\n",
       "      <td>0.230763</td>\n",
       "      <td>0.158648</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-1.055994</td>\n",
       "      <td>-0.349923</td>\n",
       "      <td>1.165309</td>\n",
       "      <td>0.825954</td>\n",
       "      <td>-0.677240</td>\n",
       "      <td>0.417914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.393328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.091645</td>\n",
       "      <td>0.289947</td>\n",
       "      <td>0.220308</td>\n",
       "      <td>0.325004</td>\n",
       "      <td>0.016952</td>\n",
       "      <td>1.031009</td>\n",
       "      <td>0.291284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.806360</td>\n",
       "      <td>-2.020944</td>\n",
       "      <td>-0.176114</td>\n",
       "      <td>-0.933132</td>\n",
       "      <td>0.601329</td>\n",
       "      <td>0.909409</td>\n",
       "      <td>0.958033</td>\n",
       "      <td>-1.338227</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>621e30c867b776a240d4aa6c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.430172</td>\n",
       "      <td>-1.050827</td>\n",
       "      <td>-0.204884</td>\n",
       "      <td>-0.501247</td>\n",
       "      <td>-0.031663</td>\n",
       "      <td>0.323896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907265</td>\n",
       "      <td>-0.263858</td>\n",
       "      <td>-0.072542</td>\n",
       "      <td>-0.470408</td>\n",
       "      <td>-0.435895</td>\n",
       "      <td>-0.150489</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.372026</td>\n",
       "      <td>-1.347951</td>\n",
       "      <td>-0.030063</td>\n",
       "      <td>-0.465745</td>\n",
       "      <td>-0.201696</td>\n",
       "      <td>-0.320111</td>\n",
       "      <td>-0.351812</td>\n",
       "      <td>-0.058874</td>\n",
       "      <td>-0.261057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.718627</td>\n",
       "      <td>0.304764</td>\n",
       "      <td>-0.434615</td>\n",
       "      <td>-0.597681</td>\n",
       "      <td>1.100467</td>\n",
       "      <td>-0.752231</td>\n",
       "      <td>2.105718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.264245</td>\n",
       "      <td>-0.164551</td>\n",
       "      <td>-0.415036</td>\n",
       "      <td>-0.438163</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-0.583472</td>\n",
       "      <td>0.629202</td>\n",
       "      <td>0.549091</td>\n",
       "      <td>-0.521309</td>\n",
       "      <td>-0.107948</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.871167</td>\n",
       "      <td>-0.605614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.802404</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.319649</td>\n",
       "      <td>0.487419</td>\n",
       "      <td>-0.492131</td>\n",
       "      <td>-0.161917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002492</td>\n",
       "      <td>-0.218789</td>\n",
       "      <td>-0.903910</td>\n",
       "      <td>0.285732</td>\n",
       "      <td>-0.680663</td>\n",
       "      <td>0.752726</td>\n",
       "      <td>-0.136363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.529785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-0.505136</td>\n",
       "      <td>-0.664642</td>\n",
       "      <td>0.559513</td>\n",
       "      <td>0.150720</td>\n",
       "      <td>-0.139577</td>\n",
       "      <td>1.373222</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>0.806430</td>\n",
       "      <td>0.148516</td>\n",
       "      <td>0.139366</td>\n",
       "      <td>-0.325912</td>\n",
       "      <td>0.488613</td>\n",
       "      <td>-0.282697</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.434303</td>\n",
       "      <td>-0.735410</td>\n",
       "      <td>0.052009</td>\n",
       "      <td>-0.421316</td>\n",
       "      <td>-0.227787</td>\n",
       "      <td>0.342170</td>\n",
       "      <td>-0.156184</td>\n",
       "      <td>-0.454850</td>\n",
       "      <td>-0.153702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.610790</td>\n",
       "      <td>-0.037926</td>\n",
       "      <td>0.363449</td>\n",
       "      <td>0.294451</td>\n",
       "      <td>-0.313709</td>\n",
       "      <td>-0.158874</td>\n",
       "      <td>-0.117078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.149173</td>\n",
       "      <td>0.220574</td>\n",
       "      <td>0.297412</td>\n",
       "      <td>-0.453426</td>\n",
       "      <td>0.477598</td>\n",
       "      <td>0.273459</td>\n",
       "      <td>-1.086549</td>\n",
       "      <td>-0.720963</td>\n",
       "      <td>0.460179</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>621e30e267b776a240e5bf90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.398160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.763172</td>\n",
       "      <td>-1.136073</td>\n",
       "      <td>-1.310834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.415651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.051721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.145553</td>\n",
       "      <td>-1.053309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.329909</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>2.071827</td>\n",
       "      <td>3.255366</td>\n",
       "      <td>0.052335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.509680</td>\n",
       "      <td>-1.248798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066052</td>\n",
       "      <td>4.713863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.681800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.686638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>3.352358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>2.071827</td>\n",
       "      <td>2.106936</td>\n",
       "      <td>-0.324301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.881812</td>\n",
       "      <td>-1.442883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-1.880910</td>\n",
       "      <td>-1.859797</td>\n",
       "      <td>-1.534995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.612836</td>\n",
       "      <td>1.107939</td>\n",
       "      <td>-1.090698</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-1.218638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.053229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.802866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.860271</td>\n",
       "      <td>-2.078616</td>\n",
       "      <td>-1.019775</td>\n",
       "      <td>-1.254214</td>\n",
       "      <td>0.972519</td>\n",
       "      <td>0.591434</td>\n",
       "      <td>-0.940508</td>\n",
       "      <td>0.390110</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>621e30e467b776a240e817c7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.465588</td>\n",
       "      <td>4.122059</td>\n",
       "      <td>3.374508</td>\n",
       "      <td>0.254568</td>\n",
       "      <td>-0.666815</td>\n",
       "      <td>2.424184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.731337</td>\n",
       "      <td>-0.312899</td>\n",
       "      <td>-0.048644</td>\n",
       "      <td>0.024321</td>\n",
       "      <td>1.604472</td>\n",
       "      <td>2.396447</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>-0.221116</td>\n",
       "      <td>4.127703</td>\n",
       "      <td>0.185990</td>\n",
       "      <td>-0.426302</td>\n",
       "      <td>4.006792</td>\n",
       "      <td>1.829085</td>\n",
       "      <td>0.549383</td>\n",
       "      <td>0.150343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.869632</td>\n",
       "      <td>1.848175</td>\n",
       "      <td>1.846673</td>\n",
       "      <td>1.561738</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.365153</td>\n",
       "      <td>-0.694702</td>\n",
       "      <td>0.198045</td>\n",
       "      <td>-0.313858</td>\n",
       "      <td>-0.164041</td>\n",
       "      <td>0.306394</td>\n",
       "      <td>4.260040</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>2.071827</td>\n",
       "      <td>-1.693723</td>\n",
       "      <td>0.495605</td>\n",
       "      <td>-2.250456</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>-0.750739</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.943652</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.820218</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.424576</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>-1.358049</td>\n",
       "      <td>-1.102324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.947717</td>\n",
       "      <td>-0.104265</td>\n",
       "      <td>-0.921761</td>\n",
       "      <td>-0.571465</td>\n",
       "      <td>0.784912</td>\n",
       "      <td>0.479034</td>\n",
       "      <td>1.196498</td>\n",
       "      <td>-0.903192</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>2.071827</td>\n",
       "      <td>0.186699</td>\n",
       "      <td>1.257299</td>\n",
       "      <td>0.055236</td>\n",
       "      <td>-1.203840</td>\n",
       "      <td>-0.878568</td>\n",
       "      <td>0.100511</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-1.512468</td>\n",
       "      <td>-0.514325</td>\n",
       "      <td>-0.695024</td>\n",
       "      <td>-0.125400</td>\n",
       "      <td>0.574010</td>\n",
       "      <td>0.749521</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-1.661025</td>\n",
       "      <td>-0.947044</td>\n",
       "      <td>3.917789</td>\n",
       "      <td>0.214441</td>\n",
       "      <td>-0.621423</td>\n",
       "      <td>-0.661996</td>\n",
       "      <td>-1.713642</td>\n",
       "      <td>-1.123447</td>\n",
       "      <td>-0.350390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.592182</td>\n",
       "      <td>1.575342</td>\n",
       "      <td>0.613956</td>\n",
       "      <td>-0.422285</td>\n",
       "      <td>0.921691</td>\n",
       "      <td>1.723628</td>\n",
       "      <td>0.189977</td>\n",
       "      <td>-2.134134</td>\n",
       "      <td>-0.514107</td>\n",
       "      <td>-0.470994</td>\n",
       "      <td>-1.162770</td>\n",
       "      <td>1.731269</td>\n",
       "      <td>0.477598</td>\n",
       "      <td>-2.906292</td>\n",
       "      <td>-1.670716</td>\n",
       "      <td>-2.078943</td>\n",
       "      <td>-1.840716</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>621e30f467b776a240f22944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.579143</td>\n",
       "      <td>0.573354</td>\n",
       "      <td>-0.490786</td>\n",
       "      <td>1.806943</td>\n",
       "      <td>-0.633712</td>\n",
       "      <td>-0.905980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793963</td>\n",
       "      <td>1.200592</td>\n",
       "      <td>1.482043</td>\n",
       "      <td>-0.008622</td>\n",
       "      <td>-1.182860</td>\n",
       "      <td>-0.640548</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.530977</td>\n",
       "      <td>-0.738063</td>\n",
       "      <td>-1.056281</td>\n",
       "      <td>-0.385075</td>\n",
       "      <td>0.793859</td>\n",
       "      <td>-0.662998</td>\n",
       "      <td>-0.862989</td>\n",
       "      <td>-1.357831</td>\n",
       "      <td>-0.525059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.754415</td>\n",
       "      <td>3.141453</td>\n",
       "      <td>-1.242833</td>\n",
       "      <td>-0.766165</td>\n",
       "      <td>-1.070808</td>\n",
       "      <td>0.126637</td>\n",
       "      <td>-0.972826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.215408</td>\n",
       "      <td>0.983462</td>\n",
       "      <td>1.923055</td>\n",
       "      <td>0.532474</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-0.660397</td>\n",
       "      <td>1.139845</td>\n",
       "      <td>-0.580946</td>\n",
       "      <td>0.434111</td>\n",
       "      <td>2.193615</td>\n",
       "      <td>0.597954</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>1.986801</td>\n",
       "      <td>1.715906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.268528</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756839</td>\n",
       "      <td>0.529777</td>\n",
       "      <td>-0.911860</td>\n",
       "      <td>1.501812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.272556</td>\n",
       "      <td>-0.844651</td>\n",
       "      <td>1.889700</td>\n",
       "      <td>0.285732</td>\n",
       "      <td>1.239276</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>0.537672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>0.444340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-1.586555</td>\n",
       "      <td>0.357388</td>\n",
       "      <td>-1.160884</td>\n",
       "      <td>0.337628</td>\n",
       "      <td>2.129660</td>\n",
       "      <td>-0.865717</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>0.636172</td>\n",
       "      <td>-0.243328</td>\n",
       "      <td>-0.377035</td>\n",
       "      <td>-0.228220</td>\n",
       "      <td>0.593458</td>\n",
       "      <td>0.677490</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.121857</td>\n",
       "      <td>-0.894307</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>-0.437586</td>\n",
       "      <td>0.288041</td>\n",
       "      <td>0.479438</td>\n",
       "      <td>-0.895160</td>\n",
       "      <td>-1.189206</td>\n",
       "      <td>0.550734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.734242</td>\n",
       "      <td>0.614768</td>\n",
       "      <td>0.617123</td>\n",
       "      <td>0.564610</td>\n",
       "      <td>1.013066</td>\n",
       "      <td>0.289357</td>\n",
       "      <td>0.351258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.242915</td>\n",
       "      <td>-0.247512</td>\n",
       "      <td>0.321819</td>\n",
       "      <td>0.224506</td>\n",
       "      <td>-0.264783</td>\n",
       "      <td>0.273459</td>\n",
       "      <td>0.081783</td>\n",
       "      <td>-0.103700</td>\n",
       "      <td>-0.115045</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>621e310d67b776a24003096d</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.139305</td>\n",
       "      <td>-0.749507</td>\n",
       "      <td>-0.334097</td>\n",
       "      <td>-0.686749</td>\n",
       "      <td>0.336152</td>\n",
       "      <td>-0.729102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450605</td>\n",
       "      <td>-0.352127</td>\n",
       "      <td>-0.322317</td>\n",
       "      <td>-0.166741</td>\n",
       "      <td>0.254947</td>\n",
       "      <td>0.101871</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>1.320647</td>\n",
       "      <td>0.471723</td>\n",
       "      <td>-0.465364</td>\n",
       "      <td>-0.432611</td>\n",
       "      <td>-0.368972</td>\n",
       "      <td>-0.496757</td>\n",
       "      <td>0.585990</td>\n",
       "      <td>0.206030</td>\n",
       "      <td>-0.617882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.599960</td>\n",
       "      <td>-0.382726</td>\n",
       "      <td>0.258374</td>\n",
       "      <td>-0.123261</td>\n",
       "      <td>0.051064</td>\n",
       "      <td>-0.656013</td>\n",
       "      <td>0.448809</td>\n",
       "      <td>0.487244</td>\n",
       "      <td>-0.353602</td>\n",
       "      <td>-0.287643</td>\n",
       "      <td>-0.571134</td>\n",
       "      <td>-1.126197</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-0.206013</td>\n",
       "      <td>-1.312835</td>\n",
       "      <td>-0.343603</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>-0.844672</td>\n",
       "      <td>0.953419</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.891877</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.704424</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.591786</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>0.955538</td>\n",
       "      <td>0.242289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.737857</td>\n",
       "      <td>0.153906</td>\n",
       "      <td>-0.957462</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>-0.513440</td>\n",
       "      <td>0.268875</td>\n",
       "      <td>-0.564060</td>\n",
       "      <td>-0.310472</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>0.502647</td>\n",
       "      <td>-1.290892</td>\n",
       "      <td>-0.634742</td>\n",
       "      <td>-1.052147</td>\n",
       "      <td>-0.764767</td>\n",
       "      <td>0.551869</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>-0.698120</td>\n",
       "      <td>-0.905748</td>\n",
       "      <td>-0.097039</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>-0.309355</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.021851</td>\n",
       "      <td>0.037110</td>\n",
       "      <td>-0.488341</td>\n",
       "      <td>-0.446091</td>\n",
       "      <td>-0.542384</td>\n",
       "      <td>0.151675</td>\n",
       "      <td>1.002862</td>\n",
       "      <td>1.003184</td>\n",
       "      <td>-0.537834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055095</td>\n",
       "      <td>0.163456</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.339254</td>\n",
       "      <td>-0.386921</td>\n",
       "      <td>0.380447</td>\n",
       "      <td>-0.430231</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>-0.697999</td>\n",
       "      <td>-0.668803</td>\n",
       "      <td>-0.956432</td>\n",
       "      <td>-1.013480</td>\n",
       "      <td>1.591170</td>\n",
       "      <td>1.545359</td>\n",
       "      <td>-0.356341</td>\n",
       "      <td>0.019752</td>\n",
       "      <td>0.843661</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>621e312a67b776a240164d59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.287583</td>\n",
       "      <td>1.101382</td>\n",
       "      <td>0.434881</td>\n",
       "      <td>0.349202</td>\n",
       "      <td>0.209849</td>\n",
       "      <td>-0.608437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.196525</td>\n",
       "      <td>-0.456346</td>\n",
       "      <td>-0.609959</td>\n",
       "      <td>-0.076470</td>\n",
       "      <td>0.191347</td>\n",
       "      <td>0.221118</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.071363</td>\n",
       "      <td>-0.316623</td>\n",
       "      <td>-0.981755</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>1.290926</td>\n",
       "      <td>-0.534423</td>\n",
       "      <td>-0.208148</td>\n",
       "      <td>0.780889</td>\n",
       "      <td>-0.283512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.394079</td>\n",
       "      <td>-0.854832</td>\n",
       "      <td>0.342750</td>\n",
       "      <td>-0.113195</td>\n",
       "      <td>-0.248109</td>\n",
       "      <td>0.682759</td>\n",
       "      <td>-0.733498</td>\n",
       "      <td>-1.187736</td>\n",
       "      <td>-0.459021</td>\n",
       "      <td>-0.400639</td>\n",
       "      <td>0.158754</td>\n",
       "      <td>0.890117</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>-1.588401</td>\n",
       "      <td>1.238386</td>\n",
       "      <td>-0.438052</td>\n",
       "      <td>-0.344080</td>\n",
       "      <td>0.389156</td>\n",
       "      <td>1.101530</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.405194</td>\n",
       "      <td>-0.755389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.575269</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.431260</td>\n",
       "      <td>0.299672</td>\n",
       "      <td>-1.424180</td>\n",
       "      <td>-0.066114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.567003</td>\n",
       "      <td>1.018405</td>\n",
       "      <td>-0.555825</td>\n",
       "      <td>-0.326551</td>\n",
       "      <td>0.028427</td>\n",
       "      <td>-0.946631</td>\n",
       "      <td>-0.047748</td>\n",
       "      <td>0.611536</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.361807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>-1.604353</td>\n",
       "      <td>0.997462</td>\n",
       "      <td>-0.384344</td>\n",
       "      <td>-0.268798</td>\n",
       "      <td>0.380473</td>\n",
       "      <td>1.336149</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.279258</td>\n",
       "      <td>-1.037300</td>\n",
       "      <td>-1.235005</td>\n",
       "      <td>-0.222453</td>\n",
       "      <td>0.795080</td>\n",
       "      <td>1.043453</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.966153</td>\n",
       "      <td>-0.596309</td>\n",
       "      <td>-0.858790</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>0.427155</td>\n",
       "      <td>-0.091512</td>\n",
       "      <td>-0.991195</td>\n",
       "      <td>-1.140688</td>\n",
       "      <td>-0.125730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.350581</td>\n",
       "      <td>-0.631921</td>\n",
       "      <td>0.851151</td>\n",
       "      <td>-0.009522</td>\n",
       "      <td>0.166289</td>\n",
       "      <td>1.024472</td>\n",
       "      <td>-0.032525</td>\n",
       "      <td>0.612810</td>\n",
       "      <td>-1.037332</td>\n",
       "      <td>-1.011674</td>\n",
       "      <td>-0.448549</td>\n",
       "      <td>0.543604</td>\n",
       "      <td>-0.635973</td>\n",
       "      <td>-0.839454</td>\n",
       "      <td>0.958033</td>\n",
       "      <td>0.019752</td>\n",
       "      <td>-0.690268</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>621e314867b776a24029ebf9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688567</td>\n",
       "      <td>0.478573</td>\n",
       "      <td>0.619237</td>\n",
       "      <td>0.714371</td>\n",
       "      <td>-0.885494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.654538</td>\n",
       "      <td>0.232783</td>\n",
       "      <td>0.427673</td>\n",
       "      <td>0.356618</td>\n",
       "      <td>0.025756</td>\n",
       "      <td>-0.067016</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>1.475106</td>\n",
       "      <td>-1.889276</td>\n",
       "      <td>-0.690391</td>\n",
       "      <td>-0.387449</td>\n",
       "      <td>-0.077625</td>\n",
       "      <td>-0.115853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.069953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.528268</td>\n",
       "      <td>-0.753294</td>\n",
       "      <td>0.061798</td>\n",
       "      <td>-0.592451</td>\n",
       "      <td>-0.731648</td>\n",
       "      <td>-0.558861</td>\n",
       "      <td>-0.502527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237477</td>\n",
       "      <td>0.349895</td>\n",
       "      <td>0.610926</td>\n",
       "      <td>0.638002</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-1.871168</td>\n",
       "      <td>0.500733</td>\n",
       "      <td>-0.056706</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>1.420113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.767617</td>\n",
       "      <td>-0.605614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.820218</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.346103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.518263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.137196</td>\n",
       "      <td>-0.326882</td>\n",
       "      <td>-0.796807</td>\n",
       "      <td>0.285732</td>\n",
       "      <td>-0.272699</td>\n",
       "      <td>-0.300580</td>\n",
       "      <td>-0.459363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-2.904939</td>\n",
       "      <td>0.466933</td>\n",
       "      <td>0.071685</td>\n",
       "      <td>-0.497067</td>\n",
       "      <td>1.350334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.459207</td>\n",
       "      <td>-0.425401</td>\n",
       "      <td>-0.579472</td>\n",
       "      <td>0.276663</td>\n",
       "      <td>0.175304</td>\n",
       "      <td>0.241908</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.917573</td>\n",
       "      <td>-1.296169</td>\n",
       "      <td>-0.777142</td>\n",
       "      <td>-0.396576</td>\n",
       "      <td>-0.026601</td>\n",
       "      <td>0.017107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.805175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.115120</td>\n",
       "      <td>-0.502837</td>\n",
       "      <td>0.188039</td>\n",
       "      <td>0.454461</td>\n",
       "      <td>-1.260179</td>\n",
       "      <td>0.098519</td>\n",
       "      <td>-0.501342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.425109</td>\n",
       "      <td>-0.384218</td>\n",
       "      <td>-0.610540</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>-1.254624</td>\n",
       "      <td>0.273459</td>\n",
       "      <td>0.811991</td>\n",
       "      <td>1.501184</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>621e323667b776a240f19134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.222775</td>\n",
       "      <td>-0.381316</td>\n",
       "      <td>0.096160</td>\n",
       "      <td>-1.226282</td>\n",
       "      <td>1.069774</td>\n",
       "      <td>-0.722265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324439</td>\n",
       "      <td>-0.772298</td>\n",
       "      <td>-0.492988</td>\n",
       "      <td>-0.181185</td>\n",
       "      <td>-0.383783</td>\n",
       "      <td>0.592355</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.054122</td>\n",
       "      <td>-1.212719</td>\n",
       "      <td>-0.963539</td>\n",
       "      <td>-0.406937</td>\n",
       "      <td>-1.337106</td>\n",
       "      <td>-0.143950</td>\n",
       "      <td>-0.833281</td>\n",
       "      <td>-0.052724</td>\n",
       "      <td>-0.525958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.713850</td>\n",
       "      <td>-0.352273</td>\n",
       "      <td>-0.339802</td>\n",
       "      <td>-0.006531</td>\n",
       "      <td>-0.290834</td>\n",
       "      <td>-0.599625</td>\n",
       "      <td>0.345982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.777986</td>\n",
       "      <td>-0.601744</td>\n",
       "      <td>-1.224918</td>\n",
       "      <td>-0.903789</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-1.190755</td>\n",
       "      <td>0.895593</td>\n",
       "      <td>-1.312596</td>\n",
       "      <td>-0.673340</td>\n",
       "      <td>-0.412275</td>\n",
       "      <td>1.562051</td>\n",
       "      <td>1.012663</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.485332</td>\n",
       "      <td>0.817253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.726692</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289070</td>\n",
       "      <td>0.879564</td>\n",
       "      <td>0.684895</td>\n",
       "      <td>-1.102324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.579543</td>\n",
       "      <td>0.244937</td>\n",
       "      <td>0.702639</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>-0.576029</td>\n",
       "      <td>0.366735</td>\n",
       "      <td>-0.701475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.414928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-1.190755</td>\n",
       "      <td>-0.074076</td>\n",
       "      <td>-0.532939</td>\n",
       "      <td>-1.053776</td>\n",
       "      <td>-0.575688</td>\n",
       "      <td>1.613349</td>\n",
       "      <td>0.801930</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>0.272496</td>\n",
       "      <td>0.044718</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>-0.166264</td>\n",
       "      <td>0.336708</td>\n",
       "      <td>0.427791</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.132924</td>\n",
       "      <td>-0.913766</td>\n",
       "      <td>-0.945756</td>\n",
       "      <td>-0.450806</td>\n",
       "      <td>-0.936226</td>\n",
       "      <td>-0.115153</td>\n",
       "      <td>0.774397</td>\n",
       "      <td>1.009513</td>\n",
       "      <td>-0.539224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.391796</td>\n",
       "      <td>-0.105805</td>\n",
       "      <td>0.356324</td>\n",
       "      <td>0.395898</td>\n",
       "      <td>-0.671648</td>\n",
       "      <td>0.653464</td>\n",
       "      <td>-2.640196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045304</td>\n",
       "      <td>0.106975</td>\n",
       "      <td>-0.508923</td>\n",
       "      <td>-0.961640</td>\n",
       "      <td>1.096249</td>\n",
       "      <td>0.909409</td>\n",
       "      <td>-0.210300</td>\n",
       "      <td>0.390110</td>\n",
       "      <td>0.268438</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>621e324e67b776a2400191cb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.057943</td>\n",
       "      <td>-0.374901</td>\n",
       "      <td>-1.255617</td>\n",
       "      <td>-1.311539</td>\n",
       "      <td>-0.615888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072887</td>\n",
       "      <td>-0.310572</td>\n",
       "      <td>-0.297781</td>\n",
       "      <td>-0.012318</td>\n",
       "      <td>0.154024</td>\n",
       "      <td>0.067256</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.889664</td>\n",
       "      <td>0.330169</td>\n",
       "      <td>-0.554794</td>\n",
       "      <td>-0.131282</td>\n",
       "      <td>0.915348</td>\n",
       "      <td>-0.344547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.303906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.159527</td>\n",
       "      <td>0.790240</td>\n",
       "      <td>0.255417</td>\n",
       "      <td>1.064155</td>\n",
       "      <td>2.114400</td>\n",
       "      <td>0.315313</td>\n",
       "      <td>0.567956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.311447</td>\n",
       "      <td>-0.312371</td>\n",
       "      <td>-1.425419</td>\n",
       "      <td>0.088462</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>1.182032</td>\n",
       "      <td>0.739424</td>\n",
       "      <td>1.392132</td>\n",
       "      <td>1.521959</td>\n",
       "      <td>1.364832</td>\n",
       "      <td>-0.096105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.829368</td>\n",
       "      <td>0.774349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.788327</td>\n",
       "      <td>-0.530726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.050301</td>\n",
       "      <td>2.059043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.223367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.295838</td>\n",
       "      <td>-0.402777</td>\n",
       "      <td>-0.805732</td>\n",
       "      <td>-0.081638</td>\n",
       "      <td>-0.227751</td>\n",
       "      <td>-1.304808</td>\n",
       "      <td>-0.131543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829338</td>\n",
       "      <td>0.821812</td>\n",
       "      <td>1.381855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>1.182032</td>\n",
       "      <td>-0.345252</td>\n",
       "      <td>1.478805</td>\n",
       "      <td>1.027627</td>\n",
       "      <td>0.305781</td>\n",
       "      <td>-0.136201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>1.485285</td>\n",
       "      <td>0.185787</td>\n",
       "      <td>0.148818</td>\n",
       "      <td>-0.013624</td>\n",
       "      <td>-0.799558</td>\n",
       "      <td>-0.235345</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.253467</td>\n",
       "      <td>1.441417</td>\n",
       "      <td>-0.456451</td>\n",
       "      <td>-0.266745</td>\n",
       "      <td>0.767930</td>\n",
       "      <td>-0.213740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.107316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.109191</td>\n",
       "      <td>-0.841325</td>\n",
       "      <td>-0.720997</td>\n",
       "      <td>-0.068912</td>\n",
       "      <td>0.347363</td>\n",
       "      <td>-0.717162</td>\n",
       "      <td>-0.417227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186470</td>\n",
       "      <td>0.232472</td>\n",
       "      <td>0.261208</td>\n",
       "      <td>0.049141</td>\n",
       "      <td>0.230138</td>\n",
       "      <td>0.114471</td>\n",
       "      <td>1.104074</td>\n",
       "      <td>1.994995</td>\n",
       "      <td>0.076696</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>621e328667b776a240281372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.397322</td>\n",
       "      <td>-0.186589</td>\n",
       "      <td>-1.428403</td>\n",
       "      <td>-0.822642</td>\n",
       "      <td>-1.309713</td>\n",
       "      <td>-0.640024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.795315</td>\n",
       "      <td>-1.291586</td>\n",
       "      <td>-1.310834</td>\n",
       "      <td>-0.159924</td>\n",
       "      <td>-0.363673</td>\n",
       "      <td>-0.667602</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.494018</td>\n",
       "      <td>-0.427401</td>\n",
       "      <td>1.278524</td>\n",
       "      <td>1.745532</td>\n",
       "      <td>-1.238338</td>\n",
       "      <td>-0.208328</td>\n",
       "      <td>-0.406580</td>\n",
       "      <td>-1.312209</td>\n",
       "      <td>2.575788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.944828</td>\n",
       "      <td>2.362543</td>\n",
       "      <td>-0.376758</td>\n",
       "      <td>-0.451127</td>\n",
       "      <td>-1.526418</td>\n",
       "      <td>-0.620285</td>\n",
       "      <td>-0.933058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.302687</td>\n",
       "      <td>-1.306226</td>\n",
       "      <td>-1.061263</td>\n",
       "      <td>-0.661337</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>2.071827</td>\n",
       "      <td>1.197285</td>\n",
       "      <td>1.295135</td>\n",
       "      <td>1.047553</td>\n",
       "      <td>3.885098</td>\n",
       "      <td>-0.463295</td>\n",
       "      <td>0.716442</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>1.337825</td>\n",
       "      <td>1.719055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>2.111061</td>\n",
       "      <td>2.464783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.638178</td>\n",
       "      <td>2.076742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.246474</td>\n",
       "      <td>0.877299</td>\n",
       "      <td>0.653811</td>\n",
       "      <td>1.006661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.346729</td>\n",
       "      <td>1.549265</td>\n",
       "      <td>1.996804</td>\n",
       "      <td>0.653103</td>\n",
       "      <td>1.793744</td>\n",
       "      <td>1.090550</td>\n",
       "      <td>2.165275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.337846</td>\n",
       "      <td>1.527876</td>\n",
       "      <td>3.994117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>2.071827</td>\n",
       "      <td>0.814149</td>\n",
       "      <td>-0.552579</td>\n",
       "      <td>0.086078</td>\n",
       "      <td>0.689515</td>\n",
       "      <td>-0.628074</td>\n",
       "      <td>-0.142565</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-1.628579</td>\n",
       "      <td>0.507854</td>\n",
       "      <td>0.864922</td>\n",
       "      <td>-0.043672</td>\n",
       "      <td>0.592862</td>\n",
       "      <td>0.193557</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-0.386717</td>\n",
       "      <td>1.031398</td>\n",
       "      <td>2.884858</td>\n",
       "      <td>3.067978</td>\n",
       "      <td>-1.097673</td>\n",
       "      <td>-0.264005</td>\n",
       "      <td>0.709970</td>\n",
       "      <td>0.277237</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.635957</td>\n",
       "      <td>3.532201</td>\n",
       "      <td>0.533358</td>\n",
       "      <td>0.118974</td>\n",
       "      <td>-1.464170</td>\n",
       "      <td>-0.837489</td>\n",
       "      <td>-1.384483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508756</td>\n",
       "      <td>0.790594</td>\n",
       "      <td>0.558127</td>\n",
       "      <td>-1.046784</td>\n",
       "      <td>0.848789</td>\n",
       "      <td>0.114471</td>\n",
       "      <td>0.811991</td>\n",
       "      <td>-0.350606</td>\n",
       "      <td>-1.265492</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>621e329067b776a2402ffad2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.012816</td>\n",
       "      <td>-0.626681</td>\n",
       "      <td>-1.161575</td>\n",
       "      <td>-1.049619</td>\n",
       "      <td>-0.947486</td>\n",
       "      <td>-0.653602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250456</td>\n",
       "      <td>-0.141085</td>\n",
       "      <td>-0.545351</td>\n",
       "      <td>0.036532</td>\n",
       "      <td>0.050673</td>\n",
       "      <td>-0.409735</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.292155</td>\n",
       "      <td>1.033225</td>\n",
       "      <td>-0.071731</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>1.492975</td>\n",
       "      <td>-0.042013</td>\n",
       "      <td>-0.781227</td>\n",
       "      <td>-0.977820</td>\n",
       "      <td>-0.802796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108266</td>\n",
       "      <td>0.748233</td>\n",
       "      <td>0.097179</td>\n",
       "      <td>0.297324</td>\n",
       "      <td>1.257865</td>\n",
       "      <td>-0.129289</td>\n",
       "      <td>0.336330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.140194</td>\n",
       "      <td>-0.557786</td>\n",
       "      <td>-1.127802</td>\n",
       "      <td>0.673616</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>2.109814</td>\n",
       "      <td>2.779153</td>\n",
       "      <td>1.375503</td>\n",
       "      <td>4.128356</td>\n",
       "      <td>0.444591</td>\n",
       "      <td>0.686820</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>1.337825</td>\n",
       "      <td>1.719055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.185039</td>\n",
       "      <td>0.517702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.237353</td>\n",
       "      <td>5.244827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.219640</td>\n",
       "      <td>0.894663</td>\n",
       "      <td>1.179572</td>\n",
       "      <td>0.173270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.603152</td>\n",
       "      <td>-1.192111</td>\n",
       "      <td>0.247450</td>\n",
       "      <td>-0.081638</td>\n",
       "      <td>-0.304158</td>\n",
       "      <td>-0.573864</td>\n",
       "      <td>-0.538420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.337846</td>\n",
       "      <td>1.527876</td>\n",
       "      <td>4.113281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>1.466930</td>\n",
       "      <td>2.280024</td>\n",
       "      <td>0.929484</td>\n",
       "      <td>1.712249</td>\n",
       "      <td>0.339671</td>\n",
       "      <td>-0.008367</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>1.804765</td>\n",
       "      <td>0.705951</td>\n",
       "      <td>0.979918</td>\n",
       "      <td>-0.062094</td>\n",
       "      <td>-0.420835</td>\n",
       "      <td>-0.130220</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-0.491994</td>\n",
       "      <td>4.112211</td>\n",
       "      <td>-0.096809</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>2.564876</td>\n",
       "      <td>-0.198215</td>\n",
       "      <td>0.749180</td>\n",
       "      <td>1.150266</td>\n",
       "      <td>-0.733370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.287578</td>\n",
       "      <td>-0.920018</td>\n",
       "      <td>-0.382414</td>\n",
       "      <td>-0.081702</td>\n",
       "      <td>0.079838</td>\n",
       "      <td>-1.378372</td>\n",
       "      <td>-0.190891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.706988</td>\n",
       "      <td>0.910745</td>\n",
       "      <td>1.678622</td>\n",
       "      <td>2.443579</td>\n",
       "      <td>-0.141052</td>\n",
       "      <td>1.386371</td>\n",
       "      <td>0.811991</td>\n",
       "      <td>0.143205</td>\n",
       "      <td>0.651920</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.896021</td>\n",
       "      <td>-1.069687</td>\n",
       "      <td>-0.983412</td>\n",
       "      <td>-0.963200</td>\n",
       "      <td>-0.390587</td>\n",
       "      <td>-0.765960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.120099</td>\n",
       "      <td>-0.434681</td>\n",
       "      <td>-0.180938</td>\n",
       "      <td>-0.094757</td>\n",
       "      <td>-1.107933</td>\n",
       "      <td>-0.946648</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.081832</td>\n",
       "      <td>-1.384300</td>\n",
       "      <td>-1.182916</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-0.503795</td>\n",
       "      <td>-0.493907</td>\n",
       "      <td>-0.708691</td>\n",
       "      <td>-0.465705</td>\n",
       "      <td>-0.678469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.302348</td>\n",
       "      <td>-1.306499</td>\n",
       "      <td>-1.211251</td>\n",
       "      <td>0.425398</td>\n",
       "      <td>-1.833194</td>\n",
       "      <td>-0.245141</td>\n",
       "      <td>-1.896983</td>\n",
       "      <td>0.872578</td>\n",
       "      <td>-0.436849</td>\n",
       "      <td>-0.332949</td>\n",
       "      <td>-0.985771</td>\n",
       "      <td>-0.293174</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.300960</td>\n",
       "      <td>-0.420835</td>\n",
       "      <td>1.343617</td>\n",
       "      <td>1.498756</td>\n",
       "      <td>-0.046600</td>\n",
       "      <td>1.459988</td>\n",
       "      <td>0.657198</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>5.639971</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.371428</td>\n",
       "      <td>2.165232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.056818</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450789</td>\n",
       "      <td>0.434691</td>\n",
       "      <td>-0.501786</td>\n",
       "      <td>0.498741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.481761</td>\n",
       "      <td>1.455507</td>\n",
       "      <td>0.461657</td>\n",
       "      <td>-0.571465</td>\n",
       "      <td>1.578284</td>\n",
       "      <td>0.847642</td>\n",
       "      <td>1.497433</td>\n",
       "      <td>-0.244614</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.023699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.300960</td>\n",
       "      <td>-1.007736</td>\n",
       "      <td>0.318303</td>\n",
       "      <td>1.388773</td>\n",
       "      <td>-0.487276</td>\n",
       "      <td>1.406425</td>\n",
       "      <td>-0.339807</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>0.630992</td>\n",
       "      <td>-0.220882</td>\n",
       "      <td>-0.344872</td>\n",
       "      <td>-0.174015</td>\n",
       "      <td>0.073531</td>\n",
       "      <td>1.055631</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.002092</td>\n",
       "      <td>-0.905466</td>\n",
       "      <td>-1.031418</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>0.013863</td>\n",
       "      <td>0.451636</td>\n",
       "      <td>-0.931637</td>\n",
       "      <td>-0.570094</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.871031</td>\n",
       "      <td>-0.137639</td>\n",
       "      <td>0.241050</td>\n",
       "      <td>-0.604802</td>\n",
       "      <td>0.215451</td>\n",
       "      <td>0.637940</td>\n",
       "      <td>-0.314549</td>\n",
       "      <td>0.513146</td>\n",
       "      <td>-0.220476</td>\n",
       "      <td>-0.178494</td>\n",
       "      <td>-0.470660</td>\n",
       "      <td>-0.031358</td>\n",
       "      <td>-1.502084</td>\n",
       "      <td>-0.362492</td>\n",
       "      <td>1.834282</td>\n",
       "      <td>-1.461679</td>\n",
       "      <td>1.227144</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>621e32d067b776a2405b7d54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.308531</td>\n",
       "      <td>0.024163</td>\n",
       "      <td>-0.049233</td>\n",
       "      <td>0.582467</td>\n",
       "      <td>-0.545193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.336972</td>\n",
       "      <td>1.385325</td>\n",
       "      <td>1.178305</td>\n",
       "      <td>-0.030264</td>\n",
       "      <td>0.546824</td>\n",
       "      <td>1.135718</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>1.593481</td>\n",
       "      <td>-0.498286</td>\n",
       "      <td>-0.410795</td>\n",
       "      <td>-0.500810</td>\n",
       "      <td>-1.191879</td>\n",
       "      <td>-0.618398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.185745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.673956</td>\n",
       "      <td>-1.609613</td>\n",
       "      <td>0.736545</td>\n",
       "      <td>-0.057808</td>\n",
       "      <td>-2.108681</td>\n",
       "      <td>-0.566114</td>\n",
       "      <td>1.730253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.401880</td>\n",
       "      <td>1.964845</td>\n",
       "      <td>0.489686</td>\n",
       "      <td>-0.842167</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.300960</td>\n",
       "      <td>-0.158351</td>\n",
       "      <td>-1.312548</td>\n",
       "      <td>0.123963</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>-0.467929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.591583</td>\n",
       "      <td>-0.605614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.824672</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.269783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.258588</td>\n",
       "      <td>1.583547</td>\n",
       "      <td>-0.689704</td>\n",
       "      <td>0.285732</td>\n",
       "      <td>2.123446</td>\n",
       "      <td>1.591347</td>\n",
       "      <td>1.562779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.300960</td>\n",
       "      <td>-0.594976</td>\n",
       "      <td>-0.887904</td>\n",
       "      <td>-0.504654</td>\n",
       "      <td>-0.118254</td>\n",
       "      <td>-0.515440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.660387</td>\n",
       "      <td>-1.161440</td>\n",
       "      <td>-0.875018</td>\n",
       "      <td>-0.145832</td>\n",
       "      <td>0.403734</td>\n",
       "      <td>0.296851</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-0.196080</td>\n",
       "      <td>-0.656357</td>\n",
       "      <td>-0.563636</td>\n",
       "      <td>-0.450806</td>\n",
       "      <td>-0.907951</td>\n",
       "      <td>-0.025746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.506904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.966085</td>\n",
       "      <td>-1.070340</td>\n",
       "      <td>0.390664</td>\n",
       "      <td>0.202890</td>\n",
       "      <td>0.248450</td>\n",
       "      <td>-0.197964</td>\n",
       "      <td>2.046327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.161507</td>\n",
       "      <td>-1.276006</td>\n",
       "      <td>-0.231785</td>\n",
       "      <td>-0.885505</td>\n",
       "      <td>-1.130894</td>\n",
       "      <td>-0.362492</td>\n",
       "      <td>-0.940508</td>\n",
       "      <td>0.390110</td>\n",
       "      <td>-0.690268</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.385816</td>\n",
       "      <td>1.415291</td>\n",
       "      <td>2.533037</td>\n",
       "      <td>2.330936</td>\n",
       "      <td>-0.649378</td>\n",
       "      <td>1.309901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.784118</td>\n",
       "      <td>1.129863</td>\n",
       "      <td>1.218448</td>\n",
       "      <td>0.339921</td>\n",
       "      <td>1.355888</td>\n",
       "      <td>0.047519</td>\n",
       "      <td>0.815251</td>\n",
       "      <td>1.345101</td>\n",
       "      <td>0.270517</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.123301</td>\n",
       "      <td>-0.539492</td>\n",
       "      <td>-0.049513</td>\n",
       "      <td>-1.164961</td>\n",
       "      <td>-0.386421</td>\n",
       "      <td>-0.975099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.364606</td>\n",
       "      <td>-1.002015</td>\n",
       "      <td>1.329412</td>\n",
       "      <td>0.059147</td>\n",
       "      <td>-0.816554</td>\n",
       "      <td>-0.592410</td>\n",
       "      <td>-1.395064</td>\n",
       "      <td>-0.210728</td>\n",
       "      <td>1.143920</td>\n",
       "      <td>1.134364</td>\n",
       "      <td>2.289609</td>\n",
       "      <td>0.861868</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>2.071827</td>\n",
       "      <td>0.162529</td>\n",
       "      <td>0.537191</td>\n",
       "      <td>1.355326</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>-2.041381</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.736553</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.829126</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.134587</td>\n",
       "      <td>1.079152</td>\n",
       "      <td>0.901591</td>\n",
       "      <td>0.449344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.230650</td>\n",
       "      <td>1.052896</td>\n",
       "      <td>-0.850359</td>\n",
       "      <td>0.408189</td>\n",
       "      <td>0.259434</td>\n",
       "      <td>0.098536</td>\n",
       "      <td>1.800854</td>\n",
       "      <td>0.743252</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>2.071827</td>\n",
       "      <td>0.162560</td>\n",
       "      <td>0.788912</td>\n",
       "      <td>1.040163</td>\n",
       "      <td>0.868164</td>\n",
       "      <td>-2.178713</td>\n",
       "      <td>0.294972</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-1.523210</td>\n",
       "      <td>0.017086</td>\n",
       "      <td>-0.064078</td>\n",
       "      <td>-0.013624</td>\n",
       "      <td>-0.124439</td>\n",
       "      <td>-0.307722</td>\n",
       "      <td>1.025089</td>\n",
       "      <td>-0.797664</td>\n",
       "      <td>-0.286542</td>\n",
       "      <td>-0.367870</td>\n",
       "      <td>-0.295219</td>\n",
       "      <td>-0.853429</td>\n",
       "      <td>-0.506856</td>\n",
       "      <td>0.861873</td>\n",
       "      <td>0.621703</td>\n",
       "      <td>-0.783084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.452278</td>\n",
       "      <td>-0.869026</td>\n",
       "      <td>-0.161477</td>\n",
       "      <td>0.017164</td>\n",
       "      <td>-1.806553</td>\n",
       "      <td>-1.264956</td>\n",
       "      <td>-1.269381</td>\n",
       "      <td>0.255542</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.023177</td>\n",
       "      <td>0.729719</td>\n",
       "      <td>-0.377442</td>\n",
       "      <td>0.477598</td>\n",
       "      <td>-0.680467</td>\n",
       "      <td>0.811991</td>\n",
       "      <td>0.390110</td>\n",
       "      <td>1.802368</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>621e331067b776a24085dd3f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.322260</td>\n",
       "      <td>-0.454793</td>\n",
       "      <td>-0.368286</td>\n",
       "      <td>-1.674542</td>\n",
       "      <td>-0.274852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.141464</td>\n",
       "      <td>-1.291586</td>\n",
       "      <td>-1.310834</td>\n",
       "      <td>-0.791465</td>\n",
       "      <td>0.407717</td>\n",
       "      <td>0.347201</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.417256</td>\n",
       "      <td>0.909288</td>\n",
       "      <td>1.052041</td>\n",
       "      <td>0.274268</td>\n",
       "      <td>0.784100</td>\n",
       "      <td>-0.601416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.406274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.939354</td>\n",
       "      <td>0.690521</td>\n",
       "      <td>0.485681</td>\n",
       "      <td>-0.148975</td>\n",
       "      <td>0.545386</td>\n",
       "      <td>0.087144</td>\n",
       "      <td>1.289334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.302687</td>\n",
       "      <td>-1.306226</td>\n",
       "      <td>-1.720687</td>\n",
       "      <td>-0.618293</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>1.249784</td>\n",
       "      <td>0.470440</td>\n",
       "      <td>-0.557842</td>\n",
       "      <td>-0.420443</td>\n",
       "      <td>-0.753365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.777972</td>\n",
       "      <td>-0.680501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.553001</td>\n",
       "      <td>1.846658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.008025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.653400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.207368</td>\n",
       "      <td>-0.593580</td>\n",
       "      <td>-0.841433</td>\n",
       "      <td>0.408189</td>\n",
       "      <td>-0.815681</td>\n",
       "      <td>-1.677123</td>\n",
       "      <td>-0.434729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>-0.423543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>0.866683</td>\n",
       "      <td>-0.709605</td>\n",
       "      <td>-1.393132</td>\n",
       "      <td>-0.819742</td>\n",
       "      <td>-0.817838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>0.800039</td>\n",
       "      <td>-1.869306</td>\n",
       "      <td>-1.534995</td>\n",
       "      <td>-0.470889</td>\n",
       "      <td>-0.133882</td>\n",
       "      <td>-0.444435</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-0.316519</td>\n",
       "      <td>2.216507</td>\n",
       "      <td>1.579751</td>\n",
       "      <td>0.552507</td>\n",
       "      <td>0.147725</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.653485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.221294</td>\n",
       "      <td>1.049668</td>\n",
       "      <td>-0.193010</td>\n",
       "      <td>0.493736</td>\n",
       "      <td>-0.383293</td>\n",
       "      <td>-0.547894</td>\n",
       "      <td>0.234553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.869787</td>\n",
       "      <td>-2.093034</td>\n",
       "      <td>-0.782106</td>\n",
       "      <td>-0.905678</td>\n",
       "      <td>-1.378354</td>\n",
       "      <td>0.273459</td>\n",
       "      <td>1.104074</td>\n",
       "      <td>-0.597511</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>621e332267b776a24092a584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479059</td>\n",
       "      <td>1.158393</td>\n",
       "      <td>-0.524642</td>\n",
       "      <td>3.081953</td>\n",
       "      <td>-1.164830</td>\n",
       "      <td>-0.523357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.397166</td>\n",
       "      <td>0.177877</td>\n",
       "      <td>0.070191</td>\n",
       "      <td>2.143606</td>\n",
       "      <td>2.960497</td>\n",
       "      <td>0.743160</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.755687</td>\n",
       "      <td>1.018711</td>\n",
       "      <td>-1.111183</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>0.865683</td>\n",
       "      <td>-0.494223</td>\n",
       "      <td>-0.604940</td>\n",
       "      <td>-0.610876</td>\n",
       "      <td>3.174008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.012146</td>\n",
       "      <td>0.301031</td>\n",
       "      <td>2.926055</td>\n",
       "      <td>-0.525826</td>\n",
       "      <td>4.008535</td>\n",
       "      <td>0.475927</td>\n",
       "      <td>-0.008953</td>\n",
       "      <td>1.272368</td>\n",
       "      <td>0.182093</td>\n",
       "      <td>-0.350291</td>\n",
       "      <td>2.670027</td>\n",
       "      <td>1.038890</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>-0.480800</td>\n",
       "      <td>0.730436</td>\n",
       "      <td>1.542219</td>\n",
       "      <td>0.265346</td>\n",
       "      <td>1.203263</td>\n",
       "      <td>1.160774</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>1.337825</td>\n",
       "      <td>1.719055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>1.479408</td>\n",
       "      <td>2.090344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.161641</td>\n",
       "      <td>-0.330294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.745711</td>\n",
       "      <td>0.452651</td>\n",
       "      <td>-0.398172</td>\n",
       "      <td>-1.102324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.958130</td>\n",
       "      <td>0.595126</td>\n",
       "      <td>1.666568</td>\n",
       "      <td>0.408189</td>\n",
       "      <td>-0.302784</td>\n",
       "      <td>-1.171644</td>\n",
       "      <td>0.291836</td>\n",
       "      <td>0.018816</td>\n",
       "      <td>1.337846</td>\n",
       "      <td>1.527876</td>\n",
       "      <td>0.198835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>-0.347558</td>\n",
       "      <td>-0.124272</td>\n",
       "      <td>1.590085</td>\n",
       "      <td>2.907596</td>\n",
       "      <td>1.109920</td>\n",
       "      <td>1.276827</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-1.375143</td>\n",
       "      <td>1.696434</td>\n",
       "      <td>1.577397</td>\n",
       "      <td>1.780627</td>\n",
       "      <td>1.959966</td>\n",
       "      <td>1.746871</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.714197</td>\n",
       "      <td>0.312763</td>\n",
       "      <td>-0.982689</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-0.332414</td>\n",
       "      <td>0.378241</td>\n",
       "      <td>-0.592734</td>\n",
       "      <td>-0.611628</td>\n",
       "      <td>-0.050061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.131878</td>\n",
       "      <td>-1.558516</td>\n",
       "      <td>1.960515</td>\n",
       "      <td>0.096497</td>\n",
       "      <td>-0.666287</td>\n",
       "      <td>-3.474547</td>\n",
       "      <td>-0.828269</td>\n",
       "      <td>-0.367894</td>\n",
       "      <td>1.698147</td>\n",
       "      <td>1.511501</td>\n",
       "      <td>2.361856</td>\n",
       "      <td>-0.228325</td>\n",
       "      <td>-1.007164</td>\n",
       "      <td>-0.998442</td>\n",
       "      <td>-1.086549</td>\n",
       "      <td>0.143205</td>\n",
       "      <td>1.802368</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>621e333567b776a240a0c217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.684420</td>\n",
       "      <td>-0.878729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.301893</td>\n",
       "      <td>-1.000030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.934372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980165</td>\n",
       "      <td>-0.977530</td>\n",
       "      <td>-0.446614</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-2.080479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.877588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.139314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.261391</td>\n",
       "      <td>-1.513220</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>1.487246</td>\n",
       "      <td>-1.312484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.494837</td>\n",
       "      <td>1.152979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.246211</td>\n",
       "      <td>-0.053125</td>\n",
       "      <td>0.367927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.530733</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.403459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.693495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.100636</td>\n",
       "      <td>0.408189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.502506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>0.757655</td>\n",
       "      <td>-1.274799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.867097</td>\n",
       "      <td>1.019811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-1.681113</td>\n",
       "      <td>-0.680726</td>\n",
       "      <td>-0.935015</td>\n",
       "      <td>0.671454</td>\n",
       "      <td>-2.526820</td>\n",
       "      <td>-1.893587</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-1.241336</td>\n",
       "      <td>-0.966200</td>\n",
       "      <td>-0.488341</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-1.254260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.943380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.766421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.453794</td>\n",
       "      <td>-0.793614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.680634</td>\n",
       "      <td>-0.651220</td>\n",
       "      <td>-0.864473</td>\n",
       "      <td>-1.265210</td>\n",
       "      <td>-0.017322</td>\n",
       "      <td>0.432446</td>\n",
       "      <td>0.373866</td>\n",
       "      <td>-0.720963</td>\n",
       "      <td>-0.498527</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>621e333967b776a240a3cd06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.367642</td>\n",
       "      <td>0.460905</td>\n",
       "      <td>0.750849</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.748161</td>\n",
       "      <td>2.482310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484751</td>\n",
       "      <td>0.295615</td>\n",
       "      <td>-0.511713</td>\n",
       "      <td>-0.000465</td>\n",
       "      <td>0.239616</td>\n",
       "      <td>1.081749</td>\n",
       "      <td>7.001161</td>\n",
       "      <td>1.179058</td>\n",
       "      <td>0.332929</td>\n",
       "      <td>-0.635622</td>\n",
       "      <td>-0.519843</td>\n",
       "      <td>0.131992</td>\n",
       "      <td>0.121932</td>\n",
       "      <td>2.542443</td>\n",
       "      <td>0.913526</td>\n",
       "      <td>1.339021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083620</td>\n",
       "      <td>0.236016</td>\n",
       "      <td>0.182987</td>\n",
       "      <td>1.598573</td>\n",
       "      <td>0.871357</td>\n",
       "      <td>1.706525</td>\n",
       "      <td>-0.336533</td>\n",
       "      <td>0.669945</td>\n",
       "      <td>0.301059</td>\n",
       "      <td>-0.273698</td>\n",
       "      <td>0.140821</td>\n",
       "      <td>-0.290152</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>-0.529574</td>\n",
       "      <td>-1.312054</td>\n",
       "      <td>-0.894542</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>1.037752</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>1.337825</td>\n",
       "      <td>1.719055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.042770</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.606445</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.169988</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>0.364817</td>\n",
       "      <td>-1.102324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.249275</td>\n",
       "      <td>0.709104</td>\n",
       "      <td>0.265301</td>\n",
       "      <td>-0.571465</td>\n",
       "      <td>-1.851109</td>\n",
       "      <td>-0.185769</td>\n",
       "      <td>-0.965398</td>\n",
       "      <td>-2.878924</td>\n",
       "      <td>1.337846</td>\n",
       "      <td>1.527876</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>0.259695</td>\n",
       "      <td>0.127676</td>\n",
       "      <td>-0.154298</td>\n",
       "      <td>-0.746268</td>\n",
       "      <td>1.133302</td>\n",
       "      <td>1.323946</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>1.102144</td>\n",
       "      <td>1.164912</td>\n",
       "      <td>-0.093104</td>\n",
       "      <td>0.512735</td>\n",
       "      <td>0.139166</td>\n",
       "      <td>6.153625</td>\n",
       "      <td>0.133722</td>\n",
       "      <td>-0.453665</td>\n",
       "      <td>-0.781990</td>\n",
       "      <td>-0.491144</td>\n",
       "      <td>-0.056516</td>\n",
       "      <td>0.209916</td>\n",
       "      <td>-0.292246</td>\n",
       "      <td>0.476445</td>\n",
       "      <td>1.507500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.710890</td>\n",
       "      <td>0.497854</td>\n",
       "      <td>0.476048</td>\n",
       "      <td>-0.444414</td>\n",
       "      <td>0.663418</td>\n",
       "      <td>1.095024</td>\n",
       "      <td>-0.021653</td>\n",
       "      <td>-0.451353</td>\n",
       "      <td>1.103452</td>\n",
       "      <td>1.151047</td>\n",
       "      <td>-0.701290</td>\n",
       "      <td>-0.319468</td>\n",
       "      <td>-1.007164</td>\n",
       "      <td>-0.203504</td>\n",
       "      <td>0.811991</td>\n",
       "      <td>-0.720963</td>\n",
       "      <td>2.185850</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>621e335a67b776a240bb12ff</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.752524</td>\n",
       "      <td>-0.919100</td>\n",
       "      <td>-0.497482</td>\n",
       "      <td>-0.498602</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.060106</td>\n",
       "      <td>-1.291586</td>\n",
       "      <td>-1.310834</td>\n",
       "      <td>-0.012541</td>\n",
       "      <td>-0.427896</td>\n",
       "      <td>-0.102491</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.757299</td>\n",
       "      <td>0.239368</td>\n",
       "      <td>0.457021</td>\n",
       "      <td>-0.364658</td>\n",
       "      <td>0.141549</td>\n",
       "      <td>-0.245731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.231298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.322720</td>\n",
       "      <td>-0.375193</td>\n",
       "      <td>-0.540108</td>\n",
       "      <td>-0.663327</td>\n",
       "      <td>-0.719623</td>\n",
       "      <td>-0.335300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.302687</td>\n",
       "      <td>-1.306226</td>\n",
       "      <td>-0.381627</td>\n",
       "      <td>-0.006614</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-1.190755</td>\n",
       "      <td>0.336936</td>\n",
       "      <td>0.451423</td>\n",
       "      <td>-0.082986</td>\n",
       "      <td>0.147554</td>\n",
       "      <td>-0.197321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>1.337825</td>\n",
       "      <td>1.719055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.809037</td>\n",
       "      <td>-0.680501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.802962</td>\n",
       "      <td>-0.082511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.164844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.388963</td>\n",
       "      <td>1.442395</td>\n",
       "      <td>-0.894985</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>0.313601</td>\n",
       "      <td>0.054145</td>\n",
       "      <td>0.378210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.337846</td>\n",
       "      <td>1.527876</td>\n",
       "      <td>0.190221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-1.190755</td>\n",
       "      <td>-0.416242</td>\n",
       "      <td>-0.918880</td>\n",
       "      <td>-0.773742</td>\n",
       "      <td>-0.323088</td>\n",
       "      <td>-0.050984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>1.051501</td>\n",
       "      <td>0.507854</td>\n",
       "      <td>0.864922</td>\n",
       "      <td>-0.194480</td>\n",
       "      <td>0.490597</td>\n",
       "      <td>0.405787</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.875077</td>\n",
       "      <td>-0.161270</td>\n",
       "      <td>-0.024436</td>\n",
       "      <td>-0.279738</td>\n",
       "      <td>-0.112741</td>\n",
       "      <td>0.729348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.982732</td>\n",
       "      <td>0.504440</td>\n",
       "      <td>0.483255</td>\n",
       "      <td>-0.101306</td>\n",
       "      <td>0.011822</td>\n",
       "      <td>-0.125237</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508756</td>\n",
       "      <td>0.790594</td>\n",
       "      <td>-0.217781</td>\n",
       "      <td>-0.449087</td>\n",
       "      <td>-1.130894</td>\n",
       "      <td>-1.475404</td>\n",
       "      <td>0.811991</td>\n",
       "      <td>0.143205</td>\n",
       "      <td>-1.648975</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>621e337667b776a240ce78ab</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.535935</td>\n",
       "      <td>-0.313637</td>\n",
       "      <td>-0.182907</td>\n",
       "      <td>-0.444693</td>\n",
       "      <td>-0.828852</td>\n",
       "      <td>-0.626244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.194533</td>\n",
       "      <td>0.152782</td>\n",
       "      <td>0.449695</td>\n",
       "      <td>-0.521301</td>\n",
       "      <td>-0.849276</td>\n",
       "      <td>0.135358</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.739139</td>\n",
       "      <td>0.324277</td>\n",
       "      <td>-0.858536</td>\n",
       "      <td>-0.470606</td>\n",
       "      <td>1.709235</td>\n",
       "      <td>-0.401200</td>\n",
       "      <td>-0.614794</td>\n",
       "      <td>-0.781092</td>\n",
       "      <td>-0.559535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.033250</td>\n",
       "      <td>-0.700989</td>\n",
       "      <td>-0.834191</td>\n",
       "      <td>-0.911859</td>\n",
       "      <td>-1.188731</td>\n",
       "      <td>-0.454709</td>\n",
       "      <td>-0.626287</td>\n",
       "      <td>0.888140</td>\n",
       "      <td>0.156737</td>\n",
       "      <td>0.285224</td>\n",
       "      <td>-0.609360</td>\n",
       "      <td>-0.220979</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.321043</td>\n",
       "      <td>0.329140</td>\n",
       "      <td>-0.091079</td>\n",
       "      <td>1.314975</td>\n",
       "      <td>0.183244</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>2.017866</td>\n",
       "      <td>1.715906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738548</td>\n",
       "      <td>-0.347993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446906</td>\n",
       "      <td>0.388599</td>\n",
       "      <td>-0.426231</td>\n",
       "      <td>0.692901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.235306</td>\n",
       "      <td>-0.013377</td>\n",
       "      <td>1.961102</td>\n",
       "      <td>0.408189</td>\n",
       "      <td>0.826140</td>\n",
       "      <td>0.163592</td>\n",
       "      <td>-0.246186</td>\n",
       "      <td>-1.495912</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.104816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.597559</td>\n",
       "      <td>-0.713596</td>\n",
       "      <td>0.777877</td>\n",
       "      <td>0.633006</td>\n",
       "      <td>-0.307783</td>\n",
       "      <td>1.272277</td>\n",
       "      <td>-1.613401</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.153942</td>\n",
       "      <td>-0.107660</td>\n",
       "      <td>-0.195755</td>\n",
       "      <td>-0.345170</td>\n",
       "      <td>1.032722</td>\n",
       "      <td>1.400850</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.070215</td>\n",
       "      <td>-0.443231</td>\n",
       "      <td>-0.893692</td>\n",
       "      <td>-0.457111</td>\n",
       "      <td>0.706010</td>\n",
       "      <td>0.186954</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-0.634745</td>\n",
       "      <td>0.225429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.554783</td>\n",
       "      <td>-0.060665</td>\n",
       "      <td>1.114266</td>\n",
       "      <td>0.152745</td>\n",
       "      <td>0.091260</td>\n",
       "      <td>-0.152083</td>\n",
       "      <td>-0.526848</td>\n",
       "      <td>-1.758233</td>\n",
       "      <td>-0.107178</td>\n",
       "      <td>-0.046173</td>\n",
       "      <td>-0.427256</td>\n",
       "      <td>-0.253755</td>\n",
       "      <td>-1.007164</td>\n",
       "      <td>-1.316417</td>\n",
       "      <td>-1.378633</td>\n",
       "      <td>-0.720963</td>\n",
       "      <td>1.802368</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>621e339967b776a240e502de</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124999</td>\n",
       "      <td>-0.682633</td>\n",
       "      <td>-0.204748</td>\n",
       "      <td>0.956394</td>\n",
       "      <td>1.342530</td>\n",
       "      <td>-0.161107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.879651</td>\n",
       "      <td>0.871533</td>\n",
       "      <td>0.759631</td>\n",
       "      <td>-0.271345</td>\n",
       "      <td>-0.451999</td>\n",
       "      <td>-0.117306</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.109334</td>\n",
       "      <td>-1.609975</td>\n",
       "      <td>0.210012</td>\n",
       "      <td>0.221605</td>\n",
       "      <td>0.436852</td>\n",
       "      <td>-0.464074</td>\n",
       "      <td>0.226617</td>\n",
       "      <td>0.125599</td>\n",
       "      <td>-0.685907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.346470</td>\n",
       "      <td>-0.548468</td>\n",
       "      <td>-1.234428</td>\n",
       "      <td>-0.395797</td>\n",
       "      <td>-0.349346</td>\n",
       "      <td>-0.602358</td>\n",
       "      <td>-0.776549</td>\n",
       "      <td>0.227184</td>\n",
       "      <td>0.882980</td>\n",
       "      <td>0.730266</td>\n",
       "      <td>1.048951</td>\n",
       "      <td>1.044961</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-0.875858</td>\n",
       "      <td>-0.031777</td>\n",
       "      <td>-0.087859</td>\n",
       "      <td>-0.379112</td>\n",
       "      <td>0.526971</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-1.513174</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.138815</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920213</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>-2.015558</td>\n",
       "      <td>1.056432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.510027</td>\n",
       "      <td>-1.966679</td>\n",
       "      <td>1.291707</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>-3.626548</td>\n",
       "      <td>-1.677123</td>\n",
       "      <td>-3.076782</td>\n",
       "      <td>-0.705619</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.368268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>-1.394380</td>\n",
       "      <td>-0.760116</td>\n",
       "      <td>-0.485134</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>0.566232</td>\n",
       "      <td>-1.429211</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>0.251195</td>\n",
       "      <td>1.068185</td>\n",
       "      <td>0.929206</td>\n",
       "      <td>-0.401234</td>\n",
       "      <td>0.653323</td>\n",
       "      <td>1.221504</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.240389</td>\n",
       "      <td>-0.958515</td>\n",
       "      <td>0.965921</td>\n",
       "      <td>0.514822</td>\n",
       "      <td>0.836006</td>\n",
       "      <td>0.698707</td>\n",
       "      <td>-1.602164</td>\n",
       "      <td>-1.587374</td>\n",
       "      <td>0.458487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.214316</td>\n",
       "      <td>-0.226999</td>\n",
       "      <td>0.857610</td>\n",
       "      <td>-0.183349</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>0.618765</td>\n",
       "      <td>-0.444048</td>\n",
       "      <td>-0.192631</td>\n",
       "      <td>1.069469</td>\n",
       "      <td>0.996567</td>\n",
       "      <td>0.913830</td>\n",
       "      <td>2.483734</td>\n",
       "      <td>0.601329</td>\n",
       "      <td>1.068396</td>\n",
       "      <td>-0.648425</td>\n",
       "      <td>1.748090</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>621e33b067b776a240f39e56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.945380</td>\n",
       "      <td>-0.180763</td>\n",
       "      <td>-0.944076</td>\n",
       "      <td>-0.453674</td>\n",
       "      <td>-0.550481</td>\n",
       "      <td>-0.770278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.982392</td>\n",
       "      <td>0.496522</td>\n",
       "      <td>0.129256</td>\n",
       "      <td>-0.715864</td>\n",
       "      <td>-0.533480</td>\n",
       "      <td>-0.588613</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.140335</td>\n",
       "      <td>1.441223</td>\n",
       "      <td>-0.732335</td>\n",
       "      <td>-0.301028</td>\n",
       "      <td>0.765151</td>\n",
       "      <td>-0.655983</td>\n",
       "      <td>-0.442477</td>\n",
       "      <td>0.030436</td>\n",
       "      <td>-0.014290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.312898</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>-0.456245</td>\n",
       "      <td>-0.484487</td>\n",
       "      <td>0.178148</td>\n",
       "      <td>-0.820419</td>\n",
       "      <td>-0.412416</td>\n",
       "      <td>0.273092</td>\n",
       "      <td>0.504060</td>\n",
       "      <td>-0.019720</td>\n",
       "      <td>-0.621270</td>\n",
       "      <td>-0.842285</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.300960</td>\n",
       "      <td>0.292688</td>\n",
       "      <td>0.817494</td>\n",
       "      <td>0.696510</td>\n",
       "      <td>0.478984</td>\n",
       "      <td>0.785457</td>\n",
       "      <td>0.568331</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.094545</td>\n",
       "      <td>-0.380951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.540199</td>\n",
       "      <td>0.784730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558197</td>\n",
       "      <td>0.456108</td>\n",
       "      <td>-0.110392</td>\n",
       "      <td>1.700582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.347057</td>\n",
       "      <td>0.885142</td>\n",
       "      <td>-0.180963</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>0.242181</td>\n",
       "      <td>0.427555</td>\n",
       "      <td>-0.617296</td>\n",
       "      <td>-0.837334</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>0.433572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>-0.300960</td>\n",
       "      <td>-0.376101</td>\n",
       "      <td>1.397114</td>\n",
       "      <td>0.693469</td>\n",
       "      <td>0.404720</td>\n",
       "      <td>0.763027</td>\n",
       "      <td>-0.389072</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>1.862948</td>\n",
       "      <td>1.201517</td>\n",
       "      <td>1.243597</td>\n",
       "      <td>-0.470889</td>\n",
       "      <td>-0.655649</td>\n",
       "      <td>-0.378979</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.096382</td>\n",
       "      <td>1.831109</td>\n",
       "      <td>-0.460961</td>\n",
       "      <td>-0.293124</td>\n",
       "      <td>0.761443</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>-0.362803</td>\n",
       "      <td>-0.077321</td>\n",
       "      <td>1.282015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.938950</td>\n",
       "      <td>-0.310006</td>\n",
       "      <td>-0.624440</td>\n",
       "      <td>0.244848</td>\n",
       "      <td>-0.538777</td>\n",
       "      <td>0.163687</td>\n",
       "      <td>-0.848599</td>\n",
       "      <td>-1.251252</td>\n",
       "      <td>1.202892</td>\n",
       "      <td>1.192411</td>\n",
       "      <td>0.215846</td>\n",
       "      <td>-0.635227</td>\n",
       "      <td>1.838630</td>\n",
       "      <td>1.704346</td>\n",
       "      <td>1.104074</td>\n",
       "      <td>0.760468</td>\n",
       "      <td>1.610626</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>621e33cf67b776a240087de9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.655906</td>\n",
       "      <td>0.401158</td>\n",
       "      <td>-0.663778</td>\n",
       "      <td>-0.134703</td>\n",
       "      <td>-0.618166</td>\n",
       "      <td>0.399851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.071882</td>\n",
       "      <td>1.565750</td>\n",
       "      <td>1.551273</td>\n",
       "      <td>-0.158289</td>\n",
       "      <td>-0.885611</td>\n",
       "      <td>0.459538</td>\n",
       "      <td>3.695862</td>\n",
       "      <td>-0.728416</td>\n",
       "      <td>1.370430</td>\n",
       "      <td>-0.432135</td>\n",
       "      <td>-0.121224</td>\n",
       "      <td>1.106097</td>\n",
       "      <td>-0.384225</td>\n",
       "      <td>0.741384</td>\n",
       "      <td>0.582436</td>\n",
       "      <td>-0.041486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.538390</td>\n",
       "      <td>1.169995</td>\n",
       "      <td>-0.820252</td>\n",
       "      <td>0.805081</td>\n",
       "      <td>-0.675846</td>\n",
       "      <td>4.246794</td>\n",
       "      <td>-0.437529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.584278</td>\n",
       "      <td>1.515918</td>\n",
       "      <td>-0.165638</td>\n",
       "      <td>0.598224</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>0.122592</td>\n",
       "      <td>-0.283761</td>\n",
       "      <td>-0.686331</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>-0.173271</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>1.116985</td>\n",
       "      <td>-0.306063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.468383</td>\n",
       "      <td>-0.171005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.910330</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>0.031584</td>\n",
       "      <td>-0.327238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044071</td>\n",
       "      <td>-0.255586</td>\n",
       "      <td>1.113201</td>\n",
       "      <td>-0.816378</td>\n",
       "      <td>0.100702</td>\n",
       "      <td>-0.902979</td>\n",
       "      <td>-0.500177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.012214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>-0.286817</td>\n",
       "      <td>1.774330</td>\n",
       "      <td>-1.316293</td>\n",
       "      <td>0.453583</td>\n",
       "      <td>-0.179935</td>\n",
       "      <td>-1.173981</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>1.248050</td>\n",
       "      <td>0.172255</td>\n",
       "      <td>0.035539</td>\n",
       "      <td>-0.227561</td>\n",
       "      <td>0.217169</td>\n",
       "      <td>0.298187</td>\n",
       "      <td>5.249027</td>\n",
       "      <td>0.483383</td>\n",
       "      <td>1.195900</td>\n",
       "      <td>-0.405419</td>\n",
       "      <td>-0.222477</td>\n",
       "      <td>1.173574</td>\n",
       "      <td>0.667067</td>\n",
       "      <td>0.350132</td>\n",
       "      <td>0.045858</td>\n",
       "      <td>0.043904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.220136</td>\n",
       "      <td>-0.734090</td>\n",
       "      <td>0.254432</td>\n",
       "      <td>-0.217110</td>\n",
       "      <td>0.852261</td>\n",
       "      <td>2.639083</td>\n",
       "      <td>0.059235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.172980</td>\n",
       "      <td>0.140364</td>\n",
       "      <td>0.371806</td>\n",
       "      <td>0.880667</td>\n",
       "      <td>-1.749544</td>\n",
       "      <td>-1.157429</td>\n",
       "      <td>1.104074</td>\n",
       "      <td>0.883921</td>\n",
       "      <td>-1.648975</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>621e33ed67b776a2401cf5f7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.045176</td>\n",
       "      <td>0.127815</td>\n",
       "      <td>-0.976718</td>\n",
       "      <td>-0.174241</td>\n",
       "      <td>-0.611985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.194012</td>\n",
       "      <td>-0.486728</td>\n",
       "      <td>-0.635454</td>\n",
       "      <td>-0.682446</td>\n",
       "      <td>-0.073912</td>\n",
       "      <td>-0.322377</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.325107</td>\n",
       "      <td>-1.385639</td>\n",
       "      <td>-0.619051</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-0.343477</td>\n",
       "      <td>-0.848911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.177622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693784</td>\n",
       "      <td>-0.276639</td>\n",
       "      <td>-0.052394</td>\n",
       "      <td>-0.839107</td>\n",
       "      <td>0.969029</td>\n",
       "      <td>1.000406</td>\n",
       "      <td>-0.432717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.489710</td>\n",
       "      <td>-0.433581</td>\n",
       "      <td>-0.295430</td>\n",
       "      <td>0.472391</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>-0.270230</td>\n",
       "      <td>0.583730</td>\n",
       "      <td>1.210250</td>\n",
       "      <td>-0.034201</td>\n",
       "      <td>0.396336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.247168</td>\n",
       "      <td>0.667477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.315128</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.751488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.203914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.332760</td>\n",
       "      <td>-1.016072</td>\n",
       "      <td>0.176048</td>\n",
       "      <td>0.530646</td>\n",
       "      <td>-0.316585</td>\n",
       "      <td>-0.757126</td>\n",
       "      <td>-0.982847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.044517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>-1.471852</td>\n",
       "      <td>1.138894</td>\n",
       "      <td>1.098978</td>\n",
       "      <td>-0.646217</td>\n",
       "      <td>0.550587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.267447</td>\n",
       "      <td>-1.190117</td>\n",
       "      <td>-1.363572</td>\n",
       "      <td>-0.445723</td>\n",
       "      <td>0.302284</td>\n",
       "      <td>0.874179</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.145286</td>\n",
       "      <td>-1.110074</td>\n",
       "      <td>-0.497753</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-0.344677</td>\n",
       "      <td>0.342797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.562551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.221449</td>\n",
       "      <td>-0.834207</td>\n",
       "      <td>0.403917</td>\n",
       "      <td>0.144352</td>\n",
       "      <td>0.959947</td>\n",
       "      <td>-0.135397</td>\n",
       "      <td>0.395514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.190203</td>\n",
       "      <td>-1.166154</td>\n",
       "      <td>-0.733336</td>\n",
       "      <td>0.123717</td>\n",
       "      <td>1.096249</td>\n",
       "      <td>1.545359</td>\n",
       "      <td>0.227825</td>\n",
       "      <td>1.501184</td>\n",
       "      <td>1.418885</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>621e341067b776a24037b105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.778303</td>\n",
       "      <td>-0.250491</td>\n",
       "      <td>-0.901297</td>\n",
       "      <td>-0.899363</td>\n",
       "      <td>-0.679218</td>\n",
       "      <td>1.545074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545608</td>\n",
       "      <td>-0.960031</td>\n",
       "      <td>-1.310834</td>\n",
       "      <td>-0.514120</td>\n",
       "      <td>-0.265497</td>\n",
       "      <td>-0.501155</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.078140</td>\n",
       "      <td>0.642799</td>\n",
       "      <td>-1.127763</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-0.679866</td>\n",
       "      <td>-0.429433</td>\n",
       "      <td>-0.342968</td>\n",
       "      <td>-0.513340</td>\n",
       "      <td>-0.718257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.562384</td>\n",
       "      <td>-0.818981</td>\n",
       "      <td>-0.237961</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.401512</td>\n",
       "      <td>-0.127584</td>\n",
       "      <td>-0.851287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.967675</td>\n",
       "      <td>-0.767006</td>\n",
       "      <td>-0.835343</td>\n",
       "      <td>-0.774393</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>-0.300030</td>\n",
       "      <td>0.204424</td>\n",
       "      <td>1.055416</td>\n",
       "      <td>-0.416507</td>\n",
       "      <td>0.165212</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.102199</td>\n",
       "      <td>-0.306063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.370403</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372380</td>\n",
       "      <td>0.738266</td>\n",
       "      <td>0.515734</td>\n",
       "      <td>0.155642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.514355</td>\n",
       "      <td>0.139281</td>\n",
       "      <td>-0.002458</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>0.355786</td>\n",
       "      <td>-0.297127</td>\n",
       "      <td>0.548128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>-0.417800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>0.105443</td>\n",
       "      <td>0.318246</td>\n",
       "      <td>1.393823</td>\n",
       "      <td>-0.389895</td>\n",
       "      <td>0.096514</td>\n",
       "      <td>-2.004461</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>0.081123</td>\n",
       "      <td>-1.826084</td>\n",
       "      <td>-1.534995</td>\n",
       "      <td>-0.350154</td>\n",
       "      <td>-0.245460</td>\n",
       "      <td>-0.097185</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.601148</td>\n",
       "      <td>0.408529</td>\n",
       "      <td>-1.000344</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-0.504993</td>\n",
       "      <td>-0.163010</td>\n",
       "      <td>0.273683</td>\n",
       "      <td>0.536503</td>\n",
       "      <td>-0.470616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103743</td>\n",
       "      <td>-0.125368</td>\n",
       "      <td>-0.227778</td>\n",
       "      <td>-0.161096</td>\n",
       "      <td>-0.842587</td>\n",
       "      <td>-0.428792</td>\n",
       "      <td>-0.212055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.826536</td>\n",
       "      <td>-2.027497</td>\n",
       "      <td>-0.298796</td>\n",
       "      <td>-0.686965</td>\n",
       "      <td>-1.378354</td>\n",
       "      <td>-1.316417</td>\n",
       "      <td>-1.232591</td>\n",
       "      <td>-1.091321</td>\n",
       "      <td>-1.265492</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>621e346f67b776a24081744f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.755827</td>\n",
       "      <td>-1.062678</td>\n",
       "      <td>0.247413</td>\n",
       "      <td>-0.349450</td>\n",
       "      <td>0.637723</td>\n",
       "      <td>1.360858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.109054</td>\n",
       "      <td>-0.712022</td>\n",
       "      <td>-0.216593</td>\n",
       "      <td>0.084306</td>\n",
       "      <td>-0.424474</td>\n",
       "      <td>-0.051035</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.062853</td>\n",
       "      <td>0.293137</td>\n",
       "      <td>-1.034400</td>\n",
       "      <td>-0.461512</td>\n",
       "      <td>0.331506</td>\n",
       "      <td>-0.346434</td>\n",
       "      <td>-0.252231</td>\n",
       "      <td>1.288236</td>\n",
       "      <td>-0.499274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.675627</td>\n",
       "      <td>-0.098637</td>\n",
       "      <td>-0.360327</td>\n",
       "      <td>0.413465</td>\n",
       "      <td>-0.245961</td>\n",
       "      <td>-0.215408</td>\n",
       "      <td>-0.189911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.717082</td>\n",
       "      <td>-0.363661</td>\n",
       "      <td>-0.136895</td>\n",
       "      <td>-0.731503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.576415</td>\n",
       "      <td>-1.312787</td>\n",
       "      <td>-0.638072</td>\n",
       "      <td>-0.525442</td>\n",
       "      <td>-1.377621</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.829368</td>\n",
       "      <td>0.774349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.050424</td>\n",
       "      <td>-0.755389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.080918</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374666</td>\n",
       "      <td>0.849842</td>\n",
       "      <td>0.637868</td>\n",
       "      <td>-1.102324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290854</td>\n",
       "      <td>-1.596313</td>\n",
       "      <td>-0.163113</td>\n",
       "      <td>-1.428662</td>\n",
       "      <td>-0.412578</td>\n",
       "      <td>-0.405571</td>\n",
       "      <td>-1.426636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829338</td>\n",
       "      <td>0.821812</td>\n",
       "      <td>-0.534092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201902</td>\n",
       "      <td>-1.014373</td>\n",
       "      <td>-0.545461</td>\n",
       "      <td>-1.009325</td>\n",
       "      <td>-1.299449</td>\n",
       "      <td>1.535304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.172697</td>\n",
       "      <td>0.296551</td>\n",
       "      <td>0.464936</td>\n",
       "      <td>-0.005593</td>\n",
       "      <td>0.744410</td>\n",
       "      <td>0.744218</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.631175</td>\n",
       "      <td>-0.151445</td>\n",
       "      <td>-1.001956</td>\n",
       "      <td>-0.470797</td>\n",
       "      <td>-0.325400</td>\n",
       "      <td>0.293057</td>\n",
       "      <td>1.502162</td>\n",
       "      <td>1.230170</td>\n",
       "      <td>-0.792438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.883786</td>\n",
       "      <td>-0.085445</td>\n",
       "      <td>0.756678</td>\n",
       "      <td>0.109248</td>\n",
       "      <td>-0.243283</td>\n",
       "      <td>-0.131516</td>\n",
       "      <td>-0.102505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.297309</td>\n",
       "      <td>0.470191</td>\n",
       "      <td>-0.900953</td>\n",
       "      <td>-0.731667</td>\n",
       "      <td>0.601329</td>\n",
       "      <td>0.909409</td>\n",
       "      <td>-1.816757</td>\n",
       "      <td>0.390110</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>621e34db67b776a240c9c2be</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.157588</td>\n",
       "      <td>-0.594750</td>\n",
       "      <td>-1.204493</td>\n",
       "      <td>-0.975233</td>\n",
       "      <td>-0.704773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.627774</td>\n",
       "      <td>-0.816368</td>\n",
       "      <td>-0.712679</td>\n",
       "      <td>0.422669</td>\n",
       "      <td>-1.257328</td>\n",
       "      <td>-1.006851</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.852166</td>\n",
       "      <td>-0.670798</td>\n",
       "      <td>-0.729849</td>\n",
       "      <td>-0.376785</td>\n",
       "      <td>-1.466894</td>\n",
       "      <td>-0.638878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.060022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.169874</td>\n",
       "      <td>-0.473614</td>\n",
       "      <td>-1.285053</td>\n",
       "      <td>-0.668939</td>\n",
       "      <td>-0.397803</td>\n",
       "      <td>2.999144</td>\n",
       "      <td>-1.652563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.822515</td>\n",
       "      <td>-0.790983</td>\n",
       "      <td>-1.097443</td>\n",
       "      <td>-1.218581</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.433167</td>\n",
       "      <td>0.972641</td>\n",
       "      <td>1.015186</td>\n",
       "      <td>0.574143</td>\n",
       "      <td>-1.088094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>1.531183</td>\n",
       "      <td>1.790793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.949932</td>\n",
       "      <td>0.448453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.135880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.747169</td>\n",
       "      <td>-1.016072</td>\n",
       "      <td>1.604092</td>\n",
       "      <td>0.530646</td>\n",
       "      <td>1.882227</td>\n",
       "      <td>-0.975258</td>\n",
       "      <td>1.365351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-0.555982</td>\n",
       "      <td>-1.118583</td>\n",
       "      <td>0.297207</td>\n",
       "      <td>-0.220803</td>\n",
       "      <td>-1.166283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-1.109626</td>\n",
       "      <td>-0.029591</td>\n",
       "      <td>-0.113305</td>\n",
       "      <td>0.129216</td>\n",
       "      <td>-0.044182</td>\n",
       "      <td>0.068763</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.712614</td>\n",
       "      <td>-0.021386</td>\n",
       "      <td>-0.657754</td>\n",
       "      <td>-0.398944</td>\n",
       "      <td>-1.097686</td>\n",
       "      <td>0.505767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.236892</td>\n",
       "      <td>-1.233373</td>\n",
       "      <td>-0.028610</td>\n",
       "      <td>0.179491</td>\n",
       "      <td>1.187734</td>\n",
       "      <td>0.910786</td>\n",
       "      <td>-0.420554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.029055</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>-0.290538</td>\n",
       "      <td>-1.158649</td>\n",
       "      <td>-0.264783</td>\n",
       "      <td>1.068396</td>\n",
       "      <td>0.227825</td>\n",
       "      <td>0.760468</td>\n",
       "      <td>0.843661</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>621e34ec67b776a240d60873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.069279</td>\n",
       "      <td>-0.207067</td>\n",
       "      <td>2.015515</td>\n",
       "      <td>1.114210</td>\n",
       "      <td>-1.180209</td>\n",
       "      <td>-0.706147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.094596</td>\n",
       "      <td>1.882817</td>\n",
       "      <td>1.885649</td>\n",
       "      <td>0.120384</td>\n",
       "      <td>-0.967763</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>1.526668</td>\n",
       "      <td>0.682149</td>\n",
       "      <td>-0.023688</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-0.834294</td>\n",
       "      <td>3.711812</td>\n",
       "      <td>-0.544497</td>\n",
       "      <td>-0.949037</td>\n",
       "      <td>-1.110345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.706322</td>\n",
       "      <td>2.406481</td>\n",
       "      <td>-0.915685</td>\n",
       "      <td>0.606128</td>\n",
       "      <td>0.021528</td>\n",
       "      <td>0.313615</td>\n",
       "      <td>-0.306073</td>\n",
       "      <td>-0.739442</td>\n",
       "      <td>1.904484</td>\n",
       "      <td>2.823885</td>\n",
       "      <td>1.263980</td>\n",
       "      <td>0.056770</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-0.662581</td>\n",
       "      <td>-0.236776</td>\n",
       "      <td>-1.187044</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>0.811672</td>\n",
       "      <td>1.190396</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.827046</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.744507</td>\n",
       "      <td>-0.347993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.221235</td>\n",
       "      <td>0.737869</td>\n",
       "      <td>0.528496</td>\n",
       "      <td>0.559474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.021117</td>\n",
       "      <td>-0.954743</td>\n",
       "      <td>0.782966</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>1.469093</td>\n",
       "      <td>0.734593</td>\n",
       "      <td>-0.036389</td>\n",
       "      <td>0.413963</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.979221</td>\n",
       "      <td>-0.997316</td>\n",
       "      <td>0.238905</td>\n",
       "      <td>0.226982</td>\n",
       "      <td>0.684567</td>\n",
       "      <td>1.184226</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-1.631789</td>\n",
       "      <td>-0.284532</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.119786</td>\n",
       "      <td>-0.397748</td>\n",
       "      <td>-0.575679</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-0.979361</td>\n",
       "      <td>0.290201</td>\n",
       "      <td>-0.148186</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-0.991347</td>\n",
       "      <td>-0.378121</td>\n",
       "      <td>0.047149</td>\n",
       "      <td>0.188767</td>\n",
       "      <td>-0.509628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.567793</td>\n",
       "      <td>-1.685313</td>\n",
       "      <td>-0.439466</td>\n",
       "      <td>-0.095012</td>\n",
       "      <td>-0.278367</td>\n",
       "      <td>-1.753335</td>\n",
       "      <td>-1.547837</td>\n",
       "      <td>-0.338162</td>\n",
       "      <td>-0.284091</td>\n",
       "      <td>-0.170616</td>\n",
       "      <td>0.260377</td>\n",
       "      <td>-0.797981</td>\n",
       "      <td>0.106408</td>\n",
       "      <td>-1.475404</td>\n",
       "      <td>-1.232591</td>\n",
       "      <td>0.019752</td>\n",
       "      <td>-1.265492</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>621e34f767b776a240de4e1a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.259151</td>\n",
       "      <td>-0.974788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.421054</td>\n",
       "      <td>-0.895489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.537298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.007735</td>\n",
       "      <td>3.040333</td>\n",
       "      <td>-1.452385</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.952054</td>\n",
       "      <td>0.227105</td>\n",
       "      <td>0.803550</td>\n",
       "      <td>0.165220</td>\n",
       "      <td>-0.789669</td>\n",
       "      <td>-0.570337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.689706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.300651</td>\n",
       "      <td>1.417464</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.276260</td>\n",
       "      <td>-0.938321</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>-0.821914</td>\n",
       "      <td>0.094860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>-1.356563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.340363</td>\n",
       "      <td>-0.755389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.815765</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.436018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.819886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.316336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.184973</td>\n",
       "      <td>0.898016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>1.294030</td>\n",
       "      <td>-1.293411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.452172</td>\n",
       "      <td>-1.466149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.775672</td>\n",
       "      <td>-1.393874</td>\n",
       "      <td>-1.534995</td>\n",
       "      <td>-0.554475</td>\n",
       "      <td>-0.558537</td>\n",
       "      <td>-2.413095</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-2.471864</td>\n",
       "      <td>-0.726373</td>\n",
       "      <td>0.415195</td>\n",
       "      <td>-0.070482</td>\n",
       "      <td>-0.551781</td>\n",
       "      <td>-4.830918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.470570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.349588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.892008</td>\n",
       "      <td>0.528065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.394030</td>\n",
       "      <td>-1.372127</td>\n",
       "      <td>-1.412987</td>\n",
       "      <td>-0.826975</td>\n",
       "      <td>1.219979</td>\n",
       "      <td>0.591434</td>\n",
       "      <td>1.396158</td>\n",
       "      <td>0.266658</td>\n",
       "      <td>0.076696</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>621e356967b776a24027bd9f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.265985</td>\n",
       "      <td>-0.638434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.371363</td>\n",
       "      <td>0.028013</td>\n",
       "      <td>-0.784135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.962727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.674830</td>\n",
       "      <td>-1.132796</td>\n",
       "      <td>-0.414332</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-1.734308</td>\n",
       "      <td>-0.557501</td>\n",
       "      <td>-0.455013</td>\n",
       "      <td>2.044084</td>\n",
       "      <td>-0.313147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.326610</td>\n",
       "      <td>1.235822</td>\n",
       "      <td>1.601376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.321499</td>\n",
       "      <td>1.196380</td>\n",
       "      <td>-1.067418</td>\n",
       "      <td>-0.401473</td>\n",
       "      <td>-1.335114</td>\n",
       "      <td>-1.125916</td>\n",
       "      <td>-0.139678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.799305</td>\n",
       "      <td>0.339733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038595</td>\n",
       "      <td>0.092837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.152091</td>\n",
       "      <td>-0.712080</td>\n",
       "      <td>1.071908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>2.608968</td>\n",
       "      <td>2.348858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>2.214610</td>\n",
       "      <td>1.116804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.373025</td>\n",
       "      <td>0.112175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.543764</td>\n",
       "      <td>-0.466576</td>\n",
       "      <td>-0.450698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.301595</td>\n",
       "      <td>-1.396315</td>\n",
       "      <td>2.112832</td>\n",
       "      <td>-0.204095</td>\n",
       "      <td>1.170420</td>\n",
       "      <td>0.997193</td>\n",
       "      <td>0.516410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.609115</td>\n",
       "      <td>2.233940</td>\n",
       "      <td>-0.154348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.410071</td>\n",
       "      <td>0.141884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.597196</td>\n",
       "      <td>-0.683697</td>\n",
       "      <td>1.004004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.682228</td>\n",
       "      <td>1.696434</td>\n",
       "      <td>1.464901</td>\n",
       "      <td>-0.437291</td>\n",
       "      <td>0.817793</td>\n",
       "      <td>0.155492</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.048234</td>\n",
       "      <td>-0.168810</td>\n",
       "      <td>-0.132321</td>\n",
       "      <td>3.055968</td>\n",
       "      <td>0.243361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.286714</td>\n",
       "      <td>-0.324181</td>\n",
       "      <td>1.879990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.312998</td>\n",
       "      <td>1.182018</td>\n",
       "      <td>0.717651</td>\n",
       "      <td>0.061590</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>-0.099818</td>\n",
       "      <td>0.527193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.698147</td>\n",
       "      <td>1.511501</td>\n",
       "      <td>-0.752278</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>-0.512243</td>\n",
       "      <td>-0.044517</td>\n",
       "      <td>1.104074</td>\n",
       "      <td>-0.597511</td>\n",
       "      <td>0.460179</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>621e362467b776a2404ad513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.132685</td>\n",
       "      <td>-0.651477</td>\n",
       "      <td>0.775909</td>\n",
       "      <td>-0.466282</td>\n",
       "      <td>-0.023823</td>\n",
       "      <td>0.220395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.418404</td>\n",
       "      <td>1.156172</td>\n",
       "      <td>0.858551</td>\n",
       "      <td>-0.881980</td>\n",
       "      <td>0.487765</td>\n",
       "      <td>0.666725</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.897977</td>\n",
       "      <td>1.440138</td>\n",
       "      <td>-0.219109</td>\n",
       "      <td>2.072307</td>\n",
       "      <td>-0.858437</td>\n",
       "      <td>-0.696627</td>\n",
       "      <td>0.949948</td>\n",
       "      <td>0.645650</td>\n",
       "      <td>-0.484152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.636712</td>\n",
       "      <td>0.318101</td>\n",
       "      <td>0.629926</td>\n",
       "      <td>-0.609778</td>\n",
       "      <td>0.366338</td>\n",
       "      <td>0.277168</td>\n",
       "      <td>1.111446</td>\n",
       "      <td>2.633424</td>\n",
       "      <td>1.170370</td>\n",
       "      <td>1.478375</td>\n",
       "      <td>-0.488209</td>\n",
       "      <td>-0.637897</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.907388</td>\n",
       "      <td>0.920033</td>\n",
       "      <td>-0.060876</td>\n",
       "      <td>-0.525048</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.850457</td>\n",
       "      <td>-0.830277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.815765</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.858644</td>\n",
       "      <td>-1.570056</td>\n",
       "      <td>1.502586</td>\n",
       "      <td>-1.102324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.211697</td>\n",
       "      <td>-0.126794</td>\n",
       "      <td>-0.939611</td>\n",
       "      <td>0.653103</td>\n",
       "      <td>-0.179306</td>\n",
       "      <td>-1.376556</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>-0.310472</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>-0.534092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.914903</td>\n",
       "      <td>0.200761</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>-0.635667</td>\n",
       "      <td>0.017330</td>\n",
       "      <td>-1.808017</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.306681</td>\n",
       "      <td>-1.253005</td>\n",
       "      <td>-1.072048</td>\n",
       "      <td>-0.518418</td>\n",
       "      <td>0.026240</td>\n",
       "      <td>0.066495</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-0.367660</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>-0.334696</td>\n",
       "      <td>0.852268</td>\n",
       "      <td>-0.452880</td>\n",
       "      <td>0.689928</td>\n",
       "      <td>1.154436</td>\n",
       "      <td>1.587917</td>\n",
       "      <td>-0.559247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051959</td>\n",
       "      <td>0.050492</td>\n",
       "      <td>0.030933</td>\n",
       "      <td>0.513257</td>\n",
       "      <td>-1.084078</td>\n",
       "      <td>0.177729</td>\n",
       "      <td>-0.447867</td>\n",
       "      <td>-0.312219</td>\n",
       "      <td>-1.253124</td>\n",
       "      <td>-1.389928</td>\n",
       "      <td>-0.662062</td>\n",
       "      <td>-0.500897</td>\n",
       "      <td>-1.378354</td>\n",
       "      <td>-0.362492</td>\n",
       "      <td>-0.794466</td>\n",
       "      <td>0.637016</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>621e366567b776a24076a727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.711002</td>\n",
       "      <td>0.343114</td>\n",
       "      <td>-1.203833</td>\n",
       "      <td>-0.399985</td>\n",
       "      <td>-0.253561</td>\n",
       "      <td>-0.089815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866225</td>\n",
       "      <td>-0.544461</td>\n",
       "      <td>-0.062335</td>\n",
       "      <td>0.497731</td>\n",
       "      <td>-0.798273</td>\n",
       "      <td>-0.463827</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.955679</td>\n",
       "      <td>-1.353336</td>\n",
       "      <td>-0.875567</td>\n",
       "      <td>-0.358538</td>\n",
       "      <td>1.575405</td>\n",
       "      <td>-0.599670</td>\n",
       "      <td>-0.858608</td>\n",
       "      <td>-0.713974</td>\n",
       "      <td>-0.271084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.331775</td>\n",
       "      <td>-1.126349</td>\n",
       "      <td>-0.754964</td>\n",
       "      <td>0.511441</td>\n",
       "      <td>0.830542</td>\n",
       "      <td>0.674515</td>\n",
       "      <td>0.608256</td>\n",
       "      <td>-1.575647</td>\n",
       "      <td>-0.547774</td>\n",
       "      <td>-0.230786</td>\n",
       "      <td>-0.324458</td>\n",
       "      <td>0.256783</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>-0.738437</td>\n",
       "      <td>0.398959</td>\n",
       "      <td>-0.171040</td>\n",
       "      <td>0.801066</td>\n",
       "      <td>0.658685</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>0.506042</td>\n",
       "      <td>0.967028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408980</td>\n",
       "      <td>-0.171005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533266</td>\n",
       "      <td>0.090706</td>\n",
       "      <td>-1.926848</td>\n",
       "      <td>1.664827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.594940</td>\n",
       "      <td>0.925309</td>\n",
       "      <td>0.434881</td>\n",
       "      <td>-0.326551</td>\n",
       "      <td>-0.401064</td>\n",
       "      <td>0.246508</td>\n",
       "      <td>0.573776</td>\n",
       "      <td>1.072540</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>0.770963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>-1.779655</td>\n",
       "      <td>2.547322</td>\n",
       "      <td>-1.119929</td>\n",
       "      <td>1.968453</td>\n",
       "      <td>0.648481</td>\n",
       "      <td>-0.639178</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>2.252795</td>\n",
       "      <td>0.287427</td>\n",
       "      <td>0.472208</td>\n",
       "      <td>-0.060149</td>\n",
       "      <td>-0.278274</td>\n",
       "      <td>-0.045764</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>1.154045</td>\n",
       "      <td>-0.787473</td>\n",
       "      <td>-0.426223</td>\n",
       "      <td>-0.295219</td>\n",
       "      <td>2.237206</td>\n",
       "      <td>0.146355</td>\n",
       "      <td>-1.936444</td>\n",
       "      <td>-2.098617</td>\n",
       "      <td>1.511198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.363827</td>\n",
       "      <td>-0.447765</td>\n",
       "      <td>-0.246928</td>\n",
       "      <td>-0.300268</td>\n",
       "      <td>0.301667</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>1.559766</td>\n",
       "      <td>0.507088</td>\n",
       "      <td>0.288179</td>\n",
       "      <td>0.476016</td>\n",
       "      <td>2.006804</td>\n",
       "      <td>1.336240</td>\n",
       "      <td>-0.017322</td>\n",
       "      <td>-0.362492</td>\n",
       "      <td>1.104074</td>\n",
       "      <td>0.390110</td>\n",
       "      <td>0.076696</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>621e367e67b776a24087d75d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.729736</td>\n",
       "      <td>0.108399</td>\n",
       "      <td>-0.223659</td>\n",
       "      <td>-0.546359</td>\n",
       "      <td>-0.008589</td>\n",
       "      <td>1.455451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.189341</td>\n",
       "      <td>-0.089197</td>\n",
       "      <td>-0.395692</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.146180</td>\n",
       "      <td>0.803459</td>\n",
       "      <td>-1.165156</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-0.416602</td>\n",
       "      <td>-0.200023</td>\n",
       "      <td>-0.230071</td>\n",
       "      <td>0.545461</td>\n",
       "      <td>-0.543518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508680</td>\n",
       "      <td>-0.279612</td>\n",
       "      <td>-0.058920</td>\n",
       "      <td>-0.100642</td>\n",
       "      <td>0.474993</td>\n",
       "      <td>0.289503</td>\n",
       "      <td>-0.281901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.442651</td>\n",
       "      <td>-0.474871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.297605</td>\n",
       "      <td>-1.311927</td>\n",
       "      <td>0.547241</td>\n",
       "      <td>-0.529279</td>\n",
       "      <td>0.165212</td>\n",
       "      <td>-1.268240</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.696003</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.746908</td>\n",
       "      <td>-0.306063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.370403</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.179821</td>\n",
       "      <td>0.601498</td>\n",
       "      <td>-0.161261</td>\n",
       "      <td>-0.399371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.486417</td>\n",
       "      <td>0.139281</td>\n",
       "      <td>-0.778956</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>-0.353822</td>\n",
       "      <td>-0.297127</td>\n",
       "      <td>-0.036389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.695930</td>\n",
       "      <td>-0.590316</td>\n",
       "      <td>-0.538399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.088040</td>\n",
       "      <td>-0.126478</td>\n",
       "      <td>1.051660</td>\n",
       "      <td>-0.526210</td>\n",
       "      <td>0.146718</td>\n",
       "      <td>-2.119990</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.079318</td>\n",
       "      <td>-1.393874</td>\n",
       "      <td>-1.534995</td>\n",
       "      <td>-0.182114</td>\n",
       "      <td>-0.242400</td>\n",
       "      <td>-0.168199</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.627503</td>\n",
       "      <td>0.092020</td>\n",
       "      <td>-1.021857</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-0.622056</td>\n",
       "      <td>-0.282741</td>\n",
       "      <td>0.300249</td>\n",
       "      <td>0.375508</td>\n",
       "      <td>-0.478374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.409347</td>\n",
       "      <td>-0.049578</td>\n",
       "      <td>-0.237468</td>\n",
       "      <td>-0.147773</td>\n",
       "      <td>-0.445563</td>\n",
       "      <td>0.246955</td>\n",
       "      <td>-0.617510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.394030</td>\n",
       "      <td>-1.372127</td>\n",
       "      <td>-0.443242</td>\n",
       "      <td>-0.719219</td>\n",
       "      <td>-2.120735</td>\n",
       "      <td>-1.952367</td>\n",
       "      <td>-0.794466</td>\n",
       "      <td>-0.227153</td>\n",
       "      <td>-0.498527</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>621e36bb67b776a240b40d64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009819</td>\n",
       "      <td>-0.196731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.893798</td>\n",
       "      <td>0.340355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.541653</td>\n",
       "      <td>1.110762</td>\n",
       "      <td>1.102819</td>\n",
       "      <td>-1.007735</td>\n",
       "      <td>-2.475898</td>\n",
       "      <td>-2.477646</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.867617</td>\n",
       "      <td>0.459823</td>\n",
       "      <td>-0.511083</td>\n",
       "      <td>-0.562063</td>\n",
       "      <td>-0.306519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.383287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.083495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.553600</td>\n",
       "      <td>-2.518787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.124703</td>\n",
       "      <td>0.667644</td>\n",
       "      <td>1.388882</td>\n",
       "      <td>-0.652821</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>1.239507</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.358546</td>\n",
       "      <td>-0.927412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.850457</td>\n",
       "      <td>-0.605614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664342</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.362748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.724988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.921761</td>\n",
       "      <td>1.142930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.343143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>1.615434</td>\n",
       "      <td>-1.455462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.757301</td>\n",
       "      <td>-0.975215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.871032</td>\n",
       "      <td>1.480329</td>\n",
       "      <td>1.410358</td>\n",
       "      <td>-0.554475</td>\n",
       "      <td>-3.375122</td>\n",
       "      <td>-2.604492</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-1.824226</td>\n",
       "      <td>-0.219433</td>\n",
       "      <td>-0.565309</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>-0.678907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.375577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.442468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.314658</td>\n",
       "      <td>0.339254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481894</td>\n",
       "      <td>1.314890</td>\n",
       "      <td>1.137472</td>\n",
       "      <td>-0.777229</td>\n",
       "      <td>1.343710</td>\n",
       "      <td>-1.475404</td>\n",
       "      <td>-0.064258</td>\n",
       "      <td>-1.585132</td>\n",
       "      <td>-1.265492</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>621e36c267b776a240ba2756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.213328</td>\n",
       "      <td>0.499725</td>\n",
       "      <td>-0.742128</td>\n",
       "      <td>0.125627</td>\n",
       "      <td>-0.503016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902293</td>\n",
       "      <td>0.225156</td>\n",
       "      <td>0.235806</td>\n",
       "      <td>-0.896230</td>\n",
       "      <td>-0.321609</td>\n",
       "      <td>-0.407593</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.250873</td>\n",
       "      <td>0.452284</td>\n",
       "      <td>0.246093</td>\n",
       "      <td>-0.536004</td>\n",
       "      <td>1.566309</td>\n",
       "      <td>-0.375078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.409055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.010074</td>\n",
       "      <td>-0.666712</td>\n",
       "      <td>-0.252970</td>\n",
       "      <td>-0.387198</td>\n",
       "      <td>0.323175</td>\n",
       "      <td>-0.530229</td>\n",
       "      <td>0.305775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229866</td>\n",
       "      <td>0.199416</td>\n",
       "      <td>-0.207269</td>\n",
       "      <td>0.562402</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>0.772179</td>\n",
       "      <td>-0.114726</td>\n",
       "      <td>1.242476</td>\n",
       "      <td>3.002006</td>\n",
       "      <td>1.211963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.332709</td>\n",
       "      <td>-0.755389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907785</td>\n",
       "      <td>-0.153306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.187296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.212767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.542621</td>\n",
       "      <td>1.543255</td>\n",
       "      <td>-0.466572</td>\n",
       "      <td>0.285732</td>\n",
       "      <td>-1.523890</td>\n",
       "      <td>0.246508</td>\n",
       "      <td>0.678645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>2.807508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.894157</td>\n",
       "      <td>0.337466</td>\n",
       "      <td>1.950126</td>\n",
       "      <td>1.540710</td>\n",
       "      <td>1.734924</td>\n",
       "      <td>1.163848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>0.322093</td>\n",
       "      <td>0.287118</td>\n",
       "      <td>0.243515</td>\n",
       "      <td>-0.526613</td>\n",
       "      <td>-0.275266</td>\n",
       "      <td>-0.479234</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-0.057478</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>0.081508</td>\n",
       "      <td>-0.493888</td>\n",
       "      <td>1.322671</td>\n",
       "      <td>-0.060255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.388217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.036979</td>\n",
       "      <td>-0.061683</td>\n",
       "      <td>-0.318953</td>\n",
       "      <td>0.343545</td>\n",
       "      <td>-0.507708</td>\n",
       "      <td>-0.935097</td>\n",
       "      <td>-0.520469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.287870</td>\n",
       "      <td>0.327154</td>\n",
       "      <td>1.296292</td>\n",
       "      <td>1.026572</td>\n",
       "      <td>1.096249</td>\n",
       "      <td>-0.362492</td>\n",
       "      <td>-1.524674</td>\n",
       "      <td>1.254279</td>\n",
       "      <td>-1.648975</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>621e36dd67b776a240ce9a45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.591654</td>\n",
       "      <td>0.433152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494703</td>\n",
       "      <td>1.116087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.117224</td>\n",
       "      <td>-0.410465</td>\n",
       "      <td>-0.560902</td>\n",
       "      <td>-1.007735</td>\n",
       "      <td>-1.230787</td>\n",
       "      <td>-1.928866</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.914776</td>\n",
       "      <td>-0.875709</td>\n",
       "      <td>0.073374</td>\n",
       "      <td>-0.313327</td>\n",
       "      <td>-0.169845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.464040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.582417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.307644</td>\n",
       "      <td>-1.447475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.412381</td>\n",
       "      <td>-0.543653</td>\n",
       "      <td>0.262059</td>\n",
       "      <td>0.538321</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-0.551225</td>\n",
       "      <td>1.140354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.322726</td>\n",
       "      <td>0.783801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.829368</td>\n",
       "      <td>0.774349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.871167</td>\n",
       "      <td>-0.605614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.539640</td>\n",
       "      <td>-0.365692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.764807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.628307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.939611</td>\n",
       "      <td>0.775559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829338</td>\n",
       "      <td>0.821812</td>\n",
       "      <td>-0.325197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092496</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>-0.138551</td>\n",
       "      <td>-0.071869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.060963</td>\n",
       "      <td>0.920683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.642469</td>\n",
       "      <td>0.567283</td>\n",
       "      <td>0.839923</td>\n",
       "      <td>-0.554475</td>\n",
       "      <td>-3.092354</td>\n",
       "      <td>-2.422209</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>-0.820239</td>\n",
       "      <td>-0.651359</td>\n",
       "      <td>0.248829</td>\n",
       "      <td>-0.285340</td>\n",
       "      <td>-0.247383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.859268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.302424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.043384</td>\n",
       "      <td>-0.101306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.568226</td>\n",
       "      <td>0.790594</td>\n",
       "      <td>-0.137636</td>\n",
       "      <td>0.125766</td>\n",
       "      <td>0.353868</td>\n",
       "      <td>-0.680467</td>\n",
       "      <td>0.373866</td>\n",
       "      <td>0.266658</td>\n",
       "      <td>0.268438</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>621e36f967b776a240e5e7c9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.141185</td>\n",
       "      <td>-0.347655</td>\n",
       "      <td>-0.164992</td>\n",
       "      <td>0.699332</td>\n",
       "      <td>-0.599146</td>\n",
       "      <td>-0.804961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736545</td>\n",
       "      <td>0.680939</td>\n",
       "      <td>0.400461</td>\n",
       "      <td>-0.473753</td>\n",
       "      <td>-0.436337</td>\n",
       "      <td>-1.037101</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>0.022196</td>\n",
       "      <td>-0.317074</td>\n",
       "      <td>-0.355876</td>\n",
       "      <td>-0.201591</td>\n",
       "      <td>0.161187</td>\n",
       "      <td>-0.385668</td>\n",
       "      <td>-0.515239</td>\n",
       "      <td>-0.134440</td>\n",
       "      <td>0.931473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.054986</td>\n",
       "      <td>-1.406466</td>\n",
       "      <td>-0.483478</td>\n",
       "      <td>-0.776314</td>\n",
       "      <td>-1.137057</td>\n",
       "      <td>-1.202084</td>\n",
       "      <td>-1.280829</td>\n",
       "      <td>-0.737699</td>\n",
       "      <td>0.690374</td>\n",
       "      <td>0.374101</td>\n",
       "      <td>0.919878</td>\n",
       "      <td>0.554224</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>0.332990</td>\n",
       "      <td>0.360462</td>\n",
       "      <td>-0.415615</td>\n",
       "      <td>0.798999</td>\n",
       "      <td>-1.059579</td>\n",
       "      <td>0.716442</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-1.204459</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.353419</td>\n",
       "      <td>-0.006512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.716280</td>\n",
       "      <td>-0.330294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604522</td>\n",
       "      <td>0.560452</td>\n",
       "      <td>-0.035343</td>\n",
       "      <td>0.953589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.329532</td>\n",
       "      <td>0.504899</td>\n",
       "      <td>-0.421946</td>\n",
       "      <td>0.530646</td>\n",
       "      <td>0.745182</td>\n",
       "      <td>1.136783</td>\n",
       "      <td>1.673832</td>\n",
       "      <td>0.413963</td>\n",
       "      <td>-1.204438</td>\n",
       "      <td>-1.296380</td>\n",
       "      <td>0.872180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>-0.076706</td>\n",
       "      <td>0.425226</td>\n",
       "      <td>-0.910376</td>\n",
       "      <td>0.928208</td>\n",
       "      <td>-1.165051</td>\n",
       "      <td>-0.029129</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>0.988510</td>\n",
       "      <td>0.940953</td>\n",
       "      <td>1.016972</td>\n",
       "      <td>-0.391018</td>\n",
       "      <td>0.587687</td>\n",
       "      <td>-0.062248</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.472279</td>\n",
       "      <td>-0.119200</td>\n",
       "      <td>-0.463482</td>\n",
       "      <td>-0.309488</td>\n",
       "      <td>0.484458</td>\n",
       "      <td>0.363168</td>\n",
       "      <td>-0.117114</td>\n",
       "      <td>-0.090090</td>\n",
       "      <td>1.649248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.575960</td>\n",
       "      <td>-0.090935</td>\n",
       "      <td>0.484957</td>\n",
       "      <td>0.314079</td>\n",
       "      <td>0.461395</td>\n",
       "      <td>0.084290</td>\n",
       "      <td>0.488631</td>\n",
       "      <td>0.348807</td>\n",
       "      <td>0.942154</td>\n",
       "      <td>0.968352</td>\n",
       "      <td>1.075555</td>\n",
       "      <td>0.730656</td>\n",
       "      <td>1.467440</td>\n",
       "      <td>0.591434</td>\n",
       "      <td>0.665950</td>\n",
       "      <td>0.143205</td>\n",
       "      <td>0.843661</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>621e375b67b776a240290cdc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.362927</td>\n",
       "      <td>-0.323991</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>-0.650742</td>\n",
       "      <td>-0.204767</td>\n",
       "      <td>-0.689624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551318</td>\n",
       "      <td>-1.291586</td>\n",
       "      <td>-1.310834</td>\n",
       "      <td>0.835388</td>\n",
       "      <td>0.246288</td>\n",
       "      <td>0.147857</td>\n",
       "      <td>-0.203348</td>\n",
       "      <td>-0.488418</td>\n",
       "      <td>0.176853</td>\n",
       "      <td>-0.260392</td>\n",
       "      <td>-0.527598</td>\n",
       "      <td>0.851497</td>\n",
       "      <td>0.438916</td>\n",
       "      <td>-0.542307</td>\n",
       "      <td>0.513776</td>\n",
       "      <td>-1.017128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818281</td>\n",
       "      <td>-0.330714</td>\n",
       "      <td>0.381471</td>\n",
       "      <td>-0.564627</td>\n",
       "      <td>-0.476262</td>\n",
       "      <td>-0.298605</td>\n",
       "      <td>-0.047254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.302687</td>\n",
       "      <td>-1.306226</td>\n",
       "      <td>-0.469779</td>\n",
       "      <td>-0.049303</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>0.933220</td>\n",
       "      <td>-0.302205</td>\n",
       "      <td>-0.206311</td>\n",
       "      <td>-0.180825</td>\n",
       "      <td>-1.283047</td>\n",
       "      <td>0.983041</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>2.608968</td>\n",
       "      <td>2.348858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121268</td>\n",
       "      <td>-0.912587</td>\n",
       "      <td>-0.680501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.789043</td>\n",
       "      <td>0.165272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.267412</td>\n",
       "      <td>0.636902</td>\n",
       "      <td>0.893967</td>\n",
       "      <td>-0.450325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.286525</td>\n",
       "      <td>0.428851</td>\n",
       "      <td>-0.939611</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>-0.501265</td>\n",
       "      <td>0.405158</td>\n",
       "      <td>-0.584656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.609115</td>\n",
       "      <td>2.233940</td>\n",
       "      <td>-0.152913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.915335</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>0.519163</td>\n",
       "      <td>-0.505154</td>\n",
       "      <td>0.117671</td>\n",
       "      <td>0.654396</td>\n",
       "      <td>-1.333752</td>\n",
       "      <td>1.070326</td>\n",
       "      <td>1.247219</td>\n",
       "      <td>0.651266</td>\n",
       "      <td>1.696434</td>\n",
       "      <td>1.464901</td>\n",
       "      <td>0.374259</td>\n",
       "      <td>0.153001</td>\n",
       "      <td>0.249899</td>\n",
       "      <td>-0.199832</td>\n",
       "      <td>0.435622</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>-0.148985</td>\n",
       "      <td>-0.487085</td>\n",
       "      <td>1.149713</td>\n",
       "      <td>0.639227</td>\n",
       "      <td>0.433935</td>\n",
       "      <td>0.911188</td>\n",
       "      <td>-1.154183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.765865</td>\n",
       "      <td>-0.580428</td>\n",
       "      <td>0.169853</td>\n",
       "      <td>0.047455</td>\n",
       "      <td>-0.012245</td>\n",
       "      <td>0.962891</td>\n",
       "      <td>-0.185443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.698147</td>\n",
       "      <td>1.511501</td>\n",
       "      <td>0.953236</td>\n",
       "      <td>0.578377</td>\n",
       "      <td>0.477598</td>\n",
       "      <td>1.227384</td>\n",
       "      <td>-0.064258</td>\n",
       "      <td>-1.461679</td>\n",
       "      <td>0.651920</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  age_std  bmi_std   bpm_std  calories_std  \\\n",
       "0   621e2e8e67b776a24055b564      0.0      0.0 -1.110783     -1.002972   \n",
       "1   621e2eaf67b776a2406b14ac      0.0      0.0 -0.876672     -0.540978   \n",
       "2   621e2ed667b776a24085d8d1      0.0      0.0 -0.757127     -1.380645   \n",
       "3   621e2f3967b776a240c654db      0.0      0.0  0.220415      0.068573   \n",
       "4   621e2f6167b776a240e082a9      0.0      0.0  0.333876     -0.530164   \n",
       "5   621e2f7a67b776a240f14425      0.0      0.0  1.676134      1.777261   \n",
       "6   621e2f9167b776a240011ccb      0.0      0.0 -0.890859     -1.445400   \n",
       "7   621e2fb367b776a24015accd      0.0      0.0  0.844661     -0.142981   \n",
       "8   621e2fce67b776a240279baa      0.0      0.0 -0.186988      0.884608   \n",
       "9   621e2ff067b776a2403eb737      0.0      0.0 -1.220759     -0.484167   \n",
       "10  621e301367b776a24057738e      0.0      0.0  1.146896     -0.170314   \n",
       "11  621e301e67b776a240608a72      0.0      0.0  1.121110      2.599655   \n",
       "12  621e30c867b776a240d4aa6c      0.0      0.0 -0.430172     -1.050827   \n",
       "13  621e30e267b776a240e5bf90      0.0      0.0       NaN     -1.398160   \n",
       "14  621e30e467b776a240e817c7      0.0      0.0  4.465588      4.122059   \n",
       "15  621e30f467b776a240f22944      0.0      0.0 -0.579143      0.573354   \n",
       "16  621e310d67b776a24003096d      0.0      0.0 -0.139305     -0.749507   \n",
       "17  621e312a67b776a240164d59      0.0      0.0 -0.287583      1.101382   \n",
       "18  621e314867b776a24029ebf9      0.0      0.0  0.688567      0.478573   \n",
       "19  621e323667b776a240f19134      0.0      0.0 -1.222775     -0.381316   \n",
       "20  621e324e67b776a2400191cb      0.0      0.0 -1.057943     -0.374901   \n",
       "21  621e328667b776a240281372      0.0      0.0 -0.397322     -0.186589   \n",
       "22  621e329067b776a2402ffad2      0.0      0.0 -1.012816     -0.626681   \n",
       "23  621e32af67b776a24045b4cf      0.0      0.0 -0.896021     -1.069687   \n",
       "24  621e32d067b776a2405b7d54      0.0      0.0 -0.308531      0.024163   \n",
       "25  621e32d967b776a240627414      0.0      0.0  0.385816      1.415291   \n",
       "26  621e331067b776a24085dd3f      0.0      0.0 -0.322260     -0.454793   \n",
       "27  621e332267b776a24092a584      0.0      0.0  0.479059      1.158393   \n",
       "28  621e333567b776a240a0c217      0.0      0.0 -0.684420     -0.878729   \n",
       "29  621e333967b776a240a3cd06      0.0      0.0  0.367642      0.460905   \n",
       "30  621e335a67b776a240bb12ff      0.0      0.0 -0.752524     -0.919100   \n",
       "31  621e337667b776a240ce78ab      0.0      0.0 -0.535935     -0.313637   \n",
       "32  621e339967b776a240e502de      0.0      0.0  0.124999     -0.682633   \n",
       "33  621e33b067b776a240f39e56      0.0      0.0 -0.945380     -0.180763   \n",
       "34  621e33cf67b776a240087de9      0.0      0.0 -0.655906      0.401158   \n",
       "35  621e33ed67b776a2401cf5f7      0.0      0.0 -1.045176      0.127815   \n",
       "36  621e341067b776a24037b105      0.0      0.0 -0.778303     -0.250491   \n",
       "37  621e346f67b776a24081744f      NaN      NaN -0.755827     -1.062678   \n",
       "38  621e34db67b776a240c9c2be      0.0      0.0 -1.157588     -0.594750   \n",
       "39  621e34ec67b776a240d60873      0.0      0.0  1.069279     -0.207067   \n",
       "40  621e34f767b776a240de4e1a      0.0      0.0  1.259151     -0.974788   \n",
       "41  621e356967b776a24027bd9f      NaN      NaN -0.265985     -0.638434   \n",
       "42  621e362467b776a2404ad513      0.0      0.0 -0.132685     -0.651477   \n",
       "43  621e366567b776a24076a727      0.0      0.0 -0.711002      0.343114   \n",
       "44  621e367e67b776a24087d75d      NaN      NaN -0.729736      0.108399   \n",
       "45  621e36bb67b776a240b40d64      0.0      0.0 -0.009819     -0.196731   \n",
       "46  621e36c267b776a240ba2756      0.0      0.0 -0.213328      0.499725   \n",
       "47  621e36dd67b776a240ce9a45      0.0      0.0  0.591654      0.433152   \n",
       "48  621e36f967b776a240e5e7c9      0.0      0.0 -0.141185     -0.347655   \n",
       "49  621e375b67b776a240290cdc      0.0      0.0 -0.362927     -0.323991   \n",
       "\n",
       "    daily1temperature1variation_std  distance_std  \\\n",
       "0                         -0.415743     -0.705350   \n",
       "1                         -0.325256      0.837708   \n",
       "2                         -0.122808     -1.404843   \n",
       "3                          0.615941     -0.472026   \n",
       "4                          0.466479     -0.358932   \n",
       "5                         -0.096650      1.375064   \n",
       "6                         -0.246466     -1.519221   \n",
       "7                          0.869333     -0.347274   \n",
       "8                         -0.688576      0.398916   \n",
       "9                         -0.276172     -1.262243   \n",
       "10                              NaN      0.247893   \n",
       "11                         0.588563      0.589562   \n",
       "12                        -0.204884     -0.501247   \n",
       "13                              NaN           NaN   \n",
       "14                         3.374508      0.254568   \n",
       "15                        -0.490786      1.806943   \n",
       "16                        -0.334097     -0.686749   \n",
       "17                         0.434881      0.349202   \n",
       "18                         0.619237      0.714371   \n",
       "19                         0.096160     -1.226282   \n",
       "20                        -1.255617     -1.311539   \n",
       "21                        -1.428403     -0.822642   \n",
       "22                        -1.161575     -1.049619   \n",
       "23                        -0.983412     -0.963200   \n",
       "24                        -0.049233      0.582467   \n",
       "25                         2.533037      2.330936   \n",
       "26                        -0.368286     -1.674542   \n",
       "27                        -0.524642      3.081953   \n",
       "28                              NaN     -0.301893   \n",
       "29                         0.750849      0.027815   \n",
       "30                        -0.497482     -0.498602   \n",
       "31                        -0.182907     -0.444693   \n",
       "32                        -0.204748      0.956394   \n",
       "33                        -0.944076     -0.453674   \n",
       "34                        -0.663778     -0.134703   \n",
       "35                        -0.976718     -0.174241   \n",
       "36                        -0.901297     -0.899363   \n",
       "37                         0.247413     -0.349450   \n",
       "38                        -1.204493     -0.975233   \n",
       "39                         2.015515      1.114210   \n",
       "40                              NaN     -0.421054   \n",
       "41                              NaN     -0.371363   \n",
       "42                         0.775909     -0.466282   \n",
       "43                        -1.203833     -0.399985   \n",
       "44                        -0.223659     -0.546359   \n",
       "45                              NaN      0.893798   \n",
       "46                        -0.742128      0.125627   \n",
       "47                              NaN      0.494703   \n",
       "48                        -0.164992      0.699332   \n",
       "49                         0.015754     -0.650742   \n",
       "\n",
       "    filteredDemographicVO2Max_std  full1sleep1breathing1rate_std  gender_std  \\\n",
       "0                       -0.186683                      -0.765453         0.0   \n",
       "1                        1.633165                      -0.782661         0.0   \n",
       "2                        0.284176                       1.996907         0.0   \n",
       "3                        0.723346                       1.977221         0.0   \n",
       "4                       -0.335899                      -0.627364         0.0   \n",
       "5                       -1.098944                            NaN         0.0   \n",
       "6                       -0.421096                      -0.761567         0.0   \n",
       "7                       -0.779647                            NaN         0.0   \n",
       "8                       -0.021646                      -0.624967         0.0   \n",
       "9                       -0.702450                            NaN         0.0   \n",
       "10                      -0.930517                            NaN         0.0   \n",
       "11                       0.852508                            NaN         0.0   \n",
       "12                      -0.031663                       0.323896         0.0   \n",
       "13                            NaN                            NaN         0.0   \n",
       "14                      -0.666815                       2.424184         0.0   \n",
       "15                      -0.633712                      -0.905980         0.0   \n",
       "16                       0.336152                      -0.729102         0.0   \n",
       "17                       0.209849                      -0.608437         0.0   \n",
       "18                      -0.885494                            NaN         0.0   \n",
       "19                       1.069774                      -0.722265         0.0   \n",
       "20                      -0.615888                            NaN         0.0   \n",
       "21                      -1.309713                      -0.640024         0.0   \n",
       "22                      -0.947486                      -0.653602         0.0   \n",
       "23                      -0.390587                      -0.765960         0.0   \n",
       "24                      -0.545193                            NaN         0.0   \n",
       "25                      -0.649378                       1.309901         0.0   \n",
       "26                      -0.274852                            NaN         0.0   \n",
       "27                      -1.164830                      -0.523357         0.0   \n",
       "28                      -1.000030                            NaN         0.0   \n",
       "29                       0.748161                       2.482310         0.0   \n",
       "30                       0.922759                            NaN         0.0   \n",
       "31                      -0.828852                      -0.626244         0.0   \n",
       "32                       1.342530                      -0.161107         0.0   \n",
       "33                      -0.550481                      -0.770278         0.0   \n",
       "34                      -0.618166                       0.399851         0.0   \n",
       "35                      -0.611985                            NaN         0.0   \n",
       "36                      -0.679218                       1.545074         0.0   \n",
       "37                       0.637723                       1.360858         NaN   \n",
       "38                      -0.704773                            NaN         0.0   \n",
       "39                      -1.180209                      -0.706147         0.0   \n",
       "40                      -0.895489                            NaN         0.0   \n",
       "41                       0.028013                      -0.784135         NaN   \n",
       "42                      -0.023823                       0.220395         0.0   \n",
       "43                      -0.253561                      -0.089815         0.0   \n",
       "44                      -0.008589                       1.455451         0.0   \n",
       "45                       0.340355                            NaN         0.0   \n",
       "46                      -0.503016                            NaN         0.0   \n",
       "47                       1.116087                            NaN         0.0   \n",
       "48                      -0.599146                      -0.804961         0.0   \n",
       "49                      -0.204767                      -0.689624         0.0   \n",
       "\n",
       "    lightly1active1minutes_std  max1goal_std  min1goal_std  \\\n",
       "0                    -1.843042      0.197341      0.306925   \n",
       "1                    -0.678570     -1.291586     -1.310834   \n",
       "2                    -0.313647      0.483443      0.523893   \n",
       "3                    -2.081226      0.239369      0.429395   \n",
       "4                     0.855571      0.327909      0.336213   \n",
       "5                    -0.086233     -0.418966     -0.534120   \n",
       "6                    -1.067913     -0.603336     -0.344057   \n",
       "7                     0.012447      0.532800      0.752366   \n",
       "8                    -1.011200     -0.143768      0.159020   \n",
       "9                     0.350310     -0.672687     -0.595560   \n",
       "10                   -0.143470      0.292438      0.788284   \n",
       "11                   -0.033518     -0.488519     -0.679074   \n",
       "12                    0.907265     -0.263858     -0.072542   \n",
       "13                   -2.763172     -1.136073     -1.310834   \n",
       "14                   -1.731337     -0.312899     -0.048644   \n",
       "15                    0.793963      1.200592      1.482043   \n",
       "16                    0.450605     -0.352127     -0.322317   \n",
       "17                   -0.196525     -0.456346     -0.609959   \n",
       "18                   -0.654538      0.232783      0.427673   \n",
       "19                    0.324439     -0.772298     -0.492988   \n",
       "20                    0.072887     -0.310572     -0.297781   \n",
       "21                   -0.795315     -1.291586     -1.310834   \n",
       "22                    0.250456     -0.141085     -0.545351   \n",
       "23                   -1.120099     -0.434681     -0.180938   \n",
       "24                    1.336972      1.385325      1.178305   \n",
       "25                   -0.784118      1.129863      1.218448   \n",
       "26                    1.141464     -1.291586     -1.310834   \n",
       "27                   -0.397166      0.177877      0.070191   \n",
       "28                   -0.934372           NaN           NaN   \n",
       "29                    0.484751      0.295615     -0.511713   \n",
       "30                   -0.060106     -1.291586     -1.310834   \n",
       "31                   -0.194533      0.152782      0.449695   \n",
       "32                   -0.879651      0.871533      0.759631   \n",
       "33                    0.982392      0.496522      0.129256   \n",
       "34                   -0.071882      1.565750      1.551273   \n",
       "35                   -0.194012     -0.486728     -0.635454   \n",
       "36                    0.545608     -0.960031     -1.310834   \n",
       "37                   -1.109054     -0.712022     -0.216593   \n",
       "38                    0.627774     -0.816368     -0.712679   \n",
       "39                   -1.094596      1.882817      1.885649   \n",
       "40                   -0.537298           NaN           NaN   \n",
       "41                   -0.962727           NaN           NaN   \n",
       "42                   -0.418404      1.156172      0.858551   \n",
       "43                    0.866225     -0.544461     -0.062335   \n",
       "44                    0.477561           NaN           NaN   \n",
       "45                    1.541653      1.110762      1.102819   \n",
       "46                    0.902293      0.225156      0.235806   \n",
       "47                   -0.117224     -0.410465     -0.560902   \n",
       "48                    0.736545      0.680939      0.400461   \n",
       "49                    0.551318     -1.291586     -1.310834   \n",
       "\n",
       "    minutesAfterWakeup_std  minutesAsleep_std  minutesAwake_std  \\\n",
       "0                -0.427242          -1.007991         -0.486219   \n",
       "1                 0.433363           0.026673         -0.194824   \n",
       "2                -0.137222          -0.316514         -0.026073   \n",
       "3                -0.510351           2.088174          1.168750   \n",
       "4                -0.129442          -0.388539         -0.017335   \n",
       "5                 0.230875          -0.609383         -1.266239   \n",
       "6                -0.129813           0.039063          0.070530   \n",
       "7                -0.510432          -0.478845         -0.742325   \n",
       "8                 0.528416          -0.310119         -0.119026   \n",
       "9                -0.547631          -0.349103         -0.493070   \n",
       "10                     NaN                NaN               NaN   \n",
       "11               -0.316663           0.207836          0.252570   \n",
       "12               -0.470408          -0.435895         -0.150489   \n",
       "13                     NaN                NaN               NaN   \n",
       "14                0.024321           1.604472          2.396447   \n",
       "15               -0.008622          -1.182860         -0.640548   \n",
       "16               -0.166741           0.254947          0.101871   \n",
       "17               -0.076470           0.191347          0.221118   \n",
       "18                0.356618           0.025756         -0.067016   \n",
       "19               -0.181185          -0.383783          0.592355   \n",
       "20               -0.012318           0.154024          0.067256   \n",
       "21               -0.159924          -0.363673         -0.667602   \n",
       "22                0.036532           0.050673         -0.409735   \n",
       "23               -0.094757          -1.107933         -0.946648   \n",
       "24               -0.030264           0.546824          1.135718   \n",
       "25                0.339921           1.355888          0.047519   \n",
       "26               -0.791465           0.407717          0.347201   \n",
       "27                2.143606           2.960497          0.743160   \n",
       "28                     NaN                NaN               NaN   \n",
       "29               -0.000465           0.239616          1.081749   \n",
       "30               -0.012541          -0.427896         -0.102491   \n",
       "31               -0.521301          -0.849276          0.135358   \n",
       "32               -0.271345          -0.451999         -0.117306   \n",
       "33               -0.715864          -0.533480         -0.588613   \n",
       "34               -0.158289          -0.885611          0.459538   \n",
       "35               -0.682446          -0.073912         -0.322377   \n",
       "36               -0.514120          -0.265497         -0.501155   \n",
       "37                0.084306          -0.424474         -0.051035   \n",
       "38                0.422669          -1.257328         -1.006851   \n",
       "39                0.120384          -0.967763         -0.023863   \n",
       "40               -1.007735           3.040333         -1.452385   \n",
       "41               -0.674830          -1.132796         -0.414332   \n",
       "42               -0.881980           0.487765          0.666725   \n",
       "43                0.497731          -0.798273         -0.463827   \n",
       "44               -0.189341          -0.089197         -0.395692   \n",
       "45               -1.007735          -2.475898         -2.477646   \n",
       "46               -0.896230          -0.321609         -0.407593   \n",
       "47               -1.007735          -1.230787         -1.928866   \n",
       "48               -0.473753          -0.436337         -1.037101   \n",
       "49                0.835388           0.246288          0.147857   \n",
       "\n",
       "    minutesToFallAsleep_std  minutes1below1default1zone11_std  \\\n",
       "0                 -0.203348                         -1.616408   \n",
       "1                  0.315262                         -0.948505   \n",
       "2                 -0.203348                         -0.158325   \n",
       "3                 -0.203348                          0.097094   \n",
       "4                 -0.203348                          1.567018   \n",
       "5                 -0.203348                          0.829987   \n",
       "6                 -0.203348                         -1.168760   \n",
       "7                 -0.203348                         -0.042371   \n",
       "8                  0.256004                         -0.582666   \n",
       "9                 -0.203348                         -1.135960   \n",
       "10                      NaN                         -0.556564   \n",
       "11                -0.203348                          2.060675   \n",
       "12                -0.203348                          0.372026   \n",
       "13                      NaN                               NaN   \n",
       "14                -0.203348                          0.052286   \n",
       "15                -0.203348                         -0.530977   \n",
       "16                -0.203348                          1.320647   \n",
       "17                -0.203348                          0.071363   \n",
       "18                -0.203348                          1.475106   \n",
       "19                -0.203348                         -0.054122   \n",
       "20                -0.203348                         -0.889664   \n",
       "21                -0.203348                         -1.494018   \n",
       "22                -0.203348                         -1.292155   \n",
       "23                -0.203348                         -1.081832   \n",
       "24                -0.203348                          1.593481   \n",
       "25                 0.815251                          1.345101   \n",
       "26                -0.203348                          0.417256   \n",
       "27                -0.203348                         -0.755687   \n",
       "28                      NaN                          0.980165   \n",
       "29                 7.001161                          1.179058   \n",
       "30                -0.203348                         -0.757299   \n",
       "31                -0.203348                         -0.739139   \n",
       "32                -0.203348                         -1.109334   \n",
       "33                -0.203348                         -1.140335   \n",
       "34                 3.695862                         -0.728416   \n",
       "35                -0.203348                         -1.325107   \n",
       "36                -0.203348                          0.078140   \n",
       "37                -0.203348                         -0.062853   \n",
       "38                -0.203348                         -0.852166   \n",
       "39                -0.203348                          1.526668   \n",
       "40                -0.203348                         -0.952054   \n",
       "41                -0.203348                         -1.734308   \n",
       "42                -0.203348                          0.897977   \n",
       "43                -0.203348                         -0.955679   \n",
       "44                -0.203348                          0.146180   \n",
       "45                -0.203348                         -0.867617   \n",
       "46                -0.203348                          0.250873   \n",
       "47                -0.203348                         -0.914776   \n",
       "48                -0.203348                          0.022196   \n",
       "49                -0.203348                         -0.488418   \n",
       "\n",
       "    minutes1in1default1zone11_std  minutes1in1default1zone12_std  \\\n",
       "0                       -1.087310                      -1.223248   \n",
       "1                        0.065829                       0.717323   \n",
       "2                        0.756393                      -0.873616   \n",
       "3                        0.151980                       2.986199   \n",
       "4                       -1.057101                       1.097759   \n",
       "5                       -0.556808                       1.778792   \n",
       "6                        0.464725                      -0.115673   \n",
       "7                       -0.369784                       1.551203   \n",
       "8                        2.487800                       0.617458   \n",
       "9                       -1.024325                      -0.627330   \n",
       "10                      -0.724686                       0.872361   \n",
       "11                      -0.176215                       0.966829   \n",
       "12                      -1.347951                      -0.030063   \n",
       "13                            NaN                            NaN   \n",
       "14                      -0.221116                       4.127703   \n",
       "15                      -0.738063                      -1.056281   \n",
       "16                       0.471723                      -0.465364   \n",
       "17                      -0.316623                      -0.981755   \n",
       "18                      -1.889276                      -0.690391   \n",
       "19                      -1.212719                      -0.963539   \n",
       "20                       0.330169                      -0.554794   \n",
       "21                      -0.427401                       1.278524   \n",
       "22                       1.033225                      -0.071731   \n",
       "23                      -1.384300                      -1.182916   \n",
       "24                      -0.498286                      -0.410795   \n",
       "25                       0.270517                       0.000128   \n",
       "26                       0.909288                       1.052041   \n",
       "27                       1.018711                      -1.111183   \n",
       "28                      -0.977530                      -0.446614   \n",
       "29                       0.332929                      -0.635622   \n",
       "30                       0.239368                       0.457021   \n",
       "31                       0.324277                      -0.858536   \n",
       "32                      -1.609975                       0.210012   \n",
       "33                       1.441223                      -0.732335   \n",
       "34                       1.370430                      -0.432135   \n",
       "35                      -1.385639                      -0.619051   \n",
       "36                       0.642799                      -1.127763   \n",
       "37                       0.293137                      -1.034400   \n",
       "38                      -0.670798                      -0.729849   \n",
       "39                       0.682149                      -0.023688   \n",
       "40                       0.227105                       0.803550   \n",
       "41                      -0.557501                      -0.455013   \n",
       "42                       1.440138                      -0.219109   \n",
       "43                      -1.353336                      -0.875567   \n",
       "44                       0.803459                      -1.165156   \n",
       "45                       0.459823                      -0.511083   \n",
       "46                       0.452284                       0.246093   \n",
       "47                      -0.875709                       0.073374   \n",
       "48                      -0.317074                      -0.355876   \n",
       "49                       0.176853                      -0.260392   \n",
       "\n",
       "    minutes1in1default1zone13_std  moderately1active1minutes_std  \\\n",
       "0                       -0.562063                      -0.100121   \n",
       "1                       -0.215531                       0.761270   \n",
       "2                       -0.406402                      -0.938611   \n",
       "3                        0.254536                       0.049347   \n",
       "4                       -0.381909                       0.047924   \n",
       "5                       -0.413649                       1.929864   \n",
       "6                        0.178337                      -0.800390   \n",
       "7                        0.100390                       0.026578   \n",
       "8                       -0.562063                       3.195118   \n",
       "9                       -0.562063                      -1.135281   \n",
       "10                       1.068204                      -0.696552   \n",
       "11                       1.517847                      -0.210185   \n",
       "12                      -0.465745                      -0.201696   \n",
       "13                            NaN                      -1.415651   \n",
       "14                       0.185990                      -0.426302   \n",
       "15                      -0.385075                       0.793859   \n",
       "16                      -0.432611                      -0.368972   \n",
       "17                      -0.562063                       1.290926   \n",
       "18                      -0.387449                      -0.077625   \n",
       "19                      -0.406937                      -1.337106   \n",
       "20                      -0.131282                       0.915348   \n",
       "21                       1.745532                      -1.238338   \n",
       "22                      -0.562063                       1.492975   \n",
       "23                      -0.562063                      -0.503795   \n",
       "24                      -0.500810                      -1.191879   \n",
       "25                      -0.123301                      -0.539492   \n",
       "26                       0.274268                       0.784100   \n",
       "27                      -0.562063                       0.865683   \n",
       "28                      -0.562063                      -2.080479   \n",
       "29                      -0.519843                       0.131992   \n",
       "30                      -0.364658                       0.141549   \n",
       "31                      -0.470606                       1.709235   \n",
       "32                       0.221605                       0.436852   \n",
       "33                      -0.301028                       0.765151   \n",
       "34                      -0.121224                       1.106097   \n",
       "35                      -0.562063                      -0.343477   \n",
       "36                      -0.562063                      -0.679866   \n",
       "37                      -0.461512                       0.331506   \n",
       "38                      -0.376785                      -1.466894   \n",
       "39                      -0.562063                      -0.834294   \n",
       "40                       0.165220                      -0.789669   \n",
       "41                       2.044084                      -0.313147   \n",
       "42                       2.072307                      -0.858437   \n",
       "43                      -0.358538                       1.575405   \n",
       "44                      -0.562063                      -0.416602   \n",
       "45                      -0.562063                      -0.306519   \n",
       "46                      -0.536004                       1.566309   \n",
       "47                      -0.313327                      -0.169845   \n",
       "48                      -0.201591                       0.161187   \n",
       "49                      -0.527598                       0.851497   \n",
       "\n",
       "    nightly1temperature_std  nremhr_std  resting1hr_std  rmssd_std  \\\n",
       "0                 -0.982470   -0.872711       -0.604984  -0.148081   \n",
       "1                  0.856707    0.439622        0.937525  -0.615973   \n",
       "2                 -0.475160   -0.260693        1.419905  -0.357474   \n",
       "3                  0.665182    3.256397       -0.038869  -0.567753   \n",
       "4                  1.259984   -0.432281       -0.520710  -0.652363   \n",
       "5                 -0.083613         NaN       -1.293488        NaN   \n",
       "6                  0.578924   -0.696156       -0.256672  -0.730785   \n",
       "7                  0.451060         NaN       -0.285114        NaN   \n",
       "8                 -0.660966    0.779022        1.200454  -0.587032   \n",
       "9                 -0.256077         NaN       -0.776072        NaN   \n",
       "10                      NaN         NaN       -1.697675        NaN   \n",
       "11                 1.065549         NaN        0.620215        NaN   \n",
       "12                -0.320111   -0.351812       -0.058874  -0.261057   \n",
       "13                      NaN         NaN             NaN        NaN   \n",
       "14                 4.006792    1.829085        0.549383   0.150343   \n",
       "15                -0.662998   -0.862989       -1.357831  -0.525059   \n",
       "16                -0.496757    0.585990        0.206030  -0.617882   \n",
       "17                -0.534423   -0.208148        0.780889  -0.283512   \n",
       "18                -0.115853         NaN       -1.069953        NaN   \n",
       "19                -0.143950   -0.833281       -0.052724  -0.525958   \n",
       "20                -0.344547         NaN       -0.303906        NaN   \n",
       "21                -0.208328   -0.406580       -1.312209   2.575788   \n",
       "22                -0.042013   -0.781227       -0.977820  -0.802796   \n",
       "23                -0.493907   -0.708691       -0.465705  -0.678469   \n",
       "24                -0.618398         NaN       -0.185745        NaN   \n",
       "25                -0.049513   -1.164961       -0.386421  -0.975099   \n",
       "26                -0.601416         NaN       -0.406274        NaN   \n",
       "27                -0.494223   -0.604940       -0.610876   3.174008   \n",
       "28                      NaN         NaN       -1.877588        NaN   \n",
       "29                 0.121932    2.542443        0.913526   1.339021   \n",
       "30                -0.245731         NaN        0.231298        NaN   \n",
       "31                -0.401200   -0.614794       -0.781092  -0.559535   \n",
       "32                -0.464074    0.226617        0.125599  -0.685907   \n",
       "33                -0.655983   -0.442477        0.030436  -0.014290   \n",
       "34                -0.384225    0.741384        0.582436  -0.041486   \n",
       "35                -0.848911         NaN       -1.177622        NaN   \n",
       "36                -0.429433   -0.342968       -0.513340  -0.718257   \n",
       "37                -0.346434   -0.252231        1.288236  -0.499274   \n",
       "38                -0.638878         NaN       -1.060022        NaN   \n",
       "39                 3.711812   -0.544497       -0.949037  -1.110345   \n",
       "40                -0.570337         NaN       -1.862264        NaN   \n",
       "41                      NaN   -0.326610        1.235822   1.601376   \n",
       "42                -0.696627    0.949948        0.645650  -0.484152   \n",
       "43                -0.599670   -0.858608       -0.713974  -0.271084   \n",
       "44                -0.200023   -0.230071        0.545461  -0.543518   \n",
       "45                      NaN         NaN       -0.383287        NaN   \n",
       "46                -0.375078         NaN       -0.409055        NaN   \n",
       "47                      NaN         NaN        1.464040        NaN   \n",
       "48                -0.385668   -0.515239       -0.134440   0.931473   \n",
       "49                 0.438916   -0.542307        0.513776  -1.017128   \n",
       "\n",
       "    scl1avg_std  sedentary1minutes_std  sleep1deep1ratio_std  \\\n",
       "0           NaN              -0.643462             -1.069866   \n",
       "1           NaN              -0.843950              0.875181   \n",
       "2           NaN               0.738702              0.193283   \n",
       "3           NaN               0.148939             -0.335704   \n",
       "4           NaN               0.520935             -0.405512   \n",
       "5           NaN               0.787985             -0.899702   \n",
       "6           1.0              -1.528013              1.307144   \n",
       "7           NaN               0.401307              0.924946   \n",
       "8           NaN              -1.643650             -0.351817   \n",
       "9           NaN               0.030526             -0.429030   \n",
       "10          NaN              -1.567742                   NaN   \n",
       "11          NaN               0.304132             -1.081224   \n",
       "12          NaN               0.718627              0.304764   \n",
       "13          NaN              -2.051721                   NaN   \n",
       "14          NaN               0.869632              1.848175   \n",
       "15          NaN               0.754415              3.141453   \n",
       "16          NaN               0.599960             -0.382726   \n",
       "17          NaN               1.394079             -0.854832   \n",
       "18          NaN               0.528268             -0.753294   \n",
       "19          NaN               0.713850             -0.352273   \n",
       "20          NaN               0.159527              0.790240   \n",
       "21          NaN              -0.944828              2.362543   \n",
       "22          NaN               0.108266              0.748233   \n",
       "23          NaN              -1.302348             -1.306499   \n",
       "24          NaN               0.673956             -1.609613   \n",
       "25          NaN              -0.364606             -1.002015   \n",
       "26          NaN               0.939354              0.690521   \n",
       "27          NaN               1.012146              0.301031   \n",
       "28          NaN              -2.139314                   NaN   \n",
       "29          NaN               0.083620              0.236016   \n",
       "30          NaN              -0.002800             -0.322720   \n",
       "31          NaN               1.033250             -0.700989   \n",
       "32          NaN              -0.346470             -0.548468   \n",
       "33          NaN              -0.312898              0.017961   \n",
       "34          NaN              -0.538390              1.169995   \n",
       "35          NaN               0.693784             -0.276639   \n",
       "36          NaN               0.562384             -0.818981   \n",
       "37          NaN              -0.675627             -0.098637   \n",
       "38          NaN               0.169874             -0.473614   \n",
       "39          NaN              -0.706322              2.406481   \n",
       "40          NaN              -1.689706                   NaN   \n",
       "41          NaN              -1.321499              1.196380   \n",
       "42          NaN               0.636712              0.318101   \n",
       "43          NaN               0.331775             -1.126349   \n",
       "44          NaN               0.508680             -0.279612   \n",
       "45          NaN              -1.083495                   NaN   \n",
       "46          NaN               1.010074             -0.666712   \n",
       "47          NaN              -1.582417                   NaN   \n",
       "48          NaN               1.054986             -1.406466   \n",
       "49          NaN               0.818281             -0.330714   \n",
       "\n",
       "    sleep1duration_std  sleep1efficiency_std  sleep1light1ratio_std  \\\n",
       "0            -1.084652             -0.657884              -1.243605   \n",
       "1            -0.025375             -0.016948               1.158586   \n",
       "2            -0.242827              0.641129               0.153160   \n",
       "3             1.793282             -0.710683               0.998823   \n",
       "4            -0.428177              0.384287               0.437003   \n",
       "5            -0.727630              0.075334              -0.644706   \n",
       "6            -0.045982             -0.665676               0.211155   \n",
       "7            -0.508278             -0.486091              -0.056428   \n",
       "8            -0.304886             -0.329054               0.261162   \n",
       "9            -0.370144             -0.344619              -0.613454   \n",
       "10                 NaN                   NaN                    NaN   \n",
       "11            0.318683              0.257709              -0.364076   \n",
       "12           -0.434615             -0.597681               1.100467   \n",
       "13                 NaN                   NaN                    NaN   \n",
       "14            1.846673              1.561738               0.087000   \n",
       "15           -1.242833             -0.766165              -1.070808   \n",
       "16            0.258374             -0.123261               0.051064   \n",
       "17            0.342750             -0.113195              -0.248109   \n",
       "18            0.061798             -0.592451              -0.731648   \n",
       "19           -0.339802             -0.006531              -0.290834   \n",
       "20            0.255417              1.064155               2.114400   \n",
       "21           -0.376758             -0.451127              -1.526418   \n",
       "22            0.097179              0.297324               1.257865   \n",
       "23           -1.211251              0.425398              -1.833194   \n",
       "24            0.736545             -0.057808              -2.108681   \n",
       "25            1.329412              0.059147              -0.816554   \n",
       "26            0.485681             -0.148975               0.545386   \n",
       "27            2.926055             -0.525826               4.008535   \n",
       "28                 NaN                   NaN                    NaN   \n",
       "29            0.182987              1.598573               0.871357   \n",
       "30           -0.375193             -0.540108              -0.663327   \n",
       "31           -0.834191             -0.911859              -1.188731   \n",
       "32           -1.234428             -0.395797              -0.349346   \n",
       "33           -0.456245             -0.484487               0.178148   \n",
       "34           -0.820252              0.805081              -0.675846   \n",
       "35           -0.052394             -0.839107               0.969029   \n",
       "36           -0.237961             -0.074079              -0.401512   \n",
       "37           -0.360327              0.413465              -0.245961   \n",
       "38           -1.285053             -0.668939              -0.397803   \n",
       "39           -0.915685              0.606128               0.021528   \n",
       "40            2.300651              1.417464                    NaN   \n",
       "41           -1.067418             -0.401473              -1.335114   \n",
       "42            0.629926             -0.609778               0.366338   \n",
       "43           -0.754964              0.511441               0.830542   \n",
       "44           -0.058920             -0.100642               0.474993   \n",
       "45           -2.553600             -2.518787                    NaN   \n",
       "46           -0.252970             -0.387198               0.323175   \n",
       "47           -1.307644             -1.447475                    NaN   \n",
       "48           -0.483478             -0.776314              -1.137057   \n",
       "49            0.381471             -0.564627              -0.476262   \n",
       "\n",
       "    sleep1rem1ratio_std  sleep1wake1ratio_std  spo2_std  step1goal_std  \\\n",
       "0             -0.989510             -1.625158       NaN       0.201760   \n",
       "1             -0.034989              0.279885       NaN      -1.302687   \n",
       "2              0.393982              0.528802       NaN       0.490771   \n",
       "3             -0.294606              0.089293       NaN       0.244005   \n",
       "4             -0.493298              1.003447  0.044596       0.333690   \n",
       "5             -0.057683             -1.652670 -1.661116      -0.421233   \n",
       "6             -0.333110              1.196886 -1.195173      -0.607263   \n",
       "7             -1.139519             -0.328717       NaN       0.540716   \n",
       "8             -0.935762              0.565254 -0.110956      -0.142904   \n",
       "9             -1.142494             -0.759035       NaN      -0.677390   \n",
       "10                  NaN                   NaN       NaN       0.297685   \n",
       "11             1.040523             -0.730425       NaN      -0.491350   \n",
       "12            -0.752231              2.105718       NaN      -0.264245   \n",
       "13                  NaN                   NaN       NaN      -1.145553   \n",
       "14             0.365153             -0.694702  0.198045      -0.313858   \n",
       "15             0.126637             -0.972826       NaN       1.215408   \n",
       "16            -0.656013              0.448809  0.487244      -0.353602   \n",
       "17             0.682759             -0.733498 -1.187736      -0.459021   \n",
       "18            -0.558861             -0.502527       NaN       0.237477   \n",
       "19            -0.599625              0.345982       NaN      -0.777986   \n",
       "20             0.315313              0.567956       NaN      -0.311447   \n",
       "21            -0.620285             -0.933058       NaN      -1.302687   \n",
       "22            -0.129289              0.336330       NaN      -0.140194   \n",
       "23            -0.245141             -1.896983  0.872578      -0.436849   \n",
       "24            -0.566114              1.730253       NaN       1.401880   \n",
       "25            -0.592410             -1.395064 -0.210728       1.143920   \n",
       "26             0.087144              1.289334       NaN      -1.302687   \n",
       "27             0.475927             -0.008953  1.272368       0.182093   \n",
       "28                  NaN                   NaN       NaN            NaN   \n",
       "29             1.706525             -0.336533  0.669945       0.301059   \n",
       "30            -0.719623             -0.335300       NaN      -1.302687   \n",
       "31            -0.454709             -0.626287  0.888140       0.156737   \n",
       "32            -0.602358             -0.776549  0.227184       0.882980   \n",
       "33            -0.820419             -0.412416  0.273092       0.504060   \n",
       "34             4.246794             -0.437529       NaN       1.584278   \n",
       "35             1.000406             -0.432717       NaN      -0.489710   \n",
       "36            -0.127584             -0.851287       NaN      -0.967675   \n",
       "37            -0.215408             -0.189911       NaN      -0.717082   \n",
       "38             2.999144             -1.652563       NaN      -0.822515   \n",
       "39             0.313615             -0.306073 -0.739442       1.904484   \n",
       "40                  NaN                   NaN       NaN            NaN   \n",
       "41            -1.125916             -0.139678       NaN            NaN   \n",
       "42             0.277168              1.111446  2.633424       1.170370   \n",
       "43             0.674515              0.608256 -1.575647      -0.547774   \n",
       "44             0.289503             -0.281901       NaN            NaN   \n",
       "45                  NaN                   NaN       NaN       1.124703   \n",
       "46            -0.530229              0.305775       NaN       0.229866   \n",
       "47                  NaN                   NaN       NaN      -0.412381   \n",
       "48            -1.202084             -1.280829 -0.737699       0.690374   \n",
       "49            -0.298605             -0.047254       NaN      -1.302687   \n",
       "\n",
       "    step1goal1label_std  steps_std  very1active1minutes_std   age_min  \\\n",
       "0              0.209111  -0.813912                -0.412437 -0.915335   \n",
       "1             -1.306226   1.010380                 0.024112  1.092496   \n",
       "2              0.509059  -1.354461                -1.205466 -0.915335   \n",
       "3              0.419955  -0.388223                 1.881868  1.092496   \n",
       "4              0.295729  -0.251209                -0.857851  1.092496   \n",
       "5             -0.360111   1.931164                 2.847318  1.092496   \n",
       "6             -0.473457  -1.554448                -0.838617  1.092496   \n",
       "7              0.195930  -0.459592                 0.513322  1.092496   \n",
       "8             -0.007300   0.405170                 1.026600 -0.915335   \n",
       "9             -0.635201  -1.168156                -0.684989 -0.915335   \n",
       "10             0.652858   0.316314                 0.059651  1.092496   \n",
       "11            -0.326684   0.809801                -0.848682 -0.915335   \n",
       "12            -0.164551  -0.415036                -0.438163 -0.915335   \n",
       "13            -1.053309        NaN                -1.329909 -0.915335   \n",
       "14            -0.164041   0.306394                 4.260040 -0.915335   \n",
       "15             0.983462   1.923055                 0.532474 -0.915335   \n",
       "16            -0.287643  -0.571134                -1.126197  1.092496   \n",
       "17            -0.400639   0.158754                 0.890117  1.092496   \n",
       "18             0.349895   0.610926                 0.638002 -0.915335   \n",
       "19            -0.601744  -1.224918                -0.903789 -0.915335   \n",
       "20            -0.312371  -1.425419                 0.088462  1.092496   \n",
       "21            -1.306226  -1.061263                -0.661337  1.092496   \n",
       "22            -0.557786  -1.127802                 0.673616  1.092496   \n",
       "23            -0.332949  -0.985771                -0.293174 -0.915335   \n",
       "24             1.964845   0.489686                -0.842167  1.092496   \n",
       "25             1.134364   2.289609                 0.861868  1.092496   \n",
       "26            -1.306226  -1.720687                -0.618293  1.092496   \n",
       "27            -0.350291   2.670027                 1.038890 -0.915335   \n",
       "28                  NaN  -0.261391                -1.513220 -0.915335   \n",
       "29            -0.273698   0.140821                -0.290152 -0.915335   \n",
       "30            -1.306226  -0.381627                -0.006614  1.092496   \n",
       "31             0.285224  -0.609360                -0.220979 -0.915335   \n",
       "32             0.730266   1.048951                 1.044961 -0.915335   \n",
       "33            -0.019720  -0.621270                -0.842285  1.092496   \n",
       "34             1.515918  -0.165638                 0.598224  1.092496   \n",
       "35            -0.433581  -0.295430                 0.472391 -0.915335   \n",
       "36            -0.767006  -0.835343                -0.774393  1.092496   \n",
       "37            -0.363661  -0.136895                -0.731503       NaN   \n",
       "38            -0.790983  -1.097443                -1.218581  1.092496   \n",
       "39             2.823885   1.263980                 0.056770 -0.915335   \n",
       "40                  NaN  -0.276260                -0.938321 -0.915335   \n",
       "41                  NaN  -0.799305                 0.339733       NaN   \n",
       "42             1.478375  -0.488209                -0.637897  1.092496   \n",
       "43            -0.230786  -0.324458                 0.256783  1.092496   \n",
       "44                  NaN  -0.442651                -0.474871       NaN   \n",
       "45             0.667644   1.388882                -0.652821 -0.915335   \n",
       "46             0.199416  -0.207269                 0.562402 -0.915335   \n",
       "47            -0.543653   0.262059                 0.538321  1.092496   \n",
       "48             0.374101   0.919878                 0.554224 -0.915335   \n",
       "49            -1.306226  -0.469779                -0.049303 -0.915335   \n",
       "\n",
       "     bmi_min   bpm_min  calories_min  daily1temperature1variation_min  \\\n",
       "0  -1.487354 -0.317875      1.374707                         0.186440   \n",
       "1  -1.487354  1.002774     -1.312867                        -0.253652   \n",
       "2  -0.004362  0.921982      0.245818                        -0.260055   \n",
       "3   0.292237  0.047765      1.057597                        -1.858666   \n",
       "4   0.588835 -1.011274      0.920193                        -0.644241   \n",
       "5   0.292237 -2.160409     -0.279987                        -0.352786   \n",
       "6  -0.894157 -0.326458      0.693867                        -0.662340   \n",
       "7  -0.300960 -0.257564      0.595134                        -2.404135   \n",
       "8   0.292237  1.008707      1.784754                         0.657767   \n",
       "9   0.588835  0.850237     -0.234992                        -0.967784   \n",
       "10 -0.004362  1.198977      0.554551                              NaN   \n",
       "11 -1.190755 -0.498555     -1.312787                        -0.360994   \n",
       "12 -0.894157 -0.583472      0.629202                         0.549091   \n",
       "13  2.071827  3.255366      0.052335                              NaN   \n",
       "14  2.071827 -1.693723      0.495605                        -2.250456   \n",
       "15 -0.894157 -0.660397      1.139845                        -0.580946   \n",
       "16 -0.894157 -0.206013     -1.312835                        -0.343603   \n",
       "17  0.588835 -1.588401      1.238386                        -0.438052   \n",
       "18  0.292237 -1.871168      0.500733                        -0.056706   \n",
       "19 -1.190755  0.895593     -1.312596                        -0.673340   \n",
       "20  1.182032  0.739424      1.392132                         1.521959   \n",
       "21  2.071827  1.197285      1.295135                         1.047553   \n",
       "22 -0.004362  2.109814      2.779153                         1.375503   \n",
       "23 -0.300960 -0.420835      1.343617                         1.498756   \n",
       "24 -0.300960 -0.158351     -1.312548                         0.123963   \n",
       "25  2.071827  0.162529      0.537191                         1.355326   \n",
       "26 -0.597559  1.249784      0.470440                        -0.557842   \n",
       "27 -0.597559 -0.480800      0.730436                         1.542219   \n",
       "28 -0.597559  1.487246     -1.312484                              NaN   \n",
       "29 -0.597559 -0.529574     -1.312054                        -0.894542   \n",
       "30 -1.190755  0.336936      0.451423                        -0.082986   \n",
       "31 -0.597559  0.004686      0.321043                         0.329140   \n",
       "32 -0.894157 -0.875858     -0.031777                        -0.087859   \n",
       "33 -0.300960  0.292688      0.817494                         0.696510   \n",
       "34  0.885434  0.122592     -0.283761                        -0.686331   \n",
       "35  0.885434 -0.270230      0.583730                         1.210250   \n",
       "36  0.885434 -0.300030      0.204424                         1.055416   \n",
       "37       NaN  0.576415     -1.312787                        -0.638072   \n",
       "38  0.292237  0.433167      0.972641                         1.015186   \n",
       "39  0.292237 -0.662581     -0.236776                        -1.187044   \n",
       "40 -0.004362 -0.821914      0.094860                              NaN   \n",
       "41       NaN  0.038595      0.092837                              NaN   \n",
       "42  0.292237  0.907388      0.920033                        -0.060876   \n",
       "43  0.885434 -0.738437      0.398959                        -0.171040   \n",
       "44       NaN -0.297605     -1.311927                         0.547241   \n",
       "45  0.292237  1.239507      0.297502                              NaN   \n",
       "46 -0.894157  0.772179     -0.114726                         1.242476   \n",
       "47  0.292237 -0.551225      1.140354                              NaN   \n",
       "48  0.588835  0.332990      0.360462                        -0.415615   \n",
       "49 -0.004362  0.933220     -0.302205                        -0.206311   \n",
       "\n",
       "    distance_min  filteredDemographicVO2Max_min  \\\n",
       "0       0.486070                       2.498475   \n",
       "1       1.436865                      -0.213089   \n",
       "2      -0.331386                      -1.377621   \n",
       "3      -0.529279                       0.695245   \n",
       "4      -0.501529                      -0.851700   \n",
       "5      -0.516388                       1.255798   \n",
       "6      -0.263289                      -0.385855   \n",
       "7      -0.113909                       0.680233   \n",
       "8      -0.088619                      -0.136862   \n",
       "9       0.661527                      -2.206836   \n",
       "10     -0.488736                       0.748887   \n",
       "11     -0.529279                      -0.127633   \n",
       "12     -0.521309                      -0.107948   \n",
       "13      3.509680                      -1.248798   \n",
       "14     -0.529279                      -0.750739   \n",
       "15      0.434111                       2.193615   \n",
       "16     -0.529279                      -0.844672   \n",
       "17     -0.344080                       0.389156   \n",
       "18     -0.529279                       1.420113   \n",
       "19     -0.412275                       1.562051   \n",
       "20      1.364832                      -0.096105   \n",
       "21      3.885098                      -0.463295   \n",
       "22      4.128356                       0.444591   \n",
       "23     -0.046600                       1.459988   \n",
       "24     -0.529279                      -0.467929   \n",
       "25     -0.529279                      -2.041381   \n",
       "26     -0.420443                      -0.753365   \n",
       "27      0.265346                       1.203263   \n",
       "28     -0.494837                       1.152979   \n",
       "29     -0.529279                       1.037752   \n",
       "30      0.147554                      -0.197321   \n",
       "31     -0.091079                       1.314975   \n",
       "32     -0.379112                       0.526971   \n",
       "33      0.478984                       0.785457   \n",
       "34      0.005161                      -0.173271   \n",
       "35     -0.034201                       0.396336   \n",
       "36     -0.416507                       0.165212   \n",
       "37     -0.525442                      -1.377621   \n",
       "38      0.574143                      -1.088094   \n",
       "39     -0.529279                       0.811672   \n",
       "40     -0.529279                      -1.356563   \n",
       "41     -0.152091                      -0.712080   \n",
       "42     -0.525048                       0.004008   \n",
       "43      0.801066                       0.658685   \n",
       "44     -0.529279                       0.165212   \n",
       "45     -0.358546                      -0.927412   \n",
       "46      3.002006                       1.211963   \n",
       "47     -0.322726                       0.783801   \n",
       "48      0.798999                      -1.059579   \n",
       "49     -0.180825                      -1.283047   \n",
       "\n",
       "    full1sleep1breathing1rate_min  gender_min  lightly1active1minutes_min  \\\n",
       "0                        0.805308   -0.801784                   -0.170125   \n",
       "1                        0.775686    1.247219                   -0.170125   \n",
       "2                       -1.268240    1.247219                   -0.170125   \n",
       "3                       -1.268240   -0.801784                   -0.170125   \n",
       "4                        0.331354    1.247219                   -0.170125   \n",
       "5                             NaN   -0.801784                   -0.170125   \n",
       "6                        0.627576    1.247219                   -0.170125   \n",
       "7                             NaN   -0.801784                   -0.170125   \n",
       "8                        0.479465   -0.801784                    6.098663   \n",
       "9                             NaN    1.247219                   -0.170125   \n",
       "10                            NaN   -0.801784                   -0.170125   \n",
       "11                            NaN    1.247219                   -0.170125   \n",
       "12                      -1.268240    1.247219                   -0.170125   \n",
       "13                            NaN   -0.801784                   -0.170125   \n",
       "14                      -1.268240   -0.801784                   -0.170125   \n",
       "15                       0.597954   -0.801784                   -0.170125   \n",
       "16                       0.953419    1.247219                   -0.170125   \n",
       "17                       1.101530   -0.801784                   -0.170125   \n",
       "18                            NaN   -0.801784                   -0.170125   \n",
       "19                       1.012663   -0.801784                   -0.170125   \n",
       "20                            NaN   -0.801784                   -0.170125   \n",
       "21                       0.716442   -0.801784                   -0.170125   \n",
       "22                       0.686820   -0.801784                   -0.170125   \n",
       "23                       0.657198   -0.801784                    5.639971   \n",
       "24                            NaN    1.247219                   -0.170125   \n",
       "25                      -1.268240   -0.801784                   -0.170125   \n",
       "26                            NaN    1.247219                   -0.170125   \n",
       "27                       1.160774   -0.801784                   -0.170125   \n",
       "28                            NaN   -0.801784                   -0.170125   \n",
       "29                      -1.268240   -0.801784                   -0.170125   \n",
       "30                            NaN    1.247219                   -0.170125   \n",
       "31                       0.183244   -0.801784                   -0.170125   \n",
       "32                      -1.268240    1.247219                   -0.170125   \n",
       "33                       0.568331   -0.801784                   -0.170125   \n",
       "34                      -1.268240   -0.801784                   -0.170125   \n",
       "35                            NaN   -0.801784                   -0.170125   \n",
       "36                      -1.268240   -0.801784                   -0.170125   \n",
       "37                      -1.268240         NaN                   -0.170125   \n",
       "38                            NaN    1.247219                   -0.170125   \n",
       "39                       1.190396   -0.801784                   -0.170125   \n",
       "40                            NaN    1.247219                   -0.170125   \n",
       "41                       1.071908         NaN                   -0.170125   \n",
       "42                      -1.268240   -0.801784                   -0.170125   \n",
       "43                      -1.268240   -0.801784                   -0.170125   \n",
       "44                      -1.268240   -0.801784                   -0.170125   \n",
       "45                            NaN    1.247219                   -0.170125   \n",
       "46                            NaN   -0.801784                   -0.170125   \n",
       "47                            NaN   -0.801784                   -0.170125   \n",
       "48                       0.716442    1.247219                   -0.170125   \n",
       "49                       0.983041    1.247219                   -0.170125   \n",
       "\n",
       "    max1goal_min  min1goal_min  mindfulness1session_min  \\\n",
       "0       0.066683     -0.170357                      0.0   \n",
       "1       2.608968      2.348858                      0.0   \n",
       "2      -0.696003     -0.800161                      0.0   \n",
       "3      -0.696003     -0.800161                      0.0   \n",
       "4       0.066683     -0.170357                      0.0   \n",
       "5      -0.696003     -0.800161                      0.0   \n",
       "6       0.066683     -0.170357                      0.0   \n",
       "7       0.066683     -0.170357                      0.0   \n",
       "8       0.066683     -0.170357                      0.0   \n",
       "9      -0.696003     -0.800161                      0.0   \n",
       "10     -0.696003     -0.800161                      0.0   \n",
       "11     -1.204459     -0.800161                      0.0   \n",
       "12      0.066683     -0.170357                      0.0   \n",
       "13     -1.204459     -0.800161                      0.0   \n",
       "14     -0.696003     -0.800161                      0.0   \n",
       "15     -0.696003     -0.800161                      0.0   \n",
       "16     -0.696003     -0.800161                      0.0   \n",
       "17     -0.696003     -0.800161                      0.0   \n",
       "18     -0.696003     -0.800161                      0.0   \n",
       "19      0.066683     -0.170357                      0.0   \n",
       "20      0.829368      0.774349                      0.0   \n",
       "21      1.337825      1.719055                      0.0   \n",
       "22      1.337825      1.719055                      0.0   \n",
       "23      0.066683     -0.170357                      0.0   \n",
       "24     -1.204459     -0.800161                      0.0   \n",
       "25     -1.204459     -0.800161                      0.0   \n",
       "26     -1.204459     -0.800161                      0.0   \n",
       "27      1.337825      1.719055                      0.0   \n",
       "28      0.066683     -0.170357                      0.0   \n",
       "29      1.337825      1.719055                      0.0   \n",
       "30      1.337825      1.719055                      0.0   \n",
       "31      0.066683     -0.170357                      0.0   \n",
       "32      0.066683     -0.170357                      0.0   \n",
       "33      0.066683     -0.170357                      0.0   \n",
       "34     -0.696003     -0.800161                      0.0   \n",
       "35     -0.696003     -0.800161                      0.0   \n",
       "36     -1.204459     -0.800161                      0.0   \n",
       "37      0.829368      0.774349                      0.0   \n",
       "38      0.066683     -0.170357                      0.0   \n",
       "39     -1.204459     -0.800161                      0.0   \n",
       "40     -0.696003     -0.800161                      0.0   \n",
       "41      2.608968      2.348858                      0.0   \n",
       "42     -1.204459     -0.800161                      0.0   \n",
       "43      0.066683     -0.170357                      0.0   \n",
       "44     -0.696003     -0.800161                      0.0   \n",
       "45      0.066683     -0.170357                      0.0   \n",
       "46      0.066683     -0.170357                      0.0   \n",
       "47      0.829368      0.774349                      0.0   \n",
       "48     -1.204459     -0.800161                      0.0   \n",
       "49      2.608968      2.348858                      0.0   \n",
       "\n",
       "    minutesAfterWakeup_min  minutesAsleep_min  minutesAwake_min  \\\n",
       "0                -0.121268           1.272309          2.015456   \n",
       "1                -0.121268           0.909886         -0.231175   \n",
       "2                -0.121268           0.816691          0.143263   \n",
       "3                -0.121268          -0.498389         -0.830277   \n",
       "4                -0.121268           0.112554          0.143263   \n",
       "5                -0.121268          -0.788327          0.367927   \n",
       "6                -0.121268          -0.125610         -0.680501   \n",
       "7                -0.121268          -0.695133         -0.755389   \n",
       "8                -0.121268           0.361073         -0.156287   \n",
       "9                -0.121268          -0.343064         -0.306063   \n",
       "10               -0.121268          -0.798682         -0.830277   \n",
       "11               -0.121268          -0.746908         -0.680501   \n",
       "12               -0.121268          -0.871167         -0.605614   \n",
       "13                     NaN                NaN               NaN   \n",
       "14               -0.121268          -0.943652         -0.830277   \n",
       "15               -0.121268           1.986801          1.715906   \n",
       "16               -0.121268          -0.891877         -0.830277   \n",
       "17               -0.121268          -0.405194         -0.755389   \n",
       "18               -0.121268          -0.767617         -0.605614   \n",
       "19               -0.121268           0.485332          0.817253   \n",
       "20               -0.121268          -0.788327         -0.530726   \n",
       "21               -0.121268           2.111061          2.464783   \n",
       "22               -0.121268           0.185039          0.517702   \n",
       "23               -0.121268           0.371428          2.165232   \n",
       "24               -0.121268          -0.591583         -0.605614   \n",
       "25               -0.121268          -0.736553         -0.830277   \n",
       "26               -0.121268          -0.777972         -0.680501   \n",
       "27               -0.121268           1.479408          2.090344   \n",
       "28                8.246211          -0.053125          0.367927   \n",
       "29               -0.121268          -0.042770         -0.830277   \n",
       "30               -0.121268          -0.809037         -0.680501   \n",
       "31               -0.121268           2.017866          1.715906   \n",
       "32               -0.121268          -1.513174         -0.830277   \n",
       "33               -0.121268          -0.094545         -0.380951   \n",
       "34               -0.121268           1.116985         -0.306063   \n",
       "35               -0.121268           0.247168          0.667477   \n",
       "36               -0.121268           0.102199         -0.306063   \n",
       "37               -0.121268           0.050424         -0.755389   \n",
       "38               -0.121268           1.531183          1.790793   \n",
       "39               -0.121268           0.827046         -0.830277   \n",
       "40               -0.121268           0.340363         -0.755389   \n",
       "41               -0.121268           2.214610          1.116804   \n",
       "42               -0.121268          -0.850457         -0.830277   \n",
       "43               -0.121268           0.506042          0.967028   \n",
       "44               -0.121268          -0.746908         -0.306063   \n",
       "45               -0.121268          -0.850457         -0.605614   \n",
       "46               -0.121268          -0.332709         -0.755389   \n",
       "47               -0.121268          -0.871167         -0.605614   \n",
       "48               -0.121268          -0.353419         -0.006512   \n",
       "49               -0.121268          -0.912587         -0.680501   \n",
       "\n",
       "    minutesToFallAsleep_min  minutes1below1default1zone11_min  \\\n",
       "0                       0.0                          2.907459   \n",
       "1                       0.0                          0.551496   \n",
       "2                       0.0                          0.137309   \n",
       "3                       0.0                          0.226382   \n",
       "4                       0.0                         -0.793497   \n",
       "5                       0.0                          0.230835   \n",
       "6                       0.0                          0.658383   \n",
       "7                       0.0                         -0.686610   \n",
       "8                       0.0                          0.970136   \n",
       "9                       0.0                          1.473395   \n",
       "10                      0.0                         -0.642074   \n",
       "11                      0.0                         -0.820218   \n",
       "12                      0.0                         -0.802404   \n",
       "13                      NaN                          0.066052   \n",
       "14                      0.0                         -0.820218   \n",
       "15                      0.0                          1.268528   \n",
       "16                      0.0                         -0.704424   \n",
       "17                      0.0                         -0.575269   \n",
       "18                      0.0                         -0.820218   \n",
       "19                      0.0                         -0.726692   \n",
       "20                      0.0                          1.050301   \n",
       "21                      0.0                          1.638178   \n",
       "22                      0.0                          1.237353   \n",
       "23                      0.0                          2.056818   \n",
       "24                      0.0                         -0.824672   \n",
       "25                      0.0                         -0.829126   \n",
       "26                      0.0                         -0.553001   \n",
       "27                      0.0                          1.161641   \n",
       "28                      0.0                         -0.530733   \n",
       "29                      0.0                         -0.606445   \n",
       "30                      0.0                          1.802962   \n",
       "31                      0.0                          0.738548   \n",
       "32                      0.0                         -0.138815   \n",
       "33                      0.0                          1.540199   \n",
       "34                      0.0                         -0.468383   \n",
       "35                      0.0                          2.315128   \n",
       "36                      0.0                         -0.370403   \n",
       "37                      0.0                         -0.080918   \n",
       "38                      0.0                          1.949932   \n",
       "39                      0.0                         -0.744507   \n",
       "40                      0.0                         -0.815765   \n",
       "41                      0.0                          2.373025   \n",
       "42                      0.0                         -0.815765   \n",
       "43                      0.0                          0.408980   \n",
       "44                      0.0                         -0.370403   \n",
       "45                      0.0                         -0.664342   \n",
       "46                      0.0                          0.907785   \n",
       "47                      0.0                         -0.539640   \n",
       "48                      0.0                          0.716280   \n",
       "49                      0.0                         -0.789043   \n",
       "\n",
       "    minutes1in1default1zone11_min  minutes1in1default1zone12_min  \\\n",
       "0                       -0.294897                            0.0   \n",
       "1                        0.890923                            0.0   \n",
       "2                       -0.029415                            0.0   \n",
       "3                       -0.365692                            0.0   \n",
       "4                       -0.365692                            0.0   \n",
       "5                       -0.365692                            0.0   \n",
       "6                       -0.100210                            0.0   \n",
       "7                        0.059079                            0.0   \n",
       "8                        0.076778                            0.0   \n",
       "9                       -0.330294                            0.0   \n",
       "10                      -0.365692                            0.0   \n",
       "11                      -0.365692                            0.0   \n",
       "12                      -0.365692                            0.0   \n",
       "13                       4.713863                            0.0   \n",
       "14                      -0.365692                            0.0   \n",
       "15                      -0.365692                            0.0   \n",
       "16                      -0.365692                            0.0   \n",
       "17                      -0.365692                            0.0   \n",
       "18                      -0.365692                            0.0   \n",
       "19                      -0.365692                            0.0   \n",
       "20                       2.059043                            0.0   \n",
       "21                       2.076742                            0.0   \n",
       "22                       5.244827                            0.0   \n",
       "23                      -0.365692                            0.0   \n",
       "24                      -0.365692                            0.0   \n",
       "25                      -0.365692                            0.0   \n",
       "26                       1.846658                            0.0   \n",
       "27                      -0.330294                            0.0   \n",
       "28                      -0.365692                            0.0   \n",
       "29                      -0.365692                            0.0   \n",
       "30                      -0.082511                            0.0   \n",
       "31                      -0.347993                            0.0   \n",
       "32                      -0.365692                            0.0   \n",
       "33                       0.784730                            0.0   \n",
       "34                      -0.171005                            0.0   \n",
       "35                      -0.365692                            0.0   \n",
       "36                      -0.365692                            0.0   \n",
       "37                      -0.365692                            0.0   \n",
       "38                       0.448453                            0.0   \n",
       "39                      -0.347993                            0.0   \n",
       "40                      -0.365692                            0.0   \n",
       "41                       0.112175                            0.0   \n",
       "42                      -0.365692                            0.0   \n",
       "43                      -0.171005                            0.0   \n",
       "44                      -0.365692                            0.0   \n",
       "45                      -0.365692                            0.0   \n",
       "46                      -0.153306                            0.0   \n",
       "47                      -0.365692                            0.0   \n",
       "48                      -0.330294                            0.0   \n",
       "49                       0.165272                            0.0   \n",
       "\n",
       "    minutes1in1default1zone13_min  moderately1active1minutes_min  \\\n",
       "0                             0.0                            0.0   \n",
       "1                             0.0                            0.0   \n",
       "2                             0.0                            0.0   \n",
       "3                             0.0                            0.0   \n",
       "4                             0.0                            0.0   \n",
       "5                             0.0                            0.0   \n",
       "6                             0.0                            0.0   \n",
       "7                             0.0                            0.0   \n",
       "8                             0.0                            0.0   \n",
       "9                             0.0                            0.0   \n",
       "10                            0.0                            0.0   \n",
       "11                            0.0                            0.0   \n",
       "12                            0.0                            0.0   \n",
       "13                            0.0                            0.0   \n",
       "14                            0.0                            0.0   \n",
       "15                            0.0                            0.0   \n",
       "16                            0.0                            0.0   \n",
       "17                            0.0                            0.0   \n",
       "18                            0.0                            0.0   \n",
       "19                            0.0                            0.0   \n",
       "20                            0.0                            0.0   \n",
       "21                            0.0                            0.0   \n",
       "22                            0.0                            0.0   \n",
       "23                            0.0                            0.0   \n",
       "24                            0.0                            0.0   \n",
       "25                            0.0                            0.0   \n",
       "26                            0.0                            0.0   \n",
       "27                            0.0                            0.0   \n",
       "28                            0.0                            0.0   \n",
       "29                            0.0                            0.0   \n",
       "30                            0.0                            0.0   \n",
       "31                            0.0                            0.0   \n",
       "32                            0.0                            0.0   \n",
       "33                            0.0                            0.0   \n",
       "34                            0.0                            0.0   \n",
       "35                            0.0                            0.0   \n",
       "36                            0.0                            0.0   \n",
       "37                            0.0                            0.0   \n",
       "38                            0.0                            0.0   \n",
       "39                            0.0                            0.0   \n",
       "40                            0.0                            0.0   \n",
       "41                            0.0                            0.0   \n",
       "42                            0.0                            0.0   \n",
       "43                            0.0                            0.0   \n",
       "44                            0.0                            0.0   \n",
       "45                            0.0                            0.0   \n",
       "46                            0.0                            0.0   \n",
       "47                            0.0                            0.0   \n",
       "48                            0.0                            0.0   \n",
       "49                            0.0                            0.0   \n",
       "\n",
       "    nightly1temperature_min  nremhr_min  resting1hr_min  rmssd_min  \\\n",
       "0                  1.000823    0.498386       -0.778488   3.892913   \n",
       "1                 -2.567073   -1.570056        0.313390  -0.426218   \n",
       "2                  0.570136    0.861285        0.970284  -1.102324   \n",
       "3                 -1.618347   -1.570056       -0.012726  -0.267625   \n",
       "4                 -1.770505    0.568598       -0.696740  -0.254918   \n",
       "5                 -0.380473         NaN       -1.784169        NaN   \n",
       "6                 -0.109511    0.604479        0.008364  -0.353026   \n",
       "7                 -1.319015         NaN        0.095775        NaN   \n",
       "8                  1.009076   -1.570056        1.547150  -0.337453   \n",
       "9                  0.796895         NaN        1.027466        NaN   \n",
       "10                      NaN         NaN        1.001538        NaN   \n",
       "11                -0.814261         NaN       -0.400439        NaN   \n",
       "12                 0.319649    0.487419       -0.492131  -0.161917   \n",
       "13                      NaN         NaN        2.681800        NaN   \n",
       "14                -2.424576   -1.570056       -1.358049  -1.102324   \n",
       "15                 0.756839    0.529777       -0.911860   1.501812   \n",
       "16                 0.591786   -1.570056        0.955538   0.242289   \n",
       "17                 0.431260    0.299672       -1.424180  -0.066114   \n",
       "18                -0.346103         NaN       -2.518263        NaN   \n",
       "19                 0.289070    0.879564        0.684895  -1.102324   \n",
       "20                 0.307831         NaN        0.223367        NaN   \n",
       "21                 0.246474    0.877299        0.653811   1.006661   \n",
       "22                -0.219640    0.894663        1.179572   0.173270   \n",
       "23                 0.450789    0.434691       -0.501786   0.498741   \n",
       "24                 0.462185         NaN       -0.269783        NaN   \n",
       "25                -0.134587    1.079152        0.901591   0.449344   \n",
       "26                 1.008025         NaN        0.653400        NaN   \n",
       "27                 0.745711    0.452651       -0.398172  -1.102324   \n",
       "28                      NaN         NaN        1.403459        NaN   \n",
       "29                -0.169988   -1.570056        0.364817  -1.102324   \n",
       "30                 0.788350         NaN        0.164844        NaN   \n",
       "31                 0.446906    0.388599       -0.426231   0.692901   \n",
       "32                 0.920213   -1.570056       -2.015558   1.056432   \n",
       "33                 0.558197    0.456108       -0.110392   1.700582   \n",
       "34                 0.910330   -1.570056        0.031584  -0.327238   \n",
       "35                 0.751488         NaN       -1.203914        NaN   \n",
       "36                 0.372380    0.738266        0.515734   0.155642   \n",
       "37                 0.374666    0.849842        0.637868  -1.102324   \n",
       "38                 0.922610         NaN        0.135880        NaN   \n",
       "39                -2.221235    0.737869        0.528496   0.559474   \n",
       "40                -2.436018         NaN        1.819886        NaN   \n",
       "41                      NaN    0.543764       -0.466576  -0.450698   \n",
       "42                 0.858644   -1.570056        1.502586  -1.102324   \n",
       "43                 0.533266    0.090706       -1.926848   1.664827   \n",
       "44                -0.179821    0.601498       -0.161261  -0.399371   \n",
       "45                      NaN         NaN        0.362748        NaN   \n",
       "46                 0.187296         NaN       -0.212767        NaN   \n",
       "47                      NaN         NaN       -0.764807        NaN   \n",
       "48                 0.604522    0.560452       -0.035343   0.953589   \n",
       "49                -1.267412    0.636902        0.893967  -0.450325   \n",
       "\n",
       "    scl1avg_min  sedentary1minutes_min  sleep1deep1ratio_min  \\\n",
       "0           NaN              -0.379651              1.312915   \n",
       "1           NaN              -0.929092              0.540417   \n",
       "2           NaN              -0.174774              0.013752   \n",
       "3           NaN              -0.877873             -0.935169   \n",
       "4           NaN               0.370010              0.221019   \n",
       "5           NaN              -0.426214              0.393449   \n",
       "6           0.0               0.113915             -1.966679   \n",
       "7           NaN               0.514355             -1.966679   \n",
       "8           NaN              -0.770778             -0.009547   \n",
       "9           NaN              -0.267900              0.153906   \n",
       "10          NaN               1.943833                   NaN   \n",
       "11          NaN               0.388636             -0.230788   \n",
       "12          NaN              -0.002492             -0.218789   \n",
       "13          NaN               0.686638                   NaN   \n",
       "14          NaN              -0.947717             -0.104265   \n",
       "15          NaN              -0.272556             -0.844651   \n",
       "16          NaN               0.737857              0.153906   \n",
       "17          NaN              -1.567003              1.018405   \n",
       "18          NaN               0.137196             -0.326882   \n",
       "19          NaN               0.579543              0.244937   \n",
       "20          NaN              -0.295838             -0.402777   \n",
       "21          NaN               0.346729              1.549265   \n",
       "22          NaN              -0.603152             -1.192111   \n",
       "23          NaN               0.481761              1.455507   \n",
       "24          NaN              -0.258588              1.583547   \n",
       "25          NaN              -0.230650              1.052896   \n",
       "26          NaN              -0.207368             -0.593580   \n",
       "27          NaN              -1.958130              0.595126   \n",
       "28          NaN               2.693495                   NaN   \n",
       "29          NaN              -0.249275              0.709104   \n",
       "30          NaN              -0.388963              1.442395   \n",
       "31          NaN              -0.235306             -0.013377   \n",
       "32          NaN              -0.510027             -1.966679   \n",
       "33          NaN              -0.347057              0.885142   \n",
       "34          NaN               0.044071             -0.255586   \n",
       "35          NaN               0.332760             -1.016072   \n",
       "36          NaN               0.514355              0.139281   \n",
       "37          NaN               0.290854             -1.596313   \n",
       "38          NaN               0.747169             -1.016072   \n",
       "39          NaN              -0.021117             -0.954743   \n",
       "40          NaN               2.316336                   NaN   \n",
       "41          NaN              -1.301595             -1.396315   \n",
       "42          NaN               0.211697             -0.126794   \n",
       "43          NaN              -1.594940              0.925309   \n",
       "44          NaN               0.486417              0.139281   \n",
       "45          NaN               1.724988                   NaN   \n",
       "46          NaN              -0.542621              1.543255   \n",
       "47          NaN               2.628307                   NaN   \n",
       "48          NaN              -1.329532              0.504899   \n",
       "49          NaN              -0.286525              0.428851   \n",
       "\n",
       "    sleep1duration_min  sleep1efficiency_min  sleep1light1ratio_min  \\\n",
       "0             1.220305              0.040819              -0.191007   \n",
       "1             0.845443              0.163276              -0.110026   \n",
       "2             0.631237             -1.428662              -0.158589   \n",
       "3            -0.645077              0.530646              -0.352513   \n",
       "4             0.184973              0.040819              -1.390649   \n",
       "5            -0.752181             -0.693922               0.578769   \n",
       "6            -0.305917              0.163276              -1.256569   \n",
       "7            -0.770031              0.285732               0.137483   \n",
       "8             0.301002              0.163276              -0.287032   \n",
       "9            -0.448721              0.040819               0.826140   \n",
       "10           -0.903910              1.632757                    NaN   \n",
       "11           -0.689704             -0.816378              -0.164256   \n",
       "12           -0.903910              0.285732              -0.680663   \n",
       "13                 NaN                   NaN                    NaN   \n",
       "14           -0.921761             -0.571465               0.784912   \n",
       "15            1.889700              0.285732               1.239276   \n",
       "16           -0.957462              0.163276              -0.513440   \n",
       "17           -0.555825             -0.326551               0.028427   \n",
       "18           -0.796807              0.285732              -0.272699   \n",
       "19            0.702639              0.163276              -0.576029   \n",
       "20           -0.805732             -0.081638              -0.227751   \n",
       "21            1.996804              0.653103               1.793744   \n",
       "22            0.247450             -0.081638              -0.304158   \n",
       "23            0.461657             -0.571465               1.578284   \n",
       "24           -0.689704              0.285732               2.123446   \n",
       "25           -0.850359              0.408189               0.259434   \n",
       "26           -0.841433              0.408189              -0.815681   \n",
       "27            1.666568              0.408189              -0.302784   \n",
       "28           -0.100636              0.408189                    NaN   \n",
       "29            0.265301             -0.571465              -1.851109   \n",
       "30           -0.894985              0.040819               0.313601   \n",
       "31            1.961102              0.408189               0.826140   \n",
       "32            1.291707              0.163276              -3.626548   \n",
       "33           -0.180963              0.163276               0.242181   \n",
       "34            1.113201             -0.816378               0.100702   \n",
       "35            0.176048              0.530646              -0.316585   \n",
       "36           -0.002458              0.163276               0.355786   \n",
       "37           -0.163113             -1.428662              -0.412578   \n",
       "38            1.604092              0.530646               1.882227   \n",
       "39            0.782966              0.163276               1.469093   \n",
       "40            0.184973              0.898016                    NaN   \n",
       "41            2.112832             -0.204095               1.170420   \n",
       "42           -0.939611              0.653103              -0.179306   \n",
       "43            0.434881             -0.326551              -0.401064   \n",
       "44           -0.778956              0.040819              -0.353822   \n",
       "45           -0.921761              1.142930                    NaN   \n",
       "46           -0.466572              0.285732              -1.523890   \n",
       "47           -0.939611              0.775559                    NaN   \n",
       "48           -0.421946              0.530646               0.745182   \n",
       "49           -0.939611              0.163276              -0.501265   \n",
       "\n",
       "    sleep1rem1ratio_min  sleep1wake1ratio_min  spo2_min  step1goal_min  \\\n",
       "0              1.152834              0.086826       NaN       0.066577   \n",
       "1             -0.187861             -0.419095       NaN       2.609115   \n",
       "2             -0.234400             -1.426636       NaN      -0.695930   \n",
       "3              0.150326             -0.894681       NaN      -0.695930   \n",
       "4              0.111041             -0.469241  0.545678       0.066577   \n",
       "5             -0.843550              0.304700  1.204256      -0.695930   \n",
       "6              0.395916              0.020797  0.282247       0.066577   \n",
       "7             -0.138219              0.378210       NaN       0.066577   \n",
       "8              0.661083             -0.608930  0.282247       0.066577   \n",
       "9              1.856076             -0.855715       NaN      -0.695930   \n",
       "10                  NaN                   NaN  0.545678      -0.695930   \n",
       "11            -1.233209              0.608543       NaN      -1.204438   \n",
       "12             0.752726             -0.136363       NaN       0.066577   \n",
       "13                  NaN                   NaN       NaN      -1.204438   \n",
       "14             0.479034              1.196498 -0.903192      -0.695930   \n",
       "15             0.145264              0.537672       NaN      -0.695930   \n",
       "16             0.268875             -0.564060 -0.310472      -0.695930   \n",
       "17            -0.946631             -0.047748  0.611536      -0.695930   \n",
       "18            -0.300580             -0.459363       NaN      -0.695930   \n",
       "19             0.366735             -0.701475       NaN       0.066577   \n",
       "20            -1.304808             -0.131543       NaN       0.829338   \n",
       "21             1.090550              2.165275       NaN       1.337846   \n",
       "22            -0.573864             -0.538420       NaN       1.337846   \n",
       "23             0.847642              1.497433 -0.244614       0.066577   \n",
       "24             1.591347              1.562779       NaN      -1.204438   \n",
       "25             0.098536              1.800854  0.743252      -1.204438   \n",
       "26            -1.677123             -0.434729       NaN      -1.204438   \n",
       "27            -1.171644              0.291836  0.018816       1.337846   \n",
       "28                  NaN                   NaN       NaN       0.066577   \n",
       "29            -0.185769             -0.965398 -2.878924       1.337846   \n",
       "30             0.054145              0.378210       NaN       1.337846   \n",
       "31             0.163592             -0.246186 -1.495912       0.066577   \n",
       "32            -1.677123             -3.076782 -0.705619       0.066577   \n",
       "33             0.427555             -0.617296 -0.837334       0.066577   \n",
       "34            -0.902979             -0.500177       NaN      -0.695930   \n",
       "35            -0.757126             -0.982847       NaN      -0.695930   \n",
       "36            -0.297127              0.548128       NaN      -1.204438   \n",
       "37            -0.405571             -1.426636       NaN       0.829338   \n",
       "38            -0.975258              1.365351       NaN       0.066577   \n",
       "39             0.734593             -0.036389  0.413963      -1.204438   \n",
       "40                  NaN                   NaN       NaN      -0.695930   \n",
       "41             0.997193              0.516410       NaN       2.609115   \n",
       "42            -1.376556              0.014527 -0.310472      -1.204438   \n",
       "43             0.246508              0.573776  1.072540       0.066577   \n",
       "44            -0.297127             -0.036389       NaN      -0.695930   \n",
       "45                  NaN                   NaN       NaN       0.066577   \n",
       "46             0.246508              0.678645       NaN       0.066577   \n",
       "47                  NaN                   NaN       NaN       0.829338   \n",
       "48             1.136783              1.673832  0.413963      -1.204438   \n",
       "49             0.405158             -0.584656       NaN       2.609115   \n",
       "\n",
       "    step1goal1label_min  steps_min  very1active1minutes_min  age_mean  \\\n",
       "0              0.115748   0.468029                      0.0 -0.915335   \n",
       "1              2.233940   1.689095                      0.0  1.092496   \n",
       "2             -0.590316  -0.320172                      0.0 -0.915335   \n",
       "3             -0.590316  -0.538399                      0.0  1.092496   \n",
       "4              0.115748  -0.508967                      0.0  1.092496   \n",
       "5             -0.590316  -0.525478                      0.0  1.092496   \n",
       "6              0.115748  -0.261308                      0.0  1.092496   \n",
       "7              0.115748  -0.135684                      0.0  1.092496   \n",
       "8              0.115748  -0.101227                      0.0 -0.915335   \n",
       "9             -0.590316   0.761631                      0.0 -0.915335   \n",
       "10            -0.590316  -0.496764                      0.0  1.092496   \n",
       "11            -1.296380  -0.538399                      0.0 -0.915335   \n",
       "12             0.115748  -0.529785                      0.0 -0.915335   \n",
       "13            -1.296380   3.352358                      0.0 -0.915335   \n",
       "14            -0.590316  -0.538399                      0.0 -0.915335   \n",
       "15            -0.590316   0.444340                      0.0 -0.915335   \n",
       "16            -0.590316  -0.538399                      0.0  1.092496   \n",
       "17            -0.590316  -0.361807                      0.0  1.092496   \n",
       "18            -0.590316  -0.538399                      0.0 -0.915335   \n",
       "19             0.115748  -0.414928                      0.0 -0.915335   \n",
       "20             0.821812   1.381855                      0.0  1.092496   \n",
       "21             1.527876   3.994117                      0.0  1.092496   \n",
       "22             1.527876   4.113281                      0.0  1.092496   \n",
       "23             0.115748  -0.023699                      0.0 -0.915335   \n",
       "24            -1.296380  -0.538399                      0.0  1.092496   \n",
       "25            -1.296380  -0.538399                      0.0  1.092496   \n",
       "26            -1.296380  -0.423543                      0.0  1.092496   \n",
       "27             1.527876   0.198835                      0.0 -0.915335   \n",
       "28             0.115748  -0.502506                      0.0 -0.915335   \n",
       "29             1.527876  -0.538399                      0.0 -0.915335   \n",
       "30             1.527876   0.190221                      0.0  1.092496   \n",
       "31             0.115748  -0.104816                      0.0 -0.915335   \n",
       "32             0.115748  -0.368268                      0.0 -0.915335   \n",
       "33             0.115748   0.433572                      0.0  1.092496   \n",
       "34            -0.590316  -0.012214                      0.0  1.092496   \n",
       "35            -0.590316  -0.044517                      0.0 -0.915335   \n",
       "36            -1.296380  -0.417800                      0.0  1.092496   \n",
       "37             0.821812  -0.534092                      0.0       NaN   \n",
       "38             0.115748   0.577143                      0.0  1.092496   \n",
       "39            -1.296380  -0.538399                      0.0 -0.915335   \n",
       "40            -0.590316  -0.538399                      0.0 -0.915335   \n",
       "41             2.233940  -0.154348                      0.0       NaN   \n",
       "42            -1.296380  -0.534092                      0.0  1.092496   \n",
       "43             0.115748   0.770963                      0.0  1.092496   \n",
       "44            -0.590316  -0.538399                      0.0       NaN   \n",
       "45             0.115748  -0.343143                      0.0 -0.915335   \n",
       "46             0.115748   2.807508                      0.0 -0.915335   \n",
       "47             0.821812  -0.325197                      0.0  1.092496   \n",
       "48            -1.296380   0.872180                      0.0 -0.915335   \n",
       "49             2.233940  -0.152913                      0.0 -0.915335   \n",
       "\n",
       "    bmi_mean  bpm_mean  calories_mean  daily1temperature1variation_mean  \\\n",
       "0  -1.487354 -1.512877       0.310669                         -0.524013   \n",
       "1  -1.487354  0.472681      -0.103172                         -0.793389   \n",
       "2  -0.004362  0.332837      -1.106954                          0.079949   \n",
       "3   0.292237 -0.295129      -0.255937                          0.710365   \n",
       "4   0.588835 -0.495544      -0.377764                          0.108849   \n",
       "5   0.292237 -2.003846       1.046362                          0.919543   \n",
       "6  -0.894157 -0.521684      -0.864867                         -1.187141   \n",
       "7  -0.300960  0.230269       0.894760                          0.550148   \n",
       "8   0.292237  1.464144       1.128118                          1.035021   \n",
       "9   0.588835  0.058565       0.353308                         -1.785434   \n",
       "10 -0.004362  1.362798      -0.240063                               NaN   \n",
       "11 -1.190755  0.673068      -0.165261                         -0.503116   \n",
       "12 -0.894157 -0.505136      -0.664642                          0.559513   \n",
       "13  2.071827  2.106936      -0.324301                               NaN   \n",
       "14  2.071827  0.186699       1.257299                          0.055236   \n",
       "15 -0.894157 -1.586555       0.357388                         -1.160884   \n",
       "16 -0.894157  0.502647      -1.290892                         -0.634742   \n",
       "17  0.588835 -1.604353       0.997462                         -0.384344   \n",
       "18  0.292237 -2.904939       0.466933                          0.071685   \n",
       "19 -1.190755 -0.074076      -0.532939                         -1.053776   \n",
       "20  1.182032 -0.345252       1.478805                          1.027627   \n",
       "21  2.071827  0.814149      -0.552579                          0.086078   \n",
       "22 -0.004362  1.466930       2.280024                          0.929484   \n",
       "23 -0.300960 -1.007736       0.318303                          1.388773   \n",
       "24 -0.300960 -0.594976      -0.887904                         -0.504654   \n",
       "25  2.071827  0.162560       0.788912                          1.040163   \n",
       "26 -0.597559  0.866683      -0.709605                         -1.393132   \n",
       "27 -0.597559 -0.347558      -0.124272                          1.590085   \n",
       "28 -0.597559  0.757655      -1.274799                               NaN   \n",
       "29 -0.597559  0.259695       0.127676                         -0.154298   \n",
       "30 -1.190755 -0.416242      -0.918880                         -0.773742   \n",
       "31 -0.597559 -0.713596       0.777877                          0.633006   \n",
       "32 -0.894157 -1.394380      -0.760116                         -0.485134   \n",
       "33 -0.300960 -0.376101       1.397114                          0.693469   \n",
       "34  0.885434 -0.286817       1.774330                         -1.316293   \n",
       "35  0.885434 -1.471852       1.138894                          1.098978   \n",
       "36  0.885434  0.105443       0.318246                          1.393823   \n",
       "37       NaN  0.201902      -1.014373                         -0.545461   \n",
       "38  0.292237 -0.555982      -1.118583                          0.297207   \n",
       "39  0.292237  0.979221      -0.997316                          0.238905   \n",
       "40 -0.004362  1.294030      -1.293411                               NaN   \n",
       "41       NaN -0.410071       0.141884                               NaN   \n",
       "42  0.292237  0.914903       0.200761                          0.123936   \n",
       "43  0.885434 -1.779655       2.547322                         -1.119929   \n",
       "44       NaN -0.088040      -0.126478                          1.051660   \n",
       "45  0.292237  1.615434      -1.455462                               NaN   \n",
       "46 -0.894157  0.337466       1.950126                          1.540710   \n",
       "47  0.292237 -0.138551      -0.071869                               NaN   \n",
       "48  0.588835 -0.076706       0.425226                         -0.910376   \n",
       "49 -0.004362  0.519163      -0.505154                          0.117671   \n",
       "\n",
       "    distance_mean  filteredDemographicVO2Max_mean  \\\n",
       "0        0.337865                        2.486255   \n",
       "1        2.586309                        0.004213   \n",
       "2       -1.317373                       -1.371431   \n",
       "3       -0.914596                        0.779307   \n",
       "4       -0.617785                       -0.945505   \n",
       "5        0.252637                        1.168094   \n",
       "6       -0.727421                       -0.404553   \n",
       "7       -0.534365                        0.592785   \n",
       "8       -0.637194                       -0.170250   \n",
       "9       -0.553965                       -2.303962   \n",
       "10       0.078603                        0.656447   \n",
       "11      -0.271767                        0.023832   \n",
       "12       0.150720                       -0.139577   \n",
       "13      -0.881812                       -1.442883   \n",
       "14      -1.203840                       -0.878568   \n",
       "15       0.337628                        2.129660   \n",
       "16      -1.052147                       -0.764767   \n",
       "17      -0.268798                        0.380473   \n",
       "18      -0.497067                        1.350334   \n",
       "19      -0.575688                        1.613349   \n",
       "20       0.305781                       -0.136201   \n",
       "21       0.689515                       -0.628074   \n",
       "22       1.712249                        0.339671   \n",
       "23      -0.487276                        1.406425   \n",
       "24      -0.118254                       -0.515440   \n",
       "25       0.868164                       -2.178713   \n",
       "26      -0.819742                       -0.817838   \n",
       "27       2.907596                        1.109920   \n",
       "28      -0.867097                        1.019811   \n",
       "29      -0.746268                        1.133302   \n",
       "30      -0.323088                       -0.050984   \n",
       "31      -0.307783                        1.272277   \n",
       "32       0.521479                        0.566232   \n",
       "33       0.404720                        0.763027   \n",
       "34       0.453583                       -0.179935   \n",
       "35      -0.646217                        0.550587   \n",
       "36      -0.389895                        0.096514   \n",
       "37      -1.009325                       -1.299449   \n",
       "38      -0.220803                       -1.166283   \n",
       "39       0.226982                        0.684567   \n",
       "40      -1.452172                       -1.466149   \n",
       "41      -0.597196                       -0.683697   \n",
       "42      -0.635667                        0.017330   \n",
       "43       1.968453                        0.648481   \n",
       "44      -0.526210                        0.146718   \n",
       "45       0.757301                       -0.975215   \n",
       "46       1.734924                        1.163848   \n",
       "47      -0.060963                        0.920683   \n",
       "48       0.928208                       -1.165051   \n",
       "49       0.654396                       -1.333752   \n",
       "\n",
       "    full1sleep1breathing1rate_mean  gender_mean  lightly1active1minutes_mean  \\\n",
       "0                         0.224010    -0.801784                    -0.031059   \n",
       "1                        -0.005455     1.247219                     2.272765   \n",
       "2                         1.321969     1.247219                    -0.362747   \n",
       "3                        -0.417111    -0.801784                    -0.919856   \n",
       "4                        -0.798003     1.247219                     0.614251   \n",
       "5                              NaN    -0.801784                    -0.326785   \n",
       "6                        -0.274580     1.247219                     1.147969   \n",
       "7                              NaN    -0.801784                    -0.080463   \n",
       "8                        -0.197162    -0.801784                     0.156100   \n",
       "9                              NaN     1.247219                     1.173876   \n",
       "10                             NaN    -0.801784                    -0.571661   \n",
       "11                             NaN     1.247219                    -1.019590   \n",
       "12                        1.373222     1.247219                     0.806430   \n",
       "13                             NaN    -0.801784                    -1.880910   \n",
       "14                        0.100511    -0.801784                    -1.512468   \n",
       "15                       -0.865717    -0.801784                     0.636172   \n",
       "16                        0.551869     1.247219                     0.036031   \n",
       "17                        1.336149    -0.801784                    -0.279258   \n",
       "18                             NaN    -0.801784                    -0.459207   \n",
       "19                        0.801930    -0.801784                     0.272496   \n",
       "20                             NaN    -0.801784                     1.485285   \n",
       "21                       -0.142565    -0.801784                    -1.628579   \n",
       "22                       -0.008367    -0.801784                     1.804765   \n",
       "23                       -0.339807    -0.801784                     0.630992   \n",
       "24                             NaN     1.247219                    -0.660387   \n",
       "25                        0.294972    -0.801784                    -1.523210   \n",
       "26                             NaN     1.247219                     0.800039   \n",
       "27                        1.276827    -0.801784                    -1.375143   \n",
       "28                             NaN    -0.801784                    -1.681113   \n",
       "29                        1.323946    -0.801784                    -0.000732   \n",
       "30                             NaN     1.247219                     1.051501   \n",
       "31                       -1.613401    -0.801784                    -0.153942   \n",
       "32                       -1.429211     1.247219                     0.251195   \n",
       "33                       -0.389072    -0.801784                     1.862948   \n",
       "34                       -1.173981    -0.801784                     1.248050   \n",
       "35                             NaN    -0.801784                    -0.267447   \n",
       "36                       -2.004461    -0.801784                     0.081123   \n",
       "37                        1.535304          NaN                     0.172697   \n",
       "38                             NaN     1.247219                    -1.109626   \n",
       "39                        1.184226    -0.801784                    -1.631789   \n",
       "40                             NaN     1.247219                    -0.775672   \n",
       "41                        1.004004          NaN                     0.682228   \n",
       "42                       -1.808017    -0.801784                    -0.306681   \n",
       "43                       -0.639178    -0.801784                     2.252795   \n",
       "44                       -2.119990    -0.801784                    -0.079318   \n",
       "45                             NaN     1.247219                    -0.871032   \n",
       "46                             NaN    -0.801784                     0.322093   \n",
       "47                             NaN    -0.801784                    -0.642469   \n",
       "48                       -0.029129     1.247219                     0.988510   \n",
       "49                        1.070326     1.247219                     0.651266   \n",
       "\n",
       "    max1goal_mean  min1goal_mean  minutesAfterWakeup_mean  minutesAsleep_mean  \\\n",
       "0        0.481441       0.620486                -0.330695            1.102600   \n",
       "1        1.696434       1.464901                 0.191421            0.603060   \n",
       "2        0.241324       0.210399                -0.231862            0.546269   \n",
       "3       -0.451207      -0.541926                -0.346858            1.044168   \n",
       "4        0.342487       0.304072                -0.227561           -0.096130   \n",
       "5       -1.116538      -1.296114                 0.417016            0.417392   \n",
       "6       -0.044130      -0.111315                -0.199234            0.265319   \n",
       "7        0.249286       0.243891                -0.346691            0.070054   \n",
       "8       -0.279068      -0.412620                 0.029757            0.407064   \n",
       "9       -0.048311      -0.131270                -0.370586            0.841068   \n",
       "10      -0.585639      -0.695024                -0.554475           -3.325221   \n",
       "11      -1.805915      -1.484996                -0.311603            0.230763   \n",
       "12       0.148516       0.139366                -0.325912            0.488613   \n",
       "13      -1.859797      -1.534995                      NaN                 NaN   \n",
       "14      -0.514325      -0.695024                -0.125400            0.574010   \n",
       "15      -0.243328      -0.377035                -0.228220            0.593458   \n",
       "16      -0.698120      -0.905748                -0.097039            0.075269   \n",
       "17      -1.037300      -1.235005                -0.222453            0.795080   \n",
       "18      -0.425401      -0.579472                 0.276663            0.175304   \n",
       "19       0.044718       0.011503                -0.166264            0.336708   \n",
       "20       0.185787       0.148818                -0.013624           -0.799558   \n",
       "21       0.507854       0.864922                -0.043672            0.592862   \n",
       "22       0.705951       0.979918                -0.062094           -0.420835   \n",
       "23      -0.220882      -0.344872                -0.174015            0.073531   \n",
       "24      -1.161440      -0.875018                -0.145832            0.403734   \n",
       "25       0.017086      -0.064078                -0.013624           -0.124439   \n",
       "26      -1.869306      -1.534995                -0.470889           -0.133882   \n",
       "27       1.696434       1.577397                 1.780627            1.959966   \n",
       "28      -0.680726      -0.935015                 0.671454           -2.526820   \n",
       "29       1.102144       1.164912                -0.093104            0.512735   \n",
       "30       0.507854       0.864922                -0.194480            0.490597   \n",
       "31      -0.107660      -0.195755                -0.345170            1.032722   \n",
       "32       1.068185       0.929206                -0.401234            0.653323   \n",
       "33       1.201517       1.243597                -0.470889           -0.655649   \n",
       "34       0.172255       0.035539                -0.227561            0.217169   \n",
       "35      -1.190117      -1.363572                -0.445723            0.302284   \n",
       "36      -1.826084      -1.534995                -0.350154           -0.245460   \n",
       "37       0.296551       0.464936                -0.005593            0.744410   \n",
       "38      -0.029591      -0.113305                 0.129216           -0.044182   \n",
       "39      -0.284532       0.064950                 0.119786           -0.397748   \n",
       "40      -1.393874      -1.534995                -0.554475           -0.558537   \n",
       "41       1.696434       1.464901                -0.437291            0.817793   \n",
       "42      -1.253005      -1.072048                -0.518418            0.026240   \n",
       "43       0.287427       0.472208                -0.060149           -0.278274   \n",
       "44      -1.393874      -1.534995                -0.182114           -0.242400   \n",
       "45       1.480329       1.410358                -0.554475           -3.375122   \n",
       "46       0.287118       0.243515                -0.526613           -0.275266   \n",
       "47       0.567283       0.839923                -0.554475           -3.092354   \n",
       "48       0.940953       1.016972                -0.391018            0.587687   \n",
       "49       1.696434       1.464901                 0.374259            0.153001   \n",
       "\n",
       "    minutesAwake_mean  minutesToFallAsleep_mean  \\\n",
       "0            1.795480                 -0.199832   \n",
       "1            0.312919                  0.082416   \n",
       "2            0.556444                 -0.199832   \n",
       "3            0.541659                 -0.199832   \n",
       "4           -0.150959                 -0.199832   \n",
       "5           -0.369630                 -0.199832   \n",
       "6           -0.092711                 -0.199832   \n",
       "7           -0.433776                 -0.199832   \n",
       "8            0.067382                  0.084866   \n",
       "9            0.217254                 -0.199832   \n",
       "10          -2.768547                 -0.199832   \n",
       "11           0.158648                 -0.199832   \n",
       "12          -0.282697                 -0.199832   \n",
       "13                NaN                       NaN   \n",
       "14           0.749521                 -0.199832   \n",
       "15           0.677490                 -0.199832   \n",
       "16          -0.309355                 -0.199832   \n",
       "17           1.043453                 -0.199832   \n",
       "18           0.241908                 -0.199832   \n",
       "19           0.427791                 -0.199832   \n",
       "20          -0.235345                 -0.199832   \n",
       "21           0.193557                 -0.199832   \n",
       "22          -0.130220                 -0.199832   \n",
       "23           1.055631                 -0.199832   \n",
       "24           0.296851                 -0.199832   \n",
       "25          -0.307722                  1.025089   \n",
       "26          -0.444435                 -0.199832   \n",
       "27           1.746871                 -0.199832   \n",
       "28          -1.893587                 -0.199832   \n",
       "29           0.139166                  6.153625   \n",
       "30           0.405787                 -0.199832   \n",
       "31           1.400850                 -0.199832   \n",
       "32           1.221504                 -0.199832   \n",
       "33          -0.378979                 -0.199832   \n",
       "34           0.298187                  5.249027   \n",
       "35           0.874179                 -0.199832   \n",
       "36          -0.097185                 -0.199832   \n",
       "37           0.744218                 -0.199832   \n",
       "38           0.068763                 -0.199832   \n",
       "39          -0.575679                 -0.199832   \n",
       "40          -2.413095                 -0.199832   \n",
       "41           0.155492                 -0.199832   \n",
       "42           0.066495                 -0.199832   \n",
       "43          -0.045764                 -0.199832   \n",
       "44          -0.168199                 -0.199832   \n",
       "45          -2.604492                 -0.199832   \n",
       "46          -0.479234                 -0.199832   \n",
       "47          -2.422209                 -0.199832   \n",
       "48          -0.062248                 -0.199832   \n",
       "49           0.249899                 -0.199832   \n",
       "\n",
       "    minutes1below1default1zone11_mean  minutes1in1default1zone11_mean  \\\n",
       "0                            1.297210                       -0.730252   \n",
       "1                            0.526364                        1.028905   \n",
       "2                            0.505688                       -0.071672   \n",
       "3                            0.183183                       -0.202169   \n",
       "4                            0.068961                       -0.883814   \n",
       "5                            0.660782                       -1.038450   \n",
       "6                            0.313348                        0.886782   \n",
       "7                            0.808931                       -0.398811   \n",
       "8                           -0.070656                        2.633101   \n",
       "9                            1.209194                       -0.675493   \n",
       "10                          -1.690245                       -0.741429   \n",
       "11                          -1.055994                       -0.349923   \n",
       "12                           0.434303                       -0.735410   \n",
       "13                          -2.612836                        1.107939   \n",
       "14                          -1.661025                       -0.947044   \n",
       "15                           1.121857                       -0.894307   \n",
       "16                           0.021851                        0.037110   \n",
       "17                           0.966153                       -0.596309   \n",
       "18                           0.917573                       -1.296169   \n",
       "19                           1.132924                       -0.913766   \n",
       "20                           0.253467                        1.441417   \n",
       "21                          -0.386717                        1.031398   \n",
       "22                          -0.491994                        4.112211   \n",
       "23                           1.002092                       -0.905466   \n",
       "24                          -0.196080                       -0.656357   \n",
       "25                          -0.797664                       -0.286542   \n",
       "26                          -0.316519                        2.216507   \n",
       "27                           0.714197                        0.312763   \n",
       "28                          -1.241336                       -0.966200   \n",
       "29                           0.133722                       -0.453665   \n",
       "30                           0.875077                       -0.161270   \n",
       "31                           1.070215                       -0.443231   \n",
       "32                           1.240389                       -0.958515   \n",
       "33                           0.096382                        1.831109   \n",
       "34                           0.483383                        1.195900   \n",
       "35                           1.145286                       -1.110074   \n",
       "36                           0.601148                        0.408529   \n",
       "37                           0.631175                       -0.151445   \n",
       "38                           0.712614                       -0.021386   \n",
       "39                          -0.979361                        0.290201   \n",
       "40                          -2.471864                       -0.726373   \n",
       "41                           1.048234                       -0.168810   \n",
       "42                          -0.367660                        0.988764   \n",
       "43                           1.154045                       -0.787473   \n",
       "44                           0.627503                        0.092020   \n",
       "45                          -1.824226                       -0.219433   \n",
       "46                          -0.057478                        0.012067   \n",
       "47                          -0.820239                       -0.651359   \n",
       "48                           0.472279                       -0.119200   \n",
       "49                           0.435622                        0.999236   \n",
       "\n",
       "    minutes1in1default1zone12_mean  minutes1in1default1zone13_mean  \\\n",
       "0                        -1.027843                       -0.502668   \n",
       "1                         0.147250                       -0.261041   \n",
       "2                        -0.939046                       -0.431078   \n",
       "3                         2.525190                        0.804163   \n",
       "4                         0.301210                       -0.357986   \n",
       "5                         0.644843                       -0.416231   \n",
       "6                        -0.420660                       -0.017843   \n",
       "7                         1.773844                        0.234930   \n",
       "8                         0.083420                       -0.502668   \n",
       "9                        -0.397494                       -0.502668   \n",
       "10                        0.994963                        1.318277   \n",
       "11                        1.165309                        0.825954   \n",
       "12                        0.052009                       -0.421316   \n",
       "13                       -1.090698                       -0.502668   \n",
       "14                        3.917789                        0.214441   \n",
       "15                       -0.920621                       -0.437586   \n",
       "16                       -0.488341                       -0.446091   \n",
       "17                       -0.858790                       -0.502668   \n",
       "18                       -0.777142                       -0.396576   \n",
       "19                       -0.945756                       -0.450806   \n",
       "20                       -0.456451                       -0.266745   \n",
       "21                        2.884858                        3.067978   \n",
       "22                       -0.096809                       -0.502668   \n",
       "23                       -1.031418                       -0.502668   \n",
       "24                       -0.563636                       -0.450806   \n",
       "25                       -0.367870                       -0.295219   \n",
       "26                        1.579751                        0.552507   \n",
       "27                       -0.982689                       -0.502668   \n",
       "28                       -0.488341                       -0.502668   \n",
       "29                       -0.781990                       -0.491144   \n",
       "30                       -0.024436                       -0.279738   \n",
       "31                       -0.893692                       -0.457111   \n",
       "32                        0.965921                        0.514822   \n",
       "33                       -0.460961                       -0.293124   \n",
       "34                       -0.405419                       -0.222477   \n",
       "35                       -0.497753                       -0.502668   \n",
       "36                       -1.000344                       -0.502668   \n",
       "37                       -1.001956                       -0.470797   \n",
       "38                       -0.657754                       -0.398944   \n",
       "39                       -0.148186                       -0.502668   \n",
       "40                        0.415195                       -0.070482   \n",
       "41                       -0.132321                        3.055968   \n",
       "42                       -0.334696                        0.852268   \n",
       "43                       -0.426223                       -0.295219   \n",
       "44                       -1.021857                       -0.502668   \n",
       "45                       -0.565309                       -0.502668   \n",
       "46                        0.081508                       -0.493888   \n",
       "47                        0.248829                       -0.285340   \n",
       "48                       -0.463482                       -0.309488   \n",
       "49                       -0.148985                       -0.487085   \n",
       "\n",
       "    moderately1active1minutes_mean  nightly1temperature_mean  nremhr_mean  \\\n",
       "0                         1.136107                  0.578562    -0.747719   \n",
       "1                         1.835488                  0.537027     1.052896   \n",
       "2                        -0.815144                  0.370190     1.607685   \n",
       "3                         0.437305                 -0.419781    -1.522436   \n",
       "4                        -0.133966                  0.837568     0.079174   \n",
       "5                         0.776002                  0.145488          NaN   \n",
       "6                        -0.739886                  0.238219     0.112472   \n",
       "7                         0.123822                  0.381938          NaN   \n",
       "8                         1.416700                  0.714738     1.238441   \n",
       "9                        -0.756741                  0.573332          NaN   \n",
       "10                       -0.374815                       NaN          NaN   \n",
       "11                       -0.677240                  0.417914          NaN   \n",
       "12                       -0.227787                  0.342170    -0.156184   \n",
       "13                       -1.218638                       NaN          NaN   \n",
       "14                       -0.621423                 -0.661996    -1.713642   \n",
       "15                        0.288041                  0.479438    -0.895160   \n",
       "16                       -0.542384                  0.151675     1.002862   \n",
       "17                        0.427155                 -0.091512    -0.991195   \n",
       "18                       -0.026601                  0.017107          NaN   \n",
       "19                       -0.936226                 -0.115153     0.774397   \n",
       "20                        0.767930                 -0.213740          NaN   \n",
       "21                       -1.097673                 -0.264005     0.709970   \n",
       "22                        2.564876                 -0.198215     0.749180   \n",
       "23                        0.013863                  0.451636    -0.931637   \n",
       "24                       -0.907951                 -0.025746          NaN   \n",
       "25                       -0.853429                 -0.506856     0.861873   \n",
       "26                        0.147725                  0.771903          NaN   \n",
       "27                       -0.332414                  0.378241    -0.592734   \n",
       "28                       -1.254260                       NaN          NaN   \n",
       "29                       -0.056516                  0.209916    -0.292246   \n",
       "30                       -0.112741                  0.729348          NaN   \n",
       "31                        0.706010                  0.186954    -0.543541   \n",
       "32                        0.836006                  0.698707    -1.602164   \n",
       "33                        0.761443                  0.445900    -0.362803   \n",
       "34                        1.173574                  0.667067     0.350132   \n",
       "35                       -0.344677                  0.342797          NaN   \n",
       "36                       -0.504993                 -0.163010     0.273683   \n",
       "37                       -0.325400                  0.293057     1.502162   \n",
       "38                       -1.097686                  0.505767          NaN   \n",
       "39                       -0.991347                 -0.378121     0.047149   \n",
       "40                       -0.551781                 -4.830918          NaN   \n",
       "41                        0.243361                       NaN    -0.286714   \n",
       "42                       -0.452880                  0.689928     1.154436   \n",
       "43                        2.237206                  0.146355    -1.936444   \n",
       "44                       -0.622056                 -0.282741     0.300249   \n",
       "45                       -0.678907                       NaN          NaN   \n",
       "46                        1.322671                 -0.060255          NaN   \n",
       "47                       -0.247383                       NaN          NaN   \n",
       "48                        0.484458                  0.363168    -0.117114   \n",
       "49                        1.149713                  0.639227     0.433935   \n",
       "\n",
       "    resting1hr_mean  rmssd_mean  scl1avg_mean  sedentary1minutes_mean  \\\n",
       "0         -0.933525    3.356337           NaN               -1.143153   \n",
       "1          0.481014   -0.864938           NaN               -1.800374   \n",
       "2          1.405968   -0.835065           NaN               -0.120562   \n",
       "3         -0.080220   -0.514731           NaN               -0.843292   \n",
       "4         -0.697311   -0.995158           NaN               -0.155453   \n",
       "5         -2.012405         NaN           NaN               -0.188973   \n",
       "6          0.085122   -0.926358           1.0               -1.070076   \n",
       "7          0.253315         NaN           NaN               -0.262119   \n",
       "8          1.799488   -0.907062           NaN               -1.075795   \n",
       "9          0.943785         NaN           NaN               -1.187702   \n",
       "10         0.654279         NaN           NaN                1.240155   \n",
       "11        -0.393328         NaN           NaN                1.091645   \n",
       "12        -0.454850   -0.153702           NaN               -0.610790   \n",
       "13         2.053229         NaN           NaN                1.802866   \n",
       "14        -1.123447   -0.350390           NaN                0.592182   \n",
       "15        -1.189206    0.550734           NaN               -0.734242   \n",
       "16         1.003184   -0.537834           NaN                0.055095   \n",
       "17        -1.140688   -0.125730           NaN               -0.350581   \n",
       "18        -2.805175         NaN           NaN               -0.115120   \n",
       "19         1.009513   -0.539224           NaN               -0.391796   \n",
       "20         0.107316         NaN           NaN               -1.109191   \n",
       "21         0.277237    0.654445           NaN                1.635957   \n",
       "22         1.150266   -0.733370           NaN               -1.287578   \n",
       "23        -0.570094    0.018145           NaN               -0.871031   \n",
       "24        -0.506904         NaN           NaN                0.966085   \n",
       "25         0.621703   -0.783084           NaN                1.452278   \n",
       "26         0.653485         NaN           NaN               -0.221294   \n",
       "27        -0.611628   -0.050061           NaN                1.131878   \n",
       "28         0.943380         NaN           NaN                1.766421   \n",
       "29         0.476445    1.507500           NaN               -0.710890   \n",
       "30         0.031003         NaN           NaN               -0.982732   \n",
       "31        -0.634745    0.225429           NaN               -0.554783   \n",
       "32        -1.587374    0.458487           NaN               -1.214316   \n",
       "33        -0.077321    1.282015           NaN               -0.938950   \n",
       "34         0.045858    0.043904           NaN               -1.220136   \n",
       "35        -1.562551         NaN           NaN               -0.221449   \n",
       "36         0.536503   -0.470616           NaN                0.103743   \n",
       "37         1.230170   -0.792438           NaN               -0.883786   \n",
       "38        -0.133438         NaN           NaN                1.236892   \n",
       "39         0.188767   -0.509628           NaN                1.567793   \n",
       "40         1.470570         NaN           NaN                1.349588   \n",
       "41        -0.324181    1.879990           NaN               -1.312998   \n",
       "42         1.587917   -0.559247           NaN                0.051959   \n",
       "43        -2.098617    1.511198           NaN               -1.363827   \n",
       "44         0.375508   -0.478374           NaN                0.409347   \n",
       "45         0.375577         NaN           NaN                1.442468   \n",
       "46        -0.388217         NaN           NaN               -0.036979   \n",
       "47        -0.859268         NaN           NaN                1.302424   \n",
       "48        -0.090090    1.649248           NaN               -0.575960   \n",
       "49         0.911188   -1.154183           NaN               -0.765865   \n",
       "\n",
       "    sleep1deep1ratio_mean  sleep1duration_mean  sleep1efficiency_mean  \\\n",
       "0                0.551802             1.241448              -0.182225   \n",
       "1                1.393204             0.564587              -0.011070   \n",
       "2                0.606062             0.556260               0.150443   \n",
       "3                0.048615             0.978434               0.186987   \n",
       "4               -0.736343            -0.110487               0.137855   \n",
       "5               -0.102054             0.287764               0.168255   \n",
       "6               -0.300397             0.206204               0.603161   \n",
       "7               -0.305118            -0.018147               0.419259   \n",
       "8               -0.197907             0.355040               0.581169   \n",
       "9               -1.015719             0.748025               0.150443   \n",
       "10                    NaN            -3.300546               1.094499   \n",
       "11               0.289947             0.220308               0.325004   \n",
       "12              -0.037926             0.363449               0.294451   \n",
       "13                    NaN                  NaN                    NaN   \n",
       "14               1.575342             0.613956              -0.422285   \n",
       "15               0.614768             0.617123               0.564610   \n",
       "16               0.163456             0.008667               0.339254   \n",
       "17              -0.631921             0.851151              -0.009522   \n",
       "18              -0.502837             0.188039               0.454461   \n",
       "19              -0.105805             0.356324               0.395898   \n",
       "20              -0.841325            -0.720997              -0.068912   \n",
       "21               3.532201             0.533358               0.118974   \n",
       "22              -0.920018            -0.382414              -0.081702   \n",
       "23              -0.137639             0.241050              -0.604802   \n",
       "24              -1.070340             0.390664               0.202890   \n",
       "25              -0.869026            -0.161477               0.017164   \n",
       "26               1.049668            -0.193010               0.493736   \n",
       "27              -1.558516             1.960515               0.096497   \n",
       "28                    NaN            -2.453794              -0.793614   \n",
       "29               0.497854             0.476048              -0.444414   \n",
       "30               0.504440             0.483255              -0.101306   \n",
       "31              -0.060665             1.114266               0.152745   \n",
       "32              -0.226999             0.857610              -0.183349   \n",
       "33              -0.310006            -0.624440               0.244848   \n",
       "34              -0.734090             0.254432              -0.217110   \n",
       "35              -0.834207             0.403917               0.144352   \n",
       "36              -0.125368            -0.227778              -0.161096   \n",
       "37              -0.085445             0.756678               0.109248   \n",
       "38              -1.233373            -0.028610               0.179491   \n",
       "39              -1.685313            -0.439466              -0.095012   \n",
       "40                    NaN            -0.892008               0.528065   \n",
       "41               1.182018             0.717651               0.061590   \n",
       "42               0.050492             0.030933               0.513257   \n",
       "43              -0.447765            -0.246928              -0.300268   \n",
       "44              -0.049578            -0.237468              -0.147773   \n",
       "45                    NaN            -3.314658               0.339254   \n",
       "46              -0.061683            -0.318953               0.343545   \n",
       "47                    NaN            -3.043384              -0.101306   \n",
       "48              -0.090935             0.484957               0.314079   \n",
       "49              -0.580428             0.169853               0.047455   \n",
       "\n",
       "    sleep1light1ratio_mean  sleep1rem1ratio_mean  sleep1wake1ratio_mean  \\\n",
       "0                 0.299354              0.050381               0.084276   \n",
       "1                 1.350585              0.466318               0.753044   \n",
       "2                 0.214440              0.378261               0.552121   \n",
       "3                 0.440661             -0.686518               0.404497   \n",
       "4                -0.666586             -1.035030               0.134312   \n",
       "5                -0.621408             -0.475220              -0.557779   \n",
       "6                 0.064059              0.076824               0.497716   \n",
       "7                -0.012359             -0.322383               0.544023   \n",
       "8                 1.113514             -0.815981               0.732975   \n",
       "9                 0.821075              0.286768              -1.216821   \n",
       "10                     NaN                   NaN                    NaN   \n",
       "11                0.016952              1.031009               0.291284   \n",
       "12               -0.313709             -0.158874              -0.117078   \n",
       "13                     NaN                   NaN                    NaN   \n",
       "14                0.921691              1.723628               0.189977   \n",
       "15                1.013066              0.289357               0.351258   \n",
       "16               -0.386921              0.380447              -0.430231   \n",
       "17                0.166289              1.024472              -0.032525   \n",
       "18               -1.260179              0.098519              -0.501342   \n",
       "19               -0.671648              0.653464              -2.640196   \n",
       "20                0.347363             -0.717162              -0.417227   \n",
       "21               -1.464170             -0.837489              -1.384483   \n",
       "22                0.079838             -1.378372              -0.190891   \n",
       "23                0.215451              0.637940              -0.314549   \n",
       "24                0.248450             -0.197964               2.046327   \n",
       "25               -1.806553             -1.264956              -1.269381   \n",
       "26               -0.383293             -0.547894               0.234553   \n",
       "27               -0.666287             -3.474547              -0.828269   \n",
       "28                     NaN                   NaN                    NaN   \n",
       "29                0.663418              1.095024              -0.021653   \n",
       "30                0.011822             -0.125237               0.097889   \n",
       "31                0.091260             -0.152083              -0.526848   \n",
       "32               -0.006114              0.618765              -0.444048   \n",
       "33               -0.538777              0.163687              -0.848599   \n",
       "34                0.852261              2.639083               0.059235   \n",
       "35                0.959947             -0.135397               0.395514   \n",
       "36               -0.842587             -0.428792              -0.212055   \n",
       "37               -0.243283             -0.131516              -0.102505   \n",
       "38                1.187734              0.910786              -0.420554   \n",
       "39               -0.278367             -1.753335              -1.547837   \n",
       "40                     NaN                   NaN                    NaN   \n",
       "41                0.003055             -0.099818               0.527193   \n",
       "42               -1.084078              0.177729              -0.447867   \n",
       "43                0.301667              0.759500               1.559766   \n",
       "44               -0.445563              0.246955              -0.617510   \n",
       "45                     NaN                   NaN                    NaN   \n",
       "46               -0.507708             -0.935097              -0.520469   \n",
       "47                     NaN                   NaN                    NaN   \n",
       "48                0.461395              0.084290               0.488631   \n",
       "49               -0.012245              0.962891              -0.185443   \n",
       "\n",
       "    spo2_mean  step1goal_mean  step1goal1label_mean  steps_mean  \\\n",
       "0         NaN        0.482325              0.630392    0.228625   \n",
       "1         NaN        1.698147              1.511501    2.768485   \n",
       "2         NaN        0.242059              0.288143   -1.230658   \n",
       "3         NaN       -0.450893             -0.402632   -0.866819   \n",
       "4    1.696286        0.343276              0.383125   -0.542573   \n",
       "5    1.441073       -1.116594             -1.091775    0.724232   \n",
       "6    0.025585       -0.043605              0.008593   -0.697705   \n",
       "7         NaN        0.250012              0.272047   -0.595418   \n",
       "8   -0.013925       -0.278702             -0.228620   -0.718472   \n",
       "9         NaN       -0.047784             -0.011925   -0.397821   \n",
       "10  -1.751222       -0.585435             -0.507039    0.027434   \n",
       "11        NaN       -1.806360             -2.020944   -0.176114   \n",
       "12        NaN        0.149173              0.220574    0.297412   \n",
       "13        NaN       -1.860271             -2.078616   -1.019775   \n",
       "14  -2.134134       -0.514107             -0.470994   -1.162770   \n",
       "15        NaN       -0.242915             -0.247512    0.321819   \n",
       "16   0.249700       -0.697999             -0.668803   -0.956432   \n",
       "17   0.612810       -1.037332             -1.011674   -0.448549   \n",
       "18        NaN       -0.425109             -0.384218   -0.610540   \n",
       "19        NaN        0.045304              0.106975   -0.508923   \n",
       "20        NaN        0.186470              0.232472    0.261208   \n",
       "21        NaN        0.508756              0.790594    0.558127   \n",
       "22        NaN        0.706988              0.910745    1.678622   \n",
       "23   0.513146       -0.220476             -0.178494   -0.470660   \n",
       "24        NaN       -1.161507             -1.276006   -0.231785   \n",
       "25   0.255542        0.017676              0.023177    0.729719   \n",
       "26        NaN       -1.869787             -2.093034   -0.782106   \n",
       "27  -0.367894        1.698147              1.511501    2.361856   \n",
       "28        NaN       -0.680634             -0.651220   -0.864473   \n",
       "29  -0.451353        1.103452              1.151047   -0.701290   \n",
       "30        NaN        0.508756              0.790594   -0.217781   \n",
       "31  -1.758233       -0.107178             -0.046173   -0.427256   \n",
       "32  -0.192631        1.069469              0.996567    0.913830   \n",
       "33  -1.251252        1.202892              1.192411    0.215846   \n",
       "34        NaN        0.172980              0.140364    0.371806   \n",
       "35        NaN       -1.190203             -1.166154   -0.733336   \n",
       "36        NaN       -1.826536             -2.027497   -0.298796   \n",
       "37        NaN        0.297309              0.470191   -0.900953   \n",
       "38        NaN       -0.029055              0.006999   -0.290538   \n",
       "39  -0.338162       -0.284091             -0.170616    0.260377   \n",
       "40        NaN       -1.394030             -1.372127   -1.412987   \n",
       "41        NaN        1.698147              1.511501   -0.752278   \n",
       "42  -0.312219       -1.253124             -1.389928   -0.662062   \n",
       "43   0.507088        0.288179              0.476016    2.006804   \n",
       "44        NaN       -1.394030             -1.372127   -0.443242   \n",
       "45        NaN        1.481894              1.314890    1.137472   \n",
       "46        NaN        0.287870              0.327154    1.296292   \n",
       "47        NaN        0.568226              0.790594   -0.137636   \n",
       "48   0.348807        0.942154              0.968352    1.075555   \n",
       "49        NaN        1.698147              1.511501    0.953236   \n",
       "\n",
       "    very1active1minutes_mean  extraversion  agreeableness  conscientiousness  \\\n",
       "0                   1.354252     -1.130894      -0.839454           1.688241   \n",
       "1                   1.432584      0.230138       1.068396          -0.502383   \n",
       "2                  -1.070591      1.219979       0.750421          -1.670716   \n",
       "3                   1.065344     -0.635973      -0.680467          -0.502383   \n",
       "4                  -0.763898      1.343710       0.432446          -0.502383   \n",
       "5                   1.990123     -0.141052      -0.044517           1.396158   \n",
       "6                  -0.975048     -1.130894       1.068396          -0.356341   \n",
       "7                   0.342921      0.848789      -0.362492          -0.648425   \n",
       "8                   0.809819      0.477598       0.591434           0.227825   \n",
       "9                  -0.468924      0.353868       0.909409          -1.524674   \n",
       "10                  0.467397     -0.512243      -0.362492          -0.356341   \n",
       "11                 -0.933132      0.601329       0.909409           0.958033   \n",
       "12                 -0.453426      0.477598       0.273459          -1.086549   \n",
       "13                 -1.254214      0.972519       0.591434          -0.940508   \n",
       "14                  1.731269      0.477598      -2.906292          -1.670716   \n",
       "15                  0.224506     -0.264783       0.273459           0.081783   \n",
       "16                 -1.013480      1.591170       1.545359          -0.356341   \n",
       "17                  0.543604     -0.635973      -0.839454           0.958033   \n",
       "18                  0.657800     -1.254624       0.273459           0.811991   \n",
       "19                 -0.961640      1.096249       0.909409          -0.210300   \n",
       "20                  0.049141      0.230138       0.114471           1.104074   \n",
       "21                 -1.046784      0.848789       0.114471           0.811991   \n",
       "22                  2.443579     -0.141052       1.386371           0.811991   \n",
       "23                 -0.031358     -1.502084      -0.362492           1.834282   \n",
       "24                 -0.885505     -1.130894      -0.362492          -0.940508   \n",
       "25                 -0.377442      0.477598      -0.680467           0.811991   \n",
       "26                 -0.905678     -1.378354       0.273459           1.104074   \n",
       "27                 -0.228325     -1.007164      -0.998442          -1.086549   \n",
       "28                 -1.265210     -0.017322       0.432446           0.373866   \n",
       "29                 -0.319468     -1.007164      -0.203504           0.811991   \n",
       "30                 -0.449087     -1.130894      -1.475404           0.811991   \n",
       "31                 -0.253755     -1.007164      -1.316417          -1.378633   \n",
       "32                  2.483734      0.601329       1.068396          -0.648425   \n",
       "33                 -0.635227      1.838630       1.704346           1.104074   \n",
       "34                  0.880667     -1.749544      -1.157429           1.104074   \n",
       "35                  0.123717      1.096249       1.545359           0.227825   \n",
       "36                 -0.686965     -1.378354      -1.316417          -1.232591   \n",
       "37                 -0.731667      0.601329       0.909409          -1.816757   \n",
       "38                 -1.158649     -0.264783       1.068396           0.227825   \n",
       "39                 -0.797981      0.106408      -1.475404          -1.232591   \n",
       "40                 -0.826975      1.219979       0.591434           1.396158   \n",
       "41                  0.625575     -0.512243      -0.044517           1.104074   \n",
       "42                 -0.500897     -1.378354      -0.362492          -0.794466   \n",
       "43                  1.336240     -0.017322      -0.362492           1.104074   \n",
       "44                 -0.719219     -2.120735      -1.952367          -0.794466   \n",
       "45                 -0.777229      1.343710      -1.475404          -0.064258   \n",
       "46                  1.026572      1.096249      -0.362492          -1.524674   \n",
       "47                  0.125766      0.353868      -0.680467           0.373866   \n",
       "48                  0.730656      1.467440       0.591434           0.665950   \n",
       "49                  0.578377      0.477598       1.227384          -0.064258   \n",
       "\n",
       "    stability  intellect    gender  ipip_extraversion_category  \\\n",
       "0    1.501184   0.651920  0.816497                           2   \n",
       "1   -1.461679   0.843661 -1.224745                           0   \n",
       "2   -0.227153  -0.498527 -1.224745                           1   \n",
       "3    1.130826   0.076696  0.816497                           0   \n",
       "4    0.513563  -1.265492 -1.224745                           1   \n",
       "5    1.130826  -0.306786  0.816497                           0   \n",
       "6   -1.461679   0.460179 -1.224745                           2   \n",
       "7   -0.350606   0.843661  0.816497                           1   \n",
       "8   -0.350606  -0.306786  0.816497                           1   \n",
       "9   -2.325848  -1.265492 -1.224745                           0   \n",
       "10   1.130826  -0.115045  0.816497                           0   \n",
       "11  -1.338227  -0.306786 -1.224745                           0   \n",
       "12  -0.720963   0.460179 -1.224745                           0   \n",
       "13   0.390110  -0.306786  0.816497                           1   \n",
       "14  -2.078943  -1.840716  0.816497                           1   \n",
       "15  -0.103700  -0.115045  0.816497                           0   \n",
       "16   0.019752   0.843661 -1.224745                           1   \n",
       "17   0.019752  -0.690268  0.816497                           0   \n",
       "18   1.501184  -0.306786  0.816497                           2   \n",
       "19   0.390110   0.268438  0.816497                           1   \n",
       "20   1.994995   0.076696  0.816497                           0   \n",
       "21  -0.350606  -1.265492  0.816497                           1   \n",
       "22   0.143205   0.651920  0.816497                           0   \n",
       "23  -1.461679   1.227144  0.816497                           2   \n",
       "24   0.390110  -0.690268 -1.224745                           2   \n",
       "25   0.390110   1.802368  0.816497                           1   \n",
       "26  -0.597511  -0.306786 -1.224745                           2   \n",
       "27   0.143205   1.802368  0.816497                           2   \n",
       "28  -0.720963  -0.498527  0.816497                           0   \n",
       "29  -0.720963   2.185850  0.816497                           2   \n",
       "30   0.143205  -1.648975 -1.224745                           2   \n",
       "31  -0.720963   1.802368  0.816497                           2   \n",
       "32   1.748090  -0.306786 -1.224745                           0   \n",
       "33   0.760468   1.610626  0.816497                           1   \n",
       "34   0.883921  -1.648975  0.816497                           2   \n",
       "35   1.501184   1.418885  0.816497                           1   \n",
       "36  -1.091321  -1.265492  0.816497                           2   \n",
       "37   0.390110  -0.306786 -1.224745                           0   \n",
       "38   0.760468   0.843661 -1.224745                           0   \n",
       "39   0.019752  -1.265492  0.816497                           0   \n",
       "40   0.266658   0.076696 -1.224745                           1   \n",
       "41  -0.597511   0.460179 -1.224745                           2   \n",
       "42   0.637016  -0.306786 -1.224745                           2   \n",
       "43   0.390110   0.076696  0.816497                           0   \n",
       "44  -0.227153  -0.498527  0.816497                           2   \n",
       "45  -1.585132  -1.265492 -1.224745                           1   \n",
       "46   1.254279  -1.648975  0.816497                           1   \n",
       "47   0.266658   0.268438  0.816497                           1   \n",
       "48   0.143205   0.843661 -1.224745                           1   \n",
       "49  -1.461679   0.651920 -1.224745                           0   \n",
       "\n",
       "    ipip_agreeableness_category  ipip_conscientiousness_category  \\\n",
       "0                             2                                1   \n",
       "1                             1                                0   \n",
       "2                             0                                2   \n",
       "3                             0                                2   \n",
       "4                             0                                0   \n",
       "5                             0                                1   \n",
       "6                             1                                0   \n",
       "7                             0                                2   \n",
       "8                             1                                0   \n",
       "9                             1                                2   \n",
       "10                            0                                0   \n",
       "11                            1                                1   \n",
       "12                            0                                2   \n",
       "13                            1                                2   \n",
       "14                            2                                2   \n",
       "15                            1                                0   \n",
       "16                            1                                0   \n",
       "17                            2                                1   \n",
       "18                            1                                1   \n",
       "19                            1                                0   \n",
       "20                            0                                1   \n",
       "21                            0                                1   \n",
       "22                            1                                1   \n",
       "23                            0                                1   \n",
       "24                            2                                2   \n",
       "25                            0                                1   \n",
       "26                            0                                1   \n",
       "27                            2                                2   \n",
       "28                            1                                0   \n",
       "29                            0                                1   \n",
       "30                            2                                1   \n",
       "31                            2                                2   \n",
       "32                            1                                0   \n",
       "33                            1                                1   \n",
       "34                            2                                1   \n",
       "35                            1                                0   \n",
       "36                            2                                2   \n",
       "37                            1                                2   \n",
       "38                            1                                0   \n",
       "39                            2                                2   \n",
       "40                            0                                1   \n",
       "41                            2                                1   \n",
       "42                            2                                2   \n",
       "43                            0                                1   \n",
       "44                            2                                2   \n",
       "45                            2                                0   \n",
       "46                            0                                2   \n",
       "47                            0                                0   \n",
       "48                            0                                1   \n",
       "49                            1                                0   \n",
       "\n",
       "    ipip_stability_category  ipip_intellect_category  \n",
       "0                         1                        0  \n",
       "1                         2                        1  \n",
       "2                         0                        0  \n",
       "3                         1                        0  \n",
       "4                         1                        2  \n",
       "5                         1                        0  \n",
       "6                         2                        1  \n",
       "7                         2                        1  \n",
       "8                         2                        0  \n",
       "9                         2                        2  \n",
       "10                        1                        0  \n",
       "11                        2                        0  \n",
       "12                        0                        1  \n",
       "13                        0                        0  \n",
       "14                        2                        2  \n",
       "15                        0                        0  \n",
       "16                        0                        1  \n",
       "17                        0                        2  \n",
       "18                        1                        0  \n",
       "19                        0                        0  \n",
       "20                        1                        0  \n",
       "21                        2                        2  \n",
       "22                        0                        0  \n",
       "23                        2                        1  \n",
       "24                        1                        2  \n",
       "25                        0                        1  \n",
       "26                        0                        0  \n",
       "27                        0                        1  \n",
       "28                        2                        2  \n",
       "29                        2                        1  \n",
       "30                        0                        2  \n",
       "31                        2                        1  \n",
       "32                        1                        0  \n",
       "33                        1                        1  \n",
       "34                        1                        2  \n",
       "35                        1                        1  \n",
       "36                        2                        2  \n",
       "37                        1                        0  \n",
       "38                        1                        1  \n",
       "39                        0                        2  \n",
       "40                        1                        0  \n",
       "41                        0                        1  \n",
       "42                        1                        0  \n",
       "43                        0                        0  \n",
       "44                        0                        2  \n",
       "45                        2                        2  \n",
       "46                        1                        2  \n",
       "47                        0                        0  \n",
       "48                        0                        1  \n",
       "49                        2                        1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_all_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7b412fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = lifesnaps_all_grouped[\"id\"]\n",
    "lifesnaps_all_grouped = lifesnaps_all_grouped.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "105dd92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lifesnaps_all_grouped.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd158e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesnaps_all_grouped = lifesnaps_all_grouped.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1c6622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9062175272048624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=15)\n",
    "lifesnaps_all_grouped_pca = pca.fit_transform(lifesnaps_all_grouped)\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b3f7777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.058838</td>\n",
       "      <td>-3.042497</td>\n",
       "      <td>0.592587</td>\n",
       "      <td>-2.670440</td>\n",
       "      <td>3.192927</td>\n",
       "      <td>-1.046873</td>\n",
       "      <td>1.029494</td>\n",
       "      <td>1.157444</td>\n",
       "      <td>0.202582</td>\n",
       "      <td>0.999176</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>1.522992</td>\n",
       "      <td>0.440319</td>\n",
       "      <td>0.404369</td>\n",
       "      <td>-0.978824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.384053</td>\n",
       "      <td>3.087423</td>\n",
       "      <td>-1.753993</td>\n",
       "      <td>1.048708</td>\n",
       "      <td>-0.984794</td>\n",
       "      <td>0.560115</td>\n",
       "      <td>0.018172</td>\n",
       "      <td>-1.017402</td>\n",
       "      <td>-0.995143</td>\n",
       "      <td>-1.006616</td>\n",
       "      <td>-1.668771</td>\n",
       "      <td>-0.273147</td>\n",
       "      <td>2.494626</td>\n",
       "      <td>0.930914</td>\n",
       "      <td>-1.094065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.468811</td>\n",
       "      <td>2.583222</td>\n",
       "      <td>-1.751520</td>\n",
       "      <td>-1.156369</td>\n",
       "      <td>-2.058581</td>\n",
       "      <td>-0.224020</td>\n",
       "      <td>-0.163810</td>\n",
       "      <td>0.849418</td>\n",
       "      <td>0.937476</td>\n",
       "      <td>2.418076</td>\n",
       "      <td>-0.613754</td>\n",
       "      <td>0.241807</td>\n",
       "      <td>-1.064986</td>\n",
       "      <td>0.076976</td>\n",
       "      <td>0.510627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.054052</td>\n",
       "      <td>-2.297926</td>\n",
       "      <td>1.693058</td>\n",
       "      <td>1.373355</td>\n",
       "      <td>0.907887</td>\n",
       "      <td>1.014382</td>\n",
       "      <td>-1.411897</td>\n",
       "      <td>1.511266</td>\n",
       "      <td>-1.255586</td>\n",
       "      <td>2.493167</td>\n",
       "      <td>-1.183379</td>\n",
       "      <td>0.580014</td>\n",
       "      <td>-0.830100</td>\n",
       "      <td>1.188165</td>\n",
       "      <td>-0.527131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.659829</td>\n",
       "      <td>-0.179576</td>\n",
       "      <td>-2.408590</td>\n",
       "      <td>0.339011</td>\n",
       "      <td>-1.717306</td>\n",
       "      <td>-0.551839</td>\n",
       "      <td>-1.397649</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>0.653088</td>\n",
       "      <td>-1.155476</td>\n",
       "      <td>0.092739</td>\n",
       "      <td>0.933833</td>\n",
       "      <td>-1.383331</td>\n",
       "      <td>-0.847976</td>\n",
       "      <td>1.304050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.146169</td>\n",
       "      <td>-6.359829</td>\n",
       "      <td>2.182085</td>\n",
       "      <td>0.853193</td>\n",
       "      <td>-0.380818</td>\n",
       "      <td>-0.375561</td>\n",
       "      <td>-2.201688</td>\n",
       "      <td>-0.104019</td>\n",
       "      <td>-0.882482</td>\n",
       "      <td>-0.580981</td>\n",
       "      <td>-0.077579</td>\n",
       "      <td>-0.569028</td>\n",
       "      <td>1.318325</td>\n",
       "      <td>-0.212884</td>\n",
       "      <td>0.461949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.508660</td>\n",
       "      <td>1.305421</td>\n",
       "      <td>-2.018848</td>\n",
       "      <td>-1.923465</td>\n",
       "      <td>0.315331</td>\n",
       "      <td>1.457401</td>\n",
       "      <td>0.150637</td>\n",
       "      <td>1.609869</td>\n",
       "      <td>-0.129712</td>\n",
       "      <td>-0.653480</td>\n",
       "      <td>0.067321</td>\n",
       "      <td>0.263321</td>\n",
       "      <td>0.426605</td>\n",
       "      <td>0.501643</td>\n",
       "      <td>-1.036744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.140740</td>\n",
       "      <td>-0.697750</td>\n",
       "      <td>-0.078105</td>\n",
       "      <td>1.213214</td>\n",
       "      <td>0.389643</td>\n",
       "      <td>0.432378</td>\n",
       "      <td>-0.717688</td>\n",
       "      <td>-0.226583</td>\n",
       "      <td>-0.349815</td>\n",
       "      <td>0.515637</td>\n",
       "      <td>-1.408285</td>\n",
       "      <td>-0.947898</td>\n",
       "      <td>-2.351937</td>\n",
       "      <td>-0.050678</td>\n",
       "      <td>-0.972121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.439955</td>\n",
       "      <td>1.247315</td>\n",
       "      <td>3.200634</td>\n",
       "      <td>-1.145203</td>\n",
       "      <td>1.117850</td>\n",
       "      <td>6.688355</td>\n",
       "      <td>-1.937410</td>\n",
       "      <td>-2.205534</td>\n",
       "      <td>2.101825</td>\n",
       "      <td>0.688882</td>\n",
       "      <td>-0.205462</td>\n",
       "      <td>0.364601</td>\n",
       "      <td>0.260554</td>\n",
       "      <td>0.313053</td>\n",
       "      <td>2.024033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.331532</td>\n",
       "      <td>2.341785</td>\n",
       "      <td>-1.232488</td>\n",
       "      <td>-0.156477</td>\n",
       "      <td>-3.362806</td>\n",
       "      <td>1.091200</td>\n",
       "      <td>1.030463</td>\n",
       "      <td>1.909391</td>\n",
       "      <td>2.530850</td>\n",
       "      <td>-0.739431</td>\n",
       "      <td>-1.875645</td>\n",
       "      <td>-0.135096</td>\n",
       "      <td>0.315817</td>\n",
       "      <td>-0.427052</td>\n",
       "      <td>-1.371479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.740709</td>\n",
       "      <td>0.828769</td>\n",
       "      <td>0.927163</td>\n",
       "      <td>0.892640</td>\n",
       "      <td>2.245123</td>\n",
       "      <td>-0.807667</td>\n",
       "      <td>-1.876103</td>\n",
       "      <td>-1.593975</td>\n",
       "      <td>-1.135828</td>\n",
       "      <td>0.018016</td>\n",
       "      <td>-0.295895</td>\n",
       "      <td>1.626210</td>\n",
       "      <td>-0.514271</td>\n",
       "      <td>1.036927</td>\n",
       "      <td>-0.852986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-4.642564</td>\n",
       "      <td>-0.717202</td>\n",
       "      <td>0.587517</td>\n",
       "      <td>1.286594</td>\n",
       "      <td>-0.691724</td>\n",
       "      <td>0.122103</td>\n",
       "      <td>-1.742016</td>\n",
       "      <td>-0.377915</td>\n",
       "      <td>-0.806137</td>\n",
       "      <td>-2.191155</td>\n",
       "      <td>0.055403</td>\n",
       "      <td>-2.169896</td>\n",
       "      <td>1.347923</td>\n",
       "      <td>1.473645</td>\n",
       "      <td>-0.216288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.152011</td>\n",
       "      <td>-0.452012</td>\n",
       "      <td>-2.155987</td>\n",
       "      <td>-0.250613</td>\n",
       "      <td>-1.886141</td>\n",
       "      <td>-0.669113</td>\n",
       "      <td>-0.398398</td>\n",
       "      <td>-0.098967</td>\n",
       "      <td>-0.009034</td>\n",
       "      <td>0.213666</td>\n",
       "      <td>-0.483635</td>\n",
       "      <td>-0.128801</td>\n",
       "      <td>-0.858140</td>\n",
       "      <td>0.793888</td>\n",
       "      <td>0.352451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-4.529264</td>\n",
       "      <td>7.711354</td>\n",
       "      <td>5.976125</td>\n",
       "      <td>-0.703807</td>\n",
       "      <td>1.147005</td>\n",
       "      <td>-1.898391</td>\n",
       "      <td>0.877395</td>\n",
       "      <td>0.956841</td>\n",
       "      <td>0.268671</td>\n",
       "      <td>1.720419</td>\n",
       "      <td>-0.547851</td>\n",
       "      <td>-0.294165</td>\n",
       "      <td>1.870524</td>\n",
       "      <td>-1.172574</td>\n",
       "      <td>-0.110628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-2.869683</td>\n",
       "      <td>-3.989463</td>\n",
       "      <td>1.656218</td>\n",
       "      <td>7.961341</td>\n",
       "      <td>-0.122130</td>\n",
       "      <td>2.838990</td>\n",
       "      <td>0.028515</td>\n",
       "      <td>1.667645</td>\n",
       "      <td>0.435478</td>\n",
       "      <td>0.391847</td>\n",
       "      <td>-0.782076</td>\n",
       "      <td>-0.714400</td>\n",
       "      <td>-0.220016</td>\n",
       "      <td>-1.030817</td>\n",
       "      <td>-0.265430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.053026</td>\n",
       "      <td>-3.722053</td>\n",
       "      <td>1.035552</td>\n",
       "      <td>-1.727658</td>\n",
       "      <td>0.068865</td>\n",
       "      <td>-1.561834</td>\n",
       "      <td>-0.147944</td>\n",
       "      <td>-0.393356</td>\n",
       "      <td>0.474368</td>\n",
       "      <td>-0.625392</td>\n",
       "      <td>-0.756562</td>\n",
       "      <td>-0.110921</td>\n",
       "      <td>-0.156703</td>\n",
       "      <td>0.723130</td>\n",
       "      <td>0.791769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3.284389</td>\n",
       "      <td>1.625994</td>\n",
       "      <td>-1.772477</td>\n",
       "      <td>-1.627435</td>\n",
       "      <td>-1.805889</td>\n",
       "      <td>-0.423319</td>\n",
       "      <td>-1.337680</td>\n",
       "      <td>-0.609734</td>\n",
       "      <td>-0.307751</td>\n",
       "      <td>0.247669</td>\n",
       "      <td>-0.124321</td>\n",
       "      <td>-0.837132</td>\n",
       "      <td>0.102442</td>\n",
       "      <td>-0.174225</td>\n",
       "      <td>0.638818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.812365</td>\n",
       "      <td>-4.664213</td>\n",
       "      <td>1.349804</td>\n",
       "      <td>0.073306</td>\n",
       "      <td>-1.233021</td>\n",
       "      <td>0.270338</td>\n",
       "      <td>0.800198</td>\n",
       "      <td>0.414237</td>\n",
       "      <td>-0.030155</td>\n",
       "      <td>-0.401531</td>\n",
       "      <td>0.788180</td>\n",
       "      <td>-0.322978</td>\n",
       "      <td>0.766548</td>\n",
       "      <td>-1.161587</td>\n",
       "      <td>1.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.722073</td>\n",
       "      <td>-5.762758</td>\n",
       "      <td>0.048869</td>\n",
       "      <td>-1.150061</td>\n",
       "      <td>1.087302</td>\n",
       "      <td>-1.449486</td>\n",
       "      <td>-0.451727</td>\n",
       "      <td>1.079882</td>\n",
       "      <td>-0.662080</td>\n",
       "      <td>-0.188961</td>\n",
       "      <td>0.447289</td>\n",
       "      <td>0.208805</td>\n",
       "      <td>0.455181</td>\n",
       "      <td>-0.668824</td>\n",
       "      <td>0.326968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.981643</td>\n",
       "      <td>0.198028</td>\n",
       "      <td>-1.838218</td>\n",
       "      <td>-2.028069</td>\n",
       "      <td>0.751388</td>\n",
       "      <td>-1.647696</td>\n",
       "      <td>-0.189821</td>\n",
       "      <td>-1.449425</td>\n",
       "      <td>-0.794569</td>\n",
       "      <td>0.622179</td>\n",
       "      <td>-1.611867</td>\n",
       "      <td>-0.764073</td>\n",
       "      <td>-0.794396</td>\n",
       "      <td>-0.221981</td>\n",
       "      <td>0.317043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.543612</td>\n",
       "      <td>1.146375</td>\n",
       "      <td>2.459289</td>\n",
       "      <td>-1.249994</td>\n",
       "      <td>-0.503723</td>\n",
       "      <td>-0.467052</td>\n",
       "      <td>-0.753764</td>\n",
       "      <td>0.361273</td>\n",
       "      <td>-0.823933</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.488224</td>\n",
       "      <td>0.089149</td>\n",
       "      <td>-0.640054</td>\n",
       "      <td>-1.390089</td>\n",
       "      <td>0.430752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.469242</td>\n",
       "      <td>5.407940</td>\n",
       "      <td>2.044487</td>\n",
       "      <td>3.587785</td>\n",
       "      <td>2.895818</td>\n",
       "      <td>-1.918990</td>\n",
       "      <td>0.156949</td>\n",
       "      <td>3.443601</td>\n",
       "      <td>0.807391</td>\n",
       "      <td>-2.174460</td>\n",
       "      <td>0.133119</td>\n",
       "      <td>-0.875390</td>\n",
       "      <td>-1.344256</td>\n",
       "      <td>1.117719</td>\n",
       "      <td>0.360861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.079178</td>\n",
       "      <td>4.858396</td>\n",
       "      <td>6.476012</td>\n",
       "      <td>-0.442104</td>\n",
       "      <td>-1.038055</td>\n",
       "      <td>-0.279410</td>\n",
       "      <td>0.301823</td>\n",
       "      <td>-0.666536</td>\n",
       "      <td>-1.570552</td>\n",
       "      <td>-0.106821</td>\n",
       "      <td>0.362887</td>\n",
       "      <td>-0.042008</td>\n",
       "      <td>-0.073851</td>\n",
       "      <td>-0.281071</td>\n",
       "      <td>0.299150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.876758</td>\n",
       "      <td>-1.746707</td>\n",
       "      <td>0.088414</td>\n",
       "      <td>-3.570328</td>\n",
       "      <td>4.062553</td>\n",
       "      <td>3.407122</td>\n",
       "      <td>-0.111886</td>\n",
       "      <td>0.391154</td>\n",
       "      <td>3.065732</td>\n",
       "      <td>-1.263998</td>\n",
       "      <td>0.081447</td>\n",
       "      <td>-0.448217</td>\n",
       "      <td>0.121440</td>\n",
       "      <td>0.831104</td>\n",
       "      <td>-0.557750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-4.280919</td>\n",
       "      <td>-0.940550</td>\n",
       "      <td>-1.085661</td>\n",
       "      <td>-0.137350</td>\n",
       "      <td>-1.536148</td>\n",
       "      <td>-0.857371</td>\n",
       "      <td>2.277370</td>\n",
       "      <td>0.153644</td>\n",
       "      <td>-0.061223</td>\n",
       "      <td>-0.828494</td>\n",
       "      <td>0.548459</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>0.655956</td>\n",
       "      <td>0.695040</td>\n",
       "      <td>0.321416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-2.359771</td>\n",
       "      <td>0.294710</td>\n",
       "      <td>0.504207</td>\n",
       "      <td>1.889936</td>\n",
       "      <td>-0.635022</td>\n",
       "      <td>-0.528786</td>\n",
       "      <td>-1.046947</td>\n",
       "      <td>-1.370748</td>\n",
       "      <td>1.660338</td>\n",
       "      <td>2.006907</td>\n",
       "      <td>3.265755</td>\n",
       "      <td>-0.870206</td>\n",
       "      <td>0.710513</td>\n",
       "      <td>-0.693567</td>\n",
       "      <td>-1.599047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-3.273264</td>\n",
       "      <td>1.403529</td>\n",
       "      <td>2.378630</td>\n",
       "      <td>-1.532735</td>\n",
       "      <td>-2.428283</td>\n",
       "      <td>1.772230</td>\n",
       "      <td>-0.252291</td>\n",
       "      <td>0.536974</td>\n",
       "      <td>-2.795754</td>\n",
       "      <td>-1.212852</td>\n",
       "      <td>1.959175</td>\n",
       "      <td>-0.317128</td>\n",
       "      <td>-1.172103</td>\n",
       "      <td>1.750999</td>\n",
       "      <td>0.296433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.609814</td>\n",
       "      <td>-1.411182</td>\n",
       "      <td>-1.390727</td>\n",
       "      <td>2.126998</td>\n",
       "      <td>1.444667</td>\n",
       "      <td>-1.462641</td>\n",
       "      <td>3.207669</td>\n",
       "      <td>-1.566901</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>2.398672</td>\n",
       "      <td>1.803648</td>\n",
       "      <td>-1.732204</td>\n",
       "      <td>0.701080</td>\n",
       "      <td>1.616227</td>\n",
       "      <td>1.054350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-4.154727</td>\n",
       "      <td>2.624511</td>\n",
       "      <td>-1.635260</td>\n",
       "      <td>-0.310684</td>\n",
       "      <td>3.984218</td>\n",
       "      <td>-1.170484</td>\n",
       "      <td>-0.189599</td>\n",
       "      <td>-1.459431</td>\n",
       "      <td>-0.333160</td>\n",
       "      <td>-1.486847</td>\n",
       "      <td>-1.598887</td>\n",
       "      <td>0.141661</td>\n",
       "      <td>0.863022</td>\n",
       "      <td>-0.643438</td>\n",
       "      <td>-0.360442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.124486</td>\n",
       "      <td>-0.184768</td>\n",
       "      <td>-3.049523</td>\n",
       "      <td>-0.183337</td>\n",
       "      <td>1.939351</td>\n",
       "      <td>0.803696</td>\n",
       "      <td>0.559215</td>\n",
       "      <td>-1.631977</td>\n",
       "      <td>-1.484942</td>\n",
       "      <td>0.206112</td>\n",
       "      <td>0.128226</td>\n",
       "      <td>-2.349665</td>\n",
       "      <td>-1.028007</td>\n",
       "      <td>-1.128141</td>\n",
       "      <td>-1.147717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.359006</td>\n",
       "      <td>0.624372</td>\n",
       "      <td>-2.263304</td>\n",
       "      <td>-0.099028</td>\n",
       "      <td>0.432633</td>\n",
       "      <td>0.860355</td>\n",
       "      <td>2.279301</td>\n",
       "      <td>2.033284</td>\n",
       "      <td>-0.700433</td>\n",
       "      <td>-0.675031</td>\n",
       "      <td>0.849690</td>\n",
       "      <td>1.651944</td>\n",
       "      <td>-0.271965</td>\n",
       "      <td>-0.617326</td>\n",
       "      <td>1.291018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.760148</td>\n",
       "      <td>-2.371987</td>\n",
       "      <td>-0.234418</td>\n",
       "      <td>-1.015324</td>\n",
       "      <td>0.623879</td>\n",
       "      <td>0.679903</td>\n",
       "      <td>2.560761</td>\n",
       "      <td>-0.600651</td>\n",
       "      <td>-0.138108</td>\n",
       "      <td>0.824874</td>\n",
       "      <td>-0.933405</td>\n",
       "      <td>-1.480644</td>\n",
       "      <td>-0.735640</td>\n",
       "      <td>0.027311</td>\n",
       "      <td>-0.301064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.653130</td>\n",
       "      <td>-2.818744</td>\n",
       "      <td>-1.607957</td>\n",
       "      <td>0.609383</td>\n",
       "      <td>-0.678972</td>\n",
       "      <td>-1.058462</td>\n",
       "      <td>-2.632719</td>\n",
       "      <td>1.045895</td>\n",
       "      <td>-0.484135</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>-1.269443</td>\n",
       "      <td>2.133972</td>\n",
       "      <td>1.222439</td>\n",
       "      <td>1.304933</td>\n",
       "      <td>0.255665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.547394</td>\n",
       "      <td>0.618854</td>\n",
       "      <td>0.405969</td>\n",
       "      <td>-2.025326</td>\n",
       "      <td>-0.656446</td>\n",
       "      <td>-0.830877</td>\n",
       "      <td>-1.013520</td>\n",
       "      <td>-1.092355</td>\n",
       "      <td>0.893541</td>\n",
       "      <td>-0.030879</td>\n",
       "      <td>0.658953</td>\n",
       "      <td>-0.584398</td>\n",
       "      <td>-1.751193</td>\n",
       "      <td>-0.584552</td>\n",
       "      <td>-0.795184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.775552</td>\n",
       "      <td>-1.669438</td>\n",
       "      <td>1.416447</td>\n",
       "      <td>0.020834</td>\n",
       "      <td>-1.027083</td>\n",
       "      <td>1.448599</td>\n",
       "      <td>1.699371</td>\n",
       "      <td>-0.912932</td>\n",
       "      <td>-0.664043</td>\n",
       "      <td>-0.876194</td>\n",
       "      <td>0.917177</td>\n",
       "      <td>2.519313</td>\n",
       "      <td>0.157846</td>\n",
       "      <td>-1.425455</td>\n",
       "      <td>-1.265923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.995396</td>\n",
       "      <td>-2.928256</td>\n",
       "      <td>0.948012</td>\n",
       "      <td>-2.259916</td>\n",
       "      <td>0.214203</td>\n",
       "      <td>-1.562274</td>\n",
       "      <td>-1.492575</td>\n",
       "      <td>1.551249</td>\n",
       "      <td>1.195748</td>\n",
       "      <td>-0.036269</td>\n",
       "      <td>-0.325929</td>\n",
       "      <td>-1.644863</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>-0.838148</td>\n",
       "      <td>0.280224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-4.140854</td>\n",
       "      <td>-0.953787</td>\n",
       "      <td>0.877725</td>\n",
       "      <td>-0.825495</td>\n",
       "      <td>-0.658693</td>\n",
       "      <td>1.030544</td>\n",
       "      <td>3.427306</td>\n",
       "      <td>-0.421493</td>\n",
       "      <td>-0.161957</td>\n",
       "      <td>-1.314179</td>\n",
       "      <td>-0.694314</td>\n",
       "      <td>-0.037875</td>\n",
       "      <td>-0.295523</td>\n",
       "      <td>-0.067236</td>\n",
       "      <td>-0.275071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.349596</td>\n",
       "      <td>2.478563</td>\n",
       "      <td>-2.991649</td>\n",
       "      <td>-0.592820</td>\n",
       "      <td>-1.103523</td>\n",
       "      <td>0.772147</td>\n",
       "      <td>-0.336779</td>\n",
       "      <td>0.470762</td>\n",
       "      <td>-0.654677</td>\n",
       "      <td>2.058086</td>\n",
       "      <td>-1.727174</td>\n",
       "      <td>0.235711</td>\n",
       "      <td>0.979919</td>\n",
       "      <td>-1.013211</td>\n",
       "      <td>0.761896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.909118</td>\n",
       "      <td>1.995648</td>\n",
       "      <td>-1.298704</td>\n",
       "      <td>-1.280191</td>\n",
       "      <td>0.109695</td>\n",
       "      <td>-1.920267</td>\n",
       "      <td>-0.466316</td>\n",
       "      <td>1.655836</td>\n",
       "      <td>1.341364</td>\n",
       "      <td>-0.367356</td>\n",
       "      <td>1.364854</td>\n",
       "      <td>-0.794393</td>\n",
       "      <td>0.530372</td>\n",
       "      <td>0.503619</td>\n",
       "      <td>1.165279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-3.426426</td>\n",
       "      <td>-0.185324</td>\n",
       "      <td>0.093438</td>\n",
       "      <td>1.929303</td>\n",
       "      <td>1.057349</td>\n",
       "      <td>-1.074352</td>\n",
       "      <td>1.739430</td>\n",
       "      <td>-1.455062</td>\n",
       "      <td>0.635660</td>\n",
       "      <td>1.263412</td>\n",
       "      <td>-0.285448</td>\n",
       "      <td>1.439035</td>\n",
       "      <td>0.381931</td>\n",
       "      <td>0.586112</td>\n",
       "      <td>0.602512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-5.484454</td>\n",
       "      <td>2.915850</td>\n",
       "      <td>-0.437903</td>\n",
       "      <td>-0.296954</td>\n",
       "      <td>0.746910</td>\n",
       "      <td>0.261581</td>\n",
       "      <td>-3.032632</td>\n",
       "      <td>-0.143216</td>\n",
       "      <td>-0.750588</td>\n",
       "      <td>-0.361446</td>\n",
       "      <td>1.073872</td>\n",
       "      <td>0.524738</td>\n",
       "      <td>0.210793</td>\n",
       "      <td>-0.813594</td>\n",
       "      <td>-0.393196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.842383</td>\n",
       "      <td>1.168107</td>\n",
       "      <td>-3.530938</td>\n",
       "      <td>0.542415</td>\n",
       "      <td>1.467264</td>\n",
       "      <td>1.665322</td>\n",
       "      <td>0.338850</td>\n",
       "      <td>3.682493</td>\n",
       "      <td>-1.122758</td>\n",
       "      <td>0.685686</td>\n",
       "      <td>1.479327</td>\n",
       "      <td>0.182758</td>\n",
       "      <td>0.357705</td>\n",
       "      <td>-0.559687</td>\n",
       "      <td>-0.449746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-3.859206</td>\n",
       "      <td>0.932301</td>\n",
       "      <td>0.995010</td>\n",
       "      <td>-1.130591</td>\n",
       "      <td>-0.640277</td>\n",
       "      <td>1.088846</td>\n",
       "      <td>1.531841</td>\n",
       "      <td>-0.076325</td>\n",
       "      <td>-1.560403</td>\n",
       "      <td>1.208993</td>\n",
       "      <td>0.432534</td>\n",
       "      <td>0.810970</td>\n",
       "      <td>-0.633337</td>\n",
       "      <td>1.474526</td>\n",
       "      <td>-0.384077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.630043</td>\n",
       "      <td>-4.048215</td>\n",
       "      <td>1.412219</td>\n",
       "      <td>-0.617767</td>\n",
       "      <td>-2.085899</td>\n",
       "      <td>-1.228449</td>\n",
       "      <td>-0.642889</td>\n",
       "      <td>-0.682331</td>\n",
       "      <td>0.270737</td>\n",
       "      <td>-1.148365</td>\n",
       "      <td>0.249665</td>\n",
       "      <td>0.473733</td>\n",
       "      <td>0.540636</td>\n",
       "      <td>-0.215294</td>\n",
       "      <td>-1.102434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-3.541994</td>\n",
       "      <td>-1.512857</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>-0.645549</td>\n",
       "      <td>-0.141283</td>\n",
       "      <td>0.367132</td>\n",
       "      <td>3.599450</td>\n",
       "      <td>-0.481612</td>\n",
       "      <td>-1.020822</td>\n",
       "      <td>-0.464140</td>\n",
       "      <td>0.256917</td>\n",
       "      <td>-0.092862</td>\n",
       "      <td>0.246512</td>\n",
       "      <td>-0.613936</td>\n",
       "      <td>0.138243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.758197</td>\n",
       "      <td>3.102408</td>\n",
       "      <td>-3.121930</td>\n",
       "      <td>2.836822</td>\n",
       "      <td>0.200779</td>\n",
       "      <td>-0.566310</td>\n",
       "      <td>0.476589</td>\n",
       "      <td>-2.541758</td>\n",
       "      <td>1.794122</td>\n",
       "      <td>-1.633776</td>\n",
       "      <td>1.174704</td>\n",
       "      <td>2.030755</td>\n",
       "      <td>-0.243470</td>\n",
       "      <td>0.194761</td>\n",
       "      <td>0.236081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3.121711</td>\n",
       "      <td>-0.512316</td>\n",
       "      <td>3.151777</td>\n",
       "      <td>1.587287</td>\n",
       "      <td>-1.260692</td>\n",
       "      <td>-2.117567</td>\n",
       "      <td>1.464426</td>\n",
       "      <td>-1.162968</td>\n",
       "      <td>1.344825</td>\n",
       "      <td>-0.799656</td>\n",
       "      <td>-2.338361</td>\n",
       "      <td>0.538934</td>\n",
       "      <td>-0.989138</td>\n",
       "      <td>-0.097682</td>\n",
       "      <td>0.526875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.539776</td>\n",
       "      <td>-0.546486</td>\n",
       "      <td>-1.266741</td>\n",
       "      <td>1.350942</td>\n",
       "      <td>3.165245</td>\n",
       "      <td>-1.325277</td>\n",
       "      <td>-1.853660</td>\n",
       "      <td>-1.246600</td>\n",
       "      <td>-0.536906</td>\n",
       "      <td>-0.242334</td>\n",
       "      <td>0.908215</td>\n",
       "      <td>0.740791</td>\n",
       "      <td>-0.780551</td>\n",
       "      <td>-1.005695</td>\n",
       "      <td>0.597065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.518201</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>-0.142766</td>\n",
       "      <td>-3.428792</td>\n",
       "      <td>-1.094768</td>\n",
       "      <td>-1.028888</td>\n",
       "      <td>-0.151944</td>\n",
       "      <td>2.207510</td>\n",
       "      <td>0.860628</td>\n",
       "      <td>1.267603</td>\n",
       "      <td>0.426842</td>\n",
       "      <td>-0.331028</td>\n",
       "      <td>0.754451</td>\n",
       "      <td>-1.357958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.618293</td>\n",
       "      <td>3.198968</td>\n",
       "      <td>-3.625486</td>\n",
       "      <td>1.374791</td>\n",
       "      <td>-1.501780</td>\n",
       "      <td>1.486402</td>\n",
       "      <td>-0.726926</td>\n",
       "      <td>-1.324375</td>\n",
       "      <td>-1.248136</td>\n",
       "      <td>-0.296907</td>\n",
       "      <td>-0.122874</td>\n",
       "      <td>-1.116912</td>\n",
       "      <td>0.681493</td>\n",
       "      <td>-0.342791</td>\n",
       "      <td>0.724378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   3.058838 -3.042497  0.592587 -2.670440  3.192927 -1.046873  1.029494   \n",
       "1   7.384053  3.087423 -1.753993  1.048708 -0.984794  0.560115  0.018172   \n",
       "2  -2.468811  2.583222 -1.751520 -1.156369 -2.058581 -0.224020 -0.163810   \n",
       "3  -1.054052 -2.297926  1.693058  1.373355  0.907887  1.014382 -1.411897   \n",
       "4  -0.659829 -0.179576 -2.408590  0.339011 -1.717306 -0.551839 -1.397649   \n",
       "5   0.146169 -6.359829  2.182085  0.853193 -0.380818 -0.375561 -2.201688   \n",
       "6  -0.508660  1.305421 -2.018848 -1.923465  0.315331  1.457401  0.150637   \n",
       "7   0.140740 -0.697750 -0.078105  1.213214  0.389643  0.432378 -0.717688   \n",
       "8   1.439955  1.247315  3.200634 -1.145203  1.117850  6.688355 -1.937410   \n",
       "9  -1.331532  2.341785 -1.232488 -0.156477 -3.362806  1.091200  1.030463   \n",
       "10 -2.740709  0.828769  0.927163  0.892640  2.245123 -0.807667 -1.876103   \n",
       "11 -4.642564 -0.717202  0.587517  1.286594 -0.691724  0.122103 -1.742016   \n",
       "12 -0.152011 -0.452012 -2.155987 -0.250613 -1.886141 -0.669113 -0.398398   \n",
       "13 -4.529264  7.711354  5.976125 -0.703807  1.147005 -1.898391  0.877395   \n",
       "14 -2.869683 -3.989463  1.656218  7.961341 -0.122130  2.838990  0.028515   \n",
       "15  1.053026 -3.722053  1.035552 -1.727658  0.068865 -1.561834 -0.147944   \n",
       "16 -3.284389  1.625994 -1.772477 -1.627435 -1.805889 -0.423319 -1.337680   \n",
       "17 -0.812365 -4.664213  1.349804  0.073306 -1.233021  0.270338  0.800198   \n",
       "18 -0.722073 -5.762758  0.048869 -1.150061  1.087302 -1.449486 -0.451727   \n",
       "19 -0.981643  0.198028 -1.838218 -2.028069  0.751388 -1.647696 -0.189821   \n",
       "20  3.543612  1.146375  2.459289 -1.249994 -0.503723 -0.467052 -0.753764   \n",
       "21  2.469242  5.407940  2.044487  3.587785  2.895818 -1.918990  0.156949   \n",
       "22  8.079178  4.858396  6.476012 -0.442104 -1.038055 -0.279410  0.301823   \n",
       "23  0.876758 -1.746707  0.088414 -3.570328  4.062553  3.407122 -0.111886   \n",
       "24 -4.280919 -0.940550 -1.085661 -0.137350 -1.536148 -0.857371  2.277370   \n",
       "25 -2.359771  0.294710  0.504207  1.889936 -0.635022 -0.528786 -1.046947   \n",
       "26 -3.273264  1.403529  2.378630 -1.532735 -2.428283  1.772230 -0.252291   \n",
       "27  4.609814 -1.411182 -1.390727  2.126998  1.444667 -1.462641  3.207669   \n",
       "28 -4.154727  2.624511 -1.635260 -0.310684  3.984218 -1.170484 -0.189599   \n",
       "29  2.124486 -0.184768 -3.049523 -0.183337  1.939351  0.803696  0.559215   \n",
       "30  2.359006  0.624372 -2.263304 -0.099028  0.432633  0.860355  2.279301   \n",
       "31  0.760148 -2.371987 -0.234418 -1.015324  0.623879  0.679903  2.560761   \n",
       "32  2.653130 -2.818744 -1.607957  0.609383 -0.678972 -1.058462 -2.632719   \n",
       "33  3.547394  0.618854  0.405969 -2.025326 -0.656446 -0.830877 -1.013520   \n",
       "34  0.775552 -1.669438  1.416447  0.020834 -1.027083  1.448599  1.699371   \n",
       "35 -0.995396 -2.928256  0.948012 -2.259916  0.214203 -1.562274 -1.492575   \n",
       "36 -4.140854 -0.953787  0.877725 -0.825495 -0.658693  1.030544  3.427306   \n",
       "37 -0.349596  2.478563 -2.991649 -0.592820 -1.103523  0.772147 -0.336779   \n",
       "38 -0.909118  1.995648 -1.298704 -1.280191  0.109695 -1.920267 -0.466316   \n",
       "39 -3.426426 -0.185324  0.093438  1.929303  1.057349 -1.074352  1.739430   \n",
       "40 -5.484454  2.915850 -0.437903 -0.296954  0.746910  0.261581 -3.032632   \n",
       "41  4.842383  1.168107 -3.530938  0.542415  1.467264  1.665322  0.338850   \n",
       "42 -3.859206  0.932301  0.995010 -1.130591 -0.640277  1.088846  1.531841   \n",
       "43  4.630043 -4.048215  1.412219 -0.617767 -2.085899 -1.228449 -0.642889   \n",
       "44 -3.541994 -1.512857  0.014779 -0.645549 -0.141283  0.367132  3.599450   \n",
       "45 -0.758197  3.102408 -3.121930  2.836822  0.200779 -0.566310  0.476589   \n",
       "46  3.121711 -0.512316  3.151777  1.587287 -1.260692 -2.117567  1.464426   \n",
       "47  0.539776 -0.546486 -1.266741  1.350942  3.165245 -1.325277 -1.853660   \n",
       "48  1.518201  0.016051  0.034396 -0.142766 -3.428792 -1.094768 -1.028888   \n",
       "49  4.618293  3.198968 -3.625486  1.374791 -1.501780  1.486402 -0.726926   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   1.157444  0.202582  0.999176  0.075532  1.522992  0.440319  0.404369   \n",
       "1  -1.017402 -0.995143 -1.006616 -1.668771 -0.273147  2.494626  0.930914   \n",
       "2   0.849418  0.937476  2.418076 -0.613754  0.241807 -1.064986  0.076976   \n",
       "3   1.511266 -1.255586  2.493167 -1.183379  0.580014 -0.830100  1.188165   \n",
       "4   0.583969  0.653088 -1.155476  0.092739  0.933833 -1.383331 -0.847976   \n",
       "5  -0.104019 -0.882482 -0.580981 -0.077579 -0.569028  1.318325 -0.212884   \n",
       "6   1.609869 -0.129712 -0.653480  0.067321  0.263321  0.426605  0.501643   \n",
       "7  -0.226583 -0.349815  0.515637 -1.408285 -0.947898 -2.351937 -0.050678   \n",
       "8  -2.205534  2.101825  0.688882 -0.205462  0.364601  0.260554  0.313053   \n",
       "9   1.909391  2.530850 -0.739431 -1.875645 -0.135096  0.315817 -0.427052   \n",
       "10 -1.593975 -1.135828  0.018016 -0.295895  1.626210 -0.514271  1.036927   \n",
       "11 -0.377915 -0.806137 -2.191155  0.055403 -2.169896  1.347923  1.473645   \n",
       "12 -0.098967 -0.009034  0.213666 -0.483635 -0.128801 -0.858140  0.793888   \n",
       "13  0.956841  0.268671  1.720419 -0.547851 -0.294165  1.870524 -1.172574   \n",
       "14  1.667645  0.435478  0.391847 -0.782076 -0.714400 -0.220016 -1.030817   \n",
       "15 -0.393356  0.474368 -0.625392 -0.756562 -0.110921 -0.156703  0.723130   \n",
       "16 -0.609734 -0.307751  0.247669 -0.124321 -0.837132  0.102442 -0.174225   \n",
       "17  0.414237 -0.030155 -0.401531  0.788180 -0.322978  0.766548 -1.161587   \n",
       "18  1.079882 -0.662080 -0.188961  0.447289  0.208805  0.455181 -0.668824   \n",
       "19 -1.449425 -0.794569  0.622179 -1.611867 -0.764073 -0.794396 -0.221981   \n",
       "20  0.361273 -0.823933  0.022153  0.488224  0.089149 -0.640054 -1.390089   \n",
       "21  3.443601  0.807391 -2.174460  0.133119 -0.875390 -1.344256  1.117719   \n",
       "22 -0.666536 -1.570552 -0.106821  0.362887 -0.042008 -0.073851 -0.281071   \n",
       "23  0.391154  3.065732 -1.263998  0.081447 -0.448217  0.121440  0.831104   \n",
       "24  0.153644 -0.061223 -0.828494  0.548459 -0.027589  0.655956  0.695040   \n",
       "25 -1.370748  1.660338  2.006907  3.265755 -0.870206  0.710513 -0.693567   \n",
       "26  0.536974 -2.795754 -1.212852  1.959175 -0.317128 -1.172103  1.750999   \n",
       "27 -1.566901  0.649518  2.398672  1.803648 -1.732204  0.701080  1.616227   \n",
       "28 -1.459431 -0.333160 -1.486847 -1.598887  0.141661  0.863022 -0.643438   \n",
       "29 -1.631977 -1.484942  0.206112  0.128226 -2.349665 -1.028007 -1.128141   \n",
       "30  2.033284 -0.700433 -0.675031  0.849690  1.651944 -0.271965 -0.617326   \n",
       "31 -0.600651 -0.138108  0.824874 -0.933405 -1.480644 -0.735640  0.027311   \n",
       "32  1.045895 -0.484135  0.998770 -1.269443  2.133972  1.222439  1.304933   \n",
       "33 -1.092355  0.893541 -0.030879  0.658953 -0.584398 -1.751193 -0.584552   \n",
       "34 -0.912932 -0.664043 -0.876194  0.917177  2.519313  0.157846 -1.425455   \n",
       "35  1.551249  1.195748 -0.036269 -0.325929 -1.644863  0.303475 -0.838148   \n",
       "36 -0.421493 -0.161957 -1.314179 -0.694314 -0.037875 -0.295523 -0.067236   \n",
       "37  0.470762 -0.654677  2.058086 -1.727174  0.235711  0.979919 -1.013211   \n",
       "38  1.655836  1.341364 -0.367356  1.364854 -0.794393  0.530372  0.503619   \n",
       "39 -1.455062  0.635660  1.263412 -0.285448  1.439035  0.381931  0.586112   \n",
       "40 -0.143216 -0.750588 -0.361446  1.073872  0.524738  0.210793 -0.813594   \n",
       "41  3.682493 -1.122758  0.685686  1.479327  0.182758  0.357705 -0.559687   \n",
       "42 -0.076325 -1.560403  1.208993  0.432534  0.810970 -0.633337  1.474526   \n",
       "43 -0.682331  0.270737 -1.148365  0.249665  0.473733  0.540636 -0.215294   \n",
       "44 -0.481612 -1.020822 -0.464140  0.256917 -0.092862  0.246512 -0.613936   \n",
       "45 -2.541758  1.794122 -1.633776  1.174704  2.030755 -0.243470  0.194761   \n",
       "46 -1.162968  1.344825 -0.799656 -2.338361  0.538934 -0.989138 -0.097682   \n",
       "47 -1.246600 -0.536906 -0.242334  0.908215  0.740791 -0.780551 -1.005695   \n",
       "48 -0.151944  2.207510  0.860628  1.267603  0.426842 -0.331028  0.754451   \n",
       "49 -1.324375 -1.248136 -0.296907 -0.122874 -1.116912  0.681493 -0.342791   \n",
       "\n",
       "          14  \n",
       "0  -0.978824  \n",
       "1  -1.094065  \n",
       "2   0.510627  \n",
       "3  -0.527131  \n",
       "4   1.304050  \n",
       "5   0.461949  \n",
       "6  -1.036744  \n",
       "7  -0.972121  \n",
       "8   2.024033  \n",
       "9  -1.371479  \n",
       "10 -0.852986  \n",
       "11 -0.216288  \n",
       "12  0.352451  \n",
       "13 -0.110628  \n",
       "14 -0.265430  \n",
       "15  0.791769  \n",
       "16  0.638818  \n",
       "17  1.345400  \n",
       "18  0.326968  \n",
       "19  0.317043  \n",
       "20  0.430752  \n",
       "21  0.360861  \n",
       "22  0.299150  \n",
       "23 -0.557750  \n",
       "24  0.321416  \n",
       "25 -1.599047  \n",
       "26  0.296433  \n",
       "27  1.054350  \n",
       "28 -0.360442  \n",
       "29 -1.147717  \n",
       "30  1.291018  \n",
       "31 -0.301064  \n",
       "32  0.255665  \n",
       "33 -0.795184  \n",
       "34 -1.265923  \n",
       "35  0.280224  \n",
       "36 -0.275071  \n",
       "37  0.761896  \n",
       "38  1.165279  \n",
       "39  0.602512  \n",
       "40 -0.393196  \n",
       "41 -0.449746  \n",
       "42 -0.384077  \n",
       "43 -1.102434  \n",
       "44  0.138243  \n",
       "45  0.236081  \n",
       "46  0.526875  \n",
       "47  0.597065  \n",
       "48 -1.357958  \n",
       "49  0.724378  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_all_grouped_pca = pd.DataFrame(lifesnaps_all_grouped_pca)\n",
    "lifesnaps_all_grouped_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f8c4310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering(lifesnaps_all_grouped_pca, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "397a6898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "silhouette(lifesnaps_all_grouped_pca, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16748b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster\n",
       "5          12\n",
       "2           9\n",
       "6           8\n",
       "1           7\n",
       "3           6\n",
       "0           5\n",
       "7           2\n",
       "4           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 8, max_iter = 500, random_state = 0)\n",
    "y = kmeans.fit_predict(lifesnaps_all_grouped)\n",
    "y = pd.DataFrame(y, columns=[\"Cluster\"])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70a51b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.137667</td>\n",
       "      <td>0.550383</td>\n",
       "      <td>1.185662</td>\n",
       "      <td>0.130018</td>\n",
       "      <td>0.132717</td>\n",
       "      <td>0.237689</td>\n",
       "      <td>0.442019</td>\n",
       "      <td>1.130410</td>\n",
       "      <td>0.477346</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.107376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.687777</td>\n",
       "      <td>0.469691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.399011</td>\n",
       "      <td>-0.547277</td>\n",
       "      <td>0.066628</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>1.076337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.490072</td>\n",
       "      <td>1.829539</td>\n",
       "      <td>0.973492</td>\n",
       "      <td>0.451844</td>\n",
       "      <td>1.434234</td>\n",
       "      <td>0.426821</td>\n",
       "      <td>0.428736</td>\n",
       "      <td>0.385960</td>\n",
       "      <td>0.738604</td>\n",
       "      <td>-0.333509</td>\n",
       "      <td>-0.314290</td>\n",
       "      <td>1.252565</td>\n",
       "      <td>-0.482196</td>\n",
       "      <td>-0.933817</td>\n",
       "      <td>0.427678</td>\n",
       "      <td>0.473683</td>\n",
       "      <td>0.830391</td>\n",
       "      <td>0.531479</td>\n",
       "      <td>0.279630</td>\n",
       "      <td>-0.012719</td>\n",
       "      <td>0.578325</td>\n",
       "      <td>1.056755</td>\n",
       "      <td>-0.306786</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141399</td>\n",
       "      <td>-0.265402</td>\n",
       "      <td>0.171501</td>\n",
       "      <td>-0.171290</td>\n",
       "      <td>0.056476</td>\n",
       "      <td>0.356920</td>\n",
       "      <td>-0.051042</td>\n",
       "      <td>0.696095</td>\n",
       "      <td>-0.089688</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>1.882600</td>\n",
       "      <td>1.988971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.875974</td>\n",
       "      <td>0.352373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218011</td>\n",
       "      <td>-0.680979</td>\n",
       "      <td>1.882675</td>\n",
       "      <td>1.830475</td>\n",
       "      <td>0.746658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.127403</td>\n",
       "      <td>-0.276357</td>\n",
       "      <td>0.738752</td>\n",
       "      <td>-0.064153</td>\n",
       "      <td>0.236187</td>\n",
       "      <td>1.271941</td>\n",
       "      <td>1.266694</td>\n",
       "      <td>0.478071</td>\n",
       "      <td>0.369794</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.586038</td>\n",
       "      <td>0.232745</td>\n",
       "      <td>0.177297</td>\n",
       "      <td>-0.400718</td>\n",
       "      <td>1.273364</td>\n",
       "      <td>1.254034</td>\n",
       "      <td>0.710051</td>\n",
       "      <td>0.084696</td>\n",
       "      <td>-0.300134</td>\n",
       "      <td>-0.044517</td>\n",
       "      <td>0.269551</td>\n",
       "      <td>-0.615147</td>\n",
       "      <td>0.432787</td>\n",
       "      <td>-0.349927</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.729268</td>\n",
       "      <td>-0.763622</td>\n",
       "      <td>0.394814</td>\n",
       "      <td>0.465241</td>\n",
       "      <td>1.350526</td>\n",
       "      <td>-1.043366</td>\n",
       "      <td>0.675427</td>\n",
       "      <td>-0.215715</td>\n",
       "      <td>0.958330</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.526517</td>\n",
       "      <td>-0.660204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510424</td>\n",
       "      <td>-0.357826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.334134</td>\n",
       "      <td>-0.501232</td>\n",
       "      <td>-0.526484</td>\n",
       "      <td>-0.433413</td>\n",
       "      <td>-0.222544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.398581</td>\n",
       "      <td>0.506550</td>\n",
       "      <td>-0.231212</td>\n",
       "      <td>0.948042</td>\n",
       "      <td>-0.323190</td>\n",
       "      <td>-0.380954</td>\n",
       "      <td>-0.504273</td>\n",
       "      <td>0.652379</td>\n",
       "      <td>-0.863699</td>\n",
       "      <td>0.441288</td>\n",
       "      <td>-0.136108</td>\n",
       "      <td>0.323102</td>\n",
       "      <td>-1.381621</td>\n",
       "      <td>-0.468772</td>\n",
       "      <td>-0.380595</td>\n",
       "      <td>-0.349778</td>\n",
       "      <td>-0.181501</td>\n",
       "      <td>1.130483</td>\n",
       "      <td>-0.209791</td>\n",
       "      <td>-0.238835</td>\n",
       "      <td>0.260279</td>\n",
       "      <td>0.705600</td>\n",
       "      <td>-0.157654</td>\n",
       "      <td>0.589692</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.215018</td>\n",
       "      <td>0.108768</td>\n",
       "      <td>-0.378641</td>\n",
       "      <td>0.451444</td>\n",
       "      <td>-0.482256</td>\n",
       "      <td>-0.084416</td>\n",
       "      <td>-0.235959</td>\n",
       "      <td>-0.436745</td>\n",
       "      <td>0.332192</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.907860</td>\n",
       "      <td>-0.695193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.397867</td>\n",
       "      <td>-0.359792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281590</td>\n",
       "      <td>0.116243</td>\n",
       "      <td>-0.907851</td>\n",
       "      <td>-0.943348</td>\n",
       "      <td>-0.445318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100493</td>\n",
       "      <td>-0.119136</td>\n",
       "      <td>-0.291804</td>\n",
       "      <td>0.283661</td>\n",
       "      <td>-0.458499</td>\n",
       "      <td>-1.004433</td>\n",
       "      <td>-0.857977</td>\n",
       "      <td>0.125961</td>\n",
       "      <td>0.113321</td>\n",
       "      <td>-0.660402</td>\n",
       "      <td>-0.260609</td>\n",
       "      <td>-0.462203</td>\n",
       "      <td>0.257841</td>\n",
       "      <td>0.424024</td>\n",
       "      <td>-1.004411</td>\n",
       "      <td>-1.047058</td>\n",
       "      <td>-0.300461</td>\n",
       "      <td>-0.640720</td>\n",
       "      <td>-1.151515</td>\n",
       "      <td>-1.130931</td>\n",
       "      <td>-1.062209</td>\n",
       "      <td>-0.165427</td>\n",
       "      <td>-0.370700</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.626681</td>\n",
       "      <td>0.250456</td>\n",
       "      <td>1.492975</td>\n",
       "      <td>0.108266</td>\n",
       "      <td>0.673616</td>\n",
       "      <td>2.109814</td>\n",
       "      <td>2.779153</td>\n",
       "      <td>4.128356</td>\n",
       "      <td>0.444591</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>1.337825</td>\n",
       "      <td>1.719055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.237353</td>\n",
       "      <td>5.244827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.179572</td>\n",
       "      <td>-0.603152</td>\n",
       "      <td>1.337846</td>\n",
       "      <td>1.527876</td>\n",
       "      <td>4.113281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.466930</td>\n",
       "      <td>2.280024</td>\n",
       "      <td>1.712249</td>\n",
       "      <td>0.339671</td>\n",
       "      <td>1.804765</td>\n",
       "      <td>0.705951</td>\n",
       "      <td>0.979918</td>\n",
       "      <td>-0.491994</td>\n",
       "      <td>4.112211</td>\n",
       "      <td>-0.096809</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>2.564876</td>\n",
       "      <td>1.150266</td>\n",
       "      <td>-1.287578</td>\n",
       "      <td>0.706988</td>\n",
       "      <td>0.910745</td>\n",
       "      <td>1.678622</td>\n",
       "      <td>2.443579</td>\n",
       "      <td>-0.141052</td>\n",
       "      <td>1.386371</td>\n",
       "      <td>0.811991</td>\n",
       "      <td>0.143205</td>\n",
       "      <td>0.651920</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.598680</td>\n",
       "      <td>0.229014</td>\n",
       "      <td>-0.482429</td>\n",
       "      <td>-0.043395</td>\n",
       "      <td>-0.493826</td>\n",
       "      <td>0.209991</td>\n",
       "      <td>0.249567</td>\n",
       "      <td>-0.109645</td>\n",
       "      <td>-0.529715</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.039246</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053433</td>\n",
       "      <td>-0.206403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161188</td>\n",
       "      <td>0.432870</td>\n",
       "      <td>-0.039299</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>-0.096860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>-0.315122</td>\n",
       "      <td>-0.553684</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.349118</td>\n",
       "      <td>0.357128</td>\n",
       "      <td>0.303746</td>\n",
       "      <td>-0.329584</td>\n",
       "      <td>-0.251297</td>\n",
       "      <td>-0.326167</td>\n",
       "      <td>-0.445904</td>\n",
       "      <td>0.255708</td>\n",
       "      <td>-0.106372</td>\n",
       "      <td>0.349914</td>\n",
       "      <td>0.401546</td>\n",
       "      <td>-0.232649</td>\n",
       "      <td>-0.513477</td>\n",
       "      <td>0.642572</td>\n",
       "      <td>0.366201</td>\n",
       "      <td>-0.551064</td>\n",
       "      <td>-0.350606</td>\n",
       "      <td>-0.051131</td>\n",
       "      <td>-0.714435</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.076418</td>\n",
       "      <td>-0.450485</td>\n",
       "      <td>-0.664613</td>\n",
       "      <td>-0.746205</td>\n",
       "      <td>-0.681638</td>\n",
       "      <td>0.728428</td>\n",
       "      <td>-0.278591</td>\n",
       "      <td>-0.001432</td>\n",
       "      <td>-0.558818</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>-0.854895</td>\n",
       "      <td>-0.721435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.603661</td>\n",
       "      <td>0.545796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.127097</td>\n",
       "      <td>1.041097</td>\n",
       "      <td>-0.854870</td>\n",
       "      <td>-0.855090</td>\n",
       "      <td>-0.028006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965797</td>\n",
       "      <td>-0.563677</td>\n",
       "      <td>-0.549746</td>\n",
       "      <td>-0.621283</td>\n",
       "      <td>-0.827011</td>\n",
       "      <td>-1.109536</td>\n",
       "      <td>-1.086231</td>\n",
       "      <td>-1.270576</td>\n",
       "      <td>0.036386</td>\n",
       "      <td>0.214996</td>\n",
       "      <td>0.109951</td>\n",
       "      <td>-0.665603</td>\n",
       "      <td>0.875813</td>\n",
       "      <td>1.067094</td>\n",
       "      <td>-1.109605</td>\n",
       "      <td>-1.171076</td>\n",
       "      <td>-0.556842</td>\n",
       "      <td>-0.763592</td>\n",
       "      <td>0.369335</td>\n",
       "      <td>0.412573</td>\n",
       "      <td>0.373866</td>\n",
       "      <td>-0.057405</td>\n",
       "      <td>0.148599</td>\n",
       "      <td>-0.204124</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.092540</td>\n",
       "      <td>-1.065650</td>\n",
       "      <td>1.345661</td>\n",
       "      <td>-1.472999</td>\n",
       "      <td>0.366713</td>\n",
       "      <td>0.293936</td>\n",
       "      <td>1.564185</td>\n",
       "      <td>-0.067610</td>\n",
       "      <td>0.661563</td>\n",
       "      <td>5.869317</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>-0.170357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.513477</td>\n",
       "      <td>-0.144457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522682</td>\n",
       "      <td>-0.144509</td>\n",
       "      <td>0.066577</td>\n",
       "      <td>0.115748</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228204</td>\n",
       "      <td>0.723210</td>\n",
       "      <td>-0.562235</td>\n",
       "      <td>0.618088</td>\n",
       "      <td>0.393546</td>\n",
       "      <td>-0.249975</td>\n",
       "      <td>-0.378746</td>\n",
       "      <td>0.465718</td>\n",
       "      <td>0.863818</td>\n",
       "      <td>-0.473999</td>\n",
       "      <td>-0.502668</td>\n",
       "      <td>0.715282</td>\n",
       "      <td>0.614697</td>\n",
       "      <td>-0.973413</td>\n",
       "      <td>-0.249589</td>\n",
       "      <td>-0.203557</td>\n",
       "      <td>-0.594566</td>\n",
       "      <td>0.389231</td>\n",
       "      <td>-0.512243</td>\n",
       "      <td>0.114471</td>\n",
       "      <td>1.031054</td>\n",
       "      <td>-0.906142</td>\n",
       "      <td>0.460179</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.137667  0.550383  1.185662  0.130018  0.132717  0.237689  0.442019   \n",
       "1 -0.141399 -0.265402  0.171501 -0.171290  0.056476  0.356920 -0.051042   \n",
       "2  0.729268 -0.763622  0.394814  0.465241  1.350526 -1.043366  0.675427   \n",
       "3 -0.215018  0.108768 -0.378641  0.451444 -0.482256 -0.084416 -0.235959   \n",
       "4 -0.626681  0.250456  1.492975  0.108266  0.673616  2.109814  2.779153   \n",
       "5 -0.598680  0.229014 -0.482429 -0.043395 -0.493826  0.209991  0.249567   \n",
       "6 -0.076418 -0.450485 -0.664613 -0.746205 -0.681638  0.728428 -0.278591   \n",
       "7 -0.092540 -1.065650  1.345661 -1.472999  0.366713  0.293936  1.564185   \n",
       "\n",
       "         7         8         9         10        11   12        13        14  \\\n",
       "0  1.130410  0.477346 -0.170125  0.066683 -0.107376  0.0  0.687777  0.469691   \n",
       "1  0.696095 -0.089688 -0.170125  1.882600  1.988971  0.0  0.875974  0.352373   \n",
       "2 -0.215715  0.958330 -0.170125 -0.526517 -0.660204  0.0  0.510424 -0.357826   \n",
       "3 -0.436745  0.332192 -0.170125 -0.907860 -0.695193  0.0 -0.397867 -0.359792   \n",
       "4  4.128356  0.444591 -0.170125  1.337825  1.719055  0.0  1.237353  5.244827   \n",
       "5 -0.109645 -0.529715 -0.170125 -0.039246 -0.170357  0.0  0.053433 -0.206403   \n",
       "6 -0.001432 -0.558818 -0.170125 -0.854895 -0.721435  0.0 -0.603661  0.545796   \n",
       "7 -0.067610  0.661563  5.869317  0.066683 -0.170357  0.0  1.513477 -0.144457   \n",
       "\n",
       "    15   16   17        18        19        20        21        22   23  \\\n",
       "0  0.0  0.0  0.0 -0.399011 -0.547277  0.066628  0.115748  1.076337  0.0   \n",
       "1  0.0  0.0  0.0  0.218011 -0.680979  1.882675  1.830475  0.746658  0.0   \n",
       "2  0.0  0.0  0.0 -1.334134 -0.501232 -0.526484 -0.433413 -0.222544  0.0   \n",
       "3  0.0  0.0  0.0  0.281590  0.116243 -0.907851 -0.943348 -0.445318  0.0   \n",
       "4  0.0  0.0  0.0  1.179572 -0.603152  1.337846  1.527876  4.113281  0.0   \n",
       "5  0.0  0.0  0.0  0.161188  0.432870 -0.039299 -0.001929 -0.096860  0.0   \n",
       "6  0.0  0.0  0.0  1.127097  1.041097 -0.854870 -0.855090 -0.028006  0.0   \n",
       "7  0.0  0.0  0.0  0.522682 -0.144509  0.066577  0.115748 -0.062463  0.0   \n",
       "\n",
       "         24        25        26        27        28        29        30  \\\n",
       "0 -0.490072  1.829539  0.973492  0.451844  1.434234  0.426821  0.428736   \n",
       "1  0.127403 -0.276357  0.738752 -0.064153  0.236187  1.271941  1.266694   \n",
       "2 -1.398581  0.506550 -0.231212  0.948042 -0.323190 -0.380954 -0.504273   \n",
       "3  0.100493 -0.119136 -0.291804  0.283661 -0.458499 -1.004433 -0.857977   \n",
       "4  1.466930  2.280024  1.712249  0.339671  1.804765  0.705951  0.979918   \n",
       "5  0.005944 -0.461180 -0.315122 -0.553684  0.175824  0.349118  0.357128   \n",
       "6  0.965797 -0.563677 -0.549746 -0.621283 -0.827011 -1.109536 -1.086231   \n",
       "7  0.228204  0.723210 -0.562235  0.618088  0.393546 -0.249975 -0.378746   \n",
       "\n",
       "         31        32        33        34        35        36        37  \\\n",
       "0  0.385960  0.738604 -0.333509 -0.314290  1.252565 -0.482196 -0.933817   \n",
       "1  0.478071  0.369794  0.137384  0.586038  0.232745  0.177297 -0.400718   \n",
       "2  0.652379 -0.863699  0.441288 -0.136108  0.323102 -1.381621 -0.468772   \n",
       "3  0.125961  0.113321 -0.660402 -0.260609 -0.462203  0.257841  0.424024   \n",
       "4 -0.491994  4.112211 -0.096809 -0.502668  2.564876  1.150266 -1.287578   \n",
       "5  0.303746 -0.329584 -0.251297 -0.326167 -0.445904  0.255708 -0.106372   \n",
       "6 -1.270576  0.036386  0.214996  0.109951 -0.665603  0.875813  1.067094   \n",
       "7  0.465718  0.863818 -0.473999 -0.502668  0.715282  0.614697 -0.973413   \n",
       "\n",
       "         38        39        40        41        42        43        44  \\\n",
       "0  0.427678  0.473683  0.830391  0.531479  0.279630 -0.012719  0.578325   \n",
       "1  1.273364  1.254034  0.710051  0.084696 -0.300134 -0.044517  0.269551   \n",
       "2 -0.380595 -0.349778 -0.181501  1.130483 -0.209791 -0.238835  0.260279   \n",
       "3 -1.004411 -1.047058 -0.300461 -0.640720 -1.151515 -1.130931 -1.062209   \n",
       "4  0.706988  0.910745  1.678622  2.443579 -0.141052  1.386371  0.811991   \n",
       "5  0.349914  0.401546 -0.232649 -0.513477  0.642572  0.366201 -0.551064   \n",
       "6 -1.109605 -1.171076 -0.556842 -0.763592  0.369335  0.412573  0.373866   \n",
       "7 -0.249589 -0.203557 -0.594566  0.389231 -0.512243  0.114471  1.031054   \n",
       "\n",
       "         45        46        47        48        49        50        51  \\\n",
       "0  1.056755 -0.306786  0.816497  0.800000  0.600000  1.200000  0.800000   \n",
       "1 -0.615147  0.432787 -0.349927  1.285714  1.142857  0.857143  1.142857   \n",
       "2  0.705600 -0.157654  0.589692  0.666667  1.111111  0.888889  0.888889   \n",
       "3 -0.165427 -0.370700  0.136083  1.666667  2.000000  2.000000  1.000000   \n",
       "4  0.143205  0.651920  0.816497  0.000000  1.000000  1.000000  0.000000   \n",
       "5 -0.350606 -0.051131 -0.714435  0.750000  0.583333  0.916667  0.916667   \n",
       "6 -0.057405  0.148599 -0.204124  0.750000  0.500000  0.750000  0.750000   \n",
       "7 -0.906142  0.460179  0.816497  1.500000  0.500000  0.500000  2.000000   \n",
       "\n",
       "         52  \n",
       "0  1.000000  \n",
       "1  1.285714  \n",
       "2  0.555556  \n",
       "3  1.500000  \n",
       "4  0.000000  \n",
       "5  0.916667  \n",
       "6  0.500000  \n",
       "7  0.500000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed2deb",
   "metadata": {},
   "source": [
    "####  Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fb429e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b3a0431c40>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "kmeans = KMeans(n_clusters = 8, max_iter = 500, random_state = 0)\n",
    "model = kmeans.fit(lifesnaps_all_grouped)\n",
    "tsne = TSNE().fit_transform(lifesnaps_all_grouped)\n",
    "plt.scatter(x = tsne[:, 0], y = tsne[:, 1], c=model.labels_, cmap='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25ebee53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.concat([pd.DataFrame(tsne, columns=['x','y']),pd.DataFrame(model.labels_, columns=['c'])], axis=1)\n",
    "mark = [\"o\", \"*\", \"P\", \"X\", \"s\", \"D\", \"^\", \"v\"]\n",
    "sns.scatterplot(data=test, x=\"x\", y=\"y\",palette='Set1', hue=\"c\", style='c', legend=False, markers = mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8569ef83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>621e2e8e67b776a24055b564</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>621e2eaf67b776a2406b14ac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621e2ed667b776a24085d8d1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>621e2f3967b776a240c654db</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>621e2f6167b776a240e082a9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>621e2f7a67b776a240f14425</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>621e2f9167b776a240011ccb</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>621e2fb367b776a24015accd</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>621e2fce67b776a240279baa</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>621e2ff067b776a2403eb737</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>621e301367b776a24057738e</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>621e301e67b776a240608a72</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>621e30c867b776a240d4aa6c</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>621e30e267b776a240e5bf90</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>621e30e467b776a240e817c7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>621e30f467b776a240f22944</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>621e310d67b776a24003096d</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>621e312a67b776a240164d59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>621e314867b776a24029ebf9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>621e323667b776a240f19134</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>621e324e67b776a2400191cb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>621e328667b776a240281372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>621e329067b776a2402ffad2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>621e32af67b776a24045b4cf</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>621e32d067b776a2405b7d54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>621e32d967b776a240627414</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>621e331067b776a24085dd3f</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>621e332267b776a24092a584</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>621e333567b776a240a0c217</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>621e333967b776a240a3cd06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>621e335a67b776a240bb12ff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>621e337667b776a240ce78ab</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>621e339967b776a240e502de</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>621e33b067b776a240f39e56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>621e33cf67b776a240087de9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>621e33ed67b776a2401cf5f7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>621e341067b776a24037b105</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>621e346f67b776a24081744f</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>621e34db67b776a240c9c2be</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>621e34ec67b776a240d60873</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>621e34f767b776a240de4e1a</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>621e356967b776a24027bd9f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>621e362467b776a2404ad513</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>621e366567b776a24076a727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>621e367e67b776a24087d75d</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>621e36bb67b776a240b40d64</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>621e36c267b776a240ba2756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>621e36dd67b776a240ce9a45</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>621e36f967b776a240e5e7c9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>621e375b67b776a240290cdc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  Cluster\n",
       "0   621e2e8e67b776a24055b564        2\n",
       "1   621e2eaf67b776a2406b14ac        1\n",
       "2   621e2ed667b776a24085d8d1        5\n",
       "3   621e2f3967b776a240c654db        2\n",
       "4   621e2f6167b776a240e082a9        5\n",
       "5   621e2f7a67b776a240f14425        2\n",
       "6   621e2f9167b776a240011ccb        5\n",
       "7   621e2fb367b776a24015accd        5\n",
       "8   621e2fce67b776a240279baa        7\n",
       "9   621e2ff067b776a2403eb737        5\n",
       "10  621e301367b776a24057738e        6\n",
       "11  621e301e67b776a240608a72        6\n",
       "12  621e30c867b776a240d4aa6c        5\n",
       "13  621e30e267b776a240e5bf90        6\n",
       "14  621e30e467b776a240e817c7        2\n",
       "15  621e30f467b776a240f22944        2\n",
       "16  621e310d67b776a24003096d        6\n",
       "17  621e312a67b776a240164d59        2\n",
       "18  621e314867b776a24029ebf9        2\n",
       "19  621e323667b776a240f19134        5\n",
       "20  621e324e67b776a2400191cb        0\n",
       "21  621e328667b776a240281372        1\n",
       "22  621e329067b776a2402ffad2        4\n",
       "23  621e32af67b776a24045b4cf        7\n",
       "24  621e32d067b776a2405b7d54        3\n",
       "25  621e32d967b776a240627414        6\n",
       "26  621e331067b776a24085dd3f        6\n",
       "27  621e332267b776a24092a584        1\n",
       "28  621e333567b776a240a0c217        6\n",
       "29  621e333967b776a240a3cd06        1\n",
       "30  621e335a67b776a240bb12ff        1\n",
       "31  621e337667b776a240ce78ab        3\n",
       "32  621e339967b776a240e502de        2\n",
       "33  621e33b067b776a240f39e56        0\n",
       "34  621e33cf67b776a240087de9        0\n",
       "35  621e33ed67b776a2401cf5f7        2\n",
       "36  621e341067b776a24037b105        3\n",
       "37  621e346f67b776a24081744f        5\n",
       "38  621e34db67b776a240c9c2be        5\n",
       "39  621e34ec67b776a240d60873        3\n",
       "40  621e34f767b776a240de4e1a        6\n",
       "41  621e356967b776a24027bd9f        1\n",
       "42  621e362467b776a2404ad513        3\n",
       "43  621e366567b776a24076a727        0\n",
       "44  621e367e67b776a24087d75d        3\n",
       "45  621e36bb67b776a240b40d64        5\n",
       "46  621e36c267b776a240ba2756        0\n",
       "47  621e36dd67b776a240ce9a45        5\n",
       "48  621e36f967b776a240e5e7c9        5\n",
       "49  621e375b67b776a240290cdc        1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = pd.concat([ids, y], axis=1)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7ae2b6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTERTAINMENT</th>\n",
       "      <th>GYM</th>\n",
       "      <th>HOME</th>\n",
       "      <th>HOME1OFFICE</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>OUTDOORS</th>\n",
       "      <th>TRANSIT</th>\n",
       "      <th>WORK/SCHOOL</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>week1cos</th>\n",
       "      <th>week1sin</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekday1cos</th>\n",
       "      <th>weekday1sin</th>\n",
       "      <th>year</th>\n",
       "      <th>dataset</th>\n",
       "      <th>stress</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.988831</td>\n",
       "      <td>-0.149042</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>-0.365341</td>\n",
       "      <td>-0.930874</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.826239</td>\n",
       "      <td>-0.563320</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5141</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>6</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5142</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.826239</td>\n",
       "      <td>-0.563320</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5143</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5144</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.988831</td>\n",
       "      <td>-0.149042</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>2021</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5146 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENTERTAINMENT  GYM  HOME  HOME1OFFICE  OTHER  OUTDOORS  TRANSIT  \\\n",
       "0               0.0  0.0   1.0          0.0    0.0       0.0      0.0   \n",
       "1               0.0  0.0   1.0          0.0    0.0       1.0      0.0   \n",
       "2               0.0  0.0   1.0          0.0    0.0       1.0      0.0   \n",
       "3               0.0  0.0   0.0          0.0    0.0       0.0      0.0   \n",
       "4               0.0  0.0   1.0          0.0    0.0       0.0      0.0   \n",
       "...             ...  ...   ...          ...    ...       ...      ...   \n",
       "5141            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "5142            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "5143            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "5144            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "5145            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "\n",
       "      WORK/SCHOOL  age   bmi  ...  week  week1cos  week1sin  weekday  \\\n",
       "0             1.0  1.0  23.0  ...    22 -0.988831 -0.149042        4   \n",
       "1             0.0  1.0  23.0  ...    29 -0.365341 -0.930874        1   \n",
       "2             1.0  1.0  23.0  ...    24 -0.900969 -0.433884        1   \n",
       "3             1.0  1.0  23.0  ...    27 -0.623490 -0.781831        0   \n",
       "4             0.0  1.0  23.0  ...    25 -0.826239 -0.563320        4   \n",
       "...           ...  ...   ...  ...   ...       ...       ...      ...   \n",
       "5141          NaN  1.0  23.0  ...    27 -0.623490 -0.781831        6   \n",
       "5142          NaN  1.0  23.0  ...    25 -0.826239 -0.563320        1   \n",
       "5143          NaN  1.0  23.0  ...    28 -0.500000 -0.866025        4   \n",
       "5144          NaN  1.0  23.0  ...    28 -0.500000 -0.866025        3   \n",
       "5145          NaN  1.0  23.0  ...    22 -0.988831 -0.149042        2   \n",
       "\n",
       "      weekday1cos  weekday1sin  year  dataset  stress  Cluster  \n",
       "0       -0.900969    -0.433884  2021    Train       1        4  \n",
       "1        0.623490     0.781831  2021    Train       0        4  \n",
       "2        0.623490     0.781831  2021    Train       0        4  \n",
       "3        1.000000     0.000000  2021    Train       0        4  \n",
       "4       -0.900969    -0.433884  2021    Train       0        4  \n",
       "...           ...          ...   ...      ...     ...      ...  \n",
       "5141     0.623490    -0.781831  2021     Test       0        6  \n",
       "5142     0.623490     0.781831  2021     Test       0        6  \n",
       "5143    -0.900969    -0.433884  2021     Test       0        6  \n",
       "5144    -0.900969     0.433884  2021     Test       0        6  \n",
       "5145    -0.222521     0.974928  2021     Test       0        6  \n",
       "\n",
       "[5146 rows x 64 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifesnaps_grouped_all = pd.merge(lifesnaps, clusters, on = \"id\")\n",
    "lifesnaps_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5882a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesnaps_grouped_all.to_csv(\"Final_CSVs/lifesnaps_clusters_all8_new.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5f00ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTERTAINMENT</th>\n",
       "      <th>GYM</th>\n",
       "      <th>HOME</th>\n",
       "      <th>HOME1OFFICE</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>OUTDOORS</th>\n",
       "      <th>TRANSIT</th>\n",
       "      <th>WORK/SCHOOL</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>...</th>\n",
       "      <th>very1active1minutes</th>\n",
       "      <th>week</th>\n",
       "      <th>week1cos</th>\n",
       "      <th>week1sin</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekday1cos</th>\n",
       "      <th>weekday1sin</th>\n",
       "      <th>year</th>\n",
       "      <th>stress</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.988831</td>\n",
       "      <td>-0.149042</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29</td>\n",
       "      <td>-0.365341</td>\n",
       "      <td>-0.930874</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.826239</td>\n",
       "      <td>-0.563320</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5141</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>6</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5142</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.826239</td>\n",
       "      <td>-0.563320</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5143</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5144</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.988831</td>\n",
       "      <td>-0.149042</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5146 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENTERTAINMENT  GYM  HOME  HOME1OFFICE  OTHER  OUTDOORS  TRANSIT  \\\n",
       "0               0.0  0.0   1.0          0.0    0.0       0.0      0.0   \n",
       "1               0.0  0.0   1.0          0.0    0.0       1.0      0.0   \n",
       "2               0.0  0.0   1.0          0.0    0.0       1.0      0.0   \n",
       "3               0.0  0.0   0.0          0.0    0.0       0.0      0.0   \n",
       "4               0.0  0.0   1.0          0.0    0.0       0.0      0.0   \n",
       "...             ...  ...   ...          ...    ...       ...      ...   \n",
       "5141            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "5142            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "5143            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "5144            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "5145            NaN  NaN   NaN          NaN    NaN       NaN      NaN   \n",
       "\n",
       "      WORK/SCHOOL  age   bmi  ...  very1active1minutes  week  week1cos  \\\n",
       "0             1.0  1.0  23.0  ...                 20.0    22 -0.988831   \n",
       "1             0.0  1.0  23.0  ...                 64.0    29 -0.365341   \n",
       "2             1.0  1.0  23.0  ...                 76.0    24 -0.900969   \n",
       "3             1.0  1.0  23.0  ...                 90.0    27 -0.623490   \n",
       "4             0.0  1.0  23.0  ...                 29.0    25 -0.826239   \n",
       "...           ...  ...   ...  ...                  ...   ...       ...   \n",
       "5141          NaN  1.0  23.0  ...                 35.0    27 -0.623490   \n",
       "5142          NaN  1.0  23.0  ...                  0.0    25 -0.826239   \n",
       "5143          NaN  1.0  23.0  ...                 42.0    28 -0.500000   \n",
       "5144          NaN  1.0  23.0  ...                 58.0    28 -0.500000   \n",
       "5145          NaN  1.0  23.0  ...                 29.0    22 -0.988831   \n",
       "\n",
       "      week1sin  weekday  weekday1cos  weekday1sin  year  stress  Cluster  \n",
       "0    -0.149042        4    -0.900969    -0.433884  2021       1        4  \n",
       "1    -0.930874        1     0.623490     0.781831  2021       0        4  \n",
       "2    -0.433884        1     0.623490     0.781831  2021       0        4  \n",
       "3    -0.781831        0     1.000000     0.000000  2021       0        4  \n",
       "4    -0.563320        4    -0.900969    -0.433884  2021       0        4  \n",
       "...        ...      ...          ...          ...   ...     ...      ...  \n",
       "5141 -0.781831        6     0.623490    -0.781831  2021       0        6  \n",
       "5142 -0.563320        1     0.623490     0.781831  2021       0        6  \n",
       "5143 -0.866025        4    -0.900969    -0.433884  2021       0        6  \n",
       "5144 -0.866025        3    -0.900969     0.433884  2021       0        6  \n",
       "5145 -0.149042        2    -0.222521     0.974928  2021       0        6  \n",
       "\n",
       "[5146 rows x 63 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop 'dataset' to run pycaret tests based on \"Cluster\".\n",
    "\n",
    "lifesnaps_grouped_all = lifesnaps_grouped_all.drop('dataset', axis = 1)\n",
    "lifesnaps_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c8a8e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_participants = lifesnaps_grouped_all[\"Cluster\"].unique()\n",
    "lifesnaps_group = lifesnaps_grouped_all.groupby('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "575ff36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant:  4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy   AUC  Recall   Prec.    F1   Kappa    MCC  \\\n",
       "lr  Logistic Regression      0.46  0.45     0.5  0.1933  0.25 -0.0006 -0.019   \n",
       "\n",
       "    TT (Sec)  \n",
       "lr     0.559  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy    AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier     0.775  0.825     0.8  0.5583  0.6233  0.5131   \n",
       "lr      Logistic Regression     0.460  0.450     0.5  0.1933  0.2500 -0.0006   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.5475     0.255  \n",
       "lr  -0.0190     0.559  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy    AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier     0.775  0.825     0.8  0.5583  0.6233  0.5131   \n",
       "nb              Naive Bayes     0.700  0.800     0.7  0.4083  0.4900  0.3123   \n",
       "lr      Logistic Regression     0.460  0.450     0.5  0.1933  0.2500 -0.0006   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.5475     0.255  \n",
       "nb   0.3579     0.005  \n",
       "lr  -0.0190     0.559  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier     0.875  0.8125     0.7  0.6000  0.6333   \n",
       "knn    K Neighbors Classifier     0.775  0.8250     0.8  0.5583  0.6233   \n",
       "nb                Naive Bayes     0.700  0.8000     0.7  0.4083  0.4900   \n",
       "lr        Logistic Regression     0.460  0.4500     0.5  0.1933  0.2500   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.5841  0.5975     0.005  \n",
       "knn  0.5131  0.5475     0.255  \n",
       "nb   0.3123  0.3579     0.005  \n",
       "lr  -0.0006 -0.0190     0.559  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier     0.875  0.8125     0.7  0.6000  0.6333   \n",
       "knn    K Neighbors Classifier     0.775  0.8250     0.8  0.5583  0.6233   \n",
       "nb                Naive Bayes     0.700  0.8000     0.7  0.4083  0.4900   \n",
       "svm       SVM - Linear Kernel     0.330  0.0000     0.8  0.1700  0.2800   \n",
       "lr        Logistic Regression     0.460  0.4500     0.5  0.1933  0.2500   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.5841  0.5975     0.005  \n",
       "knn  0.5131  0.5475     0.255  \n",
       "nb   0.3123  0.3579     0.005  \n",
       "svm  0.0000  0.0000     0.006  \n",
       "lr  -0.0006 -0.0190     0.559  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "ridge          Ridge Classifier     0.835  0.0000     0.8  0.6000  0.6667   \n",
       "dt     Decision Tree Classifier     0.875  0.8125     0.7  0.6000  0.6333   \n",
       "knn      K Neighbors Classifier     0.775  0.8250     0.8  0.5583  0.6233   \n",
       "nb                  Naive Bayes     0.700  0.8000     0.7  0.4083  0.4900   \n",
       "svm         SVM - Linear Kernel     0.330  0.0000     0.8  0.1700  0.2800   \n",
       "lr          Logistic Regression     0.460  0.4500     0.5  0.1933  0.2500   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "ridge  0.5818  0.6041     0.005  \n",
       "dt     0.5841  0.5975     0.005  \n",
       "knn    0.5131  0.5475     0.255  \n",
       "nb     0.3123  0.3579     0.005  \n",
       "svm    0.0000  0.0000     0.006  \n",
       "lr    -0.0006 -0.0190     0.559  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "ridge          Ridge Classifier     0.835  0.0000     0.8  0.6000  0.6667   \n",
       "dt     Decision Tree Classifier     0.875  0.8125     0.7  0.6000  0.6333   \n",
       "knn      K Neighbors Classifier     0.775  0.8250     0.8  0.5583  0.6233   \n",
       "rf     Random Forest Classifier     0.875  0.9000     0.6  0.5500  0.5667   \n",
       "nb                  Naive Bayes     0.700  0.8000     0.7  0.4083  0.4900   \n",
       "svm         SVM - Linear Kernel     0.330  0.0000     0.8  0.1700  0.2800   \n",
       "lr          Logistic Regression     0.460  0.4500     0.5  0.1933  0.2500   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "ridge  0.5818  0.6041     0.005  \n",
       "dt     0.5841  0.5975     0.005  \n",
       "knn    0.5131  0.5475     0.255  \n",
       "rf     0.5295  0.5362     0.035  \n",
       "nb     0.3123  0.3579     0.005  \n",
       "svm    0.0000  0.0000     0.006  \n",
       "lr    -0.0006 -0.0190     0.559  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier     0.835  0.0000     0.8  0.6000   \n",
       "dt            Decision Tree Classifier     0.875  0.8125     0.7  0.6000   \n",
       "knn             K Neighbors Classifier     0.775  0.8250     0.8  0.5583   \n",
       "rf            Random Forest Classifier     0.875  0.9000     0.6  0.5500   \n",
       "nb                         Naive Bayes     0.700  0.8000     0.7  0.4083   \n",
       "svm                SVM - Linear Kernel     0.330  0.0000     0.8  0.1700   \n",
       "lr                 Logistic Regression     0.460  0.4500     0.5  0.1933   \n",
       "qda    Quadratic Discriminant Analysis     0.790  0.5000     0.0  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.6667  0.5818  0.6041     0.005  \n",
       "dt     0.6333  0.5841  0.5975     0.005  \n",
       "knn    0.6233  0.5131  0.5475     0.255  \n",
       "rf     0.5667  0.5295  0.5362     0.035  \n",
       "nb     0.4900  0.3123  0.3579     0.005  \n",
       "svm    0.2800  0.0000  0.0000     0.006  \n",
       "lr     0.2500 -0.0006 -0.0190     0.559  \n",
       "qda    0.0000  0.0000  0.0000     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier     0.835  0.0000     0.8  0.6000   \n",
       "dt            Decision Tree Classifier     0.875  0.8125     0.7  0.6000   \n",
       "ada               Ada Boost Classifier     0.850  0.8167     0.7  0.6000   \n",
       "knn             K Neighbors Classifier     0.775  0.8250     0.8  0.5583   \n",
       "rf            Random Forest Classifier     0.875  0.9000     0.6  0.5500   \n",
       "nb                         Naive Bayes     0.700  0.8000     0.7  0.4083   \n",
       "svm                SVM - Linear Kernel     0.330  0.0000     0.8  0.1700   \n",
       "lr                 Logistic Regression     0.460  0.4500     0.5  0.1933   \n",
       "qda    Quadratic Discriminant Analysis     0.790  0.5000     0.0  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.6667  0.5818  0.6041     0.005  \n",
       "dt     0.6333  0.5841  0.5975     0.005  \n",
       "ada    0.6333  0.5545  0.5690     0.015  \n",
       "knn    0.6233  0.5131  0.5475     0.255  \n",
       "rf     0.5667  0.5295  0.5362     0.035  \n",
       "nb     0.4900  0.3123  0.3579     0.005  \n",
       "svm    0.2800  0.0000  0.0000     0.006  \n",
       "lr     0.2500 -0.0006 -0.0190     0.559  \n",
       "qda    0.0000  0.0000  0.0000     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier     0.835  0.0000     0.8  0.6000   \n",
       "dt            Decision Tree Classifier     0.875  0.8125     0.7  0.6000   \n",
       "ada               Ada Boost Classifier     0.850  0.8167     0.7  0.6000   \n",
       "gbc       Gradient Boosting Classifier     0.875  0.8375     0.7  0.6000   \n",
       "knn             K Neighbors Classifier     0.775  0.8250     0.8  0.5583   \n",
       "rf            Random Forest Classifier     0.875  0.9000     0.6  0.5500   \n",
       "nb                         Naive Bayes     0.700  0.8000     0.7  0.4083   \n",
       "svm                SVM - Linear Kernel     0.330  0.0000     0.8  0.1700   \n",
       "lr                 Logistic Regression     0.460  0.4500     0.5  0.1933   \n",
       "qda    Quadratic Discriminant Analysis     0.790  0.5000     0.0  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.6667  0.5818  0.6041     0.005  \n",
       "dt     0.6333  0.5841  0.5975     0.005  \n",
       "ada    0.6333  0.5545  0.5690     0.015  \n",
       "gbc    0.6333  0.5841  0.5975     0.014  \n",
       "knn    0.6233  0.5131  0.5475     0.255  \n",
       "rf     0.5667  0.5295  0.5362     0.035  \n",
       "nb     0.4900  0.3123  0.3579     0.005  \n",
       "svm    0.2800  0.0000  0.0000     0.006  \n",
       "lr     0.2500 -0.0006 -0.0190     0.559  \n",
       "qda    0.0000  0.0000  0.0000     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2099</td>\n",
       "      <td>0.2477</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier     0.835  0.0000     0.8  0.6000   \n",
       "dt            Decision Tree Classifier     0.875  0.8125     0.7  0.6000   \n",
       "ada               Ada Boost Classifier     0.850  0.8167     0.7  0.6000   \n",
       "gbc       Gradient Boosting Classifier     0.875  0.8375     0.7  0.6000   \n",
       "knn             K Neighbors Classifier     0.775  0.8250     0.8  0.5583   \n",
       "rf            Random Forest Classifier     0.875  0.9000     0.6  0.5500   \n",
       "nb                         Naive Bayes     0.700  0.8000     0.7  0.4083   \n",
       "lda       Linear Discriminant Analysis     0.665  0.8417     0.6  0.3167   \n",
       "svm                SVM - Linear Kernel     0.330  0.0000     0.8  0.1700   \n",
       "lr                 Logistic Regression     0.460  0.4500     0.5  0.1933   \n",
       "qda    Quadratic Discriminant Analysis     0.790  0.5000     0.0  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.6667  0.5818  0.6041     0.005  \n",
       "dt     0.6333  0.5841  0.5975     0.005  \n",
       "ada    0.6333  0.5545  0.5690     0.015  \n",
       "gbc    0.6333  0.5841  0.5975     0.014  \n",
       "knn    0.6233  0.5131  0.5475     0.255  \n",
       "rf     0.5667  0.5295  0.5362     0.035  \n",
       "nb     0.4900  0.3123  0.3579     0.005  \n",
       "lda    0.4000  0.2099  0.2477     0.006  \n",
       "svm    0.2800  0.0000  0.0000     0.006  \n",
       "lr     0.2500 -0.0006 -0.0190     0.559  \n",
       "qda    0.0000  0.0000  0.0000     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2099</td>\n",
       "      <td>0.2477</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier     0.835  0.0000     0.8  0.6000   \n",
       "dt            Decision Tree Classifier     0.875  0.8125     0.7  0.6000   \n",
       "ada               Ada Boost Classifier     0.850  0.8167     0.7  0.6000   \n",
       "gbc       Gradient Boosting Classifier     0.875  0.8375     0.7  0.6000   \n",
       "knn             K Neighbors Classifier     0.775  0.8250     0.8  0.5583   \n",
       "et              Extra Trees Classifier     0.895  0.9125     0.6  0.6000   \n",
       "rf            Random Forest Classifier     0.875  0.9000     0.6  0.5500   \n",
       "nb                         Naive Bayes     0.700  0.8000     0.7  0.4083   \n",
       "lda       Linear Discriminant Analysis     0.665  0.8417     0.6  0.3167   \n",
       "svm                SVM - Linear Kernel     0.330  0.0000     0.8  0.1700   \n",
       "lr                 Logistic Regression     0.460  0.4500     0.5  0.1933   \n",
       "qda    Quadratic Discriminant Analysis     0.790  0.5000     0.0  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.6667  0.5818  0.6041     0.005  \n",
       "dt     0.6333  0.5841  0.5975     0.005  \n",
       "ada    0.6333  0.5545  0.5690     0.015  \n",
       "gbc    0.6333  0.5841  0.5975     0.014  \n",
       "knn    0.6233  0.5131  0.5475     0.255  \n",
       "et     0.6000  0.5750  0.5750     0.030  \n",
       "rf     0.5667  0.5295  0.5362     0.035  \n",
       "nb     0.4900  0.3123  0.3579     0.005  \n",
       "lda    0.4000  0.2099  0.2477     0.006  \n",
       "svm    0.2800  0.0000  0.0000     0.006  \n",
       "lr     0.2500 -0.0006 -0.0190     0.559  \n",
       "qda    0.0000  0.0000  0.0000     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2099</td>\n",
       "      <td>0.2477</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                    Ridge Classifier     0.835  0.0000     0.8  0.6000   \n",
       "dt               Decision Tree Classifier     0.875  0.8125     0.7  0.6000   \n",
       "ada                  Ada Boost Classifier     0.850  0.8167     0.7  0.6000   \n",
       "gbc          Gradient Boosting Classifier     0.875  0.8375     0.7  0.6000   \n",
       "knn                K Neighbors Classifier     0.775  0.8250     0.8  0.5583   \n",
       "et                 Extra Trees Classifier     0.895  0.9125     0.6  0.6000   \n",
       "rf               Random Forest Classifier     0.875  0.9000     0.6  0.5500   \n",
       "lightgbm  Light Gradient Boosting Machine     0.830  0.8333     0.6  0.5000   \n",
       "nb                            Naive Bayes     0.700  0.8000     0.7  0.4083   \n",
       "lda          Linear Discriminant Analysis     0.665  0.8417     0.6  0.3167   \n",
       "svm                   SVM - Linear Kernel     0.330  0.0000     0.8  0.1700   \n",
       "lr                    Logistic Regression     0.460  0.4500     0.5  0.1933   \n",
       "qda       Quadratic Discriminant Analysis     0.790  0.5000     0.0  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "ridge     0.6667  0.5818  0.6041     0.005  \n",
       "dt        0.6333  0.5841  0.5975     0.005  \n",
       "ada       0.6333  0.5545  0.5690     0.015  \n",
       "gbc       0.6333  0.5841  0.5975     0.014  \n",
       "knn       0.6233  0.5131  0.5475     0.255  \n",
       "et        0.6000  0.5750  0.5750     0.030  \n",
       "rf        0.5667  0.5295  0.5362     0.035  \n",
       "lightgbm  0.5333  0.4545  0.4690     0.062  \n",
       "nb        0.4900  0.3123  0.3579     0.005  \n",
       "lda       0.4000  0.2099  0.2477     0.006  \n",
       "svm       0.2800  0.0000  0.0000     0.006  \n",
       "lr        0.2500 -0.0006 -0.0190     0.559  \n",
       "qda       0.0000  0.0000  0.0000     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2099</td>\n",
       "      <td>0.2477</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                    Ridge Classifier     0.835  0.0000     0.8  0.6000   \n",
       "dt               Decision Tree Classifier     0.875  0.8125     0.7  0.6000   \n",
       "ada                  Ada Boost Classifier     0.850  0.8167     0.7  0.6000   \n",
       "gbc          Gradient Boosting Classifier     0.875  0.8375     0.7  0.6000   \n",
       "knn                K Neighbors Classifier     0.775  0.8250     0.8  0.5583   \n",
       "et                 Extra Trees Classifier     0.895  0.9125     0.6  0.6000   \n",
       "rf               Random Forest Classifier     0.875  0.9000     0.6  0.5500   \n",
       "lightgbm  Light Gradient Boosting Machine     0.830  0.8333     0.6  0.5000   \n",
       "nb                            Naive Bayes     0.700  0.8000     0.7  0.4083   \n",
       "lda          Linear Discriminant Analysis     0.665  0.8417     0.6  0.3167   \n",
       "svm                   SVM - Linear Kernel     0.330  0.0000     0.8  0.1700   \n",
       "lr                    Logistic Regression     0.460  0.4500     0.5  0.1933   \n",
       "qda       Quadratic Discriminant Analysis     0.790  0.5000     0.0  0.0000   \n",
       "dummy                    Dummy Classifier     0.790  0.5000     0.0  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "ridge     0.6667  0.5818  0.6041     0.005  \n",
       "dt        0.6333  0.5841  0.5975     0.005  \n",
       "ada       0.6333  0.5545  0.5690     0.015  \n",
       "gbc       0.6333  0.5841  0.5975     0.014  \n",
       "knn       0.6233  0.5131  0.5475     0.255  \n",
       "et        0.6000  0.5750  0.5750     0.030  \n",
       "rf        0.5667  0.5295  0.5362     0.035  \n",
       "lightgbm  0.5333  0.4545  0.4690     0.062  \n",
       "nb        0.4900  0.3123  0.3579     0.005  \n",
       "lda       0.4000  0.2099  0.2477     0.006  \n",
       "svm       0.2800  0.0000  0.0000     0.006  \n",
       "lr        0.2500 -0.0006 -0.0190     0.559  \n",
       "qda       0.0000  0.0000  0.0000     0.007  \n",
       "dummy     0.0000  0.0000  0.0000     0.004  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.5362</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>0.3579</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2099</td>\n",
       "      <td>0.2477</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                    Ridge Classifier     0.835  0.0000     0.8  0.6000   \n",
       "dt               Decision Tree Classifier     0.875  0.8125     0.7  0.6000   \n",
       "ada                  Ada Boost Classifier     0.850  0.8167     0.7  0.6000   \n",
       "gbc          Gradient Boosting Classifier     0.875  0.8375     0.7  0.6000   \n",
       "knn                K Neighbors Classifier     0.775  0.8250     0.8  0.5583   \n",
       "et                 Extra Trees Classifier     0.895  0.9125     0.6  0.6000   \n",
       "rf               Random Forest Classifier     0.875  0.9000     0.6  0.5500   \n",
       "lightgbm  Light Gradient Boosting Machine     0.830  0.8333     0.6  0.5000   \n",
       "nb                            Naive Bayes     0.700  0.8000     0.7  0.4083   \n",
       "lda          Linear Discriminant Analysis     0.665  0.8417     0.6  0.3167   \n",
       "svm                   SVM - Linear Kernel     0.330  0.0000     0.8  0.1700   \n",
       "lr                    Logistic Regression     0.460  0.4500     0.5  0.1933   \n",
       "qda       Quadratic Discriminant Analysis     0.790  0.5000     0.0  0.0000   \n",
       "dummy                    Dummy Classifier     0.790  0.5000     0.0  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "ridge     0.6667  0.5818  0.6041     0.005  \n",
       "dt        0.6333  0.5841  0.5975     0.005  \n",
       "ada       0.6333  0.5545  0.5690     0.015  \n",
       "gbc       0.6333  0.5841  0.5975     0.014  \n",
       "knn       0.6233  0.5131  0.5475     0.255  \n",
       "et        0.6000  0.5750  0.5750     0.030  \n",
       "rf        0.5667  0.5295  0.5362     0.035  \n",
       "lightgbm  0.5333  0.4545  0.4690     0.062  \n",
       "nb        0.4900  0.3123  0.3579     0.005  \n",
       "lda       0.4000  0.2099  0.2477     0.006  \n",
       "svm       0.2800  0.0000  0.0000     0.006  \n",
       "lr        0.2500 -0.0006 -0.0190     0.559  \n",
       "qda       0.0000  0.0000  0.0000     0.007  \n",
       "dummy     0.0000  0.0000  0.0000     0.004  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "                max_iter=None, normalize=False, random_state=7413,\n",
      "                solver='auto', tol=0.001)\n",
      "Participant:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy   AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression     0.617  0.58  0.3738  0.1267  0.1839  0.0199   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.0225     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968  0.2790  0.1201   \n",
       "lr      Logistic Regression    0.6170  0.5800  0.3738  0.1267  0.1839  0.0199   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.1409     0.013  \n",
       "lr   0.0225     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968  0.2790  0.1201   \n",
       "nb              Naive Bayes    0.3671  0.5126  0.7119  0.1315  0.2214  0.0097   \n",
       "lr      Logistic Regression    0.6170  0.5800  0.3738  0.1267  0.1839  0.0199   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.1409     0.013  \n",
       "nb   0.0217     0.007  \n",
       "lr   0.0225     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242  0.4529   \n",
       "knn    K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968  0.2790   \n",
       "nb                Naive Bayes    0.3671  0.5126  0.7119  0.1315  0.2214   \n",
       "lr        Logistic Regression    0.6170  0.5800  0.3738  0.1267  0.1839   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.3636  0.3714     0.008  \n",
       "knn  0.1201  0.1409     0.013  \n",
       "nb   0.0097  0.0217     0.007  \n",
       "lr   0.0199  0.0225     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242  0.4529   \n",
       "knn    K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968  0.2790   \n",
       "nb                Naive Bayes    0.3671  0.5126  0.7119  0.1315  0.2214   \n",
       "lr        Logistic Regression    0.6170  0.5800  0.3738  0.1267  0.1839   \n",
       "svm       SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899  0.1593   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.3636  0.3714     0.008  \n",
       "knn  0.1201  0.1409     0.013  \n",
       "nb   0.0097  0.0217     0.007  \n",
       "lr   0.0199  0.0225     0.011  \n",
       "svm  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "ridge          Ridge Classifier    0.8165  0.0000  0.7119  0.3827  0.4947   \n",
       "dt     Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242  0.4529   \n",
       "knn      K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968  0.2790   \n",
       "nb                  Naive Bayes    0.3671  0.5126  0.7119  0.1315  0.2214   \n",
       "lr          Logistic Regression    0.6170  0.5800  0.3738  0.1267  0.1839   \n",
       "svm         SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899  0.1593   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "ridge  0.3951  0.4260     0.006  \n",
       "dt     0.3636  0.3714     0.008  \n",
       "knn    0.1201  0.1409     0.013  \n",
       "nb     0.0097  0.0217     0.007  \n",
       "lr     0.0199  0.0225     0.011  \n",
       "svm    0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "ridge          Ridge Classifier    0.8165  0.0000  0.7119  0.3827  0.4947   \n",
       "dt     Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242  0.4529   \n",
       "rf     Random Forest Classifier    0.8847  0.8810  0.3548  0.6133  0.4283   \n",
       "knn      K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968  0.2790   \n",
       "nb                  Naive Bayes    0.3671  0.5126  0.7119  0.1315  0.2214   \n",
       "lr          Logistic Regression    0.6170  0.5800  0.3738  0.1267  0.1839   \n",
       "svm         SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899  0.1593   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "ridge  0.3951  0.4260     0.006  \n",
       "dt     0.3636  0.3714     0.008  \n",
       "rf     0.3718  0.3987     0.047  \n",
       "knn    0.1201  0.1409     0.013  \n",
       "nb     0.0097  0.0217     0.007  \n",
       "lr     0.0199  0.0225     0.011  \n",
       "svm    0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier    0.8165  0.0000  0.7119  0.3827   \n",
       "dt            Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242   \n",
       "rf            Random Forest Classifier    0.8847  0.8810  0.3548  0.6133   \n",
       "knn             K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968   \n",
       "nb                         Naive Bayes    0.3671  0.5126  0.7119  0.1315   \n",
       "lr                 Logistic Regression    0.6170  0.5800  0.3738  0.1267   \n",
       "svm                SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899   \n",
       "qda    Quadratic Discriminant Analysis    0.8575  0.4910  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.4947  0.3951  0.4260     0.006  \n",
       "dt     0.4529  0.3636  0.3714     0.008  \n",
       "rf     0.4283  0.3718  0.3987     0.047  \n",
       "knn    0.2790  0.1201  0.1409     0.013  \n",
       "nb     0.2214  0.0097  0.0217     0.007  \n",
       "lr     0.1839  0.0199  0.0225     0.011  \n",
       "svm    0.1593  0.0000  0.0000     0.008  \n",
       "qda    0.0000 -0.0274 -0.0402     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4820</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier    0.8165  0.0000  0.7119  0.3827   \n",
       "dt            Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242   \n",
       "ada               Ada Boost Classifier    0.8652  0.8590  0.4500  0.4820   \n",
       "rf            Random Forest Classifier    0.8847  0.8810  0.3548  0.6133   \n",
       "knn             K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968   \n",
       "nb                         Naive Bayes    0.3671  0.5126  0.7119  0.1315   \n",
       "lr                 Logistic Regression    0.6170  0.5800  0.3738  0.1267   \n",
       "svm                SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899   \n",
       "qda    Quadratic Discriminant Analysis    0.8575  0.4910  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.4947  0.3951  0.4260     0.006  \n",
       "dt     0.4529  0.3636  0.3714     0.008  \n",
       "ada    0.4442  0.3704  0.3825     0.034  \n",
       "rf     0.4283  0.3718  0.3987     0.047  \n",
       "knn    0.2790  0.1201  0.1409     0.013  \n",
       "nb     0.2214  0.0097  0.0217     0.007  \n",
       "lr     0.1839  0.0199  0.0225     0.011  \n",
       "svm    0.1593  0.0000  0.0000     0.008  \n",
       "qda    0.0000 -0.0274 -0.0402     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4820</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8685</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.5536</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier    0.8165  0.0000  0.7119  0.3827   \n",
       "dt            Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242   \n",
       "ada               Ada Boost Classifier    0.8652  0.8590  0.4500  0.4820   \n",
       "rf            Random Forest Classifier    0.8847  0.8810  0.3548  0.6133   \n",
       "gbc       Gradient Boosting Classifier    0.8652  0.8685  0.3833  0.5536   \n",
       "knn             K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968   \n",
       "nb                         Naive Bayes    0.3671  0.5126  0.7119  0.1315   \n",
       "lr                 Logistic Regression    0.6170  0.5800  0.3738  0.1267   \n",
       "svm                SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899   \n",
       "qda    Quadratic Discriminant Analysis    0.8575  0.4910  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.4947  0.3951  0.4260     0.006  \n",
       "dt     0.4529  0.3636  0.3714     0.008  \n",
       "ada    0.4442  0.3704  0.3825     0.034  \n",
       "rf     0.4283  0.3718  0.3987     0.047  \n",
       "gbc    0.4255  0.3534  0.3752     0.082  \n",
       "knn    0.2790  0.1201  0.1409     0.013  \n",
       "nb     0.2214  0.0097  0.0217     0.007  \n",
       "lr     0.1839  0.0199  0.0225     0.011  \n",
       "svm    0.1593  0.0000  0.0000     0.008  \n",
       "qda    0.0000 -0.0274 -0.0402     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8204</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.4325</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4820</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8685</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.5536</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lda       Linear Discriminant Analysis    0.8204  0.8417  0.7119  0.3903   \n",
       "ridge                 Ridge Classifier    0.8165  0.0000  0.7119  0.3827   \n",
       "dt            Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242   \n",
       "ada               Ada Boost Classifier    0.8652  0.8590  0.4500  0.4820   \n",
       "rf            Random Forest Classifier    0.8847  0.8810  0.3548  0.6133   \n",
       "gbc       Gradient Boosting Classifier    0.8652  0.8685  0.3833  0.5536   \n",
       "knn             K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968   \n",
       "nb                         Naive Bayes    0.3671  0.5126  0.7119  0.1315   \n",
       "lr                 Logistic Regression    0.6170  0.5800  0.3738  0.1267   \n",
       "svm                SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899   \n",
       "qda    Quadratic Discriminant Analysis    0.8575  0.4910  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "lda    0.4993  0.4019  0.4325     0.012  \n",
       "ridge  0.4947  0.3951  0.4260     0.006  \n",
       "dt     0.4529  0.3636  0.3714     0.008  \n",
       "ada    0.4442  0.3704  0.3825     0.034  \n",
       "rf     0.4283  0.3718  0.3987     0.047  \n",
       "gbc    0.4255  0.3534  0.3752     0.082  \n",
       "knn    0.2790  0.1201  0.1409     0.013  \n",
       "nb     0.2214  0.0097  0.0217     0.007  \n",
       "lr     0.1839  0.0199  0.0225     0.011  \n",
       "svm    0.1593  0.0000  0.0000     0.008  \n",
       "qda    0.0000 -0.0274 -0.0402     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8204</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.4325</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8846</td>\n",
       "      <td>0.4024</td>\n",
       "      <td>0.5717</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.3999</td>\n",
       "      <td>0.4134</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4820</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8685</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.5536</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lda       Linear Discriminant Analysis    0.8204  0.8417  0.7119  0.3903   \n",
       "ridge                 Ridge Classifier    0.8165  0.0000  0.7119  0.3827   \n",
       "et              Extra Trees Classifier    0.8847  0.8846  0.4024  0.5717   \n",
       "dt            Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242   \n",
       "ada               Ada Boost Classifier    0.8652  0.8590  0.4500  0.4820   \n",
       "rf            Random Forest Classifier    0.8847  0.8810  0.3548  0.6133   \n",
       "gbc       Gradient Boosting Classifier    0.8652  0.8685  0.3833  0.5536   \n",
       "knn             K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968   \n",
       "nb                         Naive Bayes    0.3671  0.5126  0.7119  0.1315   \n",
       "lr                 Logistic Regression    0.6170  0.5800  0.3738  0.1267   \n",
       "svm                SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899   \n",
       "qda    Quadratic Discriminant Analysis    0.8575  0.4910  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "lda    0.4993  0.4019  0.4325     0.012  \n",
       "ridge  0.4947  0.3951  0.4260     0.006  \n",
       "et     0.4590  0.3999  0.4134     0.036  \n",
       "dt     0.4529  0.3636  0.3714     0.008  \n",
       "ada    0.4442  0.3704  0.3825     0.034  \n",
       "rf     0.4283  0.3718  0.3987     0.047  \n",
       "gbc    0.4255  0.3534  0.3752     0.082  \n",
       "knn    0.2790  0.1201  0.1409     0.013  \n",
       "nb     0.2214  0.0097  0.0217     0.007  \n",
       "lr     0.1839  0.0199  0.0225     0.011  \n",
       "svm    0.1593  0.0000  0.0000     0.008  \n",
       "qda    0.0000 -0.0274 -0.0402     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8204</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.4325</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8846</td>\n",
       "      <td>0.4024</td>\n",
       "      <td>0.5717</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.3999</td>\n",
       "      <td>0.4134</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4820</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8808</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.5997</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.3984</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8685</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.5536</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lda          Linear Discriminant Analysis    0.8204  0.8417  0.7119  0.3903   \n",
       "ridge                    Ridge Classifier    0.8165  0.0000  0.7119  0.3827   \n",
       "et                 Extra Trees Classifier    0.8847  0.8846  0.4024  0.5717   \n",
       "dt               Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242   \n",
       "ada                  Ada Boost Classifier    0.8652  0.8590  0.4500  0.4820   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8808  0.8694  0.3667  0.5997   \n",
       "rf               Random Forest Classifier    0.8847  0.8810  0.3548  0.6133   \n",
       "gbc          Gradient Boosting Classifier    0.8652  0.8685  0.3833  0.5536   \n",
       "knn                K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968   \n",
       "nb                            Naive Bayes    0.3671  0.5126  0.7119  0.1315   \n",
       "lr                    Logistic Regression    0.6170  0.5800  0.3738  0.1267   \n",
       "svm                   SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899   \n",
       "qda       Quadratic Discriminant Analysis    0.8575  0.4910  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lda       0.4993  0.4019  0.4325     0.012  \n",
       "ridge     0.4947  0.3951  0.4260     0.006  \n",
       "et        0.4590  0.3999  0.4134     0.036  \n",
       "dt        0.4529  0.3636  0.3714     0.008  \n",
       "ada       0.4442  0.3704  0.3825     0.034  \n",
       "lightgbm  0.4342  0.3738  0.3984     0.079  \n",
       "rf        0.4283  0.3718  0.3987     0.047  \n",
       "gbc       0.4255  0.3534  0.3752     0.082  \n",
       "knn       0.2790  0.1201  0.1409     0.013  \n",
       "nb        0.2214  0.0097  0.0217     0.007  \n",
       "lr        0.1839  0.0199  0.0225     0.011  \n",
       "svm       0.1593  0.0000  0.0000     0.008  \n",
       "qda       0.0000 -0.0274 -0.0402     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8204</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.4325</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8846</td>\n",
       "      <td>0.4024</td>\n",
       "      <td>0.5717</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.3999</td>\n",
       "      <td>0.4134</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4820</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8808</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.5997</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.3984</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8685</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.5536</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lda          Linear Discriminant Analysis    0.8204  0.8417  0.7119  0.3903   \n",
       "ridge                    Ridge Classifier    0.8165  0.0000  0.7119  0.3827   \n",
       "et                 Extra Trees Classifier    0.8847  0.8846  0.4024  0.5717   \n",
       "dt               Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242   \n",
       "ada                  Ada Boost Classifier    0.8652  0.8590  0.4500  0.4820   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8808  0.8694  0.3667  0.5997   \n",
       "rf               Random Forest Classifier    0.8847  0.8810  0.3548  0.6133   \n",
       "gbc          Gradient Boosting Classifier    0.8652  0.8685  0.3833  0.5536   \n",
       "knn                K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968   \n",
       "nb                            Naive Bayes    0.3671  0.5126  0.7119  0.1315   \n",
       "lr                    Logistic Regression    0.6170  0.5800  0.3738  0.1267   \n",
       "svm                   SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899   \n",
       "qda       Quadratic Discriminant Analysis    0.8575  0.4910  0.0000  0.0000   \n",
       "dummy                    Dummy Classifier    0.8731  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lda       0.4993  0.4019  0.4325     0.012  \n",
       "ridge     0.4947  0.3951  0.4260     0.006  \n",
       "et        0.4590  0.3999  0.4134     0.036  \n",
       "dt        0.4529  0.3636  0.3714     0.008  \n",
       "ada       0.4442  0.3704  0.3825     0.034  \n",
       "lightgbm  0.4342  0.3738  0.3984     0.079  \n",
       "rf        0.4283  0.3718  0.3987     0.047  \n",
       "gbc       0.4255  0.3534  0.3752     0.082  \n",
       "knn       0.2790  0.1201  0.1409     0.013  \n",
       "nb        0.2214  0.0097  0.0217     0.007  \n",
       "lr        0.1839  0.0199  0.0225     0.011  \n",
       "svm       0.1593  0.0000  0.0000     0.008  \n",
       "qda       0.0000 -0.0274 -0.0402     0.008  \n",
       "dummy     0.0000  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8204</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.4325</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8846</td>\n",
       "      <td>0.4024</td>\n",
       "      <td>0.5717</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.3999</td>\n",
       "      <td>0.4134</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8419</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4820</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8808</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.5997</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.3984</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.8685</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.5536</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6776</td>\n",
       "      <td>0.6529</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>0.5126</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lda          Linear Discriminant Analysis    0.8204  0.8417  0.7119  0.3903   \n",
       "ridge                    Ridge Classifier    0.8165  0.0000  0.7119  0.3827   \n",
       "et                 Extra Trees Classifier    0.8847  0.8846  0.4024  0.5717   \n",
       "dt               Decision Tree Classifier    0.8419  0.7000  0.5095  0.4242   \n",
       "ada                  Ada Boost Classifier    0.8652  0.8590  0.4500  0.4820   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8808  0.8694  0.3667  0.5997   \n",
       "rf               Random Forest Classifier    0.8847  0.8810  0.3548  0.6133   \n",
       "gbc          Gradient Boosting Classifier    0.8652  0.8685  0.3833  0.5536   \n",
       "knn                K Neighbors Classifier    0.6776  0.6529  0.4905  0.1968   \n",
       "nb                            Naive Bayes    0.3671  0.5126  0.7119  0.1315   \n",
       "lr                    Logistic Regression    0.6170  0.5800  0.3738  0.1267   \n",
       "svm                   SVM - Linear Kernel    0.3529  0.0000  0.7000  0.0899   \n",
       "qda       Quadratic Discriminant Analysis    0.8575  0.4910  0.0000  0.0000   \n",
       "dummy                    Dummy Classifier    0.8731  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lda       0.4993  0.4019  0.4325     0.012  \n",
       "ridge     0.4947  0.3951  0.4260     0.006  \n",
       "et        0.4590  0.3999  0.4134     0.036  \n",
       "dt        0.4529  0.3636  0.3714     0.008  \n",
       "ada       0.4442  0.3704  0.3825     0.034  \n",
       "lightgbm  0.4342  0.3738  0.3984     0.079  \n",
       "rf        0.4283  0.3718  0.3987     0.047  \n",
       "gbc       0.4255  0.3534  0.3752     0.082  \n",
       "knn       0.2790  0.1201  0.1409     0.013  \n",
       "nb        0.2214  0.0097  0.0217     0.007  \n",
       "lr        0.1839  0.0199  0.0225     0.011  \n",
       "svm       0.1593  0.0000  0.0000     0.008  \n",
       "qda       0.0000 -0.0274 -0.0402     0.008  \n",
       "dummy     0.0000  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "                           solver='svd', store_covariance=False, tol=0.0001)\n",
      "Participant:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression    0.5639  0.7078  0.7667  0.1017  0.1774  0.0885   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.1566      0.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr      Logistic Regression    0.5639  0.7078  0.7667  0.1017  0.1774  0.0885   \n",
       "knn  K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970  0.1614  0.0745   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "lr   0.1566     0.010  \n",
       "knn  0.1157     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "nb              Naive Bayes    0.5581  0.7009  0.8167  0.1001  0.1777  0.0873   \n",
       "lr      Logistic Regression    0.5639  0.7078  0.7667  0.1017  0.1774  0.0885   \n",
       "knn  K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970  0.1614  0.0745   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "nb   0.1738     0.007  \n",
       "lr   0.1566     0.010  \n",
       "knn  0.1157     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900  0.5921   \n",
       "nb                Naive Bayes    0.5581  0.7009  0.8167  0.1001  0.1777   \n",
       "lr        Logistic Regression    0.5639  0.7078  0.7667  0.1017  0.1774   \n",
       "knn    K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970  0.1614   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.5670  0.5818     0.010  \n",
       "nb   0.0873  0.1738     0.007  \n",
       "lr   0.0885  0.1566     0.010  \n",
       "knn  0.0745  0.1157     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900  0.5921   \n",
       "nb                Naive Bayes    0.5581  0.7009  0.8167  0.1001  0.1777   \n",
       "lr        Logistic Regression    0.5639  0.7078  0.7667  0.1017  0.1774   \n",
       "knn    K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970  0.1614   \n",
       "svm       SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518  0.0979   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.5670  0.5818     0.010  \n",
       "nb   0.0873  0.1738     0.007  \n",
       "lr   0.0885  0.1566     0.010  \n",
       "knn  0.0745  0.1157     0.011  \n",
       "svm  0.0000  0.0000     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt     Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900  0.5921   \n",
       "ridge          Ridge Classifier    0.9315  0.0000  0.8500  0.4512  0.5814   \n",
       "nb                  Naive Bayes    0.5581  0.7009  0.8167  0.1001  0.1777   \n",
       "lr          Logistic Regression    0.5639  0.7078  0.7667  0.1017  0.1774   \n",
       "knn      K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970  0.1614   \n",
       "svm         SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518  0.0979   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "dt     0.5670  0.5818     0.010  \n",
       "ridge  0.5491  0.5857     0.008  \n",
       "nb     0.0873  0.1738     0.007  \n",
       "lr     0.0885  0.1566     0.010  \n",
       "knn    0.0745  0.1157     0.011  \n",
       "svm    0.0000  0.0000     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt     Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900  0.5921   \n",
       "ridge          Ridge Classifier    0.9315  0.0000  0.8500  0.4512  0.5814   \n",
       "rf     Random Forest Classifier    0.9482  0.9762  0.5500  0.6967  0.5619   \n",
       "nb                  Naive Bayes    0.5581  0.7009  0.8167  0.1001  0.1777   \n",
       "lr          Logistic Regression    0.5639  0.7078  0.7667  0.1017  0.1774   \n",
       "knn      K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970  0.1614   \n",
       "svm         SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518  0.0979   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "dt     0.5670  0.5818     0.010  \n",
       "ridge  0.5491  0.5857     0.008  \n",
       "rf     0.5368  0.5673     0.041  \n",
       "nb     0.0873  0.1738     0.007  \n",
       "lr     0.0885  0.1566     0.010  \n",
       "knn    0.0745  0.1157     0.011  \n",
       "svm    0.0000  0.0000     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900   \n",
       "ridge                 Ridge Classifier    0.9315  0.0000  0.8500  0.4512   \n",
       "rf            Random Forest Classifier    0.9482  0.9762  0.5500  0.6967   \n",
       "nb                         Naive Bayes    0.5581  0.7009  0.8167  0.1001   \n",
       "lr                 Logistic Regression    0.5639  0.7078  0.7667  0.1017   \n",
       "knn             K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970   \n",
       "svm                SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518   \n",
       "qda    Quadratic Discriminant Analysis    0.9253  0.5141  0.0500  0.0333   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.5921  0.5670  0.5818     0.010  \n",
       "ridge  0.5814  0.5491  0.5857     0.008  \n",
       "rf     0.5619  0.5368  0.5673     0.041  \n",
       "nb     0.1777  0.0873  0.1738     0.007  \n",
       "lr     0.1774  0.0885  0.1566     0.010  \n",
       "knn    0.1614  0.0745  0.1157     0.011  \n",
       "svm    0.0979  0.0000  0.0000     0.011  \n",
       "qda    0.0400  0.0179  0.0179     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4369</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900   \n",
       "ridge                 Ridge Classifier    0.9315  0.0000  0.8500  0.4512   \n",
       "rf            Random Forest Classifier    0.9482  0.9762  0.5500  0.6967   \n",
       "ada               Ada Boost Classifier    0.9420  0.9573  0.4500  0.5000   \n",
       "nb                         Naive Bayes    0.5581  0.7009  0.8167  0.1001   \n",
       "lr                 Logistic Regression    0.5639  0.7078  0.7667  0.1017   \n",
       "knn             K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970   \n",
       "svm                SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518   \n",
       "qda    Quadratic Discriminant Analysis    0.9253  0.5141  0.0500  0.0333   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.5921  0.5670  0.5818     0.010  \n",
       "ridge  0.5814  0.5491  0.5857     0.008  \n",
       "rf     0.5619  0.5368  0.5673     0.041  \n",
       "ada    0.4538  0.4269  0.4369     0.034  \n",
       "nb     0.1777  0.0873  0.1738     0.007  \n",
       "lr     0.1774  0.0885  0.1566     0.010  \n",
       "knn    0.1614  0.0745  0.1157     0.011  \n",
       "svm    0.0979  0.0000  0.0000     0.011  \n",
       "qda    0.0400  0.0179  0.0179     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4369</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "gbc       Gradient Boosting Classifier    0.9523  0.9718  0.6833  0.6567   \n",
       "dt            Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900   \n",
       "ridge                 Ridge Classifier    0.9315  0.0000  0.8500  0.4512   \n",
       "rf            Random Forest Classifier    0.9482  0.9762  0.5500  0.6967   \n",
       "ada               Ada Boost Classifier    0.9420  0.9573  0.4500  0.5000   \n",
       "nb                         Naive Bayes    0.5581  0.7009  0.8167  0.1001   \n",
       "lr                 Logistic Regression    0.5639  0.7078  0.7667  0.1017   \n",
       "knn             K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970   \n",
       "svm                SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518   \n",
       "qda    Quadratic Discriminant Analysis    0.9253  0.5141  0.0500  0.0333   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "gbc    0.6324  0.6087  0.6278     0.087  \n",
       "dt     0.5921  0.5670  0.5818     0.010  \n",
       "ridge  0.5814  0.5491  0.5857     0.008  \n",
       "rf     0.5619  0.5368  0.5673     0.041  \n",
       "ada    0.4538  0.4269  0.4369     0.034  \n",
       "nb     0.1777  0.0873  0.1738     0.007  \n",
       "lr     0.1774  0.0885  0.1566     0.010  \n",
       "knn    0.1614  0.0745  0.1157     0.011  \n",
       "svm    0.0979  0.0000  0.0000     0.011  \n",
       "qda    0.0400  0.0179  0.0179     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.5226</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4369</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "gbc       Gradient Boosting Classifier    0.9523  0.9718  0.6833  0.6567   \n",
       "dt            Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900   \n",
       "ridge                 Ridge Classifier    0.9315  0.0000  0.8500  0.4512   \n",
       "rf            Random Forest Classifier    0.9482  0.9762  0.5500  0.6967   \n",
       "lda       Linear Discriminant Analysis    0.9232  0.9383  0.8500  0.4262   \n",
       "ada               Ada Boost Classifier    0.9420  0.9573  0.4500  0.5000   \n",
       "nb                         Naive Bayes    0.5581  0.7009  0.8167  0.1001   \n",
       "lr                 Logistic Regression    0.5639  0.7078  0.7667  0.1017   \n",
       "knn             K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970   \n",
       "svm                SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518   \n",
       "qda    Quadratic Discriminant Analysis    0.9253  0.5141  0.0500  0.0333   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "gbc    0.6324  0.6087  0.6278     0.087  \n",
       "dt     0.5921  0.5670  0.5818     0.010  \n",
       "ridge  0.5814  0.5491  0.5857     0.008  \n",
       "rf     0.5619  0.5368  0.5673     0.041  \n",
       "lda    0.5575  0.5226  0.5645     0.010  \n",
       "ada    0.4538  0.4269  0.4369     0.034  \n",
       "nb     0.1777  0.0873  0.1738     0.007  \n",
       "lr     0.1774  0.0885  0.1566     0.010  \n",
       "knn    0.1614  0.0745  0.1157     0.011  \n",
       "svm    0.0979  0.0000  0.0000     0.011  \n",
       "qda    0.0400  0.0179  0.0179     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.9747</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.5226</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4369</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "gbc       Gradient Boosting Classifier    0.9523  0.9718  0.6833  0.6567   \n",
       "dt            Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900   \n",
       "ridge                 Ridge Classifier    0.9315  0.0000  0.8500  0.4512   \n",
       "et              Extra Trees Classifier    0.9524  0.9747  0.5667  0.7150   \n",
       "rf            Random Forest Classifier    0.9482  0.9762  0.5500  0.6967   \n",
       "lda       Linear Discriminant Analysis    0.9232  0.9383  0.8500  0.4262   \n",
       "ada               Ada Boost Classifier    0.9420  0.9573  0.4500  0.5000   \n",
       "nb                         Naive Bayes    0.5581  0.7009  0.8167  0.1001   \n",
       "lr                 Logistic Regression    0.5639  0.7078  0.7667  0.1017   \n",
       "knn             K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970   \n",
       "svm                SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518   \n",
       "qda    Quadratic Discriminant Analysis    0.9253  0.5141  0.0500  0.0333   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "gbc    0.6324  0.6087  0.6278     0.087  \n",
       "dt     0.5921  0.5670  0.5818     0.010  \n",
       "ridge  0.5814  0.5491  0.5857     0.008  \n",
       "et     0.5790  0.5558  0.5870     0.050  \n",
       "rf     0.5619  0.5368  0.5673     0.041  \n",
       "lda    0.5575  0.5226  0.5645     0.010  \n",
       "ada    0.4538  0.4269  0.4369     0.034  \n",
       "nb     0.1777  0.0873  0.1738     0.007  \n",
       "lr     0.1774  0.0885  0.1566     0.010  \n",
       "knn    0.1614  0.0745  0.1157     0.011  \n",
       "svm    0.0979  0.0000  0.0000     0.011  \n",
       "qda    0.0400  0.0179  0.0179     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.6717</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.6274</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.9747</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.5226</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4369</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9565  0.9754  0.7167  0.6717   \n",
       "gbc          Gradient Boosting Classifier    0.9523  0.9718  0.6833  0.6567   \n",
       "dt               Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900   \n",
       "ridge                    Ridge Classifier    0.9315  0.0000  0.8500  0.4512   \n",
       "et                 Extra Trees Classifier    0.9524  0.9747  0.5667  0.7150   \n",
       "rf               Random Forest Classifier    0.9482  0.9762  0.5500  0.6967   \n",
       "lda          Linear Discriminant Analysis    0.9232  0.9383  0.8500  0.4262   \n",
       "ada                  Ada Boost Classifier    0.9420  0.9573  0.4500  0.5000   \n",
       "nb                            Naive Bayes    0.5581  0.7009  0.8167  0.1001   \n",
       "lr                    Logistic Regression    0.5639  0.7078  0.7667  0.1017   \n",
       "knn                K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970   \n",
       "svm                   SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518   \n",
       "qda       Quadratic Discriminant Analysis    0.9253  0.5141  0.0500  0.0333   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.6490  0.6274  0.6505     0.101  \n",
       "gbc       0.6324  0.6087  0.6278     0.087  \n",
       "dt        0.5921  0.5670  0.5818     0.010  \n",
       "ridge     0.5814  0.5491  0.5857     0.008  \n",
       "et        0.5790  0.5558  0.5870     0.050  \n",
       "rf        0.5619  0.5368  0.5673     0.041  \n",
       "lda       0.5575  0.5226  0.5645     0.010  \n",
       "ada       0.4538  0.4269  0.4369     0.034  \n",
       "nb        0.1777  0.0873  0.1738     0.007  \n",
       "lr        0.1774  0.0885  0.1566     0.010  \n",
       "knn       0.1614  0.0745  0.1157     0.011  \n",
       "svm       0.0979  0.0000  0.0000     0.011  \n",
       "qda       0.0400  0.0179  0.0179     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.6717</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.6274</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.9747</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.5226</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4369</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.9440</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9565  0.9754  0.7167  0.6717   \n",
       "gbc          Gradient Boosting Classifier    0.9523  0.9718  0.6833  0.6567   \n",
       "dt               Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900   \n",
       "ridge                    Ridge Classifier    0.9315  0.0000  0.8500  0.4512   \n",
       "et                 Extra Trees Classifier    0.9524  0.9747  0.5667  0.7150   \n",
       "rf               Random Forest Classifier    0.9482  0.9762  0.5500  0.6967   \n",
       "lda          Linear Discriminant Analysis    0.9232  0.9383  0.8500  0.4262   \n",
       "ada                  Ada Boost Classifier    0.9420  0.9573  0.4500  0.5000   \n",
       "nb                            Naive Bayes    0.5581  0.7009  0.8167  0.1001   \n",
       "lr                    Logistic Regression    0.5639  0.7078  0.7667  0.1017   \n",
       "knn                K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970   \n",
       "svm                   SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518   \n",
       "qda       Quadratic Discriminant Analysis    0.9253  0.5141  0.0500  0.0333   \n",
       "dummy                    Dummy Classifier    0.9440  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.6490  0.6274  0.6505     0.101  \n",
       "gbc       0.6324  0.6087  0.6278     0.087  \n",
       "dt        0.5921  0.5670  0.5818     0.010  \n",
       "ridge     0.5814  0.5491  0.5857     0.008  \n",
       "et        0.5790  0.5558  0.5870     0.050  \n",
       "rf        0.5619  0.5368  0.5673     0.041  \n",
       "lda       0.5575  0.5226  0.5645     0.010  \n",
       "ada       0.4538  0.4269  0.4369     0.034  \n",
       "nb        0.1777  0.0873  0.1738     0.007  \n",
       "lr        0.1774  0.0885  0.1566     0.010  \n",
       "knn       0.1614  0.0745  0.1157     0.011  \n",
       "svm       0.0979  0.0000  0.0000     0.011  \n",
       "qda       0.0400  0.0179  0.0179     0.008  \n",
       "dummy     0.0000  0.0000  0.0000     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.6717</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.6274</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.5814</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.9747</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.5226</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4369</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5639</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.9440</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "lightgbm  Light Gradient Boosting Machine    0.9565  0.9754  0.7167  0.6717   \n",
       "gbc          Gradient Boosting Classifier    0.9523  0.9718  0.6833  0.6567   \n",
       "dt               Decision Tree Classifier    0.9502  0.8096  0.6500  0.5900   \n",
       "ridge                    Ridge Classifier    0.9315  0.0000  0.8500  0.4512   \n",
       "et                 Extra Trees Classifier    0.9524  0.9747  0.5667  0.7150   \n",
       "rf               Random Forest Classifier    0.9482  0.9762  0.5500  0.6967   \n",
       "lda          Linear Discriminant Analysis    0.9232  0.9383  0.8500  0.4262   \n",
       "ada                  Ada Boost Classifier    0.9420  0.9573  0.4500  0.5000   \n",
       "nb                            Naive Bayes    0.5581  0.7009  0.8167  0.1001   \n",
       "lr                    Logistic Regression    0.5639  0.7078  0.7667  0.1017   \n",
       "knn                K Neighbors Classifier    0.6807  0.6788  0.5333  0.0970   \n",
       "svm                   SVM - Linear Kernel    0.1477  0.0000  0.9000  0.0518   \n",
       "qda       Quadratic Discriminant Analysis    0.9253  0.5141  0.0500  0.0333   \n",
       "dummy                    Dummy Classifier    0.9440  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "lightgbm  0.6490  0.6274  0.6505     0.101  \n",
       "gbc       0.6324  0.6087  0.6278     0.087  \n",
       "dt        0.5921  0.5670  0.5818     0.010  \n",
       "ridge     0.5814  0.5491  0.5857     0.008  \n",
       "et        0.5790  0.5558  0.5870     0.050  \n",
       "rf        0.5619  0.5368  0.5673     0.041  \n",
       "lda       0.5575  0.5226  0.5645     0.010  \n",
       "ada       0.4538  0.4269  0.4369     0.034  \n",
       "nb        0.1777  0.0873  0.1738     0.007  \n",
       "lr        0.1774  0.0885  0.1566     0.010  \n",
       "knn       0.1614  0.0745  0.1157     0.011  \n",
       "svm       0.0979  0.0000  0.0000     0.011  \n",
       "qda       0.0400  0.0179  0.0179     0.008  \n",
       "dummy     0.0000  0.0000  0.0000     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=1350, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "Participant:  6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression    0.8026  0.6865  0.4405  0.1811  0.2556  0.1755   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.2018     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291  0.3201  0.2013   \n",
       "lr      Logistic Regression    0.8026  0.6865  0.4405  0.1811  0.2556  0.1755   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.2284     0.017  \n",
       "lr   0.2018     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "nb              Naive Bayes    0.8040  0.7763  0.6310  0.3036  0.4076  0.3087   \n",
       "knn  K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291  0.3201  0.2013   \n",
       "lr      Logistic Regression    0.8026  0.6865  0.4405  0.1811  0.2556  0.1755   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "nb   0.3391     0.007  \n",
       "knn  0.2284     0.017  \n",
       "lr   0.2018     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913  0.5215   \n",
       "nb                Naive Bayes    0.8040  0.7763  0.6310  0.3036  0.4076   \n",
       "knn    K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291  0.3201   \n",
       "lr        Logistic Regression    0.8026  0.6865  0.4405  0.1811  0.2556   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.4613  0.4656     0.009  \n",
       "nb   0.3087  0.3391     0.007  \n",
       "knn  0.2013  0.2284     0.017  \n",
       "lr   0.1755  0.2018     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913  0.5215   \n",
       "nb                Naive Bayes    0.8040  0.7763  0.6310  0.3036  0.4076   \n",
       "knn    K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291  0.3201   \n",
       "lr        Logistic Regression    0.8026  0.6865  0.4405  0.1811  0.2556   \n",
       "svm       SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751  0.1357   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.4613  0.4656     0.009  \n",
       "nb   0.3087  0.3391     0.007  \n",
       "knn  0.2013  0.2284     0.017  \n",
       "lr   0.1755  0.2018     0.008  \n",
       "svm  0.0000  0.0000     0.016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "ridge          Ridge Classifier    0.8495  0.0000  0.7929  0.4032  0.5296   \n",
       "dt     Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913  0.5215   \n",
       "nb                  Naive Bayes    0.8040  0.7763  0.6310  0.3036  0.4076   \n",
       "knn      K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291  0.3201   \n",
       "lr          Logistic Regression    0.8026  0.6865  0.4405  0.1811  0.2556   \n",
       "svm         SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751  0.1357   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "ridge  0.4525  0.4924     0.013  \n",
       "dt     0.4613  0.4656     0.009  \n",
       "nb     0.3087  0.3391     0.007  \n",
       "knn    0.2013  0.2284     0.017  \n",
       "lr     0.1755  0.2018     0.008  \n",
       "svm    0.0000  0.0000     0.016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "rf     Random Forest Classifier    0.8981  0.9439  0.6143  0.5181  0.5595   \n",
       "ridge          Ridge Classifier    0.8495  0.0000  0.7929  0.4032  0.5296   \n",
       "dt     Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913  0.5215   \n",
       "nb                  Naive Bayes    0.8040  0.7763  0.6310  0.3036  0.4076   \n",
       "knn      K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291  0.3201   \n",
       "lr          Logistic Regression    0.8026  0.6865  0.4405  0.1811  0.2556   \n",
       "svm         SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751  0.1357   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "rf     0.5026  0.5065     0.057  \n",
       "ridge  0.4525  0.4924     0.013  \n",
       "dt     0.4613  0.4656     0.009  \n",
       "nb     0.3087  0.3391     0.007  \n",
       "knn    0.2013  0.2284     0.017  \n",
       "lr     0.1755  0.2018     0.008  \n",
       "svm    0.0000  0.0000     0.016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.8981  0.9439  0.6143  0.5181   \n",
       "ridge                 Ridge Classifier    0.8495  0.0000  0.7929  0.4032   \n",
       "dt            Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913   \n",
       "nb                         Naive Bayes    0.8040  0.7763  0.6310  0.3036   \n",
       "knn             K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291   \n",
       "lr                 Logistic Regression    0.8026  0.6865  0.4405  0.1811   \n",
       "svm                SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751   \n",
       "qda    Quadratic Discriminant Analysis    0.8699  0.5182  0.0714  0.1667   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.5595  0.5026  0.5065     0.057  \n",
       "ridge  0.5296  0.4525  0.4924     0.013  \n",
       "dt     0.5215  0.4613  0.4656     0.009  \n",
       "nb     0.4076  0.3087  0.3391     0.007  \n",
       "knn    0.3201  0.2013  0.2284     0.017  \n",
       "lr     0.2556  0.1755  0.2018     0.008  \n",
       "svm    0.1357  0.0000  0.0000     0.016  \n",
       "qda    0.1000  0.0489  0.0508     0.018  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.8981  0.9439  0.6143  0.5181   \n",
       "ridge                 Ridge Classifier    0.8495  0.0000  0.7929  0.4032   \n",
       "dt            Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913   \n",
       "ada               Ada Boost Classifier    0.8901  0.9046  0.4929  0.5053   \n",
       "nb                         Naive Bayes    0.8040  0.7763  0.6310  0.3036   \n",
       "knn             K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291   \n",
       "lr                 Logistic Regression    0.8026  0.6865  0.4405  0.1811   \n",
       "svm                SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751   \n",
       "qda    Quadratic Discriminant Analysis    0.8699  0.5182  0.0714  0.1667   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.5595  0.5026  0.5065     0.057  \n",
       "ridge  0.5296  0.4525  0.4924     0.013  \n",
       "dt     0.5215  0.4613  0.4656     0.009  \n",
       "ada    0.4870  0.4269  0.4334     0.039  \n",
       "nb     0.4076  0.3087  0.3391     0.007  \n",
       "knn    0.3201  0.2013  0.2284     0.017  \n",
       "lr     0.2556  0.1755  0.2018     0.008  \n",
       "svm    0.1357  0.0000  0.0000     0.016  \n",
       "qda    0.1000  0.0489  0.0508     0.018  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4915</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.8981  0.9439  0.6143  0.5181   \n",
       "ridge                 Ridge Classifier    0.8495  0.0000  0.7929  0.4032   \n",
       "dt            Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913   \n",
       "gbc       Gradient Boosting Classifier    0.8855  0.9355  0.5238  0.4681   \n",
       "ada               Ada Boost Classifier    0.8901  0.9046  0.4929  0.5053   \n",
       "nb                         Naive Bayes    0.8040  0.7763  0.6310  0.3036   \n",
       "knn             K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291   \n",
       "lr                 Logistic Regression    0.8026  0.6865  0.4405  0.1811   \n",
       "svm                SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751   \n",
       "qda    Quadratic Discriminant Analysis    0.8699  0.5182  0.0714  0.1667   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.5595  0.5026  0.5065     0.057  \n",
       "ridge  0.5296  0.4525  0.4924     0.013  \n",
       "dt     0.5215  0.4613  0.4656     0.009  \n",
       "gbc    0.4915  0.4277  0.4301     0.094  \n",
       "ada    0.4870  0.4269  0.4334     0.039  \n",
       "nb     0.4076  0.3087  0.3391     0.007  \n",
       "knn    0.3201  0.2013  0.2284     0.017  \n",
       "lr     0.2556  0.1755  0.2018     0.008  \n",
       "svm    0.1357  0.0000  0.0000     0.016  \n",
       "qda    0.1000  0.0489  0.0508     0.018  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>0.7619</td>\n",
       "      <td>0.4036</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>0.4460</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4915</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.8981  0.9439  0.6143  0.5181   \n",
       "ridge                 Ridge Classifier    0.8495  0.0000  0.7929  0.4032   \n",
       "lda       Linear Discriminant Analysis    0.8510  0.8515  0.7619  0.4036   \n",
       "dt            Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913   \n",
       "gbc       Gradient Boosting Classifier    0.8855  0.9355  0.5238  0.4681   \n",
       "ada               Ada Boost Classifier    0.8901  0.9046  0.4929  0.5053   \n",
       "nb                         Naive Bayes    0.8040  0.7763  0.6310  0.3036   \n",
       "knn             K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291   \n",
       "lr                 Logistic Regression    0.8026  0.6865  0.4405  0.1811   \n",
       "svm                SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751   \n",
       "qda    Quadratic Discriminant Analysis    0.8699  0.5182  0.0714  0.1667   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.5595  0.5026  0.5065     0.057  \n",
       "ridge  0.5296  0.4525  0.4924     0.013  \n",
       "lda    0.5230  0.4460  0.4804     0.012  \n",
       "dt     0.5215  0.4613  0.4656     0.009  \n",
       "gbc    0.4915  0.4277  0.4301     0.094  \n",
       "ada    0.4870  0.4269  0.4334     0.039  \n",
       "nb     0.4076  0.3087  0.3391     0.007  \n",
       "knn    0.3201  0.2013  0.2284     0.017  \n",
       "lr     0.2556  0.1755  0.2018     0.008  \n",
       "svm    0.1357  0.0000  0.0000     0.016  \n",
       "qda    0.1000  0.0489  0.0508     0.018  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.9421</td>\n",
       "      <td>0.6286</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5177</td>\n",
       "      <td>0.5213</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>0.7619</td>\n",
       "      <td>0.4036</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>0.4460</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4915</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et              Extra Trees Classifier    0.9012  0.9421  0.6286  0.5294   \n",
       "rf            Random Forest Classifier    0.8981  0.9439  0.6143  0.5181   \n",
       "ridge                 Ridge Classifier    0.8495  0.0000  0.7929  0.4032   \n",
       "lda       Linear Discriminant Analysis    0.8510  0.8515  0.7619  0.4036   \n",
       "dt            Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913   \n",
       "gbc       Gradient Boosting Classifier    0.8855  0.9355  0.5238  0.4681   \n",
       "ada               Ada Boost Classifier    0.8901  0.9046  0.4929  0.5053   \n",
       "nb                         Naive Bayes    0.8040  0.7763  0.6310  0.3036   \n",
       "knn             K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291   \n",
       "lr                 Logistic Regression    0.8026  0.6865  0.4405  0.1811   \n",
       "svm                SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751   \n",
       "qda    Quadratic Discriminant Analysis    0.8699  0.5182  0.0714  0.1667   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "et     0.5730  0.5177  0.5213     0.057  \n",
       "rf     0.5595  0.5026  0.5065     0.057  \n",
       "ridge  0.5296  0.4525  0.4924     0.013  \n",
       "lda    0.5230  0.4460  0.4804     0.012  \n",
       "dt     0.5215  0.4613  0.4656     0.009  \n",
       "gbc    0.4915  0.4277  0.4301     0.094  \n",
       "ada    0.4870  0.4269  0.4334     0.039  \n",
       "nb     0.4076  0.3087  0.3391     0.007  \n",
       "knn    0.3201  0.2013  0.2284     0.017  \n",
       "lr     0.2556  0.1755  0.2018     0.008  \n",
       "svm    0.1357  0.0000  0.0000     0.016  \n",
       "qda    0.1000  0.0489  0.0508     0.018  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.9421</td>\n",
       "      <td>0.6286</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5177</td>\n",
       "      <td>0.5213</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.9368</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.5524</td>\n",
       "      <td>0.4972</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>0.7619</td>\n",
       "      <td>0.4036</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>0.4460</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4915</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.9012  0.9421  0.6286  0.5294   \n",
       "rf               Random Forest Classifier    0.8981  0.9439  0.6143  0.5181   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9012  0.9368  0.5690  0.5423   \n",
       "ridge                    Ridge Classifier    0.8495  0.0000  0.7929  0.4032   \n",
       "lda          Linear Discriminant Analysis    0.8510  0.8515  0.7619  0.4036   \n",
       "dt               Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913   \n",
       "gbc          Gradient Boosting Classifier    0.8855  0.9355  0.5238  0.4681   \n",
       "ada                  Ada Boost Classifier    0.8901  0.9046  0.4929  0.5053   \n",
       "nb                            Naive Bayes    0.8040  0.7763  0.6310  0.3036   \n",
       "knn                K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291   \n",
       "lr                    Logistic Regression    0.8026  0.6865  0.4405  0.1811   \n",
       "svm                   SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751   \n",
       "qda       Quadratic Discriminant Analysis    0.8699  0.5182  0.0714  0.1667   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.5730  0.5177  0.5213     0.057  \n",
       "rf        0.5595  0.5026  0.5065     0.057  \n",
       "lightgbm  0.5524  0.4972  0.4990     0.112  \n",
       "ridge     0.5296  0.4525  0.4924     0.013  \n",
       "lda       0.5230  0.4460  0.4804     0.012  \n",
       "dt        0.5215  0.4613  0.4656     0.009  \n",
       "gbc       0.4915  0.4277  0.4301     0.094  \n",
       "ada       0.4870  0.4269  0.4334     0.039  \n",
       "nb        0.4076  0.3087  0.3391     0.007  \n",
       "knn       0.3201  0.2013  0.2284     0.017  \n",
       "lr        0.2556  0.1755  0.2018     0.008  \n",
       "svm       0.1357  0.0000  0.0000     0.016  \n",
       "qda       0.1000  0.0489  0.0508     0.018  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.9421</td>\n",
       "      <td>0.6286</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5177</td>\n",
       "      <td>0.5213</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.9368</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.5524</td>\n",
       "      <td>0.4972</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>0.7619</td>\n",
       "      <td>0.4036</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>0.4460</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4915</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8935</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.9012  0.9421  0.6286  0.5294   \n",
       "rf               Random Forest Classifier    0.8981  0.9439  0.6143  0.5181   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9012  0.9368  0.5690  0.5423   \n",
       "ridge                    Ridge Classifier    0.8495  0.0000  0.7929  0.4032   \n",
       "lda          Linear Discriminant Analysis    0.8510  0.8515  0.7619  0.4036   \n",
       "dt               Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913   \n",
       "gbc          Gradient Boosting Classifier    0.8855  0.9355  0.5238  0.4681   \n",
       "ada                  Ada Boost Classifier    0.8901  0.9046  0.4929  0.5053   \n",
       "nb                            Naive Bayes    0.8040  0.7763  0.6310  0.3036   \n",
       "knn                K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291   \n",
       "lr                    Logistic Regression    0.8026  0.6865  0.4405  0.1811   \n",
       "svm                   SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751   \n",
       "qda       Quadratic Discriminant Analysis    0.8699  0.5182  0.0714  0.1667   \n",
       "dummy                    Dummy Classifier    0.8935  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.5730  0.5177  0.5213     0.057  \n",
       "rf        0.5595  0.5026  0.5065     0.057  \n",
       "lightgbm  0.5524  0.4972  0.4990     0.112  \n",
       "ridge     0.5296  0.4525  0.4924     0.013  \n",
       "lda       0.5230  0.4460  0.4804     0.012  \n",
       "dt        0.5215  0.4613  0.4656     0.009  \n",
       "gbc       0.4915  0.4277  0.4301     0.094  \n",
       "ada       0.4870  0.4269  0.4334     0.039  \n",
       "nb        0.4076  0.3087  0.3391     0.007  \n",
       "knn       0.3201  0.2013  0.2284     0.017  \n",
       "lr        0.2556  0.1755  0.2018     0.008  \n",
       "svm       0.1357  0.0000  0.0000     0.016  \n",
       "qda       0.1000  0.0489  0.0508     0.018  \n",
       "dummy     0.0000  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.9421</td>\n",
       "      <td>0.6286</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5177</td>\n",
       "      <td>0.5213</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5026</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.9368</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.5524</td>\n",
       "      <td>0.4972</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>0.7619</td>\n",
       "      <td>0.4036</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>0.4460</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.4913</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4915</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8901</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.7763</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8935</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.9012  0.9421  0.6286  0.5294   \n",
       "rf               Random Forest Classifier    0.8981  0.9439  0.6143  0.5181   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9012  0.9368  0.5690  0.5423   \n",
       "ridge                    Ridge Classifier    0.8495  0.0000  0.7929  0.4032   \n",
       "lda          Linear Discriminant Analysis    0.8510  0.8515  0.7619  0.4036   \n",
       "dt               Decision Tree Classifier    0.8917  0.7482  0.5667  0.4913   \n",
       "gbc          Gradient Boosting Classifier    0.8855  0.9355  0.5238  0.4681   \n",
       "ada                  Ada Boost Classifier    0.8901  0.9046  0.4929  0.5053   \n",
       "nb                            Naive Bayes    0.8040  0.7763  0.6310  0.3036   \n",
       "knn                K Neighbors Classifier    0.7555  0.7181  0.5381  0.2291   \n",
       "lr                    Logistic Regression    0.8026  0.6865  0.4405  0.1811   \n",
       "svm                   SVM - Linear Kernel    0.3438  0.0000  0.7000  0.0751   \n",
       "qda       Quadratic Discriminant Analysis    0.8699  0.5182  0.0714  0.1667   \n",
       "dummy                    Dummy Classifier    0.8935  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.5730  0.5177  0.5213     0.057  \n",
       "rf        0.5595  0.5026  0.5065     0.057  \n",
       "lightgbm  0.5524  0.4972  0.4990     0.112  \n",
       "ridge     0.5296  0.4525  0.4924     0.013  \n",
       "lda       0.5230  0.4460  0.4804     0.012  \n",
       "dt        0.5215  0.4613  0.4656     0.009  \n",
       "gbc       0.4915  0.4277  0.4301     0.094  \n",
       "ada       0.4870  0.4269  0.4334     0.039  \n",
       "nb        0.4076  0.3087  0.3391     0.007  \n",
       "knn       0.3201  0.2013  0.2284     0.017  \n",
       "lr        0.2556  0.1755  0.2018     0.008  \n",
       "svm       0.1357  0.0000  0.0000     0.016  \n",
       "qda       0.1000  0.0489  0.0508     0.018  \n",
       "dummy     0.0000  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "                     oob_score=False, random_state=7072, verbose=0,\n",
      "                     warm_start=False)\n",
      "Participant:  5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression    0.8161  0.6261   0.219  0.0981  0.1335  0.0915   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310  0.3199  0.1469   \n",
       "lr      Logistic Regression    0.8161  0.6261  0.2190  0.0981  0.1335  0.0915   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.1717     0.022  \n",
       "lr   0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "nb              Naive Bayes    0.5797  0.7356  0.8367  0.2410  0.3730  0.1861   \n",
       "knn  K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310  0.3199  0.1469   \n",
       "lr      Logistic Regression    0.8161  0.6261  0.2190  0.0981  0.1335  0.0915   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "nb   0.2666     0.009  \n",
       "knn  0.1717     0.022  \n",
       "lr   0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624  0.4889   \n",
       "nb                Naive Bayes    0.5797  0.7356  0.8367  0.2410  0.3730   \n",
       "knn    K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310  0.3199   \n",
       "lr        Logistic Regression    0.8161  0.6261  0.2190  0.0981  0.1335   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.3937  0.3959     0.014  \n",
       "nb   0.1861  0.2666     0.009  \n",
       "knn  0.1469  0.1717     0.022  \n",
       "lr   0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624  0.4889   \n",
       "nb                Naive Bayes    0.5797  0.7356  0.8367  0.2410  0.3730   \n",
       "knn    K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310  0.3199   \n",
       "svm       SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184  0.2063   \n",
       "lr        Logistic Regression    0.8161  0.6261  0.2190  0.0981  0.1335   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.3937  0.3959     0.014  \n",
       "nb   0.1861  0.2666     0.009  \n",
       "knn  0.1469  0.1717     0.022  \n",
       "svm  0.0000  0.0000     0.014  \n",
       "lr   0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "ridge          Ridge Classifier    0.8039  0.0000  0.8086  0.4269  0.5545   \n",
       "dt     Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624  0.4889   \n",
       "nb                  Naive Bayes    0.5797  0.7356  0.8367  0.2410  0.3730   \n",
       "knn      K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310  0.3199   \n",
       "svm         SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184  0.2063   \n",
       "lr          Logistic Regression    0.8161  0.6261  0.2190  0.0981  0.1335   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "ridge  0.4466  0.4861     0.009  \n",
       "dt     0.3937  0.3959     0.014  \n",
       "nb     0.1861  0.2666     0.009  \n",
       "knn    0.1469  0.1717     0.022  \n",
       "svm    0.0000  0.0000     0.014  \n",
       "lr     0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "ridge          Ridge Classifier    0.8039  0.0000  0.8086  0.4269  0.5545   \n",
       "rf     Random Forest Classifier    0.8713  0.9137  0.5243  0.5689  0.5324   \n",
       "dt     Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624  0.4889   \n",
       "nb                  Naive Bayes    0.5797  0.7356  0.8367  0.2410  0.3730   \n",
       "knn      K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310  0.3199   \n",
       "svm         SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184  0.2063   \n",
       "lr          Logistic Regression    0.8161  0.6261  0.2190  0.0981  0.1335   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "ridge  0.4466  0.4861     0.009  \n",
       "rf     0.4603  0.4674     0.060  \n",
       "dt     0.3937  0.3959     0.014  \n",
       "nb     0.1861  0.2666     0.009  \n",
       "knn    0.1469  0.1717     0.022  \n",
       "svm    0.0000  0.0000     0.014  \n",
       "lr     0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.3658</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier    0.8039  0.0000  0.8086  0.4269   \n",
       "rf            Random Forest Classifier    0.8713  0.9137  0.5243  0.5689   \n",
       "qda    Quadratic Discriminant Analysis    0.7507  0.8283  0.9162  0.3658   \n",
       "dt            Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624   \n",
       "nb                         Naive Bayes    0.5797  0.7356  0.8367  0.2410   \n",
       "knn             K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310   \n",
       "svm                SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184   \n",
       "lr                 Logistic Regression    0.8161  0.6261  0.2190  0.0981   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.5545  0.4466  0.4861     0.009  \n",
       "rf     0.5324  0.4603  0.4674     0.060  \n",
       "qda    0.5219  0.3939  0.4702     0.012  \n",
       "dt     0.4889  0.3937  0.3959     0.014  \n",
       "nb     0.3730  0.1861  0.2666     0.009  \n",
       "knn    0.3199  0.1469  0.1717     0.022  \n",
       "svm    0.2063  0.0000  0.0000     0.014  \n",
       "lr     0.1335  0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.5241</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.3658</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier    0.8039  0.0000  0.8086  0.4269   \n",
       "ada               Ada Boost Classifier    0.8542  0.8922  0.5857  0.5241   \n",
       "rf            Random Forest Classifier    0.8713  0.9137  0.5243  0.5689   \n",
       "qda    Quadratic Discriminant Analysis    0.7507  0.8283  0.9162  0.3658   \n",
       "dt            Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624   \n",
       "nb                         Naive Bayes    0.5797  0.7356  0.8367  0.2410   \n",
       "knn             K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310   \n",
       "svm                SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184   \n",
       "lr                 Logistic Regression    0.8161  0.6261  0.2190  0.0981   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.5545  0.4466  0.4861     0.009  \n",
       "ada    0.5461  0.4610  0.4662     0.052  \n",
       "rf     0.5324  0.4603  0.4674     0.060  \n",
       "qda    0.5219  0.3939  0.4702     0.012  \n",
       "dt     0.4889  0.3937  0.3959     0.014  \n",
       "nb     0.3730  0.1861  0.2666     0.009  \n",
       "knn    0.3199  0.1469  0.1717     0.022  \n",
       "svm    0.2063  0.0000  0.0000     0.014  \n",
       "lr     0.1335  0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.5241</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.4997</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.3658</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier    0.8039  0.0000  0.8086  0.4269   \n",
       "ada               Ada Boost Classifier    0.8542  0.8922  0.5857  0.5241   \n",
       "rf            Random Forest Classifier    0.8713  0.9137  0.5243  0.5689   \n",
       "gbc       Gradient Boosting Classifier    0.8502  0.9048  0.5581  0.4997   \n",
       "qda    Quadratic Discriminant Analysis    0.7507  0.8283  0.9162  0.3658   \n",
       "dt            Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624   \n",
       "nb                         Naive Bayes    0.5797  0.7356  0.8367  0.2410   \n",
       "knn             K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310   \n",
       "svm                SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184   \n",
       "lr                 Logistic Regression    0.8161  0.6261  0.2190  0.0981   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.5545  0.4466  0.4861     0.009  \n",
       "ada    0.5461  0.4610  0.4662     0.052  \n",
       "rf     0.5324  0.4603  0.4674     0.060  \n",
       "gbc    0.5246  0.4366  0.4391     0.157  \n",
       "qda    0.5219  0.3939  0.4702     0.012  \n",
       "dt     0.4889  0.3937  0.3959     0.014  \n",
       "nb     0.3730  0.1861  0.2666     0.009  \n",
       "knn    0.3199  0.1469  0.1717     0.022  \n",
       "svm    0.2063  0.0000  0.0000     0.014  \n",
       "lr     0.1335  0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.5241</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.7681</td>\n",
       "      <td>0.4203</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>0.4627</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.4997</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.3658</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier    0.8039  0.0000  0.8086  0.4269   \n",
       "ada               Ada Boost Classifier    0.8542  0.8922  0.5857  0.5241   \n",
       "lda       Linear Discriminant Analysis    0.8019  0.8754  0.7681  0.4203   \n",
       "rf            Random Forest Classifier    0.8713  0.9137  0.5243  0.5689   \n",
       "gbc       Gradient Boosting Classifier    0.8502  0.9048  0.5581  0.4997   \n",
       "qda    Quadratic Discriminant Analysis    0.7507  0.8283  0.9162  0.3658   \n",
       "dt            Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624   \n",
       "nb                         Naive Bayes    0.5797  0.7356  0.8367  0.2410   \n",
       "knn             K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310   \n",
       "svm                SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184   \n",
       "lr                 Logistic Regression    0.8161  0.6261  0.2190  0.0981   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.5545  0.4466  0.4861     0.009  \n",
       "ada    0.5461  0.4610  0.4662     0.052  \n",
       "lda    0.5399  0.4299  0.4627     0.013  \n",
       "rf     0.5324  0.4603  0.4674     0.060  \n",
       "gbc    0.5246  0.4366  0.4391     0.157  \n",
       "qda    0.5219  0.3939  0.4702     0.012  \n",
       "dt     0.4889  0.3937  0.3959     0.014  \n",
       "nb     0.3730  0.1861  0.2666     0.009  \n",
       "knn    0.3199  0.1469  0.1717     0.022  \n",
       "svm    0.2063  0.0000  0.0000     0.014  \n",
       "lr     0.1335  0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.5241</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5429</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.7681</td>\n",
       "      <td>0.4203</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>0.4627</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.4997</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.3658</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                 Ridge Classifier    0.8039  0.0000  0.8086  0.4269   \n",
       "ada               Ada Boost Classifier    0.8542  0.8922  0.5857  0.5241   \n",
       "et              Extra Trees Classifier    0.8663  0.9065  0.5443  0.5518   \n",
       "lda       Linear Discriminant Analysis    0.8019  0.8754  0.7681  0.4203   \n",
       "rf            Random Forest Classifier    0.8713  0.9137  0.5243  0.5689   \n",
       "gbc       Gradient Boosting Classifier    0.8502  0.9048  0.5581  0.4997   \n",
       "qda    Quadratic Discriminant Analysis    0.7507  0.8283  0.9162  0.3658   \n",
       "dt            Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624   \n",
       "nb                         Naive Bayes    0.5797  0.7356  0.8367  0.2410   \n",
       "knn             K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310   \n",
       "svm                SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184   \n",
       "lr                 Logistic Regression    0.8161  0.6261  0.2190  0.0981   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "ridge  0.5545  0.4466  0.4861     0.009  \n",
       "ada    0.5461  0.4610  0.4662     0.052  \n",
       "et     0.5429  0.4652  0.4683     0.046  \n",
       "lda    0.5399  0.4299  0.4627     0.013  \n",
       "rf     0.5324  0.4603  0.4674     0.060  \n",
       "gbc    0.5246  0.4366  0.4391     0.157  \n",
       "qda    0.5219  0.3939  0.4702     0.012  \n",
       "dt     0.4889  0.3937  0.3959     0.014  \n",
       "nb     0.3730  0.1861  0.2666     0.009  \n",
       "knn    0.3199  0.1469  0.1717     0.022  \n",
       "svm    0.2063  0.0000  0.0000     0.014  \n",
       "lr     0.1335  0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.5241</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5429</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.7681</td>\n",
       "      <td>0.4203</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>0.4627</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.4997</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.3658</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8583</td>\n",
       "      <td>0.9028</td>\n",
       "      <td>0.4962</td>\n",
       "      <td>0.5268</td>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                    Ridge Classifier    0.8039  0.0000  0.8086  0.4269   \n",
       "ada                  Ada Boost Classifier    0.8542  0.8922  0.5857  0.5241   \n",
       "et                 Extra Trees Classifier    0.8663  0.9065  0.5443  0.5518   \n",
       "lda          Linear Discriminant Analysis    0.8019  0.8754  0.7681  0.4203   \n",
       "rf               Random Forest Classifier    0.8713  0.9137  0.5243  0.5689   \n",
       "gbc          Gradient Boosting Classifier    0.8502  0.9048  0.5581  0.4997   \n",
       "qda       Quadratic Discriminant Analysis    0.7507  0.8283  0.9162  0.3658   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8583  0.9028  0.4962  0.5268   \n",
       "dt               Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624   \n",
       "nb                            Naive Bayes    0.5797  0.7356  0.8367  0.2410   \n",
       "knn                K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310   \n",
       "svm                   SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184   \n",
       "lr                    Logistic Regression    0.8161  0.6261  0.2190  0.0981   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "ridge     0.5545  0.4466  0.4861     0.009  \n",
       "ada       0.5461  0.4610  0.4662     0.052  \n",
       "et        0.5429  0.4652  0.4683     0.046  \n",
       "lda       0.5399  0.4299  0.4627     0.013  \n",
       "rf        0.5324  0.4603  0.4674     0.060  \n",
       "gbc       0.5246  0.4366  0.4391     0.157  \n",
       "qda       0.5219  0.3939  0.4702     0.012  \n",
       "lightgbm  0.5010  0.4200  0.4259     0.094  \n",
       "dt        0.4889  0.3937  0.3959     0.014  \n",
       "nb        0.3730  0.1861  0.2666     0.009  \n",
       "knn       0.3199  0.1469  0.1717     0.022  \n",
       "svm       0.2063  0.0000  0.0000     0.014  \n",
       "lr        0.1335  0.0915  0.1058     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.5241</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5429</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.7681</td>\n",
       "      <td>0.4203</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>0.4627</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.4997</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.3658</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8583</td>\n",
       "      <td>0.9028</td>\n",
       "      <td>0.4962</td>\n",
       "      <td>0.5268</td>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8523</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                    Ridge Classifier    0.8039  0.0000  0.8086  0.4269   \n",
       "ada                  Ada Boost Classifier    0.8542  0.8922  0.5857  0.5241   \n",
       "et                 Extra Trees Classifier    0.8663  0.9065  0.5443  0.5518   \n",
       "lda          Linear Discriminant Analysis    0.8019  0.8754  0.7681  0.4203   \n",
       "rf               Random Forest Classifier    0.8713  0.9137  0.5243  0.5689   \n",
       "gbc          Gradient Boosting Classifier    0.8502  0.9048  0.5581  0.4997   \n",
       "qda       Quadratic Discriminant Analysis    0.7507  0.8283  0.9162  0.3658   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8583  0.9028  0.4962  0.5268   \n",
       "dt               Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624   \n",
       "nb                            Naive Bayes    0.5797  0.7356  0.8367  0.2410   \n",
       "knn                K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310   \n",
       "svm                   SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184   \n",
       "lr                    Logistic Regression    0.8161  0.6261  0.2190  0.0981   \n",
       "dummy                    Dummy Classifier    0.8523  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "ridge     0.5545  0.4466  0.4861     0.009  \n",
       "ada       0.5461  0.4610  0.4662     0.052  \n",
       "et        0.5429  0.4652  0.4683     0.046  \n",
       "lda       0.5399  0.4299  0.4627     0.013  \n",
       "rf        0.5324  0.4603  0.4674     0.060  \n",
       "gbc       0.5246  0.4366  0.4391     0.157  \n",
       "qda       0.5219  0.3939  0.4702     0.012  \n",
       "lightgbm  0.5010  0.4200  0.4259     0.094  \n",
       "dt        0.4889  0.3937  0.3959     0.014  \n",
       "nb        0.3730  0.1861  0.2666     0.009  \n",
       "knn       0.3199  0.1469  0.1717     0.022  \n",
       "svm       0.2063  0.0000  0.0000     0.014  \n",
       "lr        0.1335  0.0915  0.1058     0.009  \n",
       "dummy     0.0000  0.0000  0.0000     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.5241</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5429</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.7681</td>\n",
       "      <td>0.4203</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>0.4627</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.4997</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.3658</td>\n",
       "      <td>0.5219</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8583</td>\n",
       "      <td>0.9028</td>\n",
       "      <td>0.4962</td>\n",
       "      <td>0.5268</td>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.4889</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.7356</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8161</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8523</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "ridge                    Ridge Classifier    0.8039  0.0000  0.8086  0.4269   \n",
       "ada                  Ada Boost Classifier    0.8542  0.8922  0.5857  0.5241   \n",
       "et                 Extra Trees Classifier    0.8663  0.9065  0.5443  0.5518   \n",
       "lda          Linear Discriminant Analysis    0.8019  0.8754  0.7681  0.4203   \n",
       "rf               Random Forest Classifier    0.8713  0.9137  0.5243  0.5689   \n",
       "gbc          Gradient Boosting Classifier    0.8502  0.9048  0.5581  0.4997   \n",
       "qda       Quadratic Discriminant Analysis    0.7507  0.8283  0.9162  0.3658   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8583  0.9028  0.4962  0.5268   \n",
       "dt               Decision Tree Classifier    0.8382  0.7080  0.5233  0.4624   \n",
       "nb                            Naive Bayes    0.5797  0.7356  0.8367  0.2410   \n",
       "knn                K Neighbors Classifier    0.6754  0.6906  0.5295  0.2310   \n",
       "svm                   SVM - Linear Kernel    0.2891  0.0000  0.8000  0.1184   \n",
       "lr                    Logistic Regression    0.8161  0.6261  0.2190  0.0981   \n",
       "dummy                    Dummy Classifier    0.8523  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "ridge     0.5545  0.4466  0.4861     0.009  \n",
       "ada       0.5461  0.4610  0.4662     0.052  \n",
       "et        0.5429  0.4652  0.4683     0.046  \n",
       "lda       0.5399  0.4299  0.4627     0.013  \n",
       "rf        0.5324  0.4603  0.4674     0.060  \n",
       "gbc       0.5246  0.4366  0.4391     0.157  \n",
       "qda       0.5219  0.3939  0.4702     0.012  \n",
       "lightgbm  0.5010  0.4200  0.4259     0.094  \n",
       "dt        0.4889  0.3937  0.3959     0.014  \n",
       "nb        0.3730  0.1861  0.2666     0.009  \n",
       "knn       0.3199  0.1469  0.1717     0.022  \n",
       "svm       0.2063  0.0000  0.0000     0.014  \n",
       "lr        0.1335  0.0915  0.1058     0.009  \n",
       "dummy     0.0000  0.0000  0.0000     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "                max_iter=None, normalize=False, random_state=2417,\n",
      "                solver='auto', tol=0.001)\n",
      "Participant:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression    0.5124  0.5707  0.7069  0.3885  0.4843  0.1131   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.1191     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr      Logistic Regression    0.5124  0.5707  0.7069  0.3885  0.4843  0.1131   \n",
       "knn  K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108  0.4624  0.1590   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "lr   0.1191     0.006  \n",
       "knn  0.1632     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "nb              Naive Bayes    0.5669  0.7127  0.8333  0.4208  0.5565  0.2174   \n",
       "lr      Logistic Regression    0.5124  0.5707  0.7069  0.3885  0.4843  0.1131   \n",
       "knn  K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108  0.4624  0.1590   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "nb   0.2713     0.006  \n",
       "lr   0.1191     0.006  \n",
       "knn  0.1632     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625  0.6899   \n",
       "nb                Naive Bayes    0.5669  0.7127  0.8333  0.4208  0.5565   \n",
       "lr        Logistic Regression    0.5124  0.5707  0.7069  0.3885  0.4843   \n",
       "knn    K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108  0.4624   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.5255  0.5314     0.006  \n",
       "nb   0.2174  0.2713     0.006  \n",
       "lr   0.1131  0.1191     0.006  \n",
       "knn  0.1590  0.1632     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625  0.6899   \n",
       "nb                Naive Bayes    0.5669  0.7127  0.8333  0.4208  0.5565   \n",
       "lr        Logistic Regression    0.5124  0.5707  0.7069  0.3885  0.4843   \n",
       "knn    K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108  0.4624   \n",
       "svm       SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272  0.1930   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.5255  0.5314     0.006  \n",
       "nb   0.2174  0.2713     0.006  \n",
       "lr   0.1131  0.1191     0.006  \n",
       "knn  0.1590  0.1632     0.009  \n",
       "svm  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt     Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625  0.6899   \n",
       "ridge          Ridge Classifier    0.7612  0.0000  0.7500  0.6225  0.6714   \n",
       "nb                  Naive Bayes    0.5669  0.7127  0.8333  0.4208  0.5565   \n",
       "lr          Logistic Regression    0.5124  0.5707  0.7069  0.3885  0.4843   \n",
       "knn      K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108  0.4624   \n",
       "svm         SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272  0.1930   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "dt     0.5255  0.5314     0.006  \n",
       "ridge  0.4882  0.5021     0.005  \n",
       "nb     0.2174  0.2713     0.006  \n",
       "lr     0.1131  0.1191     0.006  \n",
       "knn    0.1590  0.1632     0.009  \n",
       "svm    0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "rf     Random Forest Classifier    0.8090  0.8803  0.6861  0.7426  0.7041   \n",
       "dt     Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625  0.6899   \n",
       "ridge          Ridge Classifier    0.7612  0.0000  0.7500  0.6225  0.6714   \n",
       "nb                  Naive Bayes    0.5669  0.7127  0.8333  0.4208  0.5565   \n",
       "lr          Logistic Regression    0.5124  0.5707  0.7069  0.3885  0.4843   \n",
       "knn      K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108  0.4624   \n",
       "svm         SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272  0.1930   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "rf     0.5643  0.5736     0.036  \n",
       "dt     0.5255  0.5314     0.006  \n",
       "ridge  0.4882  0.5021     0.005  \n",
       "nb     0.2174  0.2713     0.006  \n",
       "lr     0.1131  0.1191     0.006  \n",
       "knn    0.1590  0.1632     0.009  \n",
       "svm    0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.8090  0.8803  0.6861  0.7426   \n",
       "dt            Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625   \n",
       "ridge                 Ridge Classifier    0.7612  0.0000  0.7500  0.6225   \n",
       "nb                         Naive Bayes    0.5669  0.7127  0.8333  0.4208   \n",
       "lr                 Logistic Regression    0.5124  0.5707  0.7069  0.3885   \n",
       "knn             K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108   \n",
       "qda    Quadratic Discriminant Analysis    0.6837  0.5457  0.1458  0.4517   \n",
       "svm                SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.7041  0.5643  0.5736     0.036  \n",
       "dt     0.6899  0.5255  0.5314     0.006  \n",
       "ridge  0.6714  0.4882  0.5021     0.005  \n",
       "nb     0.5565  0.2174  0.2713     0.006  \n",
       "lr     0.4843  0.1131  0.1191     0.006  \n",
       "knn    0.4624  0.1590  0.1632     0.009  \n",
       "qda    0.2117  0.1080  0.1278     0.007  \n",
       "svm    0.1930  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.4631</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.8090  0.8803  0.6861  0.7426   \n",
       "dt            Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625   \n",
       "ridge                 Ridge Classifier    0.7612  0.0000  0.7500  0.6225   \n",
       "ada               Ada Boost Classifier    0.7610  0.8471  0.6597  0.6437   \n",
       "nb                         Naive Bayes    0.5669  0.7127  0.8333  0.4208   \n",
       "lr                 Logistic Regression    0.5124  0.5707  0.7069  0.3885   \n",
       "knn             K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108   \n",
       "qda    Quadratic Discriminant Analysis    0.6837  0.5457  0.1458  0.4517   \n",
       "svm                SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.7041  0.5643  0.5736     0.036  \n",
       "dt     0.6899  0.5255  0.5314     0.006  \n",
       "ridge  0.6714  0.4882  0.5021     0.005  \n",
       "ada    0.6400  0.4631  0.4712     0.024  \n",
       "nb     0.5565  0.2174  0.2713     0.006  \n",
       "lr     0.4843  0.1131  0.1191     0.006  \n",
       "knn    0.4624  0.1590  0.1632     0.009  \n",
       "qda    0.2117  0.1080  0.1278     0.007  \n",
       "svm    0.1930  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.4631</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.8090  0.8803  0.6861  0.7426   \n",
       "dt            Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625   \n",
       "ridge                 Ridge Classifier    0.7612  0.0000  0.7500  0.6225   \n",
       "gbc       Gradient Boosting Classifier    0.7577  0.8572  0.6389  0.6670   \n",
       "ada               Ada Boost Classifier    0.7610  0.8471  0.6597  0.6437   \n",
       "nb                         Naive Bayes    0.5669  0.7127  0.8333  0.4208   \n",
       "lr                 Logistic Regression    0.5124  0.5707  0.7069  0.3885   \n",
       "knn             K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108   \n",
       "qda    Quadratic Discriminant Analysis    0.6837  0.5457  0.1458  0.4517   \n",
       "svm                SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.7041  0.5643  0.5736     0.036  \n",
       "dt     0.6899  0.5255  0.5314     0.006  \n",
       "ridge  0.6714  0.4882  0.5021     0.005  \n",
       "gbc    0.6436  0.4615  0.4683     0.047  \n",
       "ada    0.6400  0.4631  0.4712     0.024  \n",
       "nb     0.5565  0.2174  0.2713     0.006  \n",
       "lr     0.4843  0.1131  0.1191     0.006  \n",
       "knn    0.4624  0.1590  0.1632     0.009  \n",
       "qda    0.2117  0.1080  0.1278     0.007  \n",
       "svm    0.1930  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.4631</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>0.4111</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "rf            Random Forest Classifier    0.8090  0.8803  0.6861  0.7426   \n",
       "dt            Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625   \n",
       "ridge                 Ridge Classifier    0.7612  0.0000  0.7500  0.6225   \n",
       "gbc       Gradient Boosting Classifier    0.7577  0.8572  0.6389  0.6670   \n",
       "ada               Ada Boost Classifier    0.7610  0.8471  0.6597  0.6437   \n",
       "lda       Linear Discriminant Analysis    0.7245  0.7844  0.6972  0.5779   \n",
       "nb                         Naive Bayes    0.5669  0.7127  0.8333  0.4208   \n",
       "lr                 Logistic Regression    0.5124  0.5707  0.7069  0.3885   \n",
       "knn             K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108   \n",
       "qda    Quadratic Discriminant Analysis    0.6837  0.5457  0.1458  0.4517   \n",
       "svm                SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "rf     0.7041  0.5643  0.5736     0.036  \n",
       "dt     0.6899  0.5255  0.5314     0.006  \n",
       "ridge  0.6714  0.4882  0.5021     0.005  \n",
       "gbc    0.6436  0.4615  0.4683     0.047  \n",
       "ada    0.6400  0.4631  0.4712     0.024  \n",
       "lda    0.6207  0.4111  0.4241     0.007  \n",
       "nb     0.5565  0.2174  0.2713     0.006  \n",
       "lr     0.4843  0.1131  0.1191     0.006  \n",
       "knn    0.4624  0.1590  0.1632     0.009  \n",
       "qda    0.2117  0.1080  0.1278     0.007  \n",
       "svm    0.1930  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8130</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.7147</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0.5845</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.4631</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>0.4111</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et              Extra Trees Classifier    0.8130  0.8803  0.7181  0.7268   \n",
       "rf            Random Forest Classifier    0.8090  0.8803  0.6861  0.7426   \n",
       "dt            Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625   \n",
       "ridge                 Ridge Classifier    0.7612  0.0000  0.7500  0.6225   \n",
       "gbc       Gradient Boosting Classifier    0.7577  0.8572  0.6389  0.6670   \n",
       "ada               Ada Boost Classifier    0.7610  0.8471  0.6597  0.6437   \n",
       "lda       Linear Discriminant Analysis    0.7245  0.7844  0.6972  0.5779   \n",
       "nb                         Naive Bayes    0.5669  0.7127  0.8333  0.4208   \n",
       "lr                 Logistic Regression    0.5124  0.5707  0.7069  0.3885   \n",
       "knn             K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108   \n",
       "qda    Quadratic Discriminant Analysis    0.6837  0.5457  0.1458  0.4517   \n",
       "svm                SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "et     0.7147  0.5766  0.5845     0.034  \n",
       "rf     0.7041  0.5643  0.5736     0.036  \n",
       "dt     0.6899  0.5255  0.5314     0.006  \n",
       "ridge  0.6714  0.4882  0.5021     0.005  \n",
       "gbc    0.6436  0.4615  0.4683     0.047  \n",
       "ada    0.6400  0.4631  0.4712     0.024  \n",
       "lda    0.6207  0.4111  0.4241     0.007  \n",
       "nb     0.5565  0.2174  0.2713     0.006  \n",
       "lr     0.4843  0.1131  0.1191     0.006  \n",
       "knn    0.4624  0.1590  0.1632     0.009  \n",
       "qda    0.2117  0.1080  0.1278     0.007  \n",
       "svm    0.1930  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8130</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.7147</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0.5845</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>0.8573</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.6519</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>0.4937</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.4631</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>0.4111</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.8130  0.8803  0.7181  0.7268   \n",
       "rf               Random Forest Classifier    0.8090  0.8803  0.6861  0.7426   \n",
       "dt               Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625   \n",
       "ridge                    Ridge Classifier    0.7612  0.0000  0.7500  0.6225   \n",
       "lightgbm  Light Gradient Boosting Machine    0.7724  0.8573  0.6500  0.6795   \n",
       "gbc          Gradient Boosting Classifier    0.7577  0.8572  0.6389  0.6670   \n",
       "ada                  Ada Boost Classifier    0.7610  0.8471  0.6597  0.6437   \n",
       "lda          Linear Discriminant Analysis    0.7245  0.7844  0.6972  0.5779   \n",
       "nb                            Naive Bayes    0.5669  0.7127  0.8333  0.4208   \n",
       "lr                    Logistic Regression    0.5124  0.5707  0.7069  0.3885   \n",
       "knn                K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108   \n",
       "qda       Quadratic Discriminant Analysis    0.6837  0.5457  0.1458  0.4517   \n",
       "svm                   SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.7147  0.5766  0.5845     0.034  \n",
       "rf        0.7041  0.5643  0.5736     0.036  \n",
       "dt        0.6899  0.5255  0.5314     0.006  \n",
       "ridge     0.6714  0.4882  0.5021     0.005  \n",
       "lightgbm  0.6519  0.4840  0.4937     0.013  \n",
       "gbc       0.6436  0.4615  0.4683     0.047  \n",
       "ada       0.6400  0.4631  0.4712     0.024  \n",
       "lda       0.6207  0.4111  0.4241     0.007  \n",
       "nb        0.5565  0.2174  0.2713     0.006  \n",
       "lr        0.4843  0.1131  0.1191     0.006  \n",
       "knn       0.4624  0.1590  0.1632     0.009  \n",
       "qda       0.2117  0.1080  0.1278     0.007  \n",
       "svm       0.1930  0.0000  0.0000     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8130</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.7147</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0.5845</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>0.8573</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.6519</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>0.4937</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.4631</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>0.4111</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.6728</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.8130  0.8803  0.7181  0.7268   \n",
       "rf               Random Forest Classifier    0.8090  0.8803  0.6861  0.7426   \n",
       "dt               Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625   \n",
       "ridge                    Ridge Classifier    0.7612  0.0000  0.7500  0.6225   \n",
       "lightgbm  Light Gradient Boosting Machine    0.7724  0.8573  0.6500  0.6795   \n",
       "gbc          Gradient Boosting Classifier    0.7577  0.8572  0.6389  0.6670   \n",
       "ada                  Ada Boost Classifier    0.7610  0.8471  0.6597  0.6437   \n",
       "lda          Linear Discriminant Analysis    0.7245  0.7844  0.6972  0.5779   \n",
       "nb                            Naive Bayes    0.5669  0.7127  0.8333  0.4208   \n",
       "lr                    Logistic Regression    0.5124  0.5707  0.7069  0.3885   \n",
       "knn                K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108   \n",
       "qda       Quadratic Discriminant Analysis    0.6837  0.5457  0.1458  0.4517   \n",
       "svm                   SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272   \n",
       "dummy                    Dummy Classifier    0.6728  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.7147  0.5766  0.5845     0.034  \n",
       "rf        0.7041  0.5643  0.5736     0.036  \n",
       "dt        0.6899  0.5255  0.5314     0.006  \n",
       "ridge     0.6714  0.4882  0.5021     0.005  \n",
       "lightgbm  0.6519  0.4840  0.4937     0.013  \n",
       "gbc       0.6436  0.4615  0.4683     0.047  \n",
       "ada       0.6400  0.4631  0.4712     0.024  \n",
       "lda       0.6207  0.4111  0.4241     0.007  \n",
       "nb        0.5565  0.2174  0.2713     0.006  \n",
       "lr        0.4843  0.1131  0.1191     0.006  \n",
       "knn       0.4624  0.1590  0.1632     0.009  \n",
       "qda       0.2117  0.1080  0.1278     0.007  \n",
       "svm       0.1930  0.0000  0.0000     0.006  \n",
       "dummy     0.0000  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8130</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.7147</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0.5845</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>0.8573</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.6519</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>0.4937</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.4631</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>0.4111</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.6195</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.6728</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.8130  0.8803  0.7181  0.7268   \n",
       "rf               Random Forest Classifier    0.8090  0.8803  0.6861  0.7426   \n",
       "dt               Decision Tree Classifier    0.7837  0.7698  0.7306  0.6625   \n",
       "ridge                    Ridge Classifier    0.7612  0.0000  0.7500  0.6225   \n",
       "lightgbm  Light Gradient Boosting Machine    0.7724  0.8573  0.6500  0.6795   \n",
       "gbc          Gradient Boosting Classifier    0.7577  0.8572  0.6389  0.6670   \n",
       "ada                  Ada Boost Classifier    0.7610  0.8471  0.6597  0.6437   \n",
       "lda          Linear Discriminant Analysis    0.7245  0.7844  0.6972  0.5779   \n",
       "nb                            Naive Bayes    0.5669  0.7127  0.8333  0.4208   \n",
       "lr                    Logistic Regression    0.5124  0.5707  0.7069  0.3885   \n",
       "knn                K Neighbors Classifier    0.6065  0.6195  0.5347  0.4108   \n",
       "qda       Quadratic Discriminant Analysis    0.6837  0.5457  0.1458  0.4517   \n",
       "svm                   SVM - Linear Kernel    0.5272  0.0000  0.4000  0.1272   \n",
       "dummy                    Dummy Classifier    0.6728  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.7147  0.5766  0.5845     0.034  \n",
       "rf        0.7041  0.5643  0.5736     0.036  \n",
       "dt        0.6899  0.5255  0.5314     0.006  \n",
       "ridge     0.6714  0.4882  0.5021     0.005  \n",
       "lightgbm  0.6519  0.4840  0.4937     0.013  \n",
       "gbc       0.6436  0.4615  0.4683     0.047  \n",
       "ada       0.6400  0.4631  0.4712     0.024  \n",
       "lda       0.6207  0.4111  0.4241     0.007  \n",
       "nb        0.5565  0.2174  0.2713     0.006  \n",
       "lr        0.4843  0.1131  0.1191     0.006  \n",
       "knn       0.4624  0.1590  0.1632     0.009  \n",
       "qda       0.2117  0.1080  0.1278     0.007  \n",
       "svm       0.1930  0.0000  0.0000     0.006  \n",
       "dummy     0.0000  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "                     oob_score=False, random_state=6465, verbose=0,\n",
      "                     warm_start=False)\n",
      "Participant:  3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression     0.231  0.5964  0.9375  0.1487  0.2503  0.0354   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.0462     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103  0.2910  0.1336   \n",
       "lr      Logistic Regression    0.2310  0.5964  0.9375  0.1487  0.2503  0.0354   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.1505     0.012  \n",
       "lr   0.0462     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "nb              Naive Bayes    0.6561  0.7189  0.7804  0.2428  0.3685  0.2137   \n",
       "knn  K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103  0.2910  0.1336   \n",
       "lr      Logistic Regression    0.2310  0.5964  0.9375  0.1487  0.2503  0.0354   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "nb   0.2857     0.006  \n",
       "knn  0.1505     0.012  \n",
       "lr   0.0462     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990  0.7551   \n",
       "nb                Naive Bayes    0.6561  0.7189  0.7804  0.2428  0.3685   \n",
       "knn    K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103  0.2910   \n",
       "lr        Logistic Regression    0.2310  0.5964  0.9375  0.1487  0.2503   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.7208  0.7250     0.009  \n",
       "nb   0.2137  0.2857     0.006  \n",
       "knn  0.1336  0.1505     0.012  \n",
       "lr   0.0354  0.0462     0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt   Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990  0.7551   \n",
       "nb                Naive Bayes    0.6561  0.7189  0.7804  0.2428  0.3685   \n",
       "knn    K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103  0.2910   \n",
       "lr        Logistic Regression    0.2310  0.5964  0.9375  0.1487  0.2503   \n",
       "svm       SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533  0.0941   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "dt   0.7208  0.7250     0.009  \n",
       "nb   0.2137  0.2857     0.006  \n",
       "knn  0.1336  0.1505     0.012  \n",
       "lr   0.0354  0.0462     0.006  \n",
       "svm  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt     Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990  0.7551   \n",
       "ridge          Ridge Classifier    0.9019  0.0000  0.9054  0.5955  0.7130   \n",
       "nb                  Naive Bayes    0.6561  0.7189  0.7804  0.2428  0.3685   \n",
       "knn      K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103  0.2910   \n",
       "lr          Logistic Regression    0.2310  0.5964  0.9375  0.1487  0.2503   \n",
       "svm         SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533  0.0941   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "dt     0.7208  0.7250     0.009  \n",
       "ridge  0.6582  0.6825     0.006  \n",
       "nb     0.2137  0.2857     0.006  \n",
       "knn    0.1336  0.1505     0.012  \n",
       "lr     0.0354  0.0462     0.006  \n",
       "svm    0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "dt     Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990  0.7551   \n",
       "rf     Random Forest Classifier    0.9251  0.9596  0.7964  0.6894  0.7362   \n",
       "ridge          Ridge Classifier    0.9019  0.0000  0.9054  0.5955  0.7130   \n",
       "nb                  Naive Bayes    0.6561  0.7189  0.7804  0.2428  0.3685   \n",
       "knn      K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103  0.2910   \n",
       "lr          Logistic Regression    0.2310  0.5964  0.9375  0.1487  0.2503   \n",
       "svm         SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533  0.0941   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "dt     0.7208  0.7250     0.009  \n",
       "rf     0.6931  0.6974     0.041  \n",
       "ridge  0.6582  0.6825     0.006  \n",
       "nb     0.2137  0.2857     0.006  \n",
       "knn    0.1336  0.1505     0.012  \n",
       "lr     0.0354  0.0462     0.006  \n",
       "svm    0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990   \n",
       "rf            Random Forest Classifier    0.9251  0.9596  0.7964  0.6894   \n",
       "ridge                 Ridge Classifier    0.9019  0.0000  0.9054  0.5955   \n",
       "nb                         Naive Bayes    0.6561  0.7189  0.7804  0.2428   \n",
       "knn             K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103   \n",
       "lr                 Logistic Regression    0.2310  0.5964  0.9375  0.1487   \n",
       "qda    Quadratic Discriminant Analysis    0.8752  0.5493  0.1089  0.6000   \n",
       "svm                SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.7551  0.7208  0.7250     0.009  \n",
       "rf     0.7362  0.6931  0.6974     0.041  \n",
       "ridge  0.7130  0.6582  0.6825     0.006  \n",
       "nb     0.3685  0.2137  0.2857     0.006  \n",
       "knn    0.2910  0.1336  0.1505     0.012  \n",
       "lr     0.2503  0.0354  0.0462     0.006  \n",
       "qda    0.1811  0.1488  0.2145     0.009  \n",
       "svm    0.0941  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.6647</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.6205</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990   \n",
       "rf            Random Forest Classifier    0.9251  0.9596  0.7964  0.6894   \n",
       "ridge                 Ridge Classifier    0.9019  0.0000  0.9054  0.5955   \n",
       "ada               Ada Boost Classifier    0.9108  0.9355  0.6839  0.6656   \n",
       "nb                         Naive Bayes    0.6561  0.7189  0.7804  0.2428   \n",
       "knn             K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103   \n",
       "lr                 Logistic Regression    0.2310  0.5964  0.9375  0.1487   \n",
       "qda    Quadratic Discriminant Analysis    0.8752  0.5493  0.1089  0.6000   \n",
       "svm                SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.7551  0.7208  0.7250     0.009  \n",
       "rf     0.7362  0.6931  0.6974     0.041  \n",
       "ridge  0.7130  0.6582  0.6825     0.006  \n",
       "ada    0.6647  0.6143  0.6205     0.032  \n",
       "nb     0.3685  0.2137  0.2857     0.006  \n",
       "knn    0.2910  0.1336  0.1505     0.012  \n",
       "lr     0.2503  0.0354  0.0462     0.006  \n",
       "qda    0.1811  0.1488  0.2145     0.009  \n",
       "svm    0.0941  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.6982</td>\n",
       "      <td>0.7499</td>\n",
       "      <td>0.7081</td>\n",
       "      <td>0.7149</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.6647</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.6205</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990   \n",
       "gbc       Gradient Boosting Classifier    0.9268  0.9631  0.8232  0.6982   \n",
       "rf            Random Forest Classifier    0.9251  0.9596  0.7964  0.6894   \n",
       "ridge                 Ridge Classifier    0.9019  0.0000  0.9054  0.5955   \n",
       "ada               Ada Boost Classifier    0.9108  0.9355  0.6839  0.6656   \n",
       "nb                         Naive Bayes    0.6561  0.7189  0.7804  0.2428   \n",
       "knn             K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103   \n",
       "lr                 Logistic Regression    0.2310  0.5964  0.9375  0.1487   \n",
       "qda    Quadratic Discriminant Analysis    0.8752  0.5493  0.1089  0.6000   \n",
       "svm                SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.7551  0.7208  0.7250     0.009  \n",
       "gbc    0.7499  0.7081  0.7149     0.080  \n",
       "rf     0.7362  0.6931  0.6974     0.041  \n",
       "ridge  0.7130  0.6582  0.6825     0.006  \n",
       "ada    0.6647  0.6143  0.6205     0.032  \n",
       "nb     0.3685  0.2137  0.2857     0.006  \n",
       "knn    0.2910  0.1336  0.1505     0.012  \n",
       "lr     0.2503  0.0354  0.0462     0.006  \n",
       "qda    0.1811  0.1488  0.2145     0.009  \n",
       "svm    0.0941  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.6982</td>\n",
       "      <td>0.7499</td>\n",
       "      <td>0.7081</td>\n",
       "      <td>0.7149</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9037</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.6647</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.6205</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990   \n",
       "gbc       Gradient Boosting Classifier    0.9268  0.9631  0.8232  0.6982   \n",
       "rf            Random Forest Classifier    0.9251  0.9596  0.7964  0.6894   \n",
       "ridge                 Ridge Classifier    0.9019  0.0000  0.9054  0.5955   \n",
       "lda       Linear Discriminant Analysis    0.9037  0.9547  0.8911  0.5994   \n",
       "ada               Ada Boost Classifier    0.9108  0.9355  0.6839  0.6656   \n",
       "nb                         Naive Bayes    0.6561  0.7189  0.7804  0.2428   \n",
       "knn             K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103   \n",
       "lr                 Logistic Regression    0.2310  0.5964  0.9375  0.1487   \n",
       "qda    Quadratic Discriminant Analysis    0.8752  0.5493  0.1089  0.6000   \n",
       "svm                SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.7551  0.7208  0.7250     0.009  \n",
       "gbc    0.7499  0.7081  0.7149     0.080  \n",
       "rf     0.7362  0.6931  0.6974     0.041  \n",
       "ridge  0.7130  0.6582  0.6825     0.006  \n",
       "lda    0.7125  0.6584  0.6795     0.011  \n",
       "ada    0.6647  0.6143  0.6205     0.032  \n",
       "nb     0.3685  0.2137  0.2857     0.006  \n",
       "knn    0.2910  0.1336  0.1505     0.012  \n",
       "lr     0.2503  0.0354  0.0462     0.006  \n",
       "qda    0.1811  0.1488  0.2145     0.009  \n",
       "svm    0.0941  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.6982</td>\n",
       "      <td>0.7499</td>\n",
       "      <td>0.7081</td>\n",
       "      <td>0.7149</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>0.9587</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.7323</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9037</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.6647</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.6205</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt            Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990   \n",
       "gbc       Gradient Boosting Classifier    0.9268  0.9631  0.8232  0.6982   \n",
       "rf            Random Forest Classifier    0.9251  0.9596  0.7964  0.6894   \n",
       "et              Extra Trees Classifier    0.9234  0.9587  0.7964  0.6792   \n",
       "ridge                 Ridge Classifier    0.9019  0.0000  0.9054  0.5955   \n",
       "lda       Linear Discriminant Analysis    0.9037  0.9547  0.8911  0.5994   \n",
       "ada               Ada Boost Classifier    0.9108  0.9355  0.6839  0.6656   \n",
       "nb                         Naive Bayes    0.6561  0.7189  0.7804  0.2428   \n",
       "knn             K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103   \n",
       "lr                 Logistic Regression    0.2310  0.5964  0.9375  0.1487   \n",
       "qda    Quadratic Discriminant Analysis    0.8752  0.5493  0.1089  0.6000   \n",
       "svm                SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "dt     0.7551  0.7208  0.7250     0.009  \n",
       "gbc    0.7499  0.7081  0.7149     0.080  \n",
       "rf     0.7362  0.6931  0.6974     0.041  \n",
       "et     0.7323  0.6880  0.6915     0.040  \n",
       "ridge  0.7130  0.6582  0.6825     0.006  \n",
       "lda    0.7125  0.6584  0.6795     0.011  \n",
       "ada    0.6647  0.6143  0.6205     0.032  \n",
       "nb     0.3685  0.2137  0.2857     0.006  \n",
       "knn    0.2910  0.1336  0.1505     0.012  \n",
       "lr     0.2503  0.0354  0.0462     0.006  \n",
       "qda    0.1811  0.1488  0.2145     0.009  \n",
       "svm    0.0941  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.6982</td>\n",
       "      <td>0.7499</td>\n",
       "      <td>0.7081</td>\n",
       "      <td>0.7149</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>0.9587</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.7323</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9037</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9179</td>\n",
       "      <td>0.9544</td>\n",
       "      <td>0.7536</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.6657</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.6647</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.6205</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt               Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990   \n",
       "gbc          Gradient Boosting Classifier    0.9268  0.9631  0.8232  0.6982   \n",
       "rf               Random Forest Classifier    0.9251  0.9596  0.7964  0.6894   \n",
       "et                 Extra Trees Classifier    0.9234  0.9587  0.7964  0.6792   \n",
       "ridge                    Ridge Classifier    0.9019  0.0000  0.9054  0.5955   \n",
       "lda          Linear Discriminant Analysis    0.9037  0.9547  0.8911  0.5994   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9179  0.9544  0.7536  0.6899   \n",
       "ada                  Ada Boost Classifier    0.9108  0.9355  0.6839  0.6656   \n",
       "nb                            Naive Bayes    0.6561  0.7189  0.7804  0.2428   \n",
       "knn                K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103   \n",
       "lr                    Logistic Regression    0.2310  0.5964  0.9375  0.1487   \n",
       "qda       Quadratic Discriminant Analysis    0.8752  0.5493  0.1089  0.6000   \n",
       "svm                   SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "dt        0.7551  0.7208  0.7250     0.009  \n",
       "gbc       0.7499  0.7081  0.7149     0.080  \n",
       "rf        0.7362  0.6931  0.6974     0.041  \n",
       "et        0.7323  0.6880  0.6915     0.040  \n",
       "ridge     0.7130  0.6582  0.6825     0.006  \n",
       "lda       0.7125  0.6584  0.6795     0.011  \n",
       "lightgbm  0.7124  0.6657  0.6714     0.070  \n",
       "ada       0.6647  0.6143  0.6205     0.032  \n",
       "nb        0.3685  0.2137  0.2857     0.006  \n",
       "knn       0.2910  0.1336  0.1505     0.012  \n",
       "lr        0.2503  0.0354  0.0462     0.006  \n",
       "qda       0.1811  0.1488  0.2145     0.009  \n",
       "svm       0.0941  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.6982</td>\n",
       "      <td>0.7499</td>\n",
       "      <td>0.7081</td>\n",
       "      <td>0.7149</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>0.9587</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.7323</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9037</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9179</td>\n",
       "      <td>0.9544</td>\n",
       "      <td>0.7536</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.6657</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.6647</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.6205</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt               Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990   \n",
       "gbc          Gradient Boosting Classifier    0.9268  0.9631  0.8232  0.6982   \n",
       "rf               Random Forest Classifier    0.9251  0.9596  0.7964  0.6894   \n",
       "et                 Extra Trees Classifier    0.9234  0.9587  0.7964  0.6792   \n",
       "ridge                    Ridge Classifier    0.9019  0.0000  0.9054  0.5955   \n",
       "lda          Linear Discriminant Analysis    0.9037  0.9547  0.8911  0.5994   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9179  0.9544  0.7536  0.6899   \n",
       "ada                  Ada Boost Classifier    0.9108  0.9355  0.6839  0.6656   \n",
       "nb                            Naive Bayes    0.6561  0.7189  0.7804  0.2428   \n",
       "knn                K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103   \n",
       "lr                    Logistic Regression    0.2310  0.5964  0.9375  0.1487   \n",
       "qda       Quadratic Discriminant Analysis    0.8752  0.5493  0.1089  0.6000   \n",
       "svm                   SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533   \n",
       "dummy                    Dummy Classifier    0.8699  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "dt        0.7551  0.7208  0.7250     0.009  \n",
       "gbc       0.7499  0.7081  0.7149     0.080  \n",
       "rf        0.7362  0.6931  0.6974     0.041  \n",
       "et        0.7323  0.6880  0.6915     0.040  \n",
       "ridge     0.7130  0.6582  0.6825     0.006  \n",
       "lda       0.7125  0.6584  0.6795     0.011  \n",
       "lightgbm  0.7124  0.6657  0.6714     0.070  \n",
       "ada       0.6647  0.6143  0.6205     0.032  \n",
       "nb        0.3685  0.2137  0.2857     0.006  \n",
       "knn       0.2910  0.1336  0.1505     0.012  \n",
       "lr        0.2503  0.0354  0.0462     0.006  \n",
       "qda       0.1811  0.1488  0.2145     0.009  \n",
       "svm       0.0941  0.0000  0.0000     0.008  \n",
       "dummy     0.0000  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.6982</td>\n",
       "      <td>0.7499</td>\n",
       "      <td>0.7081</td>\n",
       "      <td>0.7149</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>0.9587</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.7323</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.9037</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9179</td>\n",
       "      <td>0.9544</td>\n",
       "      <td>0.7536</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.6657</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.6647</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.6205</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "dt               Decision Tree Classifier    0.9394  0.8482  0.7250  0.7990   \n",
       "gbc          Gradient Boosting Classifier    0.9268  0.9631  0.8232  0.6982   \n",
       "rf               Random Forest Classifier    0.9251  0.9596  0.7964  0.6894   \n",
       "et                 Extra Trees Classifier    0.9234  0.9587  0.7964  0.6792   \n",
       "ridge                    Ridge Classifier    0.9019  0.0000  0.9054  0.5955   \n",
       "lda          Linear Discriminant Analysis    0.9037  0.9547  0.8911  0.5994   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9179  0.9544  0.7536  0.6899   \n",
       "ada                  Ada Boost Classifier    0.9108  0.9355  0.6839  0.6656   \n",
       "nb                            Naive Bayes    0.6561  0.7189  0.7804  0.2428   \n",
       "knn                K Neighbors Classifier    0.6899  0.6953  0.4804  0.2103   \n",
       "lr                    Logistic Regression    0.2310  0.5964  0.9375  0.1487   \n",
       "qda       Quadratic Discriminant Analysis    0.8752  0.5493  0.1089  0.6000   \n",
       "svm                   SVM - Linear Kernel    0.5765  0.0000  0.4000  0.0533   \n",
       "dummy                    Dummy Classifier    0.8699  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "dt        0.7551  0.7208  0.7250     0.009  \n",
       "gbc       0.7499  0.7081  0.7149     0.080  \n",
       "rf        0.7362  0.6931  0.6974     0.041  \n",
       "et        0.7323  0.6880  0.6915     0.040  \n",
       "ridge     0.7130  0.6582  0.6825     0.006  \n",
       "lda       0.7125  0.6584  0.6795     0.011  \n",
       "lightgbm  0.7124  0.6657  0.6714     0.070  \n",
       "ada       0.6647  0.6143  0.6205     0.032  \n",
       "nb        0.3685  0.2137  0.2857     0.006  \n",
       "knn       0.2910  0.1336  0.1505     0.012  \n",
       "lr        0.2503  0.0354  0.0462     0.006  \n",
       "qda       0.1811  0.1488  0.2145     0.009  \n",
       "svm       0.0941  0.0000  0.0000     0.008  \n",
       "dummy     0.0000  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=3741, splitter='best')\n",
      "Participant:  7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.     F1   Kappa  \\\n",
       "lr  Logistic Regression    0.5792  0.4877    0.65  0.6237  0.617  0.0477   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.0418     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.     F1   Kappa  \\\n",
       "lr      Logistic Regression    0.5792  0.4877  0.6500  0.6237  0.617  0.0477   \n",
       "knn  K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338  0.591  0.0235   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "lr   0.0418     0.005  \n",
       "knn  0.0391     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "nb              Naive Bayes    0.7181  0.7210  0.8381  0.7802  0.8018  0.2974   \n",
       "lr      Logistic Regression    0.5792  0.4877  0.6500  0.6237  0.6170  0.0477   \n",
       "knn  K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338  0.5910  0.0235   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "nb   0.3124     0.006  \n",
       "lr   0.0418     0.005  \n",
       "knn  0.0391     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "nb                Naive Bayes    0.7181  0.7210  0.8381  0.7802  0.8018   \n",
       "dt   Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015  0.7214   \n",
       "lr        Logistic Regression    0.5792  0.4877  0.6500  0.6237  0.6170   \n",
       "knn    K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338  0.5910   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "nb   0.2974  0.3124     0.006  \n",
       "dt   0.3046  0.3177     0.005  \n",
       "lr   0.0477  0.0418     0.005  \n",
       "knn  0.0235  0.0391     0.007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "nb                Naive Bayes    0.7181  0.7210  0.8381  0.7802  0.8018   \n",
       "dt   Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015  0.7214   \n",
       "lr        Logistic Regression    0.5792  0.4877  0.6500  0.6237  0.6170   \n",
       "knn    K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338  0.5910   \n",
       "svm       SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444  0.1675   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "nb   0.2974  0.3124     0.006  \n",
       "dt   0.3046  0.3177     0.005  \n",
       "lr   0.0477  0.0418     0.005  \n",
       "knn  0.0235  0.0391     0.007  \n",
       "svm  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "nb                  Naive Bayes    0.7181  0.7210  0.8381  0.7802  0.8018   \n",
       "ridge          Ridge Classifier    0.7083  0.0000  0.7738  0.8052  0.7844   \n",
       "dt     Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015  0.7214   \n",
       "lr          Logistic Regression    0.5792  0.4877  0.6500  0.6237  0.6170   \n",
       "knn      K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338  0.5910   \n",
       "svm         SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444  0.1675   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "nb     0.2974  0.3124     0.006  \n",
       "ridge  0.3154  0.3232     0.005  \n",
       "dt     0.3046  0.3177     0.005  \n",
       "lr     0.0477  0.0418     0.005  \n",
       "knn    0.0235  0.0391     0.007  \n",
       "svm    0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "rf     Random Forest Classifier    0.7417  0.7889  0.8238  0.8240  0.8143   \n",
       "nb                  Naive Bayes    0.7181  0.7210  0.8381  0.7802  0.8018   \n",
       "ridge          Ridge Classifier    0.7083  0.0000  0.7738  0.8052  0.7844   \n",
       "dt     Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015  0.7214   \n",
       "lr          Logistic Regression    0.5792  0.4877  0.6500  0.6237  0.6170   \n",
       "knn      K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338  0.5910   \n",
       "svm         SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444  0.1675   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "rf     0.3949  0.4184     0.035  \n",
       "nb     0.2974  0.3124     0.006  \n",
       "ridge  0.3154  0.3232     0.005  \n",
       "dt     0.3046  0.3177     0.005  \n",
       "lr     0.0477  0.0418     0.005  \n",
       "knn    0.0235  0.0391     0.007  \n",
       "svm    0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "qda    Quadratic Discriminant Analysis    0.6972  0.5000  1.0000  0.6972   \n",
       "rf            Random Forest Classifier    0.7417  0.7889  0.8238  0.8240   \n",
       "nb                         Naive Bayes    0.7181  0.7210  0.8381  0.7802   \n",
       "ridge                 Ridge Classifier    0.7083  0.0000  0.7738  0.8052   \n",
       "dt            Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015   \n",
       "lr                 Logistic Regression    0.5792  0.4877  0.6500  0.6237   \n",
       "knn             K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338   \n",
       "svm                SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "qda    0.8207  0.0000  0.0000     0.005  \n",
       "rf     0.8143  0.3949  0.4184     0.035  \n",
       "nb     0.8018  0.2974  0.3124     0.006  \n",
       "ridge  0.7844  0.3154  0.3232     0.005  \n",
       "dt     0.7214  0.3046  0.3177     0.005  \n",
       "lr     0.6170  0.0477  0.0418     0.005  \n",
       "knn    0.5910  0.0235  0.0391     0.007  \n",
       "svm    0.1675  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.2526</td>\n",
       "      <td>0.2761</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "qda    Quadratic Discriminant Analysis    0.6972  0.5000  1.0000  0.6972   \n",
       "rf            Random Forest Classifier    0.7417  0.7889  0.8238  0.8240   \n",
       "nb                         Naive Bayes    0.7181  0.7210  0.8381  0.7802   \n",
       "ada               Ada Boost Classifier    0.7097  0.8020  0.8524  0.7518   \n",
       "ridge                 Ridge Classifier    0.7083  0.0000  0.7738  0.8052   \n",
       "dt            Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015   \n",
       "lr                 Logistic Regression    0.5792  0.4877  0.6500  0.6237   \n",
       "knn             K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338   \n",
       "svm                SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "qda    0.8207  0.0000  0.0000     0.005  \n",
       "rf     0.8143  0.3949  0.4184     0.035  \n",
       "nb     0.8018  0.2974  0.3124     0.006  \n",
       "ada    0.7975  0.2526  0.2761     0.016  \n",
       "ridge  0.7844  0.3154  0.3232     0.005  \n",
       "dt     0.7214  0.3046  0.3177     0.005  \n",
       "lr     0.6170  0.0477  0.0418     0.005  \n",
       "knn    0.5910  0.0235  0.0391     0.007  \n",
       "svm    0.1675  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7194</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3314</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.2526</td>\n",
       "      <td>0.2761</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "qda    Quadratic Discriminant Analysis    0.6972  0.5000  1.0000  0.6972   \n",
       "rf            Random Forest Classifier    0.7417  0.7889  0.8238  0.8240   \n",
       "nb                         Naive Bayes    0.7181  0.7210  0.8381  0.7802   \n",
       "gbc       Gradient Boosting Classifier    0.7194  0.7837  0.8214  0.7850   \n",
       "ada               Ada Boost Classifier    0.7097  0.8020  0.8524  0.7518   \n",
       "ridge                 Ridge Classifier    0.7083  0.0000  0.7738  0.8052   \n",
       "dt            Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015   \n",
       "lr                 Logistic Regression    0.5792  0.4877  0.6500  0.6237   \n",
       "knn             K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338   \n",
       "svm                SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "qda    0.8207  0.0000  0.0000     0.005  \n",
       "rf     0.8143  0.3949  0.4184     0.035  \n",
       "nb     0.8018  0.2974  0.3124     0.006  \n",
       "gbc    0.7988  0.3154  0.3314     0.019  \n",
       "ada    0.7975  0.2526  0.2761     0.016  \n",
       "ridge  0.7844  0.3154  0.3232     0.005  \n",
       "dt     0.7214  0.3046  0.3177     0.005  \n",
       "lr     0.6170  0.0477  0.0418     0.005  \n",
       "knn    0.5910  0.0235  0.0391     0.007  \n",
       "svm    0.1675  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7194</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3314</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.2526</td>\n",
       "      <td>0.2761</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5901</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.7598</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "qda    Quadratic Discriminant Analysis    0.6972  0.5000  1.0000  0.6972   \n",
       "rf            Random Forest Classifier    0.7417  0.7889  0.8238  0.8240   \n",
       "nb                         Naive Bayes    0.7181  0.7210  0.8381  0.7802   \n",
       "gbc       Gradient Boosting Classifier    0.7194  0.7837  0.8214  0.7850   \n",
       "ada               Ada Boost Classifier    0.7097  0.8020  0.8524  0.7518   \n",
       "ridge                 Ridge Classifier    0.7083  0.0000  0.7738  0.8052   \n",
       "dt            Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015   \n",
       "lda       Linear Discriminant Analysis    0.5722  0.5901  0.5810  0.7598   \n",
       "lr                 Logistic Regression    0.5792  0.4877  0.6500  0.6237   \n",
       "knn             K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338   \n",
       "svm                SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "qda    0.8207  0.0000  0.0000     0.005  \n",
       "rf     0.8143  0.3949  0.4184     0.035  \n",
       "nb     0.8018  0.2974  0.3124     0.006  \n",
       "gbc    0.7988  0.3154  0.3314     0.019  \n",
       "ada    0.7975  0.2526  0.2761     0.016  \n",
       "ridge  0.7844  0.3154  0.3232     0.005  \n",
       "dt     0.7214  0.3046  0.3177     0.005  \n",
       "lda    0.6456  0.0977  0.1117     0.012  \n",
       "lr     0.6170  0.0477  0.0418     0.005  \n",
       "knn    0.5910  0.0235  0.0391     0.007  \n",
       "svm    0.1675  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7194</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3314</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.2526</td>\n",
       "      <td>0.2761</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5901</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.7598</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et              Extra Trees Classifier    0.7736  0.8321  0.8714  0.8193   \n",
       "qda    Quadratic Discriminant Analysis    0.6972  0.5000  1.0000  0.6972   \n",
       "rf            Random Forest Classifier    0.7417  0.7889  0.8238  0.8240   \n",
       "nb                         Naive Bayes    0.7181  0.7210  0.8381  0.7802   \n",
       "gbc       Gradient Boosting Classifier    0.7194  0.7837  0.8214  0.7850   \n",
       "ada               Ada Boost Classifier    0.7097  0.8020  0.8524  0.7518   \n",
       "ridge                 Ridge Classifier    0.7083  0.0000  0.7738  0.8052   \n",
       "dt            Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015   \n",
       "lda       Linear Discriminant Analysis    0.5722  0.5901  0.5810  0.7598   \n",
       "lr                 Logistic Regression    0.5792  0.4877  0.6500  0.6237   \n",
       "knn             K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338   \n",
       "svm                SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "et     0.8414  0.4394  0.4564     0.035  \n",
       "qda    0.8207  0.0000  0.0000     0.005  \n",
       "rf     0.8143  0.3949  0.4184     0.035  \n",
       "nb     0.8018  0.2974  0.3124     0.006  \n",
       "gbc    0.7988  0.3154  0.3314     0.019  \n",
       "ada    0.7975  0.2526  0.2761     0.016  \n",
       "ridge  0.7844  0.3154  0.3232     0.005  \n",
       "dt     0.7214  0.3046  0.3177     0.005  \n",
       "lda    0.6456  0.0977  0.1117     0.012  \n",
       "lr     0.6170  0.0477  0.0418     0.005  \n",
       "knn    0.5910  0.0235  0.0391     0.007  \n",
       "svm    0.1675  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.7292</td>\n",
       "      <td>0.7595</td>\n",
       "      <td>0.8048</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.3533</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7194</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3314</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.2526</td>\n",
       "      <td>0.2761</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5901</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.7598</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.7736  0.8321  0.8714  0.8193   \n",
       "qda       Quadratic Discriminant Analysis    0.6972  0.5000  1.0000  0.6972   \n",
       "rf               Random Forest Classifier    0.7417  0.7889  0.8238  0.8240   \n",
       "nb                            Naive Bayes    0.7181  0.7210  0.8381  0.7802   \n",
       "lightgbm  Light Gradient Boosting Machine    0.7292  0.7595  0.8048  0.8110   \n",
       "gbc          Gradient Boosting Classifier    0.7194  0.7837  0.8214  0.7850   \n",
       "ada                  Ada Boost Classifier    0.7097  0.8020  0.8524  0.7518   \n",
       "ridge                    Ridge Classifier    0.7083  0.0000  0.7738  0.8052   \n",
       "dt               Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015   \n",
       "lda          Linear Discriminant Analysis    0.5722  0.5901  0.5810  0.7598   \n",
       "lr                    Logistic Regression    0.5792  0.4877  0.6500  0.6237   \n",
       "knn                K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338   \n",
       "svm                   SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.8414  0.4394  0.4564     0.035  \n",
       "qda       0.8207  0.0000  0.0000     0.005  \n",
       "rf        0.8143  0.3949  0.4184     0.035  \n",
       "nb        0.8018  0.2974  0.3124     0.006  \n",
       "lightgbm  0.8013  0.3533  0.3690     0.007  \n",
       "gbc       0.7988  0.3154  0.3314     0.019  \n",
       "ada       0.7975  0.2526  0.2761     0.016  \n",
       "ridge     0.7844  0.3154  0.3232     0.005  \n",
       "dt        0.7214  0.3046  0.3177     0.005  \n",
       "lda       0.6456  0.0977  0.1117     0.012  \n",
       "lr        0.6170  0.0477  0.0418     0.005  \n",
       "knn       0.5910  0.0235  0.0391     0.007  \n",
       "svm       0.1675  0.0000  0.0000     0.005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.7292</td>\n",
       "      <td>0.7595</td>\n",
       "      <td>0.8048</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.3533</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7194</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3314</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.2526</td>\n",
       "      <td>0.2761</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5901</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.7598</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.7736  0.8321  0.8714  0.8193   \n",
       "qda       Quadratic Discriminant Analysis    0.6972  0.5000  1.0000  0.6972   \n",
       "rf               Random Forest Classifier    0.7417  0.7889  0.8238  0.8240   \n",
       "nb                            Naive Bayes    0.7181  0.7210  0.8381  0.7802   \n",
       "lightgbm  Light Gradient Boosting Machine    0.7292  0.7595  0.8048  0.8110   \n",
       "gbc          Gradient Boosting Classifier    0.7194  0.7837  0.8214  0.7850   \n",
       "ada                  Ada Boost Classifier    0.7097  0.8020  0.8524  0.7518   \n",
       "ridge                    Ridge Classifier    0.7083  0.0000  0.7738  0.8052   \n",
       "dt               Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015   \n",
       "lda          Linear Discriminant Analysis    0.5722  0.5901  0.5810  0.7598   \n",
       "lr                    Logistic Regression    0.5792  0.4877  0.6500  0.6237   \n",
       "knn                K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338   \n",
       "svm                   SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444   \n",
       "dummy                    Dummy Classifier    0.3028  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.8414  0.4394  0.4564     0.035  \n",
       "qda       0.8207  0.0000  0.0000     0.005  \n",
       "rf        0.8143  0.3949  0.4184     0.035  \n",
       "nb        0.8018  0.2974  0.3124     0.006  \n",
       "lightgbm  0.8013  0.3533  0.3690     0.007  \n",
       "gbc       0.7988  0.3154  0.3314     0.019  \n",
       "ada       0.7975  0.2526  0.2761     0.016  \n",
       "ridge     0.7844  0.3154  0.3232     0.005  \n",
       "dt        0.7214  0.3046  0.3177     0.005  \n",
       "lda       0.6456  0.0977  0.1117     0.012  \n",
       "lr        0.6170  0.0477  0.0418     0.005  \n",
       "knn       0.5910  0.0235  0.0391     0.007  \n",
       "svm       0.1675  0.0000  0.0000     0.005  \n",
       "dummy     0.0000  0.0000  0.0000     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6972</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7181</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.2974</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.7292</td>\n",
       "      <td>0.7595</td>\n",
       "      <td>0.8048</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.3533</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7194</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3314</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8524</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.2526</td>\n",
       "      <td>0.2761</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7738</td>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5901</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.7598</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.6237</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "et                 Extra Trees Classifier    0.7736  0.8321  0.8714  0.8193   \n",
       "qda       Quadratic Discriminant Analysis    0.6972  0.5000  1.0000  0.6972   \n",
       "rf               Random Forest Classifier    0.7417  0.7889  0.8238  0.8240   \n",
       "nb                            Naive Bayes    0.7181  0.7210  0.8381  0.7802   \n",
       "lightgbm  Light Gradient Boosting Machine    0.7292  0.7595  0.8048  0.8110   \n",
       "gbc          Gradient Boosting Classifier    0.7194  0.7837  0.8214  0.7850   \n",
       "ada                  Ada Boost Classifier    0.7097  0.8020  0.8524  0.7518   \n",
       "ridge                    Ridge Classifier    0.7083  0.0000  0.7738  0.8052   \n",
       "dt               Decision Tree Classifier    0.6611  0.6476  0.6786  0.8015   \n",
       "lda          Linear Discriminant Analysis    0.5722  0.5901  0.5810  0.7598   \n",
       "lr                    Logistic Regression    0.5792  0.4877  0.6500  0.6237   \n",
       "knn                K Neighbors Classifier    0.5125  0.5415  0.5167  0.7338   \n",
       "svm                   SVM - Linear Kernel    0.3917  0.0000  0.2000  0.1444   \n",
       "dummy                    Dummy Classifier    0.3028  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "et        0.8414  0.4394  0.4564     0.035  \n",
       "qda       0.8207  0.0000  0.0000     0.005  \n",
       "rf        0.8143  0.3949  0.4184     0.035  \n",
       "nb        0.8018  0.2974  0.3124     0.006  \n",
       "lightgbm  0.8013  0.3533  0.3690     0.007  \n",
       "gbc       0.7988  0.3154  0.3314     0.019  \n",
       "ada       0.7975  0.2526  0.2761     0.016  \n",
       "ridge     0.7844  0.3154  0.3232     0.005  \n",
       "dt        0.7214  0.3046  0.3177     0.005  \n",
       "lda       0.6456  0.0977  0.1117     0.012  \n",
       "lr        0.6170  0.0477  0.0418     0.005  \n",
       "knn       0.5910  0.0235  0.0391     0.007  \n",
       "svm       0.1675  0.0000  0.0000     0.005  \n",
       "dummy     0.0000  0.0000  0.0000     0.009  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "                     oob_score=False, random_state=1437, verbose=0,\n",
      "                     warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1scores = []\n",
    "\n",
    "for participant in unique_participants:\n",
    "    print(\"Participant: \",participant)\n",
    "    part_df = lifesnaps_group.get_group(participant)\n",
    "    grid = setup(data=part_df, target='stress', fix_imbalance = True, html=False, silent=True, verbose=False) #fix_imbalance = True,\n",
    "    best = compare_models(sort=\"F1\")\n",
    "    accuracies.append(pull()['Accuracy'][0])\n",
    "    precision.append(pull()['Prec.'][0])\n",
    "    recall.append(pull()['Recall'][0])\n",
    "    f1scores.append(pull()['F1'][0])\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d2524d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = statistics.mean(accuracies)\n",
    "mean_prec = statistics.mean(precision)\n",
    "mean_rec = statistics.mean(recall)\n",
    "mean_f1 = statistics.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "377079d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Lifesnaps - Cluster All Features:  0.855375\n",
      "Mean Precision Lifesnaps- Cluster All Features:  0.620425\n",
      "Mean Recall Lifesnaps- Cluster All Features:  0.7475375\n",
      "Mean F1-score Lifesnaps- Cluster All Features:  0.6567125\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy Lifesnaps - Cluster All Features: \", mean_acc)\n",
    "print(\"Mean Precision Lifesnaps- Cluster All Features: \", mean_prec)\n",
    "print(\"Mean Recall Lifesnaps- Cluster All Features: \", mean_rec)\n",
    "print(\"Mean F1-score Lifesnaps- Cluster All Features: \", mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a3f5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpyth8",
   "language": "python",
   "name": "env_python8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
