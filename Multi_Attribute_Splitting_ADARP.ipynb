{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b6546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from os import listdir\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715a922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(data, participants): \n",
    "    \n",
    "    cost = []\n",
    "    silhouette = []\n",
    "\n",
    "    for i in range(2, participants):\n",
    "        kmeans = KMeans(n_clusters = i, max_iter = 500, random_state = 0)\n",
    "        kmeans.fit_predict(data)\n",
    "        \n",
    "        # Calculate Silhoutte Score\n",
    "        score = silhouette_score(data, kmeans.labels_, metric='euclidean')\n",
    "        silhouette.append(score)\n",
    "    \n",
    "        # Calculates squared error for the clustered points\n",
    "        cost.append(kmeans.inertia_)    \n",
    "        \n",
    "    # Plot the cost against K values\n",
    "    plt.plot(range(2, participants), cost, color ='g', linewidth ='3')\n",
    "    plt.xlabel(\"Value of K\")\n",
    "    plt.ylabel(\"Squared Error (Cost)\")\n",
    "    plt.show() # clear the plot\n",
    "    \n",
    "    # Plot the Silhouette Score against K values\n",
    "    plt.plot(range(2, participants), silhouette, color ='b', linewidth ='3')\n",
    "    plt.xlabel(\"Value of K\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.show() # clear the plot\n",
    "    # the point of the elbow is the\n",
    "    # most optimal value for choosing k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69cdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette(data, clusters):\n",
    "    kmeans = KMeans(n_clusters = clusters, max_iter = 500, random_state = 0)\n",
    "    kmeans.fit_predict(data)\n",
    "    \n",
    "    #Create SilhouetteVisualizer instance with KMeans instance\n",
    "    visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')#ax[q-1][mod])#, ax = ax[x%3][y%2]\n",
    "    \n",
    "    #Fit the visualizer\n",
    "    visualizer.fit(data)\n",
    "    #fig, ax = plt.subplots(3,2, figsize = (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cfe1cb",
   "metadata": {},
   "source": [
    "## ADARP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbabf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_dataset = pd.read_csv('Final_CSVs/adarp_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85edd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EDA_0</th>\n",
       "      <th>EDA_1</th>\n",
       "      <th>EDA_2</th>\n",
       "      <th>EDA_3</th>\n",
       "      <th>EDA_4</th>\n",
       "      <th>EDA_5</th>\n",
       "      <th>EDA_6</th>\n",
       "      <th>EDA_7</th>\n",
       "      <th>EDA_8</th>\n",
       "      <th>EDA_9</th>\n",
       "      <th>EDA_10</th>\n",
       "      <th>EDA_11</th>\n",
       "      <th>EDA_12</th>\n",
       "      <th>EDA_13</th>\n",
       "      <th>EDA_14</th>\n",
       "      <th>EDA_15</th>\n",
       "      <th>EDA_16</th>\n",
       "      <th>EDA_17</th>\n",
       "      <th>EDA_18</th>\n",
       "      <th>EDA_19</th>\n",
       "      <th>EDA_20</th>\n",
       "      <th>EDA_21</th>\n",
       "      <th>EDA_22</th>\n",
       "      <th>EDA_23</th>\n",
       "      <th>EDA_24</th>\n",
       "      <th>EDA_25</th>\n",
       "      <th>EDA_26</th>\n",
       "      <th>EDA_27</th>\n",
       "      <th>EDA_28</th>\n",
       "      <th>EDA_29</th>\n",
       "      <th>EDA_30</th>\n",
       "      <th>EDA_31</th>\n",
       "      <th>EDA_32</th>\n",
       "      <th>EDA_33</th>\n",
       "      <th>EDA_34</th>\n",
       "      <th>EDA_35</th>\n",
       "      <th>EDA_36</th>\n",
       "      <th>EDA_37</th>\n",
       "      <th>EDA_38</th>\n",
       "      <th>EDA_39</th>\n",
       "      <th>EDA_40</th>\n",
       "      <th>EDA_41</th>\n",
       "      <th>EDA_42</th>\n",
       "      <th>EDA_43</th>\n",
       "      <th>EDA_44</th>\n",
       "      <th>EDA_45</th>\n",
       "      <th>EDA_46</th>\n",
       "      <th>EDA_47</th>\n",
       "      <th>EDA_48</th>\n",
       "      <th>EDA_49</th>\n",
       "      <th>EDA_50</th>\n",
       "      <th>EDA_51</th>\n",
       "      <th>EDA_52</th>\n",
       "      <th>EDA_53</th>\n",
       "      <th>EDA_54</th>\n",
       "      <th>EDA_55</th>\n",
       "      <th>EDA_56</th>\n",
       "      <th>EDA_57</th>\n",
       "      <th>EDA_58</th>\n",
       "      <th>EDA_59</th>\n",
       "      <th>EDA_60</th>\n",
       "      <th>EDA_61</th>\n",
       "      <th>EDA_62</th>\n",
       "      <th>EDA_63</th>\n",
       "      <th>EDA_64</th>\n",
       "      <th>EDA_65</th>\n",
       "      <th>EDA_66</th>\n",
       "      <th>EDA_67</th>\n",
       "      <th>EDA_68</th>\n",
       "      <th>EDA_69</th>\n",
       "      <th>EDA_70</th>\n",
       "      <th>EDA_71</th>\n",
       "      <th>EDA_72</th>\n",
       "      <th>EDA_73</th>\n",
       "      <th>EDA_74</th>\n",
       "      <th>EDA_75</th>\n",
       "      <th>EDA_76</th>\n",
       "      <th>EDA_77</th>\n",
       "      <th>EDA_78</th>\n",
       "      <th>EDA_79</th>\n",
       "      <th>EDA_80</th>\n",
       "      <th>EDA_81</th>\n",
       "      <th>EDA_82</th>\n",
       "      <th>EDA_83</th>\n",
       "      <th>EDA_84</th>\n",
       "      <th>EDA_85</th>\n",
       "      <th>EDA_86</th>\n",
       "      <th>EDA_87</th>\n",
       "      <th>EDA_88</th>\n",
       "      <th>EDA_89</th>\n",
       "      <th>EDA_90</th>\n",
       "      <th>EDA_91</th>\n",
       "      <th>EDA_92</th>\n",
       "      <th>EDA_93</th>\n",
       "      <th>EDA_94</th>\n",
       "      <th>EDA_95</th>\n",
       "      <th>EDA_96</th>\n",
       "      <th>EDA_97</th>\n",
       "      <th>EDA_98</th>\n",
       "      <th>EDA_99</th>\n",
       "      <th>EDA_100</th>\n",
       "      <th>EDA_101</th>\n",
       "      <th>EDA_102</th>\n",
       "      <th>EDA_103</th>\n",
       "      <th>EDA_104</th>\n",
       "      <th>EDA_105</th>\n",
       "      <th>EDA_106</th>\n",
       "      <th>EDA_107</th>\n",
       "      <th>EDA_108</th>\n",
       "      <th>EDA_109</th>\n",
       "      <th>EDA_110</th>\n",
       "      <th>EDA_111</th>\n",
       "      <th>EDA_112</th>\n",
       "      <th>EDA_113</th>\n",
       "      <th>EDA_114</th>\n",
       "      <th>EDA_115</th>\n",
       "      <th>EDA_116</th>\n",
       "      <th>EDA_117</th>\n",
       "      <th>EDA_118</th>\n",
       "      <th>EDA_119</th>\n",
       "      <th>EDA_120</th>\n",
       "      <th>EDA_121</th>\n",
       "      <th>EDA_122</th>\n",
       "      <th>EDA_123</th>\n",
       "      <th>EDA_124</th>\n",
       "      <th>EDA_125</th>\n",
       "      <th>EDA_126</th>\n",
       "      <th>EDA_127</th>\n",
       "      <th>EDA_128</th>\n",
       "      <th>EDA_129</th>\n",
       "      <th>EDA_130</th>\n",
       "      <th>EDA_131</th>\n",
       "      <th>EDA_132</th>\n",
       "      <th>EDA_133</th>\n",
       "      <th>EDA_134</th>\n",
       "      <th>EDA_135</th>\n",
       "      <th>EDA_136</th>\n",
       "      <th>EDA_137</th>\n",
       "      <th>EDA_138</th>\n",
       "      <th>EDA_139</th>\n",
       "      <th>EDA_140</th>\n",
       "      <th>EDA_141</th>\n",
       "      <th>EDA_142</th>\n",
       "      <th>EDA_143</th>\n",
       "      <th>EDA_144</th>\n",
       "      <th>EDA_145</th>\n",
       "      <th>EDA_146</th>\n",
       "      <th>EDA_147</th>\n",
       "      <th>EDA_148</th>\n",
       "      <th>EDA_149</th>\n",
       "      <th>EDA_150</th>\n",
       "      <th>EDA_151</th>\n",
       "      <th>EDA_152</th>\n",
       "      <th>EDA_153</th>\n",
       "      <th>EDA_154</th>\n",
       "      <th>EDA_155</th>\n",
       "      <th>EDA_156</th>\n",
       "      <th>EDA_157</th>\n",
       "      <th>EDA_158</th>\n",
       "      <th>EDA_159</th>\n",
       "      <th>EDA_160</th>\n",
       "      <th>EDA_161</th>\n",
       "      <th>EDA_162</th>\n",
       "      <th>EDA_163</th>\n",
       "      <th>EDA_164</th>\n",
       "      <th>EDA_165</th>\n",
       "      <th>EDA_166</th>\n",
       "      <th>EDA_167</th>\n",
       "      <th>EDA_168</th>\n",
       "      <th>EDA_169</th>\n",
       "      <th>EDA_170</th>\n",
       "      <th>EDA_171</th>\n",
       "      <th>EDA_172</th>\n",
       "      <th>EDA_173</th>\n",
       "      <th>EDA_174</th>\n",
       "      <th>EDA_175</th>\n",
       "      <th>EDA_176</th>\n",
       "      <th>EDA_177</th>\n",
       "      <th>EDA_178</th>\n",
       "      <th>EDA_179</th>\n",
       "      <th>EDA_180</th>\n",
       "      <th>EDA_181</th>\n",
       "      <th>EDA_182</th>\n",
       "      <th>EDA_183</th>\n",
       "      <th>EDA_184</th>\n",
       "      <th>EDA_185</th>\n",
       "      <th>EDA_186</th>\n",
       "      <th>EDA_187</th>\n",
       "      <th>EDA_188</th>\n",
       "      <th>EDA_189</th>\n",
       "      <th>EDA_190</th>\n",
       "      <th>EDA_191</th>\n",
       "      <th>EDA_192</th>\n",
       "      <th>EDA_193</th>\n",
       "      <th>EDA_194</th>\n",
       "      <th>EDA_195</th>\n",
       "      <th>EDA_196</th>\n",
       "      <th>EDA_197</th>\n",
       "      <th>EDA_198</th>\n",
       "      <th>EDA_199</th>\n",
       "      <th>EDA_200</th>\n",
       "      <th>EDA_201</th>\n",
       "      <th>EDA_202</th>\n",
       "      <th>EDA_203</th>\n",
       "      <th>EDA_204</th>\n",
       "      <th>EDA_205</th>\n",
       "      <th>EDA_206</th>\n",
       "      <th>EDA_207</th>\n",
       "      <th>EDA_208</th>\n",
       "      <th>EDA_209</th>\n",
       "      <th>EDA_210</th>\n",
       "      <th>EDA_211</th>\n",
       "      <th>EDA_212</th>\n",
       "      <th>EDA_213</th>\n",
       "      <th>EDA_214</th>\n",
       "      <th>EDA_215</th>\n",
       "      <th>EDA_216</th>\n",
       "      <th>EDA_217</th>\n",
       "      <th>EDA_218</th>\n",
       "      <th>EDA_219</th>\n",
       "      <th>EDA_220</th>\n",
       "      <th>EDA_221</th>\n",
       "      <th>EDA_222</th>\n",
       "      <th>EDA_223</th>\n",
       "      <th>EDA_224</th>\n",
       "      <th>EDA_225</th>\n",
       "      <th>EDA_226</th>\n",
       "      <th>EDA_227</th>\n",
       "      <th>EDA_228</th>\n",
       "      <th>EDA_229</th>\n",
       "      <th>EDA_230</th>\n",
       "      <th>EDA_231</th>\n",
       "      <th>EDA_232</th>\n",
       "      <th>EDA_233</th>\n",
       "      <th>EDA_234</th>\n",
       "      <th>EDA_235</th>\n",
       "      <th>EDA_236</th>\n",
       "      <th>EDA_237</th>\n",
       "      <th>EDA_238</th>\n",
       "      <th>EDA_239</th>\n",
       "      <th>TEMP_0</th>\n",
       "      <th>TEMP_1</th>\n",
       "      <th>TEMP_2</th>\n",
       "      <th>TEMP_3</th>\n",
       "      <th>TEMP_4</th>\n",
       "      <th>TEMP_5</th>\n",
       "      <th>TEMP_6</th>\n",
       "      <th>TEMP_7</th>\n",
       "      <th>TEMP_8</th>\n",
       "      <th>...</th>\n",
       "      <th>BVP_3591</th>\n",
       "      <th>BVP_3592</th>\n",
       "      <th>BVP_3593</th>\n",
       "      <th>BVP_3594</th>\n",
       "      <th>BVP_3595</th>\n",
       "      <th>BVP_3596</th>\n",
       "      <th>BVP_3597</th>\n",
       "      <th>BVP_3598</th>\n",
       "      <th>BVP_3599</th>\n",
       "      <th>BVP_3600</th>\n",
       "      <th>BVP_3601</th>\n",
       "      <th>BVP_3602</th>\n",
       "      <th>BVP_3603</th>\n",
       "      <th>BVP_3604</th>\n",
       "      <th>BVP_3605</th>\n",
       "      <th>BVP_3606</th>\n",
       "      <th>BVP_3607</th>\n",
       "      <th>BVP_3608</th>\n",
       "      <th>BVP_3609</th>\n",
       "      <th>BVP_3610</th>\n",
       "      <th>BVP_3611</th>\n",
       "      <th>BVP_3612</th>\n",
       "      <th>BVP_3613</th>\n",
       "      <th>BVP_3614</th>\n",
       "      <th>BVP_3615</th>\n",
       "      <th>BVP_3616</th>\n",
       "      <th>BVP_3617</th>\n",
       "      <th>BVP_3618</th>\n",
       "      <th>BVP_3619</th>\n",
       "      <th>BVP_3620</th>\n",
       "      <th>BVP_3621</th>\n",
       "      <th>BVP_3622</th>\n",
       "      <th>BVP_3623</th>\n",
       "      <th>BVP_3624</th>\n",
       "      <th>BVP_3625</th>\n",
       "      <th>BVP_3626</th>\n",
       "      <th>BVP_3627</th>\n",
       "      <th>BVP_3628</th>\n",
       "      <th>BVP_3629</th>\n",
       "      <th>BVP_3630</th>\n",
       "      <th>BVP_3631</th>\n",
       "      <th>BVP_3632</th>\n",
       "      <th>BVP_3633</th>\n",
       "      <th>BVP_3634</th>\n",
       "      <th>BVP_3635</th>\n",
       "      <th>BVP_3636</th>\n",
       "      <th>BVP_3637</th>\n",
       "      <th>BVP_3638</th>\n",
       "      <th>BVP_3639</th>\n",
       "      <th>BVP_3640</th>\n",
       "      <th>BVP_3641</th>\n",
       "      <th>BVP_3642</th>\n",
       "      <th>BVP_3643</th>\n",
       "      <th>BVP_3644</th>\n",
       "      <th>BVP_3645</th>\n",
       "      <th>BVP_3646</th>\n",
       "      <th>BVP_3647</th>\n",
       "      <th>BVP_3648</th>\n",
       "      <th>BVP_3649</th>\n",
       "      <th>BVP_3650</th>\n",
       "      <th>BVP_3651</th>\n",
       "      <th>BVP_3652</th>\n",
       "      <th>BVP_3653</th>\n",
       "      <th>BVP_3654</th>\n",
       "      <th>BVP_3655</th>\n",
       "      <th>BVP_3656</th>\n",
       "      <th>BVP_3657</th>\n",
       "      <th>BVP_3658</th>\n",
       "      <th>BVP_3659</th>\n",
       "      <th>BVP_3660</th>\n",
       "      <th>BVP_3661</th>\n",
       "      <th>BVP_3662</th>\n",
       "      <th>BVP_3663</th>\n",
       "      <th>BVP_3664</th>\n",
       "      <th>BVP_3665</th>\n",
       "      <th>BVP_3666</th>\n",
       "      <th>BVP_3667</th>\n",
       "      <th>BVP_3668</th>\n",
       "      <th>BVP_3669</th>\n",
       "      <th>BVP_3670</th>\n",
       "      <th>BVP_3671</th>\n",
       "      <th>BVP_3672</th>\n",
       "      <th>BVP_3673</th>\n",
       "      <th>BVP_3674</th>\n",
       "      <th>BVP_3675</th>\n",
       "      <th>BVP_3676</th>\n",
       "      <th>BVP_3677</th>\n",
       "      <th>BVP_3678</th>\n",
       "      <th>BVP_3679</th>\n",
       "      <th>BVP_3680</th>\n",
       "      <th>BVP_3681</th>\n",
       "      <th>BVP_3682</th>\n",
       "      <th>BVP_3683</th>\n",
       "      <th>BVP_3684</th>\n",
       "      <th>BVP_3685</th>\n",
       "      <th>BVP_3686</th>\n",
       "      <th>BVP_3687</th>\n",
       "      <th>BVP_3688</th>\n",
       "      <th>BVP_3689</th>\n",
       "      <th>BVP_3690</th>\n",
       "      <th>BVP_3691</th>\n",
       "      <th>BVP_3692</th>\n",
       "      <th>BVP_3693</th>\n",
       "      <th>BVP_3694</th>\n",
       "      <th>BVP_3695</th>\n",
       "      <th>BVP_3696</th>\n",
       "      <th>BVP_3697</th>\n",
       "      <th>BVP_3698</th>\n",
       "      <th>BVP_3699</th>\n",
       "      <th>BVP_3700</th>\n",
       "      <th>BVP_3701</th>\n",
       "      <th>BVP_3702</th>\n",
       "      <th>BVP_3703</th>\n",
       "      <th>BVP_3704</th>\n",
       "      <th>BVP_3705</th>\n",
       "      <th>BVP_3706</th>\n",
       "      <th>BVP_3707</th>\n",
       "      <th>BVP_3708</th>\n",
       "      <th>BVP_3709</th>\n",
       "      <th>BVP_3710</th>\n",
       "      <th>BVP_3711</th>\n",
       "      <th>BVP_3712</th>\n",
       "      <th>BVP_3713</th>\n",
       "      <th>BVP_3714</th>\n",
       "      <th>BVP_3715</th>\n",
       "      <th>BVP_3716</th>\n",
       "      <th>BVP_3717</th>\n",
       "      <th>BVP_3718</th>\n",
       "      <th>BVP_3719</th>\n",
       "      <th>BVP_3720</th>\n",
       "      <th>BVP_3721</th>\n",
       "      <th>BVP_3722</th>\n",
       "      <th>BVP_3723</th>\n",
       "      <th>BVP_3724</th>\n",
       "      <th>BVP_3725</th>\n",
       "      <th>BVP_3726</th>\n",
       "      <th>BVP_3727</th>\n",
       "      <th>BVP_3728</th>\n",
       "      <th>BVP_3729</th>\n",
       "      <th>BVP_3730</th>\n",
       "      <th>BVP_3731</th>\n",
       "      <th>BVP_3732</th>\n",
       "      <th>BVP_3733</th>\n",
       "      <th>BVP_3734</th>\n",
       "      <th>BVP_3735</th>\n",
       "      <th>BVP_3736</th>\n",
       "      <th>BVP_3737</th>\n",
       "      <th>BVP_3738</th>\n",
       "      <th>BVP_3739</th>\n",
       "      <th>BVP_3740</th>\n",
       "      <th>BVP_3741</th>\n",
       "      <th>BVP_3742</th>\n",
       "      <th>BVP_3743</th>\n",
       "      <th>BVP_3744</th>\n",
       "      <th>BVP_3745</th>\n",
       "      <th>BVP_3746</th>\n",
       "      <th>BVP_3747</th>\n",
       "      <th>BVP_3748</th>\n",
       "      <th>BVP_3749</th>\n",
       "      <th>BVP_3750</th>\n",
       "      <th>BVP_3751</th>\n",
       "      <th>BVP_3752</th>\n",
       "      <th>BVP_3753</th>\n",
       "      <th>BVP_3754</th>\n",
       "      <th>BVP_3755</th>\n",
       "      <th>BVP_3756</th>\n",
       "      <th>BVP_3757</th>\n",
       "      <th>BVP_3758</th>\n",
       "      <th>BVP_3759</th>\n",
       "      <th>BVP_3760</th>\n",
       "      <th>BVP_3761</th>\n",
       "      <th>BVP_3762</th>\n",
       "      <th>BVP_3763</th>\n",
       "      <th>BVP_3764</th>\n",
       "      <th>BVP_3765</th>\n",
       "      <th>BVP_3766</th>\n",
       "      <th>BVP_3767</th>\n",
       "      <th>BVP_3768</th>\n",
       "      <th>BVP_3769</th>\n",
       "      <th>BVP_3770</th>\n",
       "      <th>BVP_3771</th>\n",
       "      <th>BVP_3772</th>\n",
       "      <th>BVP_3773</th>\n",
       "      <th>BVP_3774</th>\n",
       "      <th>BVP_3775</th>\n",
       "      <th>BVP_3776</th>\n",
       "      <th>BVP_3777</th>\n",
       "      <th>BVP_3778</th>\n",
       "      <th>BVP_3779</th>\n",
       "      <th>BVP_3780</th>\n",
       "      <th>BVP_3781</th>\n",
       "      <th>BVP_3782</th>\n",
       "      <th>BVP_3783</th>\n",
       "      <th>BVP_3784</th>\n",
       "      <th>BVP_3785</th>\n",
       "      <th>BVP_3786</th>\n",
       "      <th>BVP_3787</th>\n",
       "      <th>BVP_3788</th>\n",
       "      <th>BVP_3789</th>\n",
       "      <th>BVP_3790</th>\n",
       "      <th>BVP_3791</th>\n",
       "      <th>BVP_3792</th>\n",
       "      <th>BVP_3793</th>\n",
       "      <th>BVP_3794</th>\n",
       "      <th>BVP_3795</th>\n",
       "      <th>BVP_3796</th>\n",
       "      <th>BVP_3797</th>\n",
       "      <th>BVP_3798</th>\n",
       "      <th>BVP_3799</th>\n",
       "      <th>BVP_3800</th>\n",
       "      <th>BVP_3801</th>\n",
       "      <th>BVP_3802</th>\n",
       "      <th>BVP_3803</th>\n",
       "      <th>BVP_3804</th>\n",
       "      <th>BVP_3805</th>\n",
       "      <th>BVP_3806</th>\n",
       "      <th>BVP_3807</th>\n",
       "      <th>BVP_3808</th>\n",
       "      <th>BVP_3809</th>\n",
       "      <th>BVP_3810</th>\n",
       "      <th>BVP_3811</th>\n",
       "      <th>BVP_3812</th>\n",
       "      <th>BVP_3813</th>\n",
       "      <th>BVP_3814</th>\n",
       "      <th>BVP_3815</th>\n",
       "      <th>BVP_3816</th>\n",
       "      <th>BVP_3817</th>\n",
       "      <th>BVP_3818</th>\n",
       "      <th>BVP_3819</th>\n",
       "      <th>BVP_3820</th>\n",
       "      <th>BVP_3821</th>\n",
       "      <th>BVP_3822</th>\n",
       "      <th>BVP_3823</th>\n",
       "      <th>BVP_3824</th>\n",
       "      <th>BVP_3825</th>\n",
       "      <th>BVP_3826</th>\n",
       "      <th>BVP_3827</th>\n",
       "      <th>BVP_3828</th>\n",
       "      <th>BVP_3829</th>\n",
       "      <th>BVP_3830</th>\n",
       "      <th>BVP_3831</th>\n",
       "      <th>BVP_3832</th>\n",
       "      <th>BVP_3833</th>\n",
       "      <th>BVP_3834</th>\n",
       "      <th>BVP_3835</th>\n",
       "      <th>BVP_3836</th>\n",
       "      <th>BVP_3837</th>\n",
       "      <th>BVP_3838</th>\n",
       "      <th>BVP_3839</th>\n",
       "      <th>stress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>279.693630</td>\n",
       "      <td>279.693630</td>\n",
       "      <td>277.263096</td>\n",
       "      <td>278.536233</td>\n",
       "      <td>277.263096</td>\n",
       "      <td>275.411261</td>\n",
       "      <td>274.253774</td>\n",
       "      <td>270.434365</td>\n",
       "      <td>275.642741</td>\n",
       "      <td>276.337179</td>\n",
       "      <td>273.906555</td>\n",
       "      <td>274.832473</td>\n",
       "      <td>273.559336</td>\n",
       "      <td>272.864898</td>\n",
       "      <td>273.559336</td>\n",
       "      <td>272.517679</td>\n",
       "      <td>272.401939</td>\n",
       "      <td>271.823241</td>\n",
       "      <td>271.128803</td>\n",
       "      <td>270.550104</td>\n",
       "      <td>269.739836</td>\n",
       "      <td>269.045398</td>\n",
       "      <td>268.466700</td>\n",
       "      <td>267.772262</td>\n",
       "      <td>267.309303</td>\n",
       "      <td>268.235220</td>\n",
       "      <td>272.286200</td>\n",
       "      <td>272.980638</td>\n",
       "      <td>273.212117</td>\n",
       "      <td>273.443596</td>\n",
       "      <td>270.665844</td>\n",
       "      <td>273.443596</td>\n",
       "      <td>272.864898</td>\n",
       "      <td>272.401939</td>\n",
       "      <td>272.286200</td>\n",
       "      <td>272.170460</td>\n",
       "      <td>271.938981</td>\n",
       "      <td>272.054720</td>\n",
       "      <td>272.170460</td>\n",
       "      <td>272.517679</td>\n",
       "      <td>272.633419</td>\n",
       "      <td>272.980638</td>\n",
       "      <td>273.096377</td>\n",
       "      <td>272.980638</td>\n",
       "      <td>272.864898</td>\n",
       "      <td>272.633419</td>\n",
       "      <td>272.749158</td>\n",
       "      <td>272.517679</td>\n",
       "      <td>272.286200</td>\n",
       "      <td>272.286200</td>\n",
       "      <td>272.170460</td>\n",
       "      <td>271.938981</td>\n",
       "      <td>271.823241</td>\n",
       "      <td>271.360282</td>\n",
       "      <td>271.591762</td>\n",
       "      <td>271.707501</td>\n",
       "      <td>272.170460</td>\n",
       "      <td>271.938981</td>\n",
       "      <td>271.938981</td>\n",
       "      <td>273.212117</td>\n",
       "      <td>273.096377</td>\n",
       "      <td>273.212117</td>\n",
       "      <td>273.675076</td>\n",
       "      <td>273.327857</td>\n",
       "      <td>272.749158</td>\n",
       "      <td>271.707501</td>\n",
       "      <td>271.823241</td>\n",
       "      <td>271.591762</td>\n",
       "      <td>271.476022</td>\n",
       "      <td>271.591762</td>\n",
       "      <td>272.517679</td>\n",
       "      <td>272.864898</td>\n",
       "      <td>273.675076</td>\n",
       "      <td>274.369514</td>\n",
       "      <td>274.948212</td>\n",
       "      <td>275.179692</td>\n",
       "      <td>275.064042</td>\n",
       "      <td>275.064042</td>\n",
       "      <td>275.179692</td>\n",
       "      <td>275.758480</td>\n",
       "      <td>274.716733</td>\n",
       "      <td>274.369514</td>\n",
       "      <td>270.897323</td>\n",
       "      <td>273.906555</td>\n",
       "      <td>273.443596</td>\n",
       "      <td>274.369514</td>\n",
       "      <td>274.716733</td>\n",
       "      <td>275.526911</td>\n",
       "      <td>273.559336</td>\n",
       "      <td>272.864898</td>\n",
       "      <td>270.318535</td>\n",
       "      <td>271.823241</td>\n",
       "      <td>272.054720</td>\n",
       "      <td>271.707501</td>\n",
       "      <td>271.476022</td>\n",
       "      <td>270.434365</td>\n",
       "      <td>269.971316</td>\n",
       "      <td>270.665844</td>\n",
       "      <td>270.318535</td>\n",
       "      <td>269.855666</td>\n",
       "      <td>269.739836</td>\n",
       "      <td>269.508447</td>\n",
       "      <td>269.392617</td>\n",
       "      <td>271.013063</td>\n",
       "      <td>272.864898</td>\n",
       "      <td>272.054720</td>\n",
       "      <td>273.906555</td>\n",
       "      <td>274.138035</td>\n",
       "      <td>273.559336</td>\n",
       "      <td>274.716733</td>\n",
       "      <td>274.253774</td>\n",
       "      <td>274.369514</td>\n",
       "      <td>273.559336</td>\n",
       "      <td>274.138035</td>\n",
       "      <td>272.401939</td>\n",
       "      <td>272.401939</td>\n",
       "      <td>270.897323</td>\n",
       "      <td>270.087146</td>\n",
       "      <td>269.624097</td>\n",
       "      <td>272.401939</td>\n",
       "      <td>271.476022</td>\n",
       "      <td>272.980638</td>\n",
       "      <td>272.401939</td>\n",
       "      <td>272.286200</td>\n",
       "      <td>271.013063</td>\n",
       "      <td>269.855666</td>\n",
       "      <td>269.739836</td>\n",
       "      <td>269.276968</td>\n",
       "      <td>268.235220</td>\n",
       "      <td>268.119481</td>\n",
       "      <td>267.772262</td>\n",
       "      <td>267.425043</td>\n",
       "      <td>268.466700</td>\n",
       "      <td>271.476022</td>\n",
       "      <td>270.087146</td>\n",
       "      <td>269.508447</td>\n",
       "      <td>269.855666</td>\n",
       "      <td>271.128803</td>\n",
       "      <td>273.790815</td>\n",
       "      <td>276.452828</td>\n",
       "      <td>276.337179</td>\n",
       "      <td>275.758480</td>\n",
       "      <td>275.989960</td>\n",
       "      <td>277.957534</td>\n",
       "      <td>274.485254</td>\n",
       "      <td>272.401939</td>\n",
       "      <td>273.906555</td>\n",
       "      <td>276.337179</td>\n",
       "      <td>274.948212</td>\n",
       "      <td>273.906555</td>\n",
       "      <td>272.980638</td>\n",
       "      <td>273.559336</td>\n",
       "      <td>272.980638</td>\n",
       "      <td>273.443596</td>\n",
       "      <td>272.286200</td>\n",
       "      <td>272.401939</td>\n",
       "      <td>273.327857</td>\n",
       "      <td>271.013063</td>\n",
       "      <td>269.045398</td>\n",
       "      <td>267.077824</td>\n",
       "      <td>267.540782</td>\n",
       "      <td>268.813919</td>\n",
       "      <td>266.267646</td>\n",
       "      <td>266.730604</td>\n",
       "      <td>267.309303</td>\n",
       "      <td>266.730604</td>\n",
       "      <td>266.036166</td>\n",
       "      <td>266.036166</td>\n",
       "      <td>264.415811</td>\n",
       "      <td>263.721373</td>\n",
       "      <td>263.258324</td>\n",
       "      <td>262.795365</td>\n",
       "      <td>263.026844</td>\n",
       "      <td>262.911105</td>\n",
       "      <td>264.647290</td>\n",
       "      <td>264.531460</td>\n",
       "      <td>265.804687</td>\n",
       "      <td>266.962084</td>\n",
       "      <td>266.614865</td>\n",
       "      <td>266.267646</td>\n",
       "      <td>266.151906</td>\n",
       "      <td>266.730604</td>\n",
       "      <td>266.499125</td>\n",
       "      <td>266.267646</td>\n",
       "      <td>265.688947</td>\n",
       "      <td>265.804687</td>\n",
       "      <td>266.036166</td>\n",
       "      <td>265.920427</td>\n",
       "      <td>267.077824</td>\n",
       "      <td>266.846344</td>\n",
       "      <td>267.540782</td>\n",
       "      <td>267.193563</td>\n",
       "      <td>266.962084</td>\n",
       "      <td>267.425043</td>\n",
       "      <td>266.962084</td>\n",
       "      <td>266.730604</td>\n",
       "      <td>267.425043</td>\n",
       "      <td>268.003741</td>\n",
       "      <td>268.929749</td>\n",
       "      <td>269.508447</td>\n",
       "      <td>270.318535</td>\n",
       "      <td>271.707501</td>\n",
       "      <td>271.938981</td>\n",
       "      <td>271.360282</td>\n",
       "      <td>271.938981</td>\n",
       "      <td>271.591762</td>\n",
       "      <td>273.675076</td>\n",
       "      <td>273.327857</td>\n",
       "      <td>274.138035</td>\n",
       "      <td>273.559336</td>\n",
       "      <td>271.476022</td>\n",
       "      <td>270.202885</td>\n",
       "      <td>268.466700</td>\n",
       "      <td>267.656522</td>\n",
       "      <td>267.888001</td>\n",
       "      <td>269.045398</td>\n",
       "      <td>272.749158</td>\n",
       "      <td>272.286200</td>\n",
       "      <td>272.054720</td>\n",
       "      <td>271.707501</td>\n",
       "      <td>270.897323</td>\n",
       "      <td>270.665844</td>\n",
       "      <td>270.550104</td>\n",
       "      <td>269.971316</td>\n",
       "      <td>268.813919</td>\n",
       "      <td>268.466700</td>\n",
       "      <td>268.119481</td>\n",
       "      <td>267.540782</td>\n",
       "      <td>267.540782</td>\n",
       "      <td>267.425043</td>\n",
       "      <td>267.772262</td>\n",
       "      <td>265.457468</td>\n",
       "      <td>268.929749</td>\n",
       "      <td>265.573208</td>\n",
       "      <td>265.804687</td>\n",
       "      <td>265.573208</td>\n",
       "      <td>268.119481</td>\n",
       "      <td>267.888001</td>\n",
       "      <td>266.267646</td>\n",
       "      <td>266.614865</td>\n",
       "      <td>-1831.016076</td>\n",
       "      <td>-1831.016076</td>\n",
       "      <td>-1831.016076</td>\n",
       "      <td>-1831.016076</td>\n",
       "      <td>-1831.016076</td>\n",
       "      <td>-1831.016076</td>\n",
       "      <td>-1831.016076</td>\n",
       "      <td>-1831.016076</td>\n",
       "      <td>-1843.006719</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.958960</td>\n",
       "      <td>-66.060810</td>\n",
       "      <td>-70.568737</td>\n",
       "      <td>-75.758506</td>\n",
       "      <td>-80.517266</td>\n",
       "      <td>-83.643846</td>\n",
       "      <td>-84.272695</td>\n",
       "      <td>-82.241301</td>\n",
       "      <td>-78.167915</td>\n",
       "      <td>-73.119461</td>\n",
       "      <td>-68.003882</td>\n",
       "      <td>-63.079077</td>\n",
       "      <td>-57.723264</td>\n",
       "      <td>-50.692876</td>\n",
       "      <td>-40.758478</td>\n",
       "      <td>-27.241761</td>\n",
       "      <td>-10.255778</td>\n",
       "      <td>9.499966</td>\n",
       "      <td>31.025672</td>\n",
       "      <td>53.226154</td>\n",
       "      <td>74.797788</td>\n",
       "      <td>94.044800</td>\n",
       "      <td>108.995857</td>\n",
       "      <td>117.898660</td>\n",
       "      <td>119.961850</td>\n",
       "      <td>115.807208</td>\n",
       "      <td>107.388406</td>\n",
       "      <td>97.189044</td>\n",
       "      <td>87.233449</td>\n",
       "      <td>78.383639</td>\n",
       "      <td>70.346386</td>\n",
       "      <td>62.316199</td>\n",
       "      <td>53.805543</td>\n",
       "      <td>45.164171</td>\n",
       "      <td>37.512000</td>\n",
       "      <td>32.159720</td>\n",
       "      <td>29.990545</td>\n",
       "      <td>31.064534</td>\n",
       "      <td>34.600925</td>\n",
       "      <td>39.331422</td>\n",
       "      <td>43.849948</td>\n",
       "      <td>46.877608</td>\n",
       "      <td>47.464062</td>\n",
       "      <td>45.040521</td>\n",
       "      <td>39.458605</td>\n",
       "      <td>31.145789</td>\n",
       "      <td>21.186661</td>\n",
       "      <td>11.259329</td>\n",
       "      <td>3.335128</td>\n",
       "      <td>-0.830112</td>\n",
       "      <td>-0.282519</td>\n",
       "      <td>4.727074</td>\n",
       "      <td>12.626546</td>\n",
       "      <td>20.829843</td>\n",
       "      <td>26.461219</td>\n",
       "      <td>27.238448</td>\n",
       "      <td>22.366636</td>\n",
       "      <td>12.845583</td>\n",
       "      <td>1.211880</td>\n",
       "      <td>-9.390228</td>\n",
       "      <td>-16.339360</td>\n",
       "      <td>-18.497937</td>\n",
       "      <td>-16.455945</td>\n",
       "      <td>-12.092865</td>\n",
       "      <td>-7.659127</td>\n",
       "      <td>-4.836373</td>\n",
       "      <td>-4.242853</td>\n",
       "      <td>-5.592405</td>\n",
       "      <td>-8.164326</td>\n",
       "      <td>-11.403957</td>\n",
       "      <td>-15.162919</td>\n",
       "      <td>-19.603722</td>\n",
       "      <td>-24.906542</td>\n",
       "      <td>-31.004256</td>\n",
       "      <td>-37.465854</td>\n",
       "      <td>-43.556502</td>\n",
       "      <td>-48.382386</td>\n",
       "      <td>-51.067359</td>\n",
       "      <td>-50.954307</td>\n",
       "      <td>-47.792399</td>\n",
       "      <td>-41.949051</td>\n",
       "      <td>-34.459391</td>\n",
       "      <td>-26.888476</td>\n",
       "      <td>-20.917945</td>\n",
       "      <td>-17.812562</td>\n",
       "      <td>-17.957410</td>\n",
       "      <td>-20.702441</td>\n",
       "      <td>-24.602716</td>\n",
       "      <td>-27.877676</td>\n",
       "      <td>-28.976395</td>\n",
       "      <td>-26.892008</td>\n",
       "      <td>-21.377217</td>\n",
       "      <td>-12.930152</td>\n",
       "      <td>-2.734323</td>\n",
       "      <td>7.496835</td>\n",
       "      <td>15.848512</td>\n",
       "      <td>20.692061</td>\n",
       "      <td>21.176063</td>\n",
       "      <td>17.505423</td>\n",
       "      <td>10.824788</td>\n",
       "      <td>2.731009</td>\n",
       "      <td>-5.320375</td>\n",
       "      <td>-12.421420</td>\n",
       "      <td>-18.218841</td>\n",
       "      <td>-22.638447</td>\n",
       "      <td>-25.574252</td>\n",
       "      <td>-26.800154</td>\n",
       "      <td>-26.157174</td>\n",
       "      <td>-23.878480</td>\n",
       "      <td>-20.734236</td>\n",
       "      <td>-17.865555</td>\n",
       "      <td>-16.325229</td>\n",
       "      <td>-16.600792</td>\n",
       "      <td>-18.342491</td>\n",
       "      <td>-20.469272</td>\n",
       "      <td>-21.592721</td>\n",
       "      <td>-20.543462</td>\n",
       "      <td>-16.865756</td>\n",
       "      <td>-11.054204</td>\n",
       "      <td>-4.500752</td>\n",
       "      <td>0.883324</td>\n",
       "      <td>3.211478</td>\n",
       "      <td>1.169486</td>\n",
       "      <td>-5.595938</td>\n",
       "      <td>-16.427682</td>\n",
       "      <td>-29.827814</td>\n",
       "      <td>-43.945116</td>\n",
       "      <td>-57.066152</td>\n",
       "      <td>-68.081605</td>\n",
       "      <td>-76.708845</td>\n",
       "      <td>-83.403611</td>\n",
       "      <td>-89.003192</td>\n",
       "      <td>-94.214158</td>\n",
       "      <td>-99.223751</td>\n",
       "      <td>-103.558569</td>\n",
       "      <td>-106.307132</td>\n",
       "      <td>-106.487308</td>\n",
       "      <td>-103.491444</td>\n",
       "      <td>-97.217088</td>\n",
       "      <td>-87.939802</td>\n",
       "      <td>-76.058799</td>\n",
       "      <td>-61.835511</td>\n",
       "      <td>-45.337062</td>\n",
       "      <td>-26.630577</td>\n",
       "      <td>-6.108203</td>\n",
       "      <td>15.141941</td>\n",
       "      <td>35.166182</td>\n",
       "      <td>51.420864</td>\n",
       "      <td>61.330532</td>\n",
       "      <td>63.068698</td>\n",
       "      <td>56.253814</td>\n",
       "      <td>42.253096</td>\n",
       "      <td>23.815108</td>\n",
       "      <td>4.183014</td>\n",
       "      <td>-14.025338</td>\n",
       "      <td>-29.644105</td>\n",
       "      <td>-43.068967</td>\n",
       "      <td>-55.649476</td>\n",
       "      <td>-68.660993</td>\n",
       "      <td>-82.389681</td>\n",
       "      <td>-95.807477</td>\n",
       "      <td>-106.946580</td>\n",
       "      <td>-113.779128</td>\n",
       "      <td>-115.065088</td>\n",
       "      <td>-110.762067</td>\n",
       "      <td>-101.876928</td>\n",
       "      <td>-89.872275</td>\n",
       "      <td>-76.041135</td>\n",
       "      <td>-61.093611</td>\n",
       "      <td>-45.255806</td>\n",
       "      <td>-28.598379</td>\n",
       "      <td>-11.389826</td>\n",
       "      <td>5.808129</td>\n",
       "      <td>22.356038</td>\n",
       "      <td>37.893549</td>\n",
       "      <td>52.639700</td>\n",
       "      <td>67.463574</td>\n",
       "      <td>83.552211</td>\n",
       "      <td>101.707570</td>\n",
       "      <td>121.745943</td>\n",
       "      <td>142.148200</td>\n",
       "      <td>160.374217</td>\n",
       "      <td>173.714290</td>\n",
       "      <td>180.331334</td>\n",
       "      <td>179.992179</td>\n",
       "      <td>174.085240</td>\n",
       "      <td>164.963400</td>\n",
       "      <td>154.855892</td>\n",
       "      <td>145.112268</td>\n",
       "      <td>136.036355</td>\n",
       "      <td>127.267801</td>\n",
       "      <td>118.449786</td>\n",
       "      <td>109.656502</td>\n",
       "      <td>101.354284</td>\n",
       "      <td>94.051866</td>\n",
       "      <td>87.922357</td>\n",
       "      <td>82.626602</td>\n",
       "      <td>77.567549</td>\n",
       "      <td>72.215268</td>\n",
       "      <td>66.386052</td>\n",
       "      <td>60.242411</td>\n",
       "      <td>54.112902</td>\n",
       "      <td>48.258955</td>\n",
       "      <td>42.691171</td>\n",
       "      <td>37.229371</td>\n",
       "      <td>31.636857</td>\n",
       "      <td>25.864166</td>\n",
       "      <td>20.130337</td>\n",
       "      <td>14.898174</td>\n",
       "      <td>10.683473</td>\n",
       "      <td>7.807727</td>\n",
       "      <td>6.228539</td>\n",
       "      <td>5.521967</td>\n",
       "      <td>5.052097</td>\n",
       "      <td>4.267803</td>\n",
       "      <td>2.960645</td>\n",
       "      <td>1.370859</td>\n",
       "      <td>0.038971</td>\n",
       "      <td>-0.434432</td>\n",
       "      <td>0.385191</td>\n",
       "      <td>2.621491</td>\n",
       "      <td>6.083692</td>\n",
       "      <td>10.266596</td>\n",
       "      <td>14.350580</td>\n",
       "      <td>17.198064</td>\n",
       "      <td>17.455963</td>\n",
       "      <td>13.820652</td>\n",
       "      <td>5.557296</td>\n",
       "      <td>-7.069140</td>\n",
       "      <td>-22.567790</td>\n",
       "      <td>-38.568105</td>\n",
       "      <td>-52.540560</td>\n",
       "      <td>-62.690463</td>\n",
       "      <td>-68.505548</td>\n",
       "      <td>-70.816037</td>\n",
       "      <td>-71.225849</td>\n",
       "      <td>-71.271776</td>\n",
       "      <td>-71.755777</td>\n",
       "      <td>-72.483546</td>\n",
       "      <td>-72.511809</td>\n",
       "      <td>-70.688854</td>\n",
       "      <td>-66.219788</td>\n",
       "      <td>-58.931501</td>\n",
       "      <td>-49.297396</td>\n",
       "      <td>-38.197155</td>\n",
       "      <td>-26.658840</td>\n",
       "      <td>-15.717577</td>\n",
       "      <td>-6.359035</td>\n",
       "      <td>0.593630</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-26.705520</td>\n",
       "      <td>-8.030028</td>\n",
       "      <td>9.632398</td>\n",
       "      <td>26.103795</td>\n",
       "      <td>19.179108</td>\n",
       "      <td>6.905367</td>\n",
       "      <td>16.839128</td>\n",
       "      <td>32.247524</td>\n",
       "      <td>-1.041654</td>\n",
       "      <td>22.086138</td>\n",
       "      <td>39.576458</td>\n",
       "      <td>19.983846</td>\n",
       "      <td>-0.202799</td>\n",
       "      <td>-17.869706</td>\n",
       "      <td>1.641230</td>\n",
       "      <td>9.588252</td>\n",
       "      <td>12.593879</td>\n",
       "      <td>-17.869706</td>\n",
       "      <td>-17.961445</td>\n",
       "      <td>-21.140247</td>\n",
       "      <td>-19.904037</td>\n",
       "      <td>-31.118154</td>\n",
       "      <td>-38.228698</td>\n",
       "      <td>-31.208860</td>\n",
       "      <td>-36.595168</td>\n",
       "      <td>-11.252977</td>\n",
       "      <td>-5.513496</td>\n",
       "      <td>6.321122</td>\n",
       "      <td>-11.512102</td>\n",
       "      <td>-15.618053</td>\n",
       "      <td>-32.045303</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-38.061313</td>\n",
       "      <td>-40.003904</td>\n",
       "      <td>-25.787572</td>\n",
       "      <td>-34.396863</td>\n",
       "      <td>-17.443196</td>\n",
       "      <td>-5.469314</td>\n",
       "      <td>9.632398</td>\n",
       "      <td>9.768250</td>\n",
       "      <td>4.249502</td>\n",
       "      <td>-13.322253</td>\n",
       "      <td>26.103795</td>\n",
       "      <td>22.402057</td>\n",
       "      <td>-0.997508</td>\n",
       "      <td>-1.225134</td>\n",
       "      <td>9.591663</td>\n",
       "      <td>3.278172</td>\n",
       "      <td>26.059649</td>\n",
       "      <td>42.755260</td>\n",
       "      <td>60.866873</td>\n",
       "      <td>77.929098</td>\n",
       "      <td>95.090058</td>\n",
       "      <td>75.196691</td>\n",
       "      <td>68.309258</td>\n",
       "      <td>52.043706</td>\n",
       "      <td>34.176330</td>\n",
       "      <td>60.911019</td>\n",
       "      <td>70.114516</td>\n",
       "      <td>51.703939</td>\n",
       "      <td>34.088037</td>\n",
       "      <td>18.217841</td>\n",
       "      <td>41.872257</td>\n",
       "      <td>30.359142</td>\n",
       "      <td>27.621841</td>\n",
       "      <td>33.273029</td>\n",
       "      <td>24.398858</td>\n",
       "      <td>16.981665</td>\n",
       "      <td>-0.997508</td>\n",
       "      <td>-17.869706</td>\n",
       "      <td>-34.208662</td>\n",
       "      <td>-13.990312</td>\n",
       "      <td>-7.853442</td>\n",
       "      <td>9.632398</td>\n",
       "      <td>24.735145</td>\n",
       "      <td>25.845773</td>\n",
       "      <td>-0.997508</td>\n",
       "      <td>-17.516532</td>\n",
       "      <td>-24.407342</td>\n",
       "      <td>-10.941572</td>\n",
       "      <td>6.365269</td>\n",
       "      <td>26.059649</td>\n",
       "      <td>42.931846</td>\n",
       "      <td>60.822692</td>\n",
       "      <td>34.043890</td>\n",
       "      <td>60.911019</td>\n",
       "      <td>67.200628</td>\n",
       "      <td>37.266839</td>\n",
       "      <td>60.866873</td>\n",
       "      <td>72.277875</td>\n",
       "      <td>72.719376</td>\n",
       "      <td>76.149652</td>\n",
       "      <td>78.931100</td>\n",
       "      <td>77.032655</td>\n",
       "      <td>81.889168</td>\n",
       "      <td>82.154083</td>\n",
       "      <td>80.829579</td>\n",
       "      <td>81.138606</td>\n",
       "      <td>56.679452</td>\n",
       "      <td>51.116557</td>\n",
       "      <td>33.999743</td>\n",
       "      <td>60.911019</td>\n",
       "      <td>77.752511</td>\n",
       "      <td>81.756728</td>\n",
       "      <td>81.182787</td>\n",
       "      <td>80.741286</td>\n",
       "      <td>76.193799</td>\n",
       "      <td>69.924489</td>\n",
       "      <td>66.215892</td>\n",
       "      <td>51.160704</td>\n",
       "      <td>38.017401</td>\n",
       "      <td>60.866873</td>\n",
       "      <td>68.480951</td>\n",
       "      <td>69.231513</td>\n",
       "      <td>69.099073</td>\n",
       "      <td>69.275660</td>\n",
       "      <td>69.187367</td>\n",
       "      <td>70.158662</td>\n",
       "      <td>72.101288</td>\n",
       "      <td>75.059322</td>\n",
       "      <td>77.606630</td>\n",
       "      <td>77.650777</td>\n",
       "      <td>77.959804</td>\n",
       "      <td>74.957589</td>\n",
       "      <td>56.149657</td>\n",
       "      <td>75.134210</td>\n",
       "      <td>51.160704</td>\n",
       "      <td>35.368394</td>\n",
       "      <td>17.025812</td>\n",
       "      <td>4.830301</td>\n",
       "      <td>0.238703</td>\n",
       "      <td>0.320138</td>\n",
       "      <td>-17.781412</td>\n",
       "      <td>-20.080623</td>\n",
       "      <td>-27.232971</td>\n",
       "      <td>5.835474</td>\n",
       "      <td>-17.825559</td>\n",
       "      <td>-11.912868</td>\n",
       "      <td>-24.319049</td>\n",
       "      <td>-19.948183</td>\n",
       "      <td>-27.895205</td>\n",
       "      <td>-34.296956</td>\n",
       "      <td>-12.754102</td>\n",
       "      <td>-5.513496</td>\n",
       "      <td>-27.541997</td>\n",
       "      <td>-33.193220</td>\n",
       "      <td>-39.023407</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-21.681621</td>\n",
       "      <td>-6.970439</td>\n",
       "      <td>-10.411743</td>\n",
       "      <td>9.279190</td>\n",
       "      <td>6.015507</td>\n",
       "      <td>10.872020</td>\n",
       "      <td>-17.560679</td>\n",
       "      <td>9.588252</td>\n",
       "      <td>15.772680</td>\n",
       "      <td>25.088353</td>\n",
       "      <td>42.931846</td>\n",
       "      <td>37.599748</td>\n",
       "      <td>48.328217</td>\n",
       "      <td>41.882320</td>\n",
       "      <td>16.981665</td>\n",
       "      <td>-0.997508</td>\n",
       "      <td>5.662299</td>\n",
       "      <td>17.936005</td>\n",
       "      <td>-4.271496</td>\n",
       "      <td>24.955878</td>\n",
       "      <td>42.622785</td>\n",
       "      <td>44.972829</td>\n",
       "      <td>45.988271</td>\n",
       "      <td>46.959567</td>\n",
       "      <td>47.268628</td>\n",
       "      <td>47.356922</td>\n",
       "      <td>48.019191</td>\n",
       "      <td>48.548985</td>\n",
       "      <td>44.972829</td>\n",
       "      <td>35.524682</td>\n",
       "      <td>45.458477</td>\n",
       "      <td>23.250976</td>\n",
       "      <td>39.542374</td>\n",
       "      <td>41.308345</td>\n",
       "      <td>43.074349</td>\n",
       "      <td>40.160462</td>\n",
       "      <td>23.339269</td>\n",
       "      <td>16.981665</td>\n",
       "      <td>-0.997508</td>\n",
       "      <td>-17.516532</td>\n",
       "      <td>-34.208662</td>\n",
       "      <td>-36.595168</td>\n",
       "      <td>-38.758527</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-45.964188</td>\n",
       "      <td>-45.875860</td>\n",
       "      <td>-45.787567</td>\n",
       "      <td>-45.743420</td>\n",
       "      <td>-45.787567</td>\n",
       "      <td>-45.699274</td>\n",
       "      <td>-45.699274</td>\n",
       "      <td>-45.699274</td>\n",
       "      <td>-45.699274</td>\n",
       "      <td>-45.743420</td>\n",
       "      <td>-23.977420</td>\n",
       "      <td>-31.968589</td>\n",
       "      <td>-30.909000</td>\n",
       "      <td>-32.807445</td>\n",
       "      <td>-25.390218</td>\n",
       "      <td>-17.487342</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-45.920041</td>\n",
       "      <td>-45.787567</td>\n",
       "      <td>-45.787567</td>\n",
       "      <td>-45.699274</td>\n",
       "      <td>-45.699274</td>\n",
       "      <td>-22.255562</td>\n",
       "      <td>-5.469314</td>\n",
       "      <td>9.588252</td>\n",
       "      <td>-5.286938</td>\n",
       "      <td>0.408431</td>\n",
       "      <td>-17.737265</td>\n",
       "      <td>-7.144648</td>\n",
       "      <td>-25.290345</td>\n",
       "      <td>-34.076222</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-46.096628</td>\n",
       "      <td>-45.920041</td>\n",
       "      <td>-45.787567</td>\n",
       "      <td>-45.831714</td>\n",
       "      <td>-45.787567</td>\n",
       "      <td>-45.787567</td>\n",
       "      <td>237.318204</td>\n",
       "      <td>237.318204</td>\n",
       "      <td>237.318204</td>\n",
       "      <td>237.318204</td>\n",
       "      <td>252.045754</td>\n",
       "      <td>252.045754</td>\n",
       "      <td>252.045754</td>\n",
       "      <td>252.045754</td>\n",
       "      <td>266.773304</td>\n",
       "      <td>...</td>\n",
       "      <td>22.954543</td>\n",
       "      <td>8.774783</td>\n",
       "      <td>-2.938060</td>\n",
       "      <td>-11.064180</td>\n",
       "      <td>-15.794109</td>\n",
       "      <td>-18.508387</td>\n",
       "      <td>-21.841065</td>\n",
       "      <td>-28.366028</td>\n",
       "      <td>-39.871623</td>\n",
       "      <td>-56.478189</td>\n",
       "      <td>-76.587911</td>\n",
       "      <td>-97.426342</td>\n",
       "      <td>-115.727660</td>\n",
       "      <td>-128.617136</td>\n",
       "      <td>-131.999955</td>\n",
       "      <td>-124.111168</td>\n",
       "      <td>-106.826033</td>\n",
       "      <td>-85.944146</td>\n",
       "      <td>-67.682941</td>\n",
       "      <td>-55.739451</td>\n",
       "      <td>-50.414519</td>\n",
       "      <td>-49.649040</td>\n",
       "      <td>-50.865785</td>\n",
       "      <td>-52.149384</td>\n",
       "      <td>-53.432983</td>\n",
       "      <td>-55.555602</td>\n",
       "      <td>-59.249292</td>\n",
       "      <td>-63.969194</td>\n",
       "      <td>-68.251201</td>\n",
       "      <td>-70.764916</td>\n",
       "      <td>-70.734832</td>\n",
       "      <td>-68.411651</td>\n",
       "      <td>-64.490656</td>\n",
       "      <td>-60.064913</td>\n",
       "      <td>-55.953384</td>\n",
       "      <td>-52.303149</td>\n",
       "      <td>-48.385497</td>\n",
       "      <td>-42.970312</td>\n",
       "      <td>-35.947286</td>\n",
       "      <td>-27.199423</td>\n",
       "      <td>-17.208074</td>\n",
       "      <td>-6.748746</td>\n",
       "      <td>3.483279</td>\n",
       "      <td>12.852885</td>\n",
       "      <td>20.818554</td>\n",
       "      <td>27.320118</td>\n",
       "      <td>32.531397</td>\n",
       "      <td>36.388880</td>\n",
       "      <td>39.220153</td>\n",
       "      <td>41.392912</td>\n",
       "      <td>43.298255</td>\n",
       "      <td>45.952363</td>\n",
       "      <td>50.114033</td>\n",
       "      <td>55.756521</td>\n",
       "      <td>62.191231</td>\n",
       "      <td>68.401980</td>\n",
       "      <td>73.626630</td>\n",
       "      <td>77.784957</td>\n",
       "      <td>81.294798</td>\n",
       "      <td>84.821354</td>\n",
       "      <td>90.169684</td>\n",
       "      <td>98.392742</td>\n",
       "      <td>109.353476</td>\n",
       "      <td>121.788344</td>\n",
       "      <td>133.822087</td>\n",
       "      <td>143.987257</td>\n",
       "      <td>151.916157</td>\n",
       "      <td>158.093478</td>\n",
       "      <td>163.532061</td>\n",
       "      <td>170.979611</td>\n",
       "      <td>182.669055</td>\n",
       "      <td>199.496240</td>\n",
       "      <td>220.438295</td>\n",
       "      <td>242.874541</td>\n",
       "      <td>263.328561</td>\n",
       "      <td>278.370740</td>\n",
       "      <td>285.066181</td>\n",
       "      <td>271.481422</td>\n",
       "      <td>232.064228</td>\n",
       "      <td>169.859805</td>\n",
       "      <td>96.250067</td>\n",
       "      <td>25.164072</td>\n",
       "      <td>-33.510453</td>\n",
       "      <td>-76.584568</td>\n",
       "      <td>-105.569175</td>\n",
       "      <td>-124.004201</td>\n",
       "      <td>-135.105329</td>\n",
       "      <td>-139.521044</td>\n",
       "      <td>-137.331571</td>\n",
       "      <td>-129.536380</td>\n",
       "      <td>-118.689298</td>\n",
       "      <td>-107.935811</td>\n",
       "      <td>-99.632529</td>\n",
       "      <td>-94.551615</td>\n",
       "      <td>-92.298631</td>\n",
       "      <td>-91.757112</td>\n",
       "      <td>-90.593850</td>\n",
       "      <td>-87.137492</td>\n",
       "      <td>-81.073823</td>\n",
       "      <td>-73.502593</td>\n",
       "      <td>-65.316305</td>\n",
       "      <td>-57.718333</td>\n",
       "      <td>-51.146572</td>\n",
       "      <td>-45.290151</td>\n",
       "      <td>-39.503926</td>\n",
       "      <td>-32.414045</td>\n",
       "      <td>-23.151406</td>\n",
       "      <td>-11.742749</td>\n",
       "      <td>0.354505</td>\n",
       "      <td>11.836702</td>\n",
       "      <td>21.313275</td>\n",
       "      <td>28.326273</td>\n",
       "      <td>33.153141</td>\n",
       "      <td>36.405594</td>\n",
       "      <td>39.113186</td>\n",
       "      <td>41.820778</td>\n",
       "      <td>44.752332</td>\n",
       "      <td>47.563548</td>\n",
       "      <td>49.846616</td>\n",
       "      <td>50.602068</td>\n",
       "      <td>49.097850</td>\n",
       "      <td>45.962392</td>\n",
       "      <td>41.844177</td>\n",
       "      <td>37.592255</td>\n",
       "      <td>33.811654</td>\n",
       "      <td>30.666167</td>\n",
       "      <td>28.002030</td>\n",
       "      <td>25.568540</td>\n",
       "      <td>22.302716</td>\n",
       "      <td>17.602870</td>\n",
       "      <td>11.539201</td>\n",
       "      <td>4.753507</td>\n",
       "      <td>-1.898479</td>\n",
       "      <td>-7.694732</td>\n",
       "      <td>-12.374521</td>\n",
       "      <td>-16.084925</td>\n",
       "      <td>-19.120102</td>\n",
       "      <td>-23.241659</td>\n",
       "      <td>-29.679712</td>\n",
       "      <td>-38.397490</td>\n",
       "      <td>-48.171563</td>\n",
       "      <td>-57.310523</td>\n",
       "      <td>-64.697904</td>\n",
       "      <td>-69.909183</td>\n",
       "      <td>-73.348828</td>\n",
       "      <td>-75.879257</td>\n",
       "      <td>-80.645956</td>\n",
       "      <td>-89.878511</td>\n",
       "      <td>-103.603664</td>\n",
       "      <td>-119.822475</td>\n",
       "      <td>-135.720387</td>\n",
       "      <td>-149.458910</td>\n",
       "      <td>-160.473128</td>\n",
       "      <td>-169.251075</td>\n",
       "      <td>-176.808934</td>\n",
       "      <td>-186.984132</td>\n",
       "      <td>-202.534403</td>\n",
       "      <td>-223.904325</td>\n",
       "      <td>-249.158471</td>\n",
       "      <td>-275.278379</td>\n",
       "      <td>-299.539742</td>\n",
       "      <td>-319.595981</td>\n",
       "      <td>-333.311105</td>\n",
       "      <td>-322.069584</td>\n",
       "      <td>-276.418242</td>\n",
       "      <td>-200.572234</td>\n",
       "      <td>-111.405541</td>\n",
       "      <td>-27.720886</td>\n",
       "      <td>39.066388</td>\n",
       "      <td>86.332257</td>\n",
       "      <td>116.998245</td>\n",
       "      <td>135.847767</td>\n",
       "      <td>147.176199</td>\n",
       "      <td>151.164048</td>\n",
       "      <td>147.390132</td>\n",
       "      <td>136.997658</td>\n",
       "      <td>123.389500</td>\n",
       "      <td>110.847666</td>\n",
       "      <td>102.270281</td>\n",
       "      <td>98.800552</td>\n",
       "      <td>99.856847</td>\n",
       "      <td>103.878123</td>\n",
       "      <td>107.237543</td>\n",
       "      <td>107.418049</td>\n",
       "      <td>104.282590</td>\n",
       "      <td>99.983870</td>\n",
       "      <td>97.674060</td>\n",
       "      <td>99.091367</td>\n",
       "      <td>104.760597</td>\n",
       "      <td>114.063349</td>\n",
       "      <td>125.361697</td>\n",
       "      <td>134.925180</td>\n",
       "      <td>138.869573</td>\n",
       "      <td>134.584224</td>\n",
       "      <td>121.785001</td>\n",
       "      <td>102.597866</td>\n",
       "      <td>80.532661</td>\n",
       "      <td>59.640746</td>\n",
       "      <td>43.251457</td>\n",
       "      <td>33.263450</td>\n",
       "      <td>29.275601</td>\n",
       "      <td>29.703468</td>\n",
       "      <td>32.528055</td>\n",
       "      <td>36.054610</td>\n",
       "      <td>39.059703</td>\n",
       "      <td>40.466982</td>\n",
       "      <td>39.978947</td>\n",
       "      <td>37.578884</td>\n",
       "      <td>33.681288</td>\n",
       "      <td>28.877819</td>\n",
       "      <td>23.950670</td>\n",
       "      <td>19.708776</td>\n",
       "      <td>16.683626</td>\n",
       "      <td>15.172723</td>\n",
       "      <td>14.684688</td>\n",
       "      <td>14.303619</td>\n",
       "      <td>12.899683</td>\n",
       "      <td>9.553633</td>\n",
       "      <td>3.981342</td>\n",
       "      <td>-3.162022</td>\n",
       "      <td>-10.723224</td>\n",
       "      <td>-17.645969</td>\n",
       "      <td>-23.034411</td>\n",
       "      <td>-26.504141</td>\n",
       "      <td>-27.801111</td>\n",
       "      <td>-27.316418</td>\n",
       "      <td>-25.648408</td>\n",
       "      <td>-23.669526</td>\n",
       "      <td>-22.125195</td>\n",
       "      <td>-21.680615</td>\n",
       "      <td>-23.462278</td>\n",
       "      <td>-26.306921</td>\n",
       "      <td>-29.596144</td>\n",
       "      <td>-32.320450</td>\n",
       "      <td>-33.914921</td>\n",
       "      <td>-34.346130</td>\n",
       "      <td>-34.078713</td>\n",
       "      <td>-33.737757</td>\n",
       "      <td>-33.911578</td>\n",
       "      <td>-35.613015</td>\n",
       "      <td>-39.189711</td>\n",
       "      <td>-44.564783</td>\n",
       "      <td>-51.019549</td>\n",
       "      <td>-57.948980</td>\n",
       "      <td>-65.329675</td>\n",
       "      <td>-73.666385</td>\n",
       "      <td>-83.570824</td>\n",
       "      <td>-90.660705</td>\n",
       "      <td>-92.425654</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>424.691455</td>\n",
       "      <td>423.525565</td>\n",
       "      <td>424.108510</td>\n",
       "      <td>423.525565</td>\n",
       "      <td>423.525565</td>\n",
       "      <td>422.651376</td>\n",
       "      <td>422.359904</td>\n",
       "      <td>422.068431</td>\n",
       "      <td>421.485487</td>\n",
       "      <td>421.194015</td>\n",
       "      <td>418.279519</td>\n",
       "      <td>417.988274</td>\n",
       "      <td>418.862464</td>\n",
       "      <td>417.113857</td>\n",
       "      <td>415.947968</td>\n",
       "      <td>415.656723</td>\n",
       "      <td>414.782307</td>\n",
       "      <td>419.153936</td>\n",
       "      <td>414.490834</td>\n",
       "      <td>408.370599</td>\n",
       "      <td>406.330520</td>\n",
       "      <td>403.416025</td>\n",
       "      <td>402.541835</td>\n",
       "      <td>401.084474</td>\n",
       "      <td>400.210284</td>\n",
       "      <td>401.958891</td>\n",
       "      <td>420.028353</td>\n",
       "      <td>420.611070</td>\n",
       "      <td>420.028353</td>\n",
       "      <td>418.570991</td>\n",
       "      <td>419.445408</td>\n",
       "      <td>425.565644</td>\n",
       "      <td>426.731533</td>\n",
       "      <td>424.399982</td>\n",
       "      <td>428.188667</td>\n",
       "      <td>414.782307</td>\n",
       "      <td>412.159283</td>\n",
       "      <td>410.119205</td>\n",
       "      <td>410.410677</td>\n",
       "      <td>417.113857</td>\n",
       "      <td>436.640454</td>\n",
       "      <td>437.223398</td>\n",
       "      <td>443.343634</td>\n",
       "      <td>440.720611</td>\n",
       "      <td>442.760917</td>\n",
       "      <td>438.097815</td>\n",
       "      <td>437.806343</td>\n",
       "      <td>438.680532</td>\n",
       "      <td>438.680532</td>\n",
       "      <td>439.554949</td>\n",
       "      <td>443.343634</td>\n",
       "      <td>443.343634</td>\n",
       "      <td>442.469445</td>\n",
       "      <td>436.640454</td>\n",
       "      <td>447.715263</td>\n",
       "      <td>481.522637</td>\n",
       "      <td>500.175044</td>\n",
       "      <td>458.186650</td>\n",
       "      <td>450.026336</td>\n",
       "      <td>471.010066</td>\n",
       "      <td>485.290844</td>\n",
       "      <td>462.266807</td>\n",
       "      <td>457.020989</td>\n",
       "      <td>468.678743</td>\n",
       "      <td>449.734864</td>\n",
       "      <td>440.408660</td>\n",
       "      <td>434.288425</td>\n",
       "      <td>433.414008</td>\n",
       "      <td>432.248346</td>\n",
       "      <td>432.248346</td>\n",
       "      <td>429.333851</td>\n",
       "      <td>443.031683</td>\n",
       "      <td>429.042378</td>\n",
       "      <td>429.042378</td>\n",
       "      <td>422.922143</td>\n",
       "      <td>411.847333</td>\n",
       "      <td>405.144153</td>\n",
       "      <td>401.063996</td>\n",
       "      <td>398.732445</td>\n",
       "      <td>396.400894</td>\n",
       "      <td>394.943760</td>\n",
       "      <td>394.069343</td>\n",
       "      <td>392.903681</td>\n",
       "      <td>392.320737</td>\n",
       "      <td>391.446547</td>\n",
       "      <td>390.572130</td>\n",
       "      <td>389.697714</td>\n",
       "      <td>390.280658</td>\n",
       "      <td>388.240580</td>\n",
       "      <td>387.949107</td>\n",
       "      <td>387.949107</td>\n",
       "      <td>386.491973</td>\n",
       "      <td>386.200501</td>\n",
       "      <td>386.783446</td>\n",
       "      <td>385.617556</td>\n",
       "      <td>386.783446</td>\n",
       "      <td>385.034612</td>\n",
       "      <td>384.451895</td>\n",
       "      <td>383.577478</td>\n",
       "      <td>383.286006</td>\n",
       "      <td>382.411816</td>\n",
       "      <td>382.703061</td>\n",
       "      <td>383.868950</td>\n",
       "      <td>381.245927</td>\n",
       "      <td>382.120344</td>\n",
       "      <td>381.828872</td>\n",
       "      <td>381.245927</td>\n",
       "      <td>379.788793</td>\n",
       "      <td>381.266405</td>\n",
       "      <td>381.557877</td>\n",
       "      <td>380.100744</td>\n",
       "      <td>380.100744</td>\n",
       "      <td>379.517799</td>\n",
       "      <td>378.060665</td>\n",
       "      <td>379.226327</td>\n",
       "      <td>377.477720</td>\n",
       "      <td>377.477720</td>\n",
       "      <td>376.603303</td>\n",
       "      <td>375.437642</td>\n",
       "      <td>376.020586</td>\n",
       "      <td>376.020586</td>\n",
       "      <td>374.271753</td>\n",
       "      <td>373.689036</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>373.689036</td>\n",
       "      <td>374.271753</td>\n",
       "      <td>373.106091</td>\n",
       "      <td>372.523146</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>372.231674</td>\n",
       "      <td>372.814619</td>\n",
       "      <td>372.231674</td>\n",
       "      <td>371.648957</td>\n",
       "      <td>372.231674</td>\n",
       "      <td>371.940202</td>\n",
       "      <td>371.648957</td>\n",
       "      <td>372.523146</td>\n",
       "      <td>374.563225</td>\n",
       "      <td>376.020586</td>\n",
       "      <td>380.392216</td>\n",
       "      <td>379.809271</td>\n",
       "      <td>386.220979</td>\n",
       "      <td>382.723767</td>\n",
       "      <td>381.849350</td>\n",
       "      <td>381.266405</td>\n",
       "      <td>383.015239</td>\n",
       "      <td>396.130127</td>\n",
       "      <td>406.039048</td>\n",
       "      <td>383.889428</td>\n",
       "      <td>380.100744</td>\n",
       "      <td>383.015239</td>\n",
       "      <td>405.456103</td>\n",
       "      <td>408.370599</td>\n",
       "      <td>408.662071</td>\n",
       "      <td>406.913465</td>\n",
       "      <td>406.330520</td>\n",
       "      <td>402.833307</td>\n",
       "      <td>402.250363</td>\n",
       "      <td>403.124552</td>\n",
       "      <td>401.667418</td>\n",
       "      <td>402.250363</td>\n",
       "      <td>399.627340</td>\n",
       "      <td>397.587261</td>\n",
       "      <td>398.752923</td>\n",
       "      <td>399.044395</td>\n",
       "      <td>397.004317</td>\n",
       "      <td>397.004317</td>\n",
       "      <td>397.295789</td>\n",
       "      <td>395.838655</td>\n",
       "      <td>395.547183</td>\n",
       "      <td>397.878733</td>\n",
       "      <td>395.255710</td>\n",
       "      <td>394.381293</td>\n",
       "      <td>384.763845</td>\n",
       "      <td>381.849350</td>\n",
       "      <td>379.517799</td>\n",
       "      <td>378.060665</td>\n",
       "      <td>378.060665</td>\n",
       "      <td>377.477720</td>\n",
       "      <td>377.186248</td>\n",
       "      <td>377.769193</td>\n",
       "      <td>375.729114</td>\n",
       "      <td>376.312059</td>\n",
       "      <td>375.146170</td>\n",
       "      <td>374.854697</td>\n",
       "      <td>375.437642</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>373.980508</td>\n",
       "      <td>375.437642</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>374.271753</td>\n",
       "      <td>373.689036</td>\n",
       "      <td>373.689036</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>373.106091</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>374.563225</td>\n",
       "      <td>372.523146</td>\n",
       "      <td>372.523146</td>\n",
       "      <td>372.231674</td>\n",
       "      <td>372.814619</td>\n",
       "      <td>369.900123</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>372.523146</td>\n",
       "      <td>373.397563</td>\n",
       "      <td>372.814619</td>\n",
       "      <td>371.940202</td>\n",
       "      <td>372.231674</td>\n",
       "      <td>371.648957</td>\n",
       "      <td>371.940202</td>\n",
       "      <td>371.648957</td>\n",
       "      <td>372.231674</td>\n",
       "      <td>372.814619</td>\n",
       "      <td>372.523146</td>\n",
       "      <td>371.648957</td>\n",
       "      <td>372.523146</td>\n",
       "      <td>374.271753</td>\n",
       "      <td>374.271753</td>\n",
       "      <td>373.689036</td>\n",
       "      <td>375.146170</td>\n",
       "      <td>375.146170</td>\n",
       "      <td>376.020586</td>\n",
       "      <td>374.271753</td>\n",
       "      <td>373.689036</td>\n",
       "      <td>373.106091</td>\n",
       "      <td>373.689036</td>\n",
       "      <td>372.814619</td>\n",
       "      <td>372.231674</td>\n",
       "      <td>371.357485</td>\n",
       "      <td>371.940202</td>\n",
       "      <td>370.483068</td>\n",
       "      <td>371.357485</td>\n",
       "      <td>369.900123</td>\n",
       "      <td>369.608878</td>\n",
       "      <td>369.900123</td>\n",
       "      <td>369.900123</td>\n",
       "      <td>369.608878</td>\n",
       "      <td>369.025934</td>\n",
       "      <td>369.900123</td>\n",
       "      <td>-2282.154114</td>\n",
       "      <td>-2282.154114</td>\n",
       "      <td>-2282.154114</td>\n",
       "      <td>-2282.154114</td>\n",
       "      <td>-2282.154114</td>\n",
       "      <td>-2282.154114</td>\n",
       "      <td>-2282.154114</td>\n",
       "      <td>-2282.154114</td>\n",
       "      <td>-2310.992450</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.019060</td>\n",
       "      <td>-46.956865</td>\n",
       "      <td>-40.932037</td>\n",
       "      <td>-33.693116</td>\n",
       "      <td>-25.915611</td>\n",
       "      <td>-17.864250</td>\n",
       "      <td>-9.411234</td>\n",
       "      <td>-0.264450</td>\n",
       "      <td>9.786058</td>\n",
       "      <td>20.612491</td>\n",
       "      <td>31.758422</td>\n",
       "      <td>42.639626</td>\n",
       "      <td>52.754034</td>\n",
       "      <td>61.982975</td>\n",
       "      <td>70.499890</td>\n",
       "      <td>78.733822</td>\n",
       "      <td>87.122938</td>\n",
       "      <td>95.986737</td>\n",
       "      <td>105.334349</td>\n",
       "      <td>114.855402</td>\n",
       "      <td>123.755716</td>\n",
       "      <td>130.903353</td>\n",
       "      <td>135.002061</td>\n",
       "      <td>134.865133</td>\n",
       "      <td>129.807929</td>\n",
       "      <td>119.885221</td>\n",
       "      <td>105.845546</td>\n",
       "      <td>88.802587</td>\n",
       "      <td>69.906536</td>\n",
       "      <td>49.942448</td>\n",
       "      <td>29.439776</td>\n",
       "      <td>8.864077</td>\n",
       "      <td>-11.017855</td>\n",
       "      <td>-28.982796</td>\n",
       "      <td>-43.643210</td>\n",
       "      <td>-53.867160</td>\n",
       "      <td>-59.389919</td>\n",
       "      <td>-60.868740</td>\n",
       "      <td>-59.672903</td>\n",
       "      <td>-57.272101</td>\n",
       "      <td>-54.633956</td>\n",
       "      <td>-52.041455</td>\n",
       "      <td>-49.284640</td>\n",
       "      <td>-46.144426</td>\n",
       "      <td>-42.748614</td>\n",
       "      <td>-39.736200</td>\n",
       "      <td>-37.956138</td>\n",
       "      <td>-38.093066</td>\n",
       "      <td>-40.302169</td>\n",
       "      <td>-44.218307</td>\n",
       "      <td>-49.156840</td>\n",
       "      <td>-54.405743</td>\n",
       "      <td>-59.472076</td>\n",
       "      <td>-64.109367</td>\n",
       "      <td>-68.226333</td>\n",
       "      <td>-71.768201</td>\n",
       "      <td>-74.661944</td>\n",
       "      <td>-76.834534</td>\n",
       "      <td>-78.212941</td>\n",
       "      <td>-78.760653</td>\n",
       "      <td>-78.550697</td>\n",
       "      <td>-77.774772</td>\n",
       "      <td>-76.697606</td>\n",
       "      <td>-75.702596</td>\n",
       "      <td>-75.136628</td>\n",
       "      <td>-75.300941</td>\n",
       "      <td>-76.259437</td>\n",
       "      <td>-77.774772</td>\n",
       "      <td>-79.335750</td>\n",
       "      <td>-80.248603</td>\n",
       "      <td>-79.883462</td>\n",
       "      <td>-77.875186</td>\n",
       "      <td>-74.150747</td>\n",
       "      <td>-68.901844</td>\n",
       "      <td>-62.374947</td>\n",
       "      <td>-54.706985</td>\n",
       "      <td>-45.925342</td>\n",
       "      <td>-36.002633</td>\n",
       "      <td>-25.121429</td>\n",
       "      <td>-13.756413</td>\n",
       "      <td>-2.619610</td>\n",
       "      <td>7.531312</td>\n",
       "      <td>16.212541</td>\n",
       "      <td>23.232378</td>\n",
       "      <td>28.782522</td>\n",
       "      <td>33.173344</td>\n",
       "      <td>36.660441</td>\n",
       "      <td>39.307714</td>\n",
       "      <td>40.996491</td>\n",
       "      <td>41.562460</td>\n",
       "      <td>40.905206</td>\n",
       "      <td>39.134272</td>\n",
       "      <td>36.486999</td>\n",
       "      <td>33.273758</td>\n",
       "      <td>29.750146</td>\n",
       "      <td>26.089607</td>\n",
       "      <td>22.383425</td>\n",
       "      <td>18.649858</td>\n",
       "      <td>14.907161</td>\n",
       "      <td>11.182723</td>\n",
       "      <td>7.449155</td>\n",
       "      <td>3.578660</td>\n",
       "      <td>-0.583949</td>\n",
       "      <td>-5.084312</td>\n",
       "      <td>-9.739861</td>\n",
       "      <td>-13.993755</td>\n",
       "      <td>-16.997040</td>\n",
       "      <td>-17.891636</td>\n",
       "      <td>-16.184601</td>\n",
       "      <td>-11.930708</td>\n",
       "      <td>-5.750695</td>\n",
       "      <td>1.488227</td>\n",
       "      <td>9.037519</td>\n",
       "      <td>16.486397</td>\n",
       "      <td>23.780090</td>\n",
       "      <td>30.991626</td>\n",
       "      <td>37.974949</td>\n",
       "      <td>44.319275</td>\n",
       "      <td>49.431250</td>\n",
       "      <td>52.817934</td>\n",
       "      <td>54.397169</td>\n",
       "      <td>54.552354</td>\n",
       "      <td>53.931614</td>\n",
       "      <td>53.201332</td>\n",
       "      <td>52.671877</td>\n",
       "      <td>52.215451</td>\n",
       "      <td>51.412141</td>\n",
       "      <td>49.832905</td>\n",
       "      <td>47.240404</td>\n",
       "      <td>43.780692</td>\n",
       "      <td>39.782397</td>\n",
       "      <td>35.574146</td>\n",
       "      <td>31.311124</td>\n",
       "      <td>26.856403</td>\n",
       "      <td>21.899613</td>\n",
       "      <td>16.148641</td>\n",
       "      <td>9.430045</td>\n",
       "      <td>1.825982</td>\n",
       "      <td>-6.453591</td>\n",
       "      <td>-14.952250</td>\n",
       "      <td>-23.076639</td>\n",
       "      <td>-30.069090</td>\n",
       "      <td>-35.153680</td>\n",
       "      <td>-37.664025</td>\n",
       "      <td>-37.326269</td>\n",
       "      <td>-34.414269</td>\n",
       "      <td>-29.749592</td>\n",
       "      <td>-24.464175</td>\n",
       "      <td>-19.580413</td>\n",
       "      <td>-15.646018</td>\n",
       "      <td>-12.606219</td>\n",
       "      <td>-10.059360</td>\n",
       "      <td>-7.603786</td>\n",
       "      <td>-5.294269</td>\n",
       "      <td>-3.642005</td>\n",
       "      <td>-3.404663</td>\n",
       "      <td>-5.175598</td>\n",
       "      <td>-9.036965</td>\n",
       "      <td>-14.459310</td>\n",
       "      <td>-20.557165</td>\n",
       "      <td>-26.381166</td>\n",
       "      <td>-31.274056</td>\n",
       "      <td>-34.980238</td>\n",
       "      <td>-37.536225</td>\n",
       "      <td>-39.088075</td>\n",
       "      <td>-39.763586</td>\n",
       "      <td>-39.562758</td>\n",
       "      <td>-38.467335</td>\n",
       "      <td>-36.550345</td>\n",
       "      <td>-33.985229</td>\n",
       "      <td>-31.137128</td>\n",
       "      <td>-28.480727</td>\n",
       "      <td>-26.445065</td>\n",
       "      <td>-25.340514</td>\n",
       "      <td>-25.313128</td>\n",
       "      <td>-26.198595</td>\n",
       "      <td>-27.549617</td>\n",
       "      <td>-28.708940</td>\n",
       "      <td>-28.882382</td>\n",
       "      <td>-27.339661</td>\n",
       "      <td>-23.587836</td>\n",
       "      <td>-17.590394</td>\n",
       "      <td>-9.703347</td>\n",
       "      <td>-0.583949</td>\n",
       "      <td>9.046647</td>\n",
       "      <td>18.613343</td>\n",
       "      <td>27.741870</td>\n",
       "      <td>36.240529</td>\n",
       "      <td>43.954134</td>\n",
       "      <td>50.690987</td>\n",
       "      <td>56.140717</td>\n",
       "      <td>60.056855</td>\n",
       "      <td>62.348116</td>\n",
       "      <td>63.133169</td>\n",
       "      <td>62.749771</td>\n",
       "      <td>61.572191</td>\n",
       "      <td>59.929056</td>\n",
       "      <td>58.002937</td>\n",
       "      <td>55.775576</td>\n",
       "      <td>53.146561</td>\n",
       "      <td>49.951576</td>\n",
       "      <td>46.154109</td>\n",
       "      <td>41.900216</td>\n",
       "      <td>37.472880</td>\n",
       "      <td>33.182472</td>\n",
       "      <td>29.248077</td>\n",
       "      <td>25.651438</td>\n",
       "      <td>22.100441</td>\n",
       "      <td>18.156917</td>\n",
       "      <td>13.382698</td>\n",
       "      <td>7.540440</td>\n",
       "      <td>0.666660</td>\n",
       "      <td>-6.800475</td>\n",
       "      <td>-14.130683</td>\n",
       "      <td>-20.401981</td>\n",
       "      <td>-24.646746</td>\n",
       "      <td>-26.235109</td>\n",
       "      <td>-24.893216</td>\n",
       "      <td>-20.967949</td>\n",
       "      <td>-15.134821</td>\n",
       "      <td>-8.315811</td>\n",
       "      <td>-1.177303</td>\n",
       "      <td>5.869920</td>\n",
       "      <td>12.716315</td>\n",
       "      <td>19.307111</td>\n",
       "      <td>25.432353</td>\n",
       "      <td>30.717770</td>\n",
       "      <td>34.661294</td>\n",
       "      <td>36.934297</td>\n",
       "      <td>37.463751</td>\n",
       "      <td>36.550899</td>\n",
       "      <td>34.606523</td>\n",
       "      <td>32.087049</td>\n",
       "      <td>29.238949</td>\n",
       "      <td>26.089607</td>\n",
       "      <td>22.575124</td>\n",
       "      <td>18.604215</td>\n",
       "      <td>14.186008</td>\n",
       "      <td>9.457431</td>\n",
       "      <td>4.719725</td>\n",
       "      <td>0.246747</td>\n",
       "      <td>-3.760676</td>\n",
       "      <td>-7.348187</td>\n",
       "      <td>-10.789642</td>\n",
       "      <td>-14.541466</td>\n",
       "      <td>-19.078344</td>\n",
       "      <td>-24.665003</td>\n",
       "      <td>-31.201028</td>\n",
       "      <td>-38.184351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>-96.859455</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-95.469222</td>\n",
       "      <td>-95.353392</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.426480</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.310650</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.658231</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.658231</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.658231</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.658231</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.774061</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-95.700882</td>\n",
       "      <td>-95.353392</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-96.396044</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-95.237471</td>\n",
       "      <td>-95.816803</td>\n",
       "      <td>-95.700882</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-95.469222</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-96.280214</td>\n",
       "      <td>-95.700882</td>\n",
       "      <td>-95.816803</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-95.585052</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-96.280214</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-96.280214</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-96.280214</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-96.280214</td>\n",
       "      <td>-96.396044</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.280214</td>\n",
       "      <td>-96.396044</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.396044</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-95.700882</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-96.048463</td>\n",
       "      <td>-96.396044</td>\n",
       "      <td>-96.396044</td>\n",
       "      <td>-96.859455</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-96.164293</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-96.627704</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-96.511874</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.554526</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.554526</td>\n",
       "      <td>-97.554526</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-96.859455</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-96.859455</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.554526</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.554526</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.902106</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.554526</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-97.091115</td>\n",
       "      <td>-97.207035</td>\n",
       "      <td>-96.859455</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.438696</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.554526</td>\n",
       "      <td>-97.554526</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.322865</td>\n",
       "      <td>-96.859455</td>\n",
       "      <td>-96.975285</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.786276</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>-97.670446</td>\n",
       "      <td>279.337205</td>\n",
       "      <td>279.337205</td>\n",
       "      <td>279.337205</td>\n",
       "      <td>279.337205</td>\n",
       "      <td>267.346561</td>\n",
       "      <td>267.346561</td>\n",
       "      <td>267.346561</td>\n",
       "      <td>267.346561</td>\n",
       "      <td>291.327848</td>\n",
       "      <td>...</td>\n",
       "      <td>38.112586</td>\n",
       "      <td>49.364740</td>\n",
       "      <td>60.722880</td>\n",
       "      <td>72.013895</td>\n",
       "      <td>83.304911</td>\n",
       "      <td>94.627722</td>\n",
       "      <td>106.010592</td>\n",
       "      <td>117.481783</td>\n",
       "      <td>128.949441</td>\n",
       "      <td>140.007288</td>\n",
       "      <td>149.440020</td>\n",
       "      <td>154.866491</td>\n",
       "      <td>153.241376</td>\n",
       "      <td>141.999820</td>\n",
       "      <td>120.399924</td>\n",
       "      <td>90.550803</td>\n",
       "      <td>57.168824</td>\n",
       "      <td>26.146795</td>\n",
       "      <td>2.589695</td>\n",
       "      <td>-10.905824</td>\n",
       "      <td>-14.763706</td>\n",
       "      <td>-11.513476</td>\n",
       "      <td>-4.309978</td>\n",
       "      <td>4.359657</td>\n",
       "      <td>13.361380</td>\n",
       "      <td>22.606871</td>\n",
       "      <td>32.502407</td>\n",
       "      <td>43.507261</td>\n",
       "      <td>55.833404</td>\n",
       "      <td>69.487902</td>\n",
       "      <td>84.612068</td>\n",
       "      <td>101.612183</td>\n",
       "      <td>120.965181</td>\n",
       "      <td>142.957225</td>\n",
       "      <td>166.934735</td>\n",
       "      <td>190.813325</td>\n",
       "      <td>211.197918</td>\n",
       "      <td>224.590984</td>\n",
       "      <td>228.565450</td>\n",
       "      <td>223.502864</td>\n",
       "      <td>212.522740</td>\n",
       "      <td>200.087078</td>\n",
       "      <td>189.824124</td>\n",
       "      <td>182.542903</td>\n",
       "      <td>176.000049</td>\n",
       "      <td>166.549653</td>\n",
       "      <td>152.096730</td>\n",
       "      <td>133.111148</td>\n",
       "      <td>113.210557</td>\n",
       "      <td>96.867554</td>\n",
       "      <td>86.594002</td>\n",
       "      <td>81.340641</td>\n",
       "      <td>76.910437</td>\n",
       "      <td>68.438643</td>\n",
       "      <td>53.084840</td>\n",
       "      <td>31.990142</td>\n",
       "      <td>7.987903</td>\n",
       "      <td>-15.102860</td>\n",
       "      <td>-35.226021</td>\n",
       "      <td>-52.759598</td>\n",
       "      <td>-69.388762</td>\n",
       "      <td>-86.187504</td>\n",
       "      <td>-102.099498</td>\n",
       "      <td>-113.959304</td>\n",
       "      <td>-117.417972</td>\n",
       "      <td>-111.666479</td>\n",
       "      <td>-98.425325</td>\n",
       "      <td>-82.432075</td>\n",
       "      <td>-69.466485</td>\n",
       "      <td>-63.831576</td>\n",
       "      <td>-66.460023</td>\n",
       "      <td>-74.571465</td>\n",
       "      <td>-83.135114</td>\n",
       "      <td>-86.014394</td>\n",
       "      <td>-81.167312</td>\n",
       "      <td>-68.152262</td>\n",
       "      <td>-48.537832</td>\n",
       "      <td>-24.503796</td>\n",
       "      <td>2.568498</td>\n",
       "      <td>32.477677</td>\n",
       "      <td>65.516969</td>\n",
       "      <td>93.843427</td>\n",
       "      <td>122.908253</td>\n",
       "      <td>147.157793</td>\n",
       "      <td>161.854484</td>\n",
       "      <td>164.094317</td>\n",
       "      <td>154.258839</td>\n",
       "      <td>135.958632</td>\n",
       "      <td>114.422328</td>\n",
       "      <td>94.415750</td>\n",
       "      <td>78.641538</td>\n",
       "      <td>68.590556</td>\n",
       "      <td>62.295002</td>\n",
       "      <td>59.101298</td>\n",
       "      <td>59.578234</td>\n",
       "      <td>65.202544</td>\n",
       "      <td>77.051751</td>\n",
       "      <td>94.352159</td>\n",
       "      <td>113.807610</td>\n",
       "      <td>130.281329</td>\n",
       "      <td>139.618674</td>\n",
       "      <td>137.869909</td>\n",
       "      <td>124.823063</td>\n",
       "      <td>103.050056</td>\n",
       "      <td>76.398173</td>\n",
       "      <td>48.446197</td>\n",
       "      <td>21.246720</td>\n",
       "      <td>-4.854038</td>\n",
       "      <td>-30.672167</td>\n",
       "      <td>-56.522092</td>\n",
       "      <td>-83.057391</td>\n",
       "      <td>-109.794064</td>\n",
       "      <td>-136.014939</td>\n",
       "      <td>-160.974583</td>\n",
       "      <td>-184.051214</td>\n",
       "      <td>-204.481735</td>\n",
       "      <td>-221.280476</td>\n",
       "      <td>-233.550093</td>\n",
       "      <td>-240.742993</td>\n",
       "      <td>-243.565747</td>\n",
       "      <td>-243.180665</td>\n",
       "      <td>-240.986760</td>\n",
       "      <td>-238.100415</td>\n",
       "      <td>-235.259997</td>\n",
       "      <td>-232.864719</td>\n",
       "      <td>-231.243137</td>\n",
       "      <td>-230.635485</td>\n",
       "      <td>-231.038231</td>\n",
       "      <td>-232.091023</td>\n",
       "      <td>-232.543229</td>\n",
       "      <td>-230.635485</td>\n",
       "      <td>-224.573100</td>\n",
       "      <td>-213.176099</td>\n",
       "      <td>-196.048802</td>\n",
       "      <td>-173.636348</td>\n",
       "      <td>-146.747763</td>\n",
       "      <td>-116.386377</td>\n",
       "      <td>-83.340020</td>\n",
       "      <td>-48.120955</td>\n",
       "      <td>-11.093066</td>\n",
       "      <td>27.400959</td>\n",
       "      <td>66.647483</td>\n",
       "      <td>105.643174</td>\n",
       "      <td>143.027882</td>\n",
       "      <td>177.512113</td>\n",
       "      <td>208.099601</td>\n",
       "      <td>234.482987</td>\n",
       "      <td>257.050887</td>\n",
       "      <td>276.661784</td>\n",
       "      <td>294.591040</td>\n",
       "      <td>312.015098</td>\n",
       "      <td>330.135129</td>\n",
       "      <td>349.844946</td>\n",
       "      <td>371.367119</td>\n",
       "      <td>393.927953</td>\n",
       "      <td>415.435995</td>\n",
       "      <td>432.807059</td>\n",
       "      <td>442.720260</td>\n",
       "      <td>442.815647</td>\n",
       "      <td>432.457307</td>\n",
       "      <td>412.941797</td>\n",
       "      <td>386.989419</td>\n",
       "      <td>357.546577</td>\n",
       "      <td>326.842505</td>\n",
       "      <td>295.834606</td>\n",
       "      <td>264.565277</td>\n",
       "      <td>232.716558</td>\n",
       "      <td>200.108275</td>\n",
       "      <td>166.910005</td>\n",
       "      <td>133.708202</td>\n",
       "      <td>101.354284</td>\n",
       "      <td>70.735001</td>\n",
       "      <td>42.712368</td>\n",
       "      <td>17.639672</td>\n",
       "      <td>-4.574942</td>\n",
       "      <td>-24.574453</td>\n",
       "      <td>-43.157289</td>\n",
       "      <td>-61.241991</td>\n",
       "      <td>-79.623453</td>\n",
       "      <td>-98.672625</td>\n",
       "      <td>-118.629743</td>\n",
       "      <td>-139.445344</td>\n",
       "      <td>-161.345534</td>\n",
       "      <td>-184.567012</td>\n",
       "      <td>-209.219298</td>\n",
       "      <td>-235.023295</td>\n",
       "      <td>-261.540931</td>\n",
       "      <td>-288.662685</td>\n",
       "      <td>-316.960880</td>\n",
       "      <td>-347.431784</td>\n",
       "      <td>-380.622989</td>\n",
       "      <td>-415.810258</td>\n",
       "      <td>-450.294489</td>\n",
       "      <td>-479.924573</td>\n",
       "      <td>-500.379823</td>\n",
       "      <td>-508.770361</td>\n",
       "      <td>-504.516800</td>\n",
       "      <td>-489.226589</td>\n",
       "      <td>-465.453985</td>\n",
       "      <td>-435.378761</td>\n",
       "      <td>-400.484719</td>\n",
       "      <td>-361.471363</td>\n",
       "      <td>-319.137121</td>\n",
       "      <td>-274.951661</td>\n",
       "      <td>-230.805063</td>\n",
       "      <td>-188.537945</td>\n",
       "      <td>-149.309085</td>\n",
       "      <td>-113.330455</td>\n",
       "      <td>-79.817760</td>\n",
       "      <td>-47.697012</td>\n",
       "      <td>-16.063797</td>\n",
       "      <td>15.353912</td>\n",
       "      <td>45.937867</td>\n",
       "      <td>74.730663</td>\n",
       "      <td>100.810224</td>\n",
       "      <td>123.703147</td>\n",
       "      <td>143.388234</td>\n",
       "      <td>160.271764</td>\n",
       "      <td>174.696425</td>\n",
       "      <td>186.782333</td>\n",
       "      <td>196.275124</td>\n",
       "      <td>202.817978</td>\n",
       "      <td>206.382632</td>\n",
       "      <td>207.516679</td>\n",
       "      <td>207.431891</td>\n",
       "      <td>207.721585</td>\n",
       "      <td>210.187520</td>\n",
       "      <td>216.341760</td>\n",
       "      <td>227.353679</td>\n",
       "      <td>243.643689</td>\n",
       "      <td>264.784314</td>\n",
       "      <td>289.327081</td>\n",
       "      <td>314.643545</td>\n",
       "      <td>337.299765</td>\n",
       "      <td>353.904200</td>\n",
       "      <td>362.167556</td>\n",
       "      <td>361.722416</td>\n",
       "      <td>354.271617</td>\n",
       "      <td>342.924076</td>\n",
       "      <td>330.947686</td>\n",
       "      <td>320.811915</td>\n",
       "      <td>313.534227</td>\n",
       "      <td>308.846124</td>\n",
       "      <td>305.850260</td>\n",
       "      <td>303.458515</td>\n",
       "      <td>300.575702</td>\n",
       "      <td>296.011249</td>\n",
       "      <td>288.440334</td>\n",
       "      <td>276.393287</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>-110.091983</td>\n",
       "      <td>-109.777949</td>\n",
       "      <td>-111.034574</td>\n",
       "      <td>-110.406261</td>\n",
       "      <td>-109.463670</td>\n",
       "      <td>-107.578732</td>\n",
       "      <td>-105.379515</td>\n",
       "      <td>-104.436924</td>\n",
       "      <td>-102.551986</td>\n",
       "      <td>-100.981081</td>\n",
       "      <td>-100.038490</td>\n",
       "      <td>-99.096143</td>\n",
       "      <td>-97.525239</td>\n",
       "      <td>-96.582648</td>\n",
       "      <td>-97.525239</td>\n",
       "      <td>-96.896927</td>\n",
       "      <td>-95.326022</td>\n",
       "      <td>-92.498493</td>\n",
       "      <td>-90.613310</td>\n",
       "      <td>-89.356684</td>\n",
       "      <td>-87.785780</td>\n",
       "      <td>-86.215121</td>\n",
       "      <td>-84.958251</td>\n",
       "      <td>-84.958251</td>\n",
       "      <td>-84.329938</td>\n",
       "      <td>-84.329938</td>\n",
       "      <td>-84.958251</td>\n",
       "      <td>-85.272530</td>\n",
       "      <td>-84.644217</td>\n",
       "      <td>-84.329938</td>\n",
       "      <td>-83.701626</td>\n",
       "      <td>-83.387347</td>\n",
       "      <td>-83.073313</td>\n",
       "      <td>-81.502409</td>\n",
       "      <td>-81.502409</td>\n",
       "      <td>-81.188375</td>\n",
       "      <td>-81.502409</td>\n",
       "      <td>-81.502409</td>\n",
       "      <td>-81.502409</td>\n",
       "      <td>-80.559817</td>\n",
       "      <td>-81.502409</td>\n",
       "      <td>-80.874096</td>\n",
       "      <td>-81.188375</td>\n",
       "      <td>-80.559817</td>\n",
       "      <td>-80.559817</td>\n",
       "      <td>-79.931504</td>\n",
       "      <td>-80.245783</td>\n",
       "      <td>-80.245783</td>\n",
       "      <td>-80.559817</td>\n",
       "      <td>-80.874096</td>\n",
       "      <td>-80.874096</td>\n",
       "      <td>-81.502409</td>\n",
       "      <td>-81.816688</td>\n",
       "      <td>-81.816688</td>\n",
       "      <td>-81.816688</td>\n",
       "      <td>-82.130721</td>\n",
       "      <td>-82.445000</td>\n",
       "      <td>-82.130721</td>\n",
       "      <td>-82.445000</td>\n",
       "      <td>-82.759034</td>\n",
       "      <td>-82.759034</td>\n",
       "      <td>-82.759034</td>\n",
       "      <td>-81.502409</td>\n",
       "      <td>-82.759034</td>\n",
       "      <td>-83.073313</td>\n",
       "      <td>-83.073313</td>\n",
       "      <td>-83.387347</td>\n",
       "      <td>-83.701626</td>\n",
       "      <td>-83.073313</td>\n",
       "      <td>-83.073313</td>\n",
       "      <td>-82.445000</td>\n",
       "      <td>-82.445000</td>\n",
       "      <td>-83.073313</td>\n",
       "      <td>-82.445000</td>\n",
       "      <td>-83.073313</td>\n",
       "      <td>-82.445000</td>\n",
       "      <td>-82.759034</td>\n",
       "      <td>-81.816688</td>\n",
       "      <td>-80.874096</td>\n",
       "      <td>-79.931504</td>\n",
       "      <td>-79.617471</td>\n",
       "      <td>-78.046567</td>\n",
       "      <td>-78.674879</td>\n",
       "      <td>-79.617471</td>\n",
       "      <td>-79.617471</td>\n",
       "      <td>-78.989158</td>\n",
       "      <td>-79.303192</td>\n",
       "      <td>-79.617471</td>\n",
       "      <td>-79.617471</td>\n",
       "      <td>-79.303192</td>\n",
       "      <td>-78.360600</td>\n",
       "      <td>-79.303192</td>\n",
       "      <td>-79.617471</td>\n",
       "      <td>-78.674879</td>\n",
       "      <td>-78.360600</td>\n",
       "      <td>-78.360600</td>\n",
       "      <td>-77.732288</td>\n",
       "      <td>-76.789941</td>\n",
       "      <td>-76.161629</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-74.276445</td>\n",
       "      <td>-73.648133</td>\n",
       "      <td>-73.333854</td>\n",
       "      <td>-73.648133</td>\n",
       "      <td>-71.448916</td>\n",
       "      <td>-70.192291</td>\n",
       "      <td>-69.563978</td>\n",
       "      <td>-69.563978</td>\n",
       "      <td>-69.878012</td>\n",
       "      <td>-69.563978</td>\n",
       "      <td>-70.192291</td>\n",
       "      <td>-70.192291</td>\n",
       "      <td>-70.192291</td>\n",
       "      <td>-70.506324</td>\n",
       "      <td>-70.820603</td>\n",
       "      <td>-70.820603</td>\n",
       "      <td>-77.103975</td>\n",
       "      <td>-76.789941</td>\n",
       "      <td>-77.732288</td>\n",
       "      <td>-77.732288</td>\n",
       "      <td>-78.046567</td>\n",
       "      <td>-78.046567</td>\n",
       "      <td>-77.103975</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-76.475662</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-74.904758</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.533071</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-75.533071</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-74.904758</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-76.161629</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-76.161629</td>\n",
       "      <td>-74.590724</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.533071</td>\n",
       "      <td>-74.276445</td>\n",
       "      <td>-73.962412</td>\n",
       "      <td>-71.448916</td>\n",
       "      <td>-72.391508</td>\n",
       "      <td>-72.705541</td>\n",
       "      <td>-72.391508</td>\n",
       "      <td>-72.705541</td>\n",
       "      <td>-72.705541</td>\n",
       "      <td>-72.077229</td>\n",
       "      <td>-71.763195</td>\n",
       "      <td>-72.391508</td>\n",
       "      <td>-71.134882</td>\n",
       "      <td>-76.161629</td>\n",
       "      <td>-75.533071</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-74.276445</td>\n",
       "      <td>-75.533071</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-75.847350</td>\n",
       "      <td>-71.763195</td>\n",
       "      <td>-69.878012</td>\n",
       "      <td>-67.678795</td>\n",
       "      <td>-68.621386</td>\n",
       "      <td>-70.192291</td>\n",
       "      <td>-73.019820</td>\n",
       "      <td>-76.161629</td>\n",
       "      <td>-79.303192</td>\n",
       "      <td>-75.219037</td>\n",
       "      <td>-74.276445</td>\n",
       "      <td>-70.192291</td>\n",
       "      <td>-65.479578</td>\n",
       "      <td>-61.709702</td>\n",
       "      <td>-58.253615</td>\n",
       "      <td>-53.541147</td>\n",
       "      <td>-51.970243</td>\n",
       "      <td>-51.656210</td>\n",
       "      <td>-51.027897</td>\n",
       "      <td>-49.771026</td>\n",
       "      <td>-48.828680</td>\n",
       "      <td>-49.142714</td>\n",
       "      <td>-48.828680</td>\n",
       "      <td>-48.828680</td>\n",
       "      <td>-49.142714</td>\n",
       "      <td>-50.399339</td>\n",
       "      <td>-52.912835</td>\n",
       "      <td>-55.740364</td>\n",
       "      <td>-56.368677</td>\n",
       "      <td>-56.996990</td>\n",
       "      <td>-57.939581</td>\n",
       "      <td>-56.682956</td>\n",
       "      <td>-57.311269</td>\n",
       "      <td>-58.567894</td>\n",
       "      <td>-57.311269</td>\n",
       "      <td>-59.824519</td>\n",
       "      <td>-60.138798</td>\n",
       "      <td>-60.138798</td>\n",
       "      <td>-60.138798</td>\n",
       "      <td>-60.452832</td>\n",
       "      <td>-60.452832</td>\n",
       "      <td>-60.452832</td>\n",
       "      <td>-61.081390</td>\n",
       "      <td>-60.767111</td>\n",
       "      <td>-61.081390</td>\n",
       "      <td>-61.709702</td>\n",
       "      <td>-60.767111</td>\n",
       "      <td>-60.138798</td>\n",
       "      <td>-59.824519</td>\n",
       "      <td>-59.824519</td>\n",
       "      <td>-60.767111</td>\n",
       "      <td>-60.767111</td>\n",
       "      <td>-61.395423</td>\n",
       "      <td>-59.510485</td>\n",
       "      <td>-57.625302</td>\n",
       "      <td>-54.797773</td>\n",
       "      <td>-51.027897</td>\n",
       "      <td>-46.629463</td>\n",
       "      <td>-43.801934</td>\n",
       "      <td>-42.545063</td>\n",
       "      <td>-39.717534</td>\n",
       "      <td>-39.089221</td>\n",
       "      <td>-39.089221</td>\n",
       "      <td>-38.146630</td>\n",
       "      <td>-35.947413</td>\n",
       "      <td>-35.947413</td>\n",
       "      <td>-34.376754</td>\n",
       "      <td>-33.119883</td>\n",
       "      <td>-32.177537</td>\n",
       "      <td>-31.549224</td>\n",
       "      <td>-31.234945</td>\n",
       "      <td>548.889178</td>\n",
       "      <td>548.889178</td>\n",
       "      <td>548.889178</td>\n",
       "      <td>548.889178</td>\n",
       "      <td>548.889178</td>\n",
       "      <td>548.889178</td>\n",
       "      <td>548.889178</td>\n",
       "      <td>548.889178</td>\n",
       "      <td>565.200219</td>\n",
       "      <td>...</td>\n",
       "      <td>459.286333</td>\n",
       "      <td>473.555295</td>\n",
       "      <td>478.348699</td>\n",
       "      <td>470.880469</td>\n",
       "      <td>452.379188</td>\n",
       "      <td>426.917555</td>\n",
       "      <td>373.382344</td>\n",
       "      <td>283.076742</td>\n",
       "      <td>161.539043</td>\n",
       "      <td>27.855797</td>\n",
       "      <td>-96.414771</td>\n",
       "      <td>-197.729239</td>\n",
       "      <td>-272.353493</td>\n",
       "      <td>-322.691489</td>\n",
       "      <td>-353.541469</td>\n",
       "      <td>-370.117650</td>\n",
       "      <td>-373.077855</td>\n",
       "      <td>-363.611970</td>\n",
       "      <td>-344.646343</td>\n",
       "      <td>-321.085626</td>\n",
       "      <td>-297.998929</td>\n",
       "      <td>-278.786618</td>\n",
       "      <td>-264.643416</td>\n",
       "      <td>-254.969543</td>\n",
       "      <td>-248.052724</td>\n",
       "      <td>-238.533634</td>\n",
       "      <td>-222.407288</td>\n",
       "      <td>-198.744996</td>\n",
       "      <td>-170.120006</td>\n",
       "      <td>-141.001650</td>\n",
       "      <td>-115.520669</td>\n",
       "      <td>-96.013305</td>\n",
       "      <td>-83.427597</td>\n",
       "      <td>-77.405611</td>\n",
       "      <td>-74.479265</td>\n",
       "      <td>-71.233680</td>\n",
       "      <td>-65.569628</td>\n",
       "      <td>-57.201728</td>\n",
       "      <td>-47.024814</td>\n",
       "      <td>-36.175566</td>\n",
       "      <td>-25.679414</td>\n",
       "      <td>-16.513420</td>\n",
       "      <td>-10.264098</td>\n",
       "      <td>-7.594109</td>\n",
       "      <td>-8.856549</td>\n",
       "      <td>-13.886963</td>\n",
       "      <td>-21.988832</td>\n",
       "      <td>-31.812649</td>\n",
       "      <td>-41.399457</td>\n",
       "      <td>-48.577470</td>\n",
       "      <td>-51.605393</td>\n",
       "      <td>-50.594473</td>\n",
       "      <td>-46.642696</td>\n",
       "      <td>-41.505870</td>\n",
       "      <td>-36.896269</td>\n",
       "      <td>-34.100520</td>\n",
       "      <td>-33.790956</td>\n",
       "      <td>-35.895023</td>\n",
       "      <td>-39.667834</td>\n",
       "      <td>-44.030750</td>\n",
       "      <td>-49.138555</td>\n",
       "      <td>-55.769995</td>\n",
       "      <td>-64.602241</td>\n",
       "      <td>-75.751379</td>\n",
       "      <td>-88.917519</td>\n",
       "      <td>-103.733055</td>\n",
       "      <td>-119.772336</td>\n",
       "      <td>-136.382376</td>\n",
       "      <td>-145.050165</td>\n",
       "      <td>-138.892746</td>\n",
       "      <td>-115.854418</td>\n",
       "      <td>-80.230382</td>\n",
       "      <td>-39.604954</td>\n",
       "      <td>-0.474139</td>\n",
       "      <td>33.611752</td>\n",
       "      <td>61.661145</td>\n",
       "      <td>84.177084</td>\n",
       "      <td>101.551359</td>\n",
       "      <td>114.461142</td>\n",
       "      <td>122.495293</td>\n",
       "      <td>125.305553</td>\n",
       "      <td>123.419148</td>\n",
       "      <td>118.233952</td>\n",
       "      <td>111.447731</td>\n",
       "      <td>104.492216</td>\n",
       "      <td>98.242895</td>\n",
       "      <td>93.613947</td>\n",
       "      <td>89.908853</td>\n",
       "      <td>85.623328</td>\n",
       "      <td>79.509440</td>\n",
       "      <td>71.068986</td>\n",
       "      <td>60.471259</td>\n",
       "      <td>48.117724</td>\n",
       "      <td>34.477563</td>\n",
       "      <td>20.242460</td>\n",
       "      <td>6.481376</td>\n",
       "      <td>-5.997920</td>\n",
       "      <td>-16.542441</td>\n",
       "      <td>-24.639473</td>\n",
       "      <td>-30.216460</td>\n",
       "      <td>-34.013455</td>\n",
       "      <td>-37.583114</td>\n",
       "      <td>-42.768310</td>\n",
       "      <td>-50.904037</td>\n",
       "      <td>-62.353065</td>\n",
       "      <td>-76.815505</td>\n",
       "      <td>-93.599674</td>\n",
       "      <td>-111.849435</td>\n",
       "      <td>-130.776366</td>\n",
       "      <td>-149.727483</td>\n",
       "      <td>-168.132026</td>\n",
       "      <td>-185.307986</td>\n",
       "      <td>-200.287978</td>\n",
       "      <td>-211.867604</td>\n",
       "      <td>-218.929531</td>\n",
       "      <td>-220.656317</td>\n",
       "      <td>-216.704540</td>\n",
       "      <td>-207.369253</td>\n",
       "      <td>-193.588821</td>\n",
       "      <td>-176.712751</td>\n",
       "      <td>-158.196958</td>\n",
       "      <td>-139.178125</td>\n",
       "      <td>-120.309236</td>\n",
       "      <td>-101.822466</td>\n",
       "      <td>-83.722650</td>\n",
       "      <td>-66.004952</td>\n",
       "      <td>-48.785459</td>\n",
       "      <td>-32.359223</td>\n",
       "      <td>-17.118037</td>\n",
       "      <td>-3.477876</td>\n",
       "      <td>8.256531</td>\n",
       "      <td>18.002958</td>\n",
       "      <td>25.950044</td>\n",
       "      <td>32.586321</td>\n",
       "      <td>38.477709</td>\n",
       "      <td>44.078882</td>\n",
       "      <td>49.525272</td>\n",
       "      <td>54.652425</td>\n",
       "      <td>59.174960</td>\n",
       "      <td>62.938096</td>\n",
       "      <td>66.077268</td>\n",
       "      <td>68.916550</td>\n",
       "      <td>71.760668</td>\n",
       "      <td>74.624135</td>\n",
       "      <td>77.144178</td>\n",
       "      <td>78.687161</td>\n",
       "      <td>78.653303</td>\n",
       "      <td>76.747550</td>\n",
       "      <td>73.119847</td>\n",
       "      <td>68.268400</td>\n",
       "      <td>62.846195</td>\n",
       "      <td>57.409478</td>\n",
       "      <td>52.238793</td>\n",
       "      <td>47.377672</td>\n",
       "      <td>42.690681</td>\n",
       "      <td>38.027874</td>\n",
       "      <td>33.316698</td>\n",
       "      <td>28.537805</td>\n",
       "      <td>23.696032</td>\n",
       "      <td>18.670455</td>\n",
       "      <td>13.286945</td>\n",
       "      <td>7.342350</td>\n",
       "      <td>0.749606</td>\n",
       "      <td>-6.365527</td>\n",
       "      <td>-13.688649</td>\n",
       "      <td>-20.784434</td>\n",
       "      <td>-27.251418</td>\n",
       "      <td>-32.823569</td>\n",
       "      <td>-37.413821</td>\n",
       "      <td>-41.123752</td>\n",
       "      <td>-44.166185</td>\n",
       "      <td>-46.797478</td>\n",
       "      <td>-49.264315</td>\n",
       "      <td>-51.706968</td>\n",
       "      <td>-54.120600</td>\n",
       "      <td>-56.311732</td>\n",
       "      <td>-57.975638</td>\n",
       "      <td>-58.768895</td>\n",
       "      <td>-58.439984</td>\n",
       "      <td>-56.959881</td>\n",
       "      <td>-54.565598</td>\n",
       "      <td>-51.673110</td>\n",
       "      <td>-48.799970</td>\n",
       "      <td>-46.357317</td>\n",
       "      <td>-44.591835</td>\n",
       "      <td>-43.527709</td>\n",
       "      <td>-42.990809</td>\n",
       "      <td>-42.652224</td>\n",
       "      <td>-42.086302</td>\n",
       "      <td>-40.819025</td>\n",
       "      <td>-38.381209</td>\n",
       "      <td>-34.424595</td>\n",
       "      <td>-28.755705</td>\n",
       "      <td>-21.422910</td>\n",
       "      <td>-12.663218</td>\n",
       "      <td>-2.844238</td>\n",
       "      <td>7.666425</td>\n",
       "      <td>18.578553</td>\n",
       "      <td>29.688996</td>\n",
       "      <td>40.871993</td>\n",
       "      <td>51.967925</td>\n",
       "      <td>62.763967</td>\n",
       "      <td>72.974739</td>\n",
       "      <td>82.242309</td>\n",
       "      <td>90.228091</td>\n",
       "      <td>96.641869</td>\n",
       "      <td>101.295001</td>\n",
       "      <td>104.177816</td>\n",
       "      <td>105.416071</td>\n",
       "      <td>105.275800</td>\n",
       "      <td>104.129446</td>\n",
       "      <td>102.383312</td>\n",
       "      <td>100.409842</td>\n",
       "      <td>98.465394</td>\n",
       "      <td>96.695075</td>\n",
       "      <td>95.089212</td>\n",
       "      <td>93.546229</td>\n",
       "      <td>91.906508</td>\n",
       "      <td>90.010429</td>\n",
       "      <td>87.712884</td>\n",
       "      <td>84.888113</td>\n",
       "      <td>81.439378</td>\n",
       "      <td>77.323145</td>\n",
       "      <td>72.520067</td>\n",
       "      <td>67.083351</td>\n",
       "      <td>61.133919</td>\n",
       "      <td>54.826554</td>\n",
       "      <td>48.349896</td>\n",
       "      <td>41.849054</td>\n",
       "      <td>35.464298</td>\n",
       "      <td>29.268183</td>\n",
       "      <td>23.289729</td>\n",
       "      <td>17.504754</td>\n",
       "      <td>11.869723</td>\n",
       "      <td>6.345941</td>\n",
       "      <td>0.894714</td>\n",
       "      <td>-4.440427</td>\n",
       "      <td>-9.620785</td>\n",
       "      <td>-14.578645</td>\n",
       "      <td>-19.323680</td>\n",
       "      <td>-23.899421</td>\n",
       "      <td>-28.421957</td>\n",
       "      <td>-33.007372</td>\n",
       "      <td>-37.737896</td>\n",
       "      <td>-42.661897</td>\n",
       "      <td>-47.750354</td>\n",
       "      <td>-52.916202</td>\n",
       "      <td>-57.980475</td>\n",
       "      <td>-62.672303</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16352</th>\n",
       "      <td>7</td>\n",
       "      <td>-50.075140</td>\n",
       "      <td>-49.147074</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.260811</td>\n",
       "      <td>-50.632153</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.260811</td>\n",
       "      <td>-50.260811</td>\n",
       "      <td>-50.446482</td>\n",
       "      <td>-50.260811</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.632153</td>\n",
       "      <td>-50.260811</td>\n",
       "      <td>-50.260811</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.260811</td>\n",
       "      <td>-50.632153</td>\n",
       "      <td>-49.889613</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-50.446482</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-50.632153</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-50.446482</td>\n",
       "      <td>-50.632153</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-50.817679</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-51.745889</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-53.230967</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-54.715901</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-53.230967</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-53.230967</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-51.931560</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-53.230967</td>\n",
       "      <td>-53.416494</td>\n",
       "      <td>-53.973362</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-53.230967</td>\n",
       "      <td>-53.416494</td>\n",
       "      <td>-53.602165</td>\n",
       "      <td>-53.416494</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-53.045296</td>\n",
       "      <td>-53.602165</td>\n",
       "      <td>-53.416494</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-53.602165</td>\n",
       "      <td>-52.488428</td>\n",
       "      <td>-54.159033</td>\n",
       "      <td>-53.416494</td>\n",
       "      <td>-53.416494</td>\n",
       "      <td>-53.416494</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-50.632153</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-50.632153</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-50.446482</td>\n",
       "      <td>-50.632153</td>\n",
       "      <td>-51.003350</td>\n",
       "      <td>-51.189021</td>\n",
       "      <td>-51.374547</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.859625</td>\n",
       "      <td>-52.117086</td>\n",
       "      <td>-52.673954</td>\n",
       "      <td>-52.302757</td>\n",
       "      <td>-51.560218</td>\n",
       "      <td>-47.847667</td>\n",
       "      <td>-47.290799</td>\n",
       "      <td>-47.290799</td>\n",
       "      <td>-46.919456</td>\n",
       "      <td>-46.919456</td>\n",
       "      <td>-46.919456</td>\n",
       "      <td>1702.123736</td>\n",
       "      <td>1702.123736</td>\n",
       "      <td>1702.123736</td>\n",
       "      <td>1702.123736</td>\n",
       "      <td>1689.175370</td>\n",
       "      <td>1689.175370</td>\n",
       "      <td>1689.175370</td>\n",
       "      <td>1689.175370</td>\n",
       "      <td>1702.123736</td>\n",
       "      <td>...</td>\n",
       "      <td>2.254561</td>\n",
       "      <td>2.132944</td>\n",
       "      <td>2.260493</td>\n",
       "      <td>2.560087</td>\n",
       "      <td>2.933837</td>\n",
       "      <td>3.271992</td>\n",
       "      <td>3.455900</td>\n",
       "      <td>3.402507</td>\n",
       "      <td>3.082150</td>\n",
       "      <td>2.503727</td>\n",
       "      <td>1.688003</td>\n",
       "      <td>0.599382</td>\n",
       "      <td>-0.842226</td>\n",
       "      <td>-2.773268</td>\n",
       "      <td>-5.264935</td>\n",
       "      <td>-8.257902</td>\n",
       "      <td>-11.491136</td>\n",
       "      <td>-14.561226</td>\n",
       "      <td>-17.035096</td>\n",
       "      <td>-18.601286</td>\n",
       "      <td>-19.179709</td>\n",
       "      <td>-18.897914</td>\n",
       "      <td>-17.981336</td>\n",
       "      <td>-16.655413</td>\n",
       "      <td>-15.083290</td>\n",
       "      <td>-13.395482</td>\n",
       "      <td>-11.746236</td>\n",
       "      <td>-10.349122</td>\n",
       "      <td>-9.402882</td>\n",
       "      <td>-9.014301</td>\n",
       "      <td>-9.091424</td>\n",
       "      <td>-9.373219</td>\n",
       "      <td>-9.536364</td>\n",
       "      <td>-9.340590</td>\n",
       "      <td>-8.732505</td>\n",
       "      <td>-7.821860</td>\n",
       "      <td>-6.789598</td>\n",
       "      <td>-5.745471</td>\n",
       "      <td>-4.698377</td>\n",
       "      <td>-3.594924</td>\n",
       "      <td>-2.381720</td>\n",
       "      <td>-1.082493</td>\n",
       "      <td>0.184104</td>\n",
       "      <td>1.260860</td>\n",
       "      <td>2.029124</td>\n",
       "      <td>2.438470</td>\n",
       "      <td>2.521525</td>\n",
       "      <td>2.337616</td>\n",
       "      <td>1.949035</td>\n",
       "      <td>1.379511</td>\n",
       "      <td>0.649808</td>\n",
       "      <td>-0.183714</td>\n",
       "      <td>-0.996472</td>\n",
       "      <td>-1.595658</td>\n",
       "      <td>-1.785500</td>\n",
       "      <td>-1.414716</td>\n",
       "      <td>-0.444745</td>\n",
       "      <td>1.047289</td>\n",
       "      <td>2.874511</td>\n",
       "      <td>4.802587</td>\n",
       "      <td>6.582349</td>\n",
       "      <td>8.015058</td>\n",
       "      <td>8.964264</td>\n",
       "      <td>9.382509</td>\n",
       "      <td>9.278689</td>\n",
       "      <td>8.715098</td>\n",
       "      <td>7.786655</td>\n",
       "      <td>6.600147</td>\n",
       "      <td>5.280157</td>\n",
       "      <td>3.945335</td>\n",
       "      <td>2.711366</td>\n",
       "      <td>1.646475</td>\n",
       "      <td>0.762527</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>-0.637553</td>\n",
       "      <td>-1.260470</td>\n",
       "      <td>-1.838892</td>\n",
       "      <td>-2.319428</td>\n",
       "      <td>-2.657583</td>\n",
       "      <td>-2.841492</td>\n",
       "      <td>-2.894885</td>\n",
       "      <td>-2.868188</td>\n",
       "      <td>-2.740639</td>\n",
       "      <td>-2.381720</td>\n",
       "      <td>-1.563029</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>2.521525</td>\n",
       "      <td>6.060286</td>\n",
       "      <td>10.462231</td>\n",
       "      <td>15.377341</td>\n",
       "      <td>20.363642</td>\n",
       "      <td>24.967293</td>\n",
       "      <td>28.829377</td>\n",
       "      <td>31.727424</td>\n",
       "      <td>33.545747</td>\n",
       "      <td>34.207226</td>\n",
       "      <td>33.673297</td>\n",
       "      <td>31.946928</td>\n",
       "      <td>29.128971</td>\n",
       "      <td>25.468593</td>\n",
       "      <td>21.327680</td>\n",
       "      <td>17.091845</td>\n",
       "      <td>13.063650</td>\n",
       "      <td>9.355812</td>\n",
       "      <td>5.926804</td>\n",
       "      <td>2.655007</td>\n",
       "      <td>-0.533733</td>\n",
       "      <td>-3.633486</td>\n",
       "      <td>-6.567127</td>\n",
       "      <td>-9.275332</td>\n",
       "      <td>-11.764033</td>\n",
       "      <td>-14.119252</td>\n",
       "      <td>-16.400314</td>\n",
       "      <td>-18.610185</td>\n",
       "      <td>-20.615384</td>\n",
       "      <td>-22.226069</td>\n",
       "      <td>-23.276129</td>\n",
       "      <td>-23.697339</td>\n",
       "      <td>-23.507498</td>\n",
       "      <td>-22.789660</td>\n",
       "      <td>-21.600186</td>\n",
       "      <td>-19.945007</td>\n",
       "      <td>-17.791495</td>\n",
       "      <td>-15.107020</td>\n",
       "      <td>-11.947942</td>\n",
       "      <td>-8.489271</td>\n",
       "      <td>-4.983139</td>\n",
       "      <td>-1.735073</td>\n",
       "      <td>0.973132</td>\n",
       "      <td>2.951634</td>\n",
       "      <td>4.108480</td>\n",
       "      <td>4.476297</td>\n",
       "      <td>4.203400</td>\n",
       "      <td>3.538956</td>\n",
       "      <td>2.779591</td>\n",
       "      <td>2.168539</td>\n",
       "      <td>1.824452</td>\n",
       "      <td>1.664273</td>\n",
       "      <td>1.456634</td>\n",
       "      <td>0.910840</td>\n",
       "      <td>-0.133287</td>\n",
       "      <td>-1.619388</td>\n",
       "      <td>-3.259736</td>\n",
       "      <td>-4.653883</td>\n",
       "      <td>-5.472574</td>\n",
       "      <td>-5.611988</td>\n",
       "      <td>-5.214508</td>\n",
       "      <td>-4.588625</td>\n",
       "      <td>-4.025034</td>\n",
       "      <td>-3.654250</td>\n",
       "      <td>-3.413982</td>\n",
       "      <td>-3.078793</td>\n",
       "      <td>-2.396551</td>\n",
       "      <td>-1.174448</td>\n",
       "      <td>0.617180</td>\n",
       "      <td>2.883410</td>\n",
       "      <td>5.419571</td>\n",
       "      <td>7.982429</td>\n",
       "      <td>10.322816</td>\n",
       "      <td>12.206398</td>\n",
       "      <td>13.449265</td>\n",
       "      <td>13.926835</td>\n",
       "      <td>13.582747</td>\n",
       "      <td>12.494126</td>\n",
       "      <td>10.835981</td>\n",
       "      <td>8.860445</td>\n",
       "      <td>6.828550</td>\n",
       "      <td>4.930137</td>\n",
       "      <td>3.239363</td>\n",
       "      <td>1.741396</td>\n",
       "      <td>0.368013</td>\n",
       "      <td>-0.928247</td>\n",
       "      <td>-2.150351</td>\n",
       "      <td>-3.262702</td>\n",
       "      <td>-4.259369</td>\n",
       "      <td>-5.190778</td>\n",
       "      <td>-6.142951</td>\n",
       "      <td>-7.213774</td>\n",
       "      <td>-8.462574</td>\n",
       "      <td>-9.892317</td>\n",
       "      <td>-11.437743</td>\n",
       "      <td>-12.968339</td>\n",
       "      <td>-14.320958</td>\n",
       "      <td>-15.299828</td>\n",
       "      <td>-15.726970</td>\n",
       "      <td>-15.468905</td>\n",
       "      <td>-14.484103</td>\n",
       "      <td>-12.873418</td>\n",
       "      <td>-10.823725</td>\n",
       "      <td>-8.557495</td>\n",
       "      <td>-6.252703</td>\n",
       "      <td>-4.016135</td>\n",
       "      <td>-1.886353</td>\n",
       "      <td>0.104015</td>\n",
       "      <td>1.865979</td>\n",
       "      <td>3.239363</td>\n",
       "      <td>4.034323</td>\n",
       "      <td>4.102547</td>\n",
       "      <td>3.387676</td>\n",
       "      <td>1.957934</td>\n",
       "      <td>-0.035400</td>\n",
       "      <td>-2.363922</td>\n",
       "      <td>-4.799230</td>\n",
       "      <td>-7.115887</td>\n",
       "      <td>-9.115154</td>\n",
       "      <td>-10.645749</td>\n",
       "      <td>-11.657247</td>\n",
       "      <td>-12.206007</td>\n",
       "      <td>-12.475938</td>\n",
       "      <td>-12.713240</td>\n",
       "      <td>-13.152248</td>\n",
       "      <td>-13.908647</td>\n",
       "      <td>-14.940909</td>\n",
       "      <td>-16.068092</td>\n",
       "      <td>-17.049927</td>\n",
       "      <td>-17.649114</td>\n",
       "      <td>-17.705473</td>\n",
       "      <td>-17.135949</td>\n",
       "      <td>-15.922744</td>\n",
       "      <td>-14.083657</td>\n",
       "      <td>-11.680978</td>\n",
       "      <td>-8.824459</td>\n",
       "      <td>-5.677246</td>\n",
       "      <td>-2.470708</td>\n",
       "      <td>0.563787</td>\n",
       "      <td>3.197835</td>\n",
       "      <td>5.262359</td>\n",
       "      <td>6.680236</td>\n",
       "      <td>7.463332</td>\n",
       "      <td>7.718431</td>\n",
       "      <td>7.584949</td>\n",
       "      <td>7.190435</td>\n",
       "      <td>6.588282</td>\n",
       "      <td>5.775524</td>\n",
       "      <td>4.725464</td>\n",
       "      <td>3.464799</td>\n",
       "      <td>2.103281</td>\n",
       "      <td>0.851515</td>\n",
       "      <td>-0.082860</td>\n",
       "      <td>-0.587126</td>\n",
       "      <td>-0.723575</td>\n",
       "      <td>-0.711710</td>\n",
       "      <td>-0.815529</td>\n",
       "      <td>-1.254537</td>\n",
       "      <td>-2.085093</td>\n",
       "      <td>-3.218208</td>\n",
       "      <td>-4.493704</td>\n",
       "      <td>-5.736572</td>\n",
       "      <td>-6.810362</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16353</th>\n",
       "      <td>6</td>\n",
       "      <td>28.103361</td>\n",
       "      <td>35.282196</td>\n",
       "      <td>47.029379</td>\n",
       "      <td>70.523745</td>\n",
       "      <td>77.547103</td>\n",
       "      <td>49.484387</td>\n",
       "      <td>14.569084</td>\n",
       "      <td>55.847477</td>\n",
       "      <td>21.421672</td>\n",
       "      <td>32.516233</td>\n",
       "      <td>6.085007</td>\n",
       "      <td>-6.641171</td>\n",
       "      <td>-3.385711</td>\n",
       "      <td>-24.922214</td>\n",
       "      <td>5.587863</td>\n",
       "      <td>13.419319</td>\n",
       "      <td>-19.048622</td>\n",
       "      <td>36.097940</td>\n",
       "      <td>15.377183</td>\n",
       "      <td>45.234639</td>\n",
       "      <td>12.114076</td>\n",
       "      <td>16.845676</td>\n",
       "      <td>33.324332</td>\n",
       "      <td>50.129362</td>\n",
       "      <td>22.719268</td>\n",
       "      <td>-6.975065</td>\n",
       "      <td>-1.264597</td>\n",
       "      <td>-12.196035</td>\n",
       "      <td>36.261191</td>\n",
       "      <td>-16.275014</td>\n",
       "      <td>-20.027491</td>\n",
       "      <td>-24.432716</td>\n",
       "      <td>-42.542989</td>\n",
       "      <td>-41.727244</td>\n",
       "      <td>-10.890793</td>\n",
       "      <td>15.703557</td>\n",
       "      <td>21.577149</td>\n",
       "      <td>66.608018</td>\n",
       "      <td>44.426540</td>\n",
       "      <td>-4.520057</td>\n",
       "      <td>74.928971</td>\n",
       "      <td>90.599528</td>\n",
       "      <td>64.168430</td>\n",
       "      <td>61.231570</td>\n",
       "      <td>60.252575</td>\n",
       "      <td>93.536388</td>\n",
       "      <td>101.857341</td>\n",
       "      <td>102.509963</td>\n",
       "      <td>0.537663</td>\n",
       "      <td>5.432386</td>\n",
       "      <td>60.742072</td>\n",
       "      <td>6.900752</td>\n",
       "      <td>8.042871</td>\n",
       "      <td>80.647086</td>\n",
       "      <td>46.221281</td>\n",
       "      <td>40.673936</td>\n",
       "      <td>31.047740</td>\n",
       "      <td>64.331553</td>\n",
       "      <td>117.030754</td>\n",
       "      <td>117.846627</td>\n",
       "      <td>66.289417</td>\n",
       "      <td>99.573103</td>\n",
       "      <td>99.409980</td>\n",
       "      <td>61.068447</td>\n",
       "      <td>1.516658</td>\n",
       "      <td>-16.430491</td>\n",
       "      <td>-40.748248</td>\n",
       "      <td>-40.422002</td>\n",
       "      <td>-3.548835</td>\n",
       "      <td>-21.495857</td>\n",
       "      <td>4.608995</td>\n",
       "      <td>12.929948</td>\n",
       "      <td>-6.322443</td>\n",
       "      <td>-11.053916</td>\n",
       "      <td>-21.169610</td>\n",
       "      <td>-20.516988</td>\n",
       "      <td>-23.453848</td>\n",
       "      <td>-24.269593</td>\n",
       "      <td>-24.759091</td>\n",
       "      <td>6.240485</td>\n",
       "      <td>57.471320</td>\n",
       "      <td>58.613439</td>\n",
       "      <td>-3.548835</td>\n",
       "      <td>4.608995</td>\n",
       "      <td>43.440026</td>\n",
       "      <td>53.555592</td>\n",
       "      <td>79.497320</td>\n",
       "      <td>19.790055</td>\n",
       "      <td>-8.599035</td>\n",
       "      <td>-50.700819</td>\n",
       "      <td>-57.390155</td>\n",
       "      <td>-52.658683</td>\n",
       "      <td>-78.763534</td>\n",
       "      <td>-77.375328</td>\n",
       "      <td>9.097057</td>\n",
       "      <td>52.576724</td>\n",
       "      <td>3.793123</td>\n",
       "      <td>8.198348</td>\n",
       "      <td>21.577149</td>\n",
       "      <td>-27.043201</td>\n",
       "      <td>-14.806520</td>\n",
       "      <td>-12.032912</td>\n",
       "      <td>-3.711958</td>\n",
       "      <td>44.255770</td>\n",
       "      <td>47.518877</td>\n",
       "      <td>48.824119</td>\n",
       "      <td>74.439601</td>\n",
       "      <td>-6.314797</td>\n",
       "      <td>79.497320</td>\n",
       "      <td>104.141452</td>\n",
       "      <td>95.820499</td>\n",
       "      <td>138.404134</td>\n",
       "      <td>43.627363</td>\n",
       "      <td>36.105587</td>\n",
       "      <td>21.911042</td>\n",
       "      <td>17.179569</td>\n",
       "      <td>7.553374</td>\n",
       "      <td>26.968889</td>\n",
       "      <td>102.183588</td>\n",
       "      <td>82.768073</td>\n",
       "      <td>98.594235</td>\n",
       "      <td>94.515256</td>\n",
       "      <td>80.810209</td>\n",
       "      <td>86.520677</td>\n",
       "      <td>72.815630</td>\n",
       "      <td>25.337399</td>\n",
       "      <td>-4.846431</td>\n",
       "      <td>-17.906503</td>\n",
       "      <td>45.234639</td>\n",
       "      <td>31.855966</td>\n",
       "      <td>-8.117184</td>\n",
       "      <td>-1.754094</td>\n",
       "      <td>5.914238</td>\n",
       "      <td>-21.006486</td>\n",
       "      <td>-50.537695</td>\n",
       "      <td>-27.532699</td>\n",
       "      <td>-19.048622</td>\n",
       "      <td>78.192077</td>\n",
       "      <td>96.962618</td>\n",
       "      <td>78.852345</td>\n",
       "      <td>97.125741</td>\n",
       "      <td>69.389400</td>\n",
       "      <td>80.320839</td>\n",
       "      <td>33.495102</td>\n",
       "      <td>98.757358</td>\n",
       "      <td>74.120873</td>\n",
       "      <td>102.020465</td>\n",
       "      <td>93.699511</td>\n",
       "      <td>99.409980</td>\n",
       "      <td>100.225724</td>\n",
       "      <td>43.937042</td>\n",
       "      <td>9.021740</td>\n",
       "      <td>91.741648</td>\n",
       "      <td>35.126719</td>\n",
       "      <td>48.342268</td>\n",
       "      <td>41.163434</td>\n",
       "      <td>18.811186</td>\n",
       "      <td>40.837059</td>\n",
       "      <td>29.742497</td>\n",
       "      <td>35.126719</td>\n",
       "      <td>53.073740</td>\n",
       "      <td>49.647511</td>\n",
       "      <td>4.127144</td>\n",
       "      <td>8.695493</td>\n",
       "      <td>34.310974</td>\n",
       "      <td>8.695493</td>\n",
       "      <td>-8.272661</td>\n",
       "      <td>-10.401295</td>\n",
       "      <td>22.066646</td>\n",
       "      <td>50.292485</td>\n",
       "      <td>-8.606682</td>\n",
       "      <td>7.219480</td>\n",
       "      <td>27.287617</td>\n",
       "      <td>8.524723</td>\n",
       "      <td>52.413473</td>\n",
       "      <td>25.656000</td>\n",
       "      <td>32.182213</td>\n",
       "      <td>50.292485</td>\n",
       "      <td>34.466451</td>\n",
       "      <td>63.018664</td>\n",
       "      <td>77.220856</td>\n",
       "      <td>80.483962</td>\n",
       "      <td>83.420694</td>\n",
       "      <td>53.073740</td>\n",
       "      <td>50.463255</td>\n",
       "      <td>57.968464</td>\n",
       "      <td>46.221281</td>\n",
       "      <td>41.489681</td>\n",
       "      <td>-14.146253</td>\n",
       "      <td>-6.159320</td>\n",
       "      <td>1.509012</td>\n",
       "      <td>3.793123</td>\n",
       "      <td>23.208638</td>\n",
       "      <td>29.408604</td>\n",
       "      <td>12.277327</td>\n",
       "      <td>21.903396</td>\n",
       "      <td>64.160783</td>\n",
       "      <td>40.673936</td>\n",
       "      <td>35.126719</td>\n",
       "      <td>31.863612</td>\n",
       "      <td>-15.940993</td>\n",
       "      <td>-13.501278</td>\n",
       "      <td>-17.253882</td>\n",
       "      <td>13.419319</td>\n",
       "      <td>-21.332733</td>\n",
       "      <td>-28.674818</td>\n",
       "      <td>-15.622392</td>\n",
       "      <td>1.509012</td>\n",
       "      <td>25.819251</td>\n",
       "      <td>32.508587</td>\n",
       "      <td>31.692715</td>\n",
       "      <td>-11.869788</td>\n",
       "      <td>-64.568989</td>\n",
       "      <td>27.777115</td>\n",
       "      <td>18.150919</td>\n",
       "      <td>40.340043</td>\n",
       "      <td>56.818698</td>\n",
       "      <td>35.445319</td>\n",
       "      <td>-2.569839</td>\n",
       "      <td>9.014221</td>\n",
       "      <td>-2.733090</td>\n",
       "      <td>-18.722248</td>\n",
       "      <td>14.724561</td>\n",
       "      <td>31.529591</td>\n",
       "      <td>-59.511142</td>\n",
       "      <td>25.329753</td>\n",
       "      <td>1.998510</td>\n",
       "      <td>41.645285</td>\n",
       "      <td>11.950953</td>\n",
       "      <td>34.140077</td>\n",
       "      <td>61.060800</td>\n",
       "      <td>11.142854</td>\n",
       "      <td>-8.925282</td>\n",
       "      <td>49.313617</td>\n",
       "      <td>13.419319</td>\n",
       "      <td>31.529591</td>\n",
       "      <td>30.713847</td>\n",
       "      <td>1.019514</td>\n",
       "      <td>-6.811941</td>\n",
       "      <td>-35.364155</td>\n",
       "      <td>530.440199</td>\n",
       "      <td>530.440199</td>\n",
       "      <td>530.440199</td>\n",
       "      <td>530.440199</td>\n",
       "      <td>524.355934</td>\n",
       "      <td>524.355934</td>\n",
       "      <td>524.355934</td>\n",
       "      <td>524.355934</td>\n",
       "      <td>530.440199</td>\n",
       "      <td>...</td>\n",
       "      <td>29.748289</td>\n",
       "      <td>41.216813</td>\n",
       "      <td>52.225963</td>\n",
       "      <td>60.387782</td>\n",
       "      <td>63.825171</td>\n",
       "      <td>61.496617</td>\n",
       "      <td>53.366479</td>\n",
       "      <td>40.325785</td>\n",
       "      <td>24.010067</td>\n",
       "      <td>6.502350</td>\n",
       "      <td>-10.023254</td>\n",
       "      <td>-23.681728</td>\n",
       "      <td>-33.372156</td>\n",
       "      <td>-38.845050</td>\n",
       "      <td>-40.464741</td>\n",
       "      <td>-39.027216</td>\n",
       "      <td>-35.375980</td>\n",
       "      <td>-30.192175</td>\n",
       "      <td>-23.998538</td>\n",
       "      <td>-17.329686</td>\n",
       "      <td>-10.866761</td>\n",
       "      <td>-5.417628</td>\n",
       "      <td>-1.726791</td>\n",
       "      <td>-0.186302</td>\n",
       "      <td>-0.649637</td>\n",
       "      <td>-2.518816</td>\n",
       "      <td>-4.898852</td>\n",
       "      <td>-7.017519</td>\n",
       "      <td>-8.498606</td>\n",
       "      <td>-9.571800</td>\n",
       "      <td>-10.973685</td>\n",
       "      <td>-13.634889</td>\n",
       "      <td>-18.212795</td>\n",
       "      <td>-24.754922</td>\n",
       "      <td>-32.599932</td>\n",
       "      <td>-40.642947</td>\n",
       "      <td>-47.707811</td>\n",
       "      <td>-52.919337</td>\n",
       "      <td>-55.731026</td>\n",
       "      <td>-55.818149</td>\n",
       "      <td>-52.994579</td>\n",
       "      <td>-47.363280</td>\n",
       "      <td>-39.383627</td>\n",
       "      <td>-29.934767</td>\n",
       "      <td>-20.117615</td>\n",
       "      <td>-11.025166</td>\n",
       "      <td>-3.322722</td>\n",
       "      <td>2.965958</td>\n",
       "      <td>8.466573</td>\n",
       "      <td>14.006789</td>\n",
       "      <td>20.196465</td>\n",
       "      <td>27.162327</td>\n",
       "      <td>34.381636</td>\n",
       "      <td>40.860402</td>\n",
       "      <td>45.533350</td>\n",
       "      <td>47.616376</td>\n",
       "      <td>46.903554</td>\n",
       "      <td>43.854257</td>\n",
       "      <td>39.442677</td>\n",
       "      <td>34.892492</td>\n",
       "      <td>31.344219</td>\n",
       "      <td>29.467120</td>\n",
       "      <td>29.277034</td>\n",
       "      <td>30.140341</td>\n",
       "      <td>31.047210</td>\n",
       "      <td>31.134333</td>\n",
       "      <td>30.104700</td>\n",
       "      <td>28.405806</td>\n",
       "      <td>27.043523</td>\n",
       "      <td>27.031643</td>\n",
       "      <td>28.789938</td>\n",
       "      <td>31.870916</td>\n",
       "      <td>35.130100</td>\n",
       "      <td>37.256687</td>\n",
       "      <td>37.300249</td>\n",
       "      <td>34.888532</td>\n",
       "      <td>30.088860</td>\n",
       "      <td>23.170520</td>\n",
       "      <td>14.489924</td>\n",
       "      <td>4.486646</td>\n",
       "      <td>-6.055208</td>\n",
       "      <td>-16.014925</td>\n",
       "      <td>-24.176744</td>\n",
       "      <td>-29.558555</td>\n",
       "      <td>-31.716823</td>\n",
       "      <td>-30.944599</td>\n",
       "      <td>-28.081428</td>\n",
       "      <td>-24.117342</td>\n",
       "      <td>-19.844366</td>\n",
       "      <td>-15.666434</td>\n",
       "      <td>-11.829072</td>\n",
       "      <td>-8.510486</td>\n",
       "      <td>-5.948285</td>\n",
       "      <td>-4.451357</td>\n",
       "      <td>-4.296913</td>\n",
       "      <td>-5.449309</td>\n",
       "      <td>-7.437292</td>\n",
       "      <td>-9.302512</td>\n",
       "      <td>-10.023254</td>\n",
       "      <td>-9.060944</td>\n",
       "      <td>-6.621506</td>\n",
       "      <td>-3.603891</td>\n",
       "      <td>-1.283257</td>\n",
       "      <td>-0.784281</td>\n",
       "      <td>-2.633660</td>\n",
       "      <td>-6.498742</td>\n",
       "      <td>-11.108329</td>\n",
       "      <td>-14.834807</td>\n",
       "      <td>-16.723787</td>\n",
       "      <td>-16.850511</td>\n",
       "      <td>-15.995124</td>\n",
       "      <td>-14.985292</td>\n",
       "      <td>-14.363552</td>\n",
       "      <td>-14.363552</td>\n",
       "      <td>-14.989252</td>\n",
       "      <td>-16.189170</td>\n",
       "      <td>-17.147521</td>\n",
       "      <td>-16.917833</td>\n",
       "      <td>-14.553638</td>\n",
       "      <td>-9.413395</td>\n",
       "      <td>-1.560466</td>\n",
       "      <td>8.110162</td>\n",
       "      <td>18.145120</td>\n",
       "      <td>26.013890</td>\n",
       "      <td>29.791850</td>\n",
       "      <td>28.892902</td>\n",
       "      <td>23.875422</td>\n",
       "      <td>15.788845</td>\n",
       "      <td>5.603402</td>\n",
       "      <td>-5.873043</td>\n",
       "      <td>-17.994988</td>\n",
       "      <td>-29.685279</td>\n",
       "      <td>-39.288584</td>\n",
       "      <td>-45.086208</td>\n",
       "      <td>-46.183163</td>\n",
       "      <td>-43.054664</td>\n",
       "      <td>-37.328322</td>\n",
       "      <td>-31.114884</td>\n",
       "      <td>-26.342933</td>\n",
       "      <td>-24.133183</td>\n",
       "      <td>-24.727201</td>\n",
       "      <td>-27.578492</td>\n",
       "      <td>-31.130725</td>\n",
       "      <td>-33.871132</td>\n",
       "      <td>-34.397829</td>\n",
       "      <td>-31.859388</td>\n",
       "      <td>-26.303332</td>\n",
       "      <td>-18.739491</td>\n",
       "      <td>-10.680635</td>\n",
       "      <td>-3.283120</td>\n",
       "      <td>3.405532</td>\n",
       "      <td>10.632762</td>\n",
       "      <td>20.208346</td>\n",
       "      <td>33.447046</td>\n",
       "      <td>50.067694</td>\n",
       "      <td>67.868460</td>\n",
       "      <td>83.439674</td>\n",
       "      <td>93.530075</td>\n",
       "      <td>96.484328</td>\n",
       "      <td>93.027139</td>\n",
       "      <td>85.780108</td>\n",
       "      <td>77.927179</td>\n",
       "      <td>71.792944</td>\n",
       "      <td>67.884300</td>\n",
       "      <td>64.957767</td>\n",
       "      <td>60.993681</td>\n",
       "      <td>54.352550</td>\n",
       "      <td>44.563119</td>\n",
       "      <td>32.294650</td>\n",
       "      <td>18.663897</td>\n",
       "      <td>4.415364</td>\n",
       "      <td>-10.248982</td>\n",
       "      <td>-25.416263</td>\n",
       "      <td>-40.912236</td>\n",
       "      <td>-55.794388</td>\n",
       "      <td>-68.534113</td>\n",
       "      <td>-77.448356</td>\n",
       "      <td>-81.329279</td>\n",
       "      <td>-79.879873</td>\n",
       "      <td>-73.805040</td>\n",
       "      <td>-64.415582</td>\n",
       "      <td>-53.327230</td>\n",
       "      <td>-42.120074</td>\n",
       "      <td>-32.085115</td>\n",
       "      <td>-24.473753</td>\n",
       "      <td>-19.939409</td>\n",
       "      <td>-17.975187</td>\n",
       "      <td>-16.676266</td>\n",
       "      <td>-13.116113</td>\n",
       "      <td>-4.475118</td>\n",
       "      <td>10.244669</td>\n",
       "      <td>29.423558</td>\n",
       "      <td>48.828175</td>\n",
       "      <td>62.724256</td>\n",
       "      <td>66.529937</td>\n",
       "      <td>58.966097</td>\n",
       "      <td>42.567216</td>\n",
       "      <td>22.445817</td>\n",
       "      <td>4.062913</td>\n",
       "      <td>-9.243110</td>\n",
       "      <td>-16.997036</td>\n",
       "      <td>-21.060125</td>\n",
       "      <td>-24.469793</td>\n",
       "      <td>-29.328868</td>\n",
       "      <td>-35.775952</td>\n",
       "      <td>-42.369562</td>\n",
       "      <td>-47.260317</td>\n",
       "      <td>-49.216619</td>\n",
       "      <td>-48.582999</td>\n",
       "      <td>-46.915786</td>\n",
       "      <td>-44.310023</td>\n",
       "      <td>-41.276567</td>\n",
       "      <td>-38.183709</td>\n",
       "      <td>-34.710679</td>\n",
       "      <td>-29.285306</td>\n",
       "      <td>-20.477986</td>\n",
       "      <td>-5.643355</td>\n",
       "      <td>16.085855</td>\n",
       "      <td>43.414683</td>\n",
       "      <td>73.507679</td>\n",
       "      <td>101.977023</td>\n",
       "      <td>124.415096</td>\n",
       "      <td>137.590434</td>\n",
       "      <td>140.073433</td>\n",
       "      <td>132.121500</td>\n",
       "      <td>115.469171</td>\n",
       "      <td>92.547963</td>\n",
       "      <td>66.205207</td>\n",
       "      <td>39.054584</td>\n",
       "      <td>12.505901</td>\n",
       "      <td>-12.553775</td>\n",
       "      <td>-35.724471</td>\n",
       "      <td>-57.065589</td>\n",
       "      <td>-76.794935</td>\n",
       "      <td>-94.983793</td>\n",
       "      <td>-111.319312</td>\n",
       "      <td>-124.910463</td>\n",
       "      <td>-134.549410</td>\n",
       "      <td>-139.547088</td>\n",
       "      <td>-139.186717</td>\n",
       "      <td>-132.985160</td>\n",
       "      <td>-120.966178</td>\n",
       "      <td>-103.604987</td>\n",
       "      <td>-81.654010</td>\n",
       "      <td>-56.186441</td>\n",
       "      <td>-28.596244</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>26.877198</td>\n",
       "      <td>46.408538</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16354</th>\n",
       "      <td>1</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.520024</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.520024</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-43.933027</td>\n",
       "      <td>-45.699239</td>\n",
       "      <td>-45.831679</td>\n",
       "      <td>-45.831679</td>\n",
       "      <td>-45.787532</td>\n",
       "      <td>-45.566765</td>\n",
       "      <td>-44.992754</td>\n",
       "      <td>-44.507038</td>\n",
       "      <td>-44.286270</td>\n",
       "      <td>-45.345997</td>\n",
       "      <td>-44.595331</td>\n",
       "      <td>-43.844700</td>\n",
       "      <td>-44.109649</td>\n",
       "      <td>-44.551184</td>\n",
       "      <td>-43.800553</td>\n",
       "      <td>-43.800553</td>\n",
       "      <td>-43.623932</td>\n",
       "      <td>-43.712225</td>\n",
       "      <td>-43.712225</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.564205</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.564205</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.564205</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.652498</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.784973</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.608352</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.564205</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-42.696645</td>\n",
       "      <td>-42.740826</td>\n",
       "      <td>-43.226508</td>\n",
       "      <td>-44.109649</td>\n",
       "      <td>-45.125194</td>\n",
       "      <td>-44.551184</td>\n",
       "      <td>-43.977174</td>\n",
       "      <td>-44.153795</td>\n",
       "      <td>-44.330416</td>\n",
       "      <td>-44.242089</td>\n",
       "      <td>-43.447311</td>\n",
       "      <td>-43.844700</td>\n",
       "      <td>-43.756406</td>\n",
       "      <td>-43.712225</td>\n",
       "      <td>-43.668078</td>\n",
       "      <td>-43.623932</td>\n",
       "      <td>-43.491457</td>\n",
       "      <td>-43.756406</td>\n",
       "      <td>-43.668078</td>\n",
       "      <td>-44.374563</td>\n",
       "      <td>-45.743386</td>\n",
       "      <td>-45.743386</td>\n",
       "      <td>-45.743386</td>\n",
       "      <td>-45.743386</td>\n",
       "      <td>-45.787532</td>\n",
       "      <td>-45.699239</td>\n",
       "      <td>-45.787532</td>\n",
       "      <td>-45.610911</td>\n",
       "      <td>-45.566765</td>\n",
       "      <td>-45.566765</td>\n",
       "      <td>-45.655058</td>\n",
       "      <td>-45.610911</td>\n",
       "      <td>-2104.362238</td>\n",
       "      <td>-2104.362238</td>\n",
       "      <td>-2104.362238</td>\n",
       "      <td>-2104.362238</td>\n",
       "      <td>-2089.634688</td>\n",
       "      <td>-2089.634688</td>\n",
       "      <td>-2089.634688</td>\n",
       "      <td>-2089.634688</td>\n",
       "      <td>-2089.634688</td>\n",
       "      <td>...</td>\n",
       "      <td>-63.431018</td>\n",
       "      <td>-63.588125</td>\n",
       "      <td>-63.544670</td>\n",
       "      <td>-63.424333</td>\n",
       "      <td>-63.347451</td>\n",
       "      <td>-63.431018</td>\n",
       "      <td>-63.745233</td>\n",
       "      <td>-64.283408</td>\n",
       "      <td>-65.005433</td>\n",
       "      <td>-65.854480</td>\n",
       "      <td>-66.763696</td>\n",
       "      <td>-67.606059</td>\n",
       "      <td>-68.264572</td>\n",
       "      <td>-68.585471</td>\n",
       "      <td>-68.451763</td>\n",
       "      <td>-67.813306</td>\n",
       "      <td>-66.713556</td>\n",
       "      <td>-65.276192</td>\n",
       "      <td>-63.651637</td>\n",
       "      <td>-61.990312</td>\n",
       "      <td>-60.322301</td>\n",
       "      <td>-58.584094</td>\n",
       "      <td>-56.585155</td>\n",
       "      <td>-54.138294</td>\n",
       "      <td>-51.096432</td>\n",
       "      <td>-47.432825</td>\n",
       "      <td>-43.214330</td>\n",
       "      <td>-38.598052</td>\n",
       "      <td>-33.774527</td>\n",
       "      <td>-28.930945</td>\n",
       "      <td>-24.241128</td>\n",
       "      <td>-19.872211</td>\n",
       "      <td>-15.967930</td>\n",
       "      <td>-12.635252</td>\n",
       "      <td>-9.837406</td>\n",
       "      <td>-7.420630</td>\n",
       "      <td>-5.080735</td>\n",
       "      <td>-2.503509</td>\n",
       "      <td>0.545040</td>\n",
       "      <td>4.101679</td>\n",
       "      <td>7.995932</td>\n",
       "      <td>11.926955</td>\n",
       "      <td>15.593904</td>\n",
       "      <td>18.822958</td>\n",
       "      <td>21.607433</td>\n",
       "      <td>24.087721</td>\n",
       "      <td>26.451014</td>\n",
       "      <td>28.861106</td>\n",
       "      <td>31.351422</td>\n",
       "      <td>33.845081</td>\n",
       "      <td>36.154891</td>\n",
       "      <td>38.093661</td>\n",
       "      <td>39.507626</td>\n",
       "      <td>40.306532</td>\n",
       "      <td>40.473668</td>\n",
       "      <td>40.005689</td>\n",
       "      <td>38.919309</td>\n",
       "      <td>37.161046</td>\n",
       "      <td>34.670729</td>\n",
       "      <td>31.418276</td>\n",
       "      <td>27.483911</td>\n",
       "      <td>23.051482</td>\n",
       "      <td>18.438547</td>\n",
       "      <td>13.989405</td>\n",
       "      <td>9.984843</td>\n",
       "      <td>6.615395</td>\n",
       "      <td>3.891089</td>\n",
       "      <td>1.691588</td>\n",
       "      <td>-0.153586</td>\n",
       "      <td>-1.821596</td>\n",
       "      <td>-3.449494</td>\n",
       "      <td>-5.114162</td>\n",
       "      <td>-6.822285</td>\n",
       "      <td>-8.527065</td>\n",
       "      <td>-10.121536</td>\n",
       "      <td>-11.458619</td>\n",
       "      <td>-12.401262</td>\n",
       "      <td>-12.892640</td>\n",
       "      <td>-12.989579</td>\n",
       "      <td>-12.926067</td>\n",
       "      <td>-13.043062</td>\n",
       "      <td>-13.671491</td>\n",
       "      <td>-15.005231</td>\n",
       "      <td>-17.007512</td>\n",
       "      <td>-19.414260</td>\n",
       "      <td>-21.804295</td>\n",
       "      <td>-23.736380</td>\n",
       "      <td>-24.862872</td>\n",
       "      <td>-25.033350</td>\n",
       "      <td>-24.321353</td>\n",
       "      <td>-22.964214</td>\n",
       "      <td>-21.332974</td>\n",
       "      <td>-19.781958</td>\n",
       "      <td>-18.575241</td>\n",
       "      <td>-17.796390</td>\n",
       "      <td>-17.345125</td>\n",
       "      <td>-16.987455</td>\n",
       "      <td>-16.449280</td>\n",
       "      <td>-15.506636</td>\n",
       "      <td>-14.089329</td>\n",
       "      <td>-12.277582</td>\n",
       "      <td>-10.225160</td>\n",
       "      <td>-8.119255</td>\n",
       "      <td>-6.083547</td>\n",
       "      <td>-4.114693</td>\n",
       "      <td>-2.102384</td>\n",
       "      <td>0.097117</td>\n",
       "      <td>2.564034</td>\n",
       "      <td>5.218143</td>\n",
       "      <td>7.792027</td>\n",
       "      <td>9.964786</td>\n",
       "      <td>11.475690</td>\n",
       "      <td>12.291310</td>\n",
       "      <td>12.612210</td>\n",
       "      <td>12.759289</td>\n",
       "      <td>13.060132</td>\n",
       "      <td>13.651792</td>\n",
       "      <td>14.484125</td>\n",
       "      <td>15.336516</td>\n",
       "      <td>15.988343</td>\n",
       "      <td>16.319271</td>\n",
       "      <td>16.346013</td>\n",
       "      <td>16.202276</td>\n",
       "      <td>16.031798</td>\n",
       "      <td>15.974972</td>\n",
       "      <td>16.101995</td>\n",
       "      <td>16.442951</td>\n",
       "      <td>16.984470</td>\n",
       "      <td>17.693124</td>\n",
       "      <td>18.485345</td>\n",
       "      <td>19.287595</td>\n",
       "      <td>20.073131</td>\n",
       "      <td>20.845296</td>\n",
       "      <td>21.744484</td>\n",
       "      <td>22.984628</td>\n",
       "      <td>24.849858</td>\n",
       "      <td>27.530708</td>\n",
       "      <td>31.047236</td>\n",
       "      <td>35.101939</td>\n",
       "      <td>39.163327</td>\n",
       "      <td>42.606314</td>\n",
       "      <td>44.882697</td>\n",
       "      <td>45.758487</td>\n",
       "      <td>45.337306</td>\n",
       "      <td>43.970139</td>\n",
       "      <td>42.094880</td>\n",
       "      <td>40.039116</td>\n",
       "      <td>37.933211</td>\n",
       "      <td>35.787193</td>\n",
       "      <td>33.560951</td>\n",
       "      <td>31.261169</td>\n",
       "      <td>28.958044</td>\n",
       "      <td>26.725116</td>\n",
       "      <td>24.575756</td>\n",
       "      <td>22.463166</td>\n",
       "      <td>20.353918</td>\n",
       "      <td>18.248013</td>\n",
       "      <td>16.245732</td>\n",
       "      <td>14.500839</td>\n",
       "      <td>13.163756</td>\n",
       "      <td>12.294653</td>\n",
       "      <td>11.826674</td>\n",
       "      <td>11.555915</td>\n",
       "      <td>11.261756</td>\n",
       "      <td>10.797120</td>\n",
       "      <td>10.108523</td>\n",
       "      <td>9.279532</td>\n",
       "      <td>8.443855</td>\n",
       "      <td>7.735201</td>\n",
       "      <td>7.243823</td>\n",
       "      <td>7.013177</td>\n",
       "      <td>7.046604</td>\n",
       "      <td>7.340762</td>\n",
       "      <td>7.878938</td>\n",
       "      <td>8.567535</td>\n",
       "      <td>9.286217</td>\n",
       "      <td>9.884561</td>\n",
       "      <td>10.242231</td>\n",
       "      <td>10.359226</td>\n",
       "      <td>10.315771</td>\n",
       "      <td>10.279001</td>\n",
       "      <td>10.372597</td>\n",
       "      <td>10.623299</td>\n",
       "      <td>10.930828</td>\n",
       "      <td>11.158133</td>\n",
       "      <td>11.238357</td>\n",
       "      <td>11.151447</td>\n",
       "      <td>10.980969</td>\n",
       "      <td>10.783749</td>\n",
       "      <td>10.536389</td>\n",
       "      <td>10.111865</td>\n",
       "      <td>9.373127</td>\n",
       "      <td>8.256663</td>\n",
       "      <td>6.849384</td>\n",
       "      <td>5.425391</td>\n",
       "      <td>4.318955</td>\n",
       "      <td>3.830920</td>\n",
       "      <td>4.068252</td>\n",
       "      <td>4.890558</td>\n",
       "      <td>5.943511</td>\n",
       "      <td>6.805929</td>\n",
       "      <td>7.133514</td>\n",
       "      <td>6.805929</td>\n",
       "      <td>5.956881</td>\n",
       "      <td>4.867159</td>\n",
       "      <td>3.850976</td>\n",
       "      <td>3.098867</td>\n",
       "      <td>2.630888</td>\n",
       "      <td>2.370157</td>\n",
       "      <td>2.206365</td>\n",
       "      <td>2.059286</td>\n",
       "      <td>1.858723</td>\n",
       "      <td>1.544509</td>\n",
       "      <td>1.026389</td>\n",
       "      <td>0.234168</td>\n",
       "      <td>-0.838841</td>\n",
       "      <td>-2.099041</td>\n",
       "      <td>-3.362584</td>\n",
       "      <td>-4.425565</td>\n",
       "      <td>-5.134218</td>\n",
       "      <td>-5.391607</td>\n",
       "      <td>-5.167646</td>\n",
       "      <td>-4.479048</td>\n",
       "      <td>-3.379298</td>\n",
       "      <td>-2.015473</td>\n",
       "      <td>-0.624908</td>\n",
       "      <td>0.504927</td>\n",
       "      <td>1.086558</td>\n",
       "      <td>0.976249</td>\n",
       "      <td>0.237511</td>\n",
       "      <td>-0.902352</td>\n",
       "      <td>-2.129125</td>\n",
       "      <td>-3.165364</td>\n",
       "      <td>-3.960929</td>\n",
       "      <td>-4.649526</td>\n",
       "      <td>-5.525315</td>\n",
       "      <td>-6.885797</td>\n",
       "      <td>-8.851308</td>\n",
       "      <td>-11.358338</td>\n",
       "      <td>-14.149498</td>\n",
       "      <td>-16.913916</td>\n",
       "      <td>-19.377490</td>\n",
       "      <td>-21.413199</td>\n",
       "      <td>-23.027726</td>\n",
       "      <td>-24.297954</td>\n",
       "      <td>-25.327508</td>\n",
       "      <td>-26.169870</td>\n",
       "      <td>-26.891894</td>\n",
       "      <td>-27.516981</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16355</th>\n",
       "      <td>5</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.568335</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.357364</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.357364</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.146557</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.568335</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-65.255777</td>\n",
       "      <td>-65.255777</td>\n",
       "      <td>-65.255777</td>\n",
       "      <td>-65.255777</td>\n",
       "      <td>-65.466749</td>\n",
       "      <td>-65.466749</td>\n",
       "      <td>-65.677720</td>\n",
       "      <td>-65.677720</td>\n",
       "      <td>-65.255777</td>\n",
       "      <td>-65.255777</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-65.044971</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.833999</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.412056</td>\n",
       "      <td>-64.623028</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.357364</td>\n",
       "      <td>-63.357364</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-64.201250</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>6819.633542</td>\n",
       "      <td>6819.633542</td>\n",
       "      <td>6819.633542</td>\n",
       "      <td>6819.633542</td>\n",
       "      <td>6761.305782</td>\n",
       "      <td>6761.305782</td>\n",
       "      <td>6761.305782</td>\n",
       "      <td>6761.305782</td>\n",
       "      <td>6761.305782</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.195681</td>\n",
       "      <td>-34.068512</td>\n",
       "      <td>-36.989166</td>\n",
       "      <td>-41.143362</td>\n",
       "      <td>-44.937245</td>\n",
       "      <td>-47.344983</td>\n",
       "      <td>-48.027458</td>\n",
       "      <td>-47.506064</td>\n",
       "      <td>-46.700659</td>\n",
       "      <td>-46.526861</td>\n",
       "      <td>-47.866377</td>\n",
       "      <td>-51.079521</td>\n",
       "      <td>-55.954343</td>\n",
       "      <td>-61.626092</td>\n",
       "      <td>-66.835793</td>\n",
       "      <td>-70.290558</td>\n",
       "      <td>-71.125636</td>\n",
       "      <td>-69.268965</td>\n",
       "      <td>-65.445409</td>\n",
       "      <td>-61.075025</td>\n",
       "      <td>-57.755907</td>\n",
       "      <td>-56.450303</td>\n",
       "      <td>-57.319293</td>\n",
       "      <td>-59.451498</td>\n",
       "      <td>-61.185239</td>\n",
       "      <td>-60.875793</td>\n",
       "      <td>-57.213319</td>\n",
       "      <td>-49.858696</td>\n",
       "      <td>-39.371470</td>\n",
       "      <td>-27.370931</td>\n",
       "      <td>-15.773093</td>\n",
       "      <td>-6.850050</td>\n",
       "      <td>-2.895086</td>\n",
       "      <td>-5.459666</td>\n",
       "      <td>-14.891386</td>\n",
       "      <td>-29.261514</td>\n",
       "      <td>-44.466719</td>\n",
       "      <td>-55.577074</td>\n",
       "      <td>-58.578269</td>\n",
       "      <td>-52.215566</td>\n",
       "      <td>-38.680517</td>\n",
       "      <td>-22.513065</td>\n",
       "      <td>-8.571074</td>\n",
       "      <td>-0.148229</td>\n",
       "      <td>1.933108</td>\n",
       "      <td>-1.161345</td>\n",
       "      <td>-7.651216</td>\n",
       "      <td>-16.082539</td>\n",
       "      <td>-25.726208</td>\n",
       "      <td>-36.192239</td>\n",
       "      <td>-46.738810</td>\n",
       "      <td>-55.962821</td>\n",
       "      <td>-62.499321</td>\n",
       "      <td>-65.318240</td>\n",
       "      <td>-64.398382</td>\n",
       "      <td>-60.913944</td>\n",
       "      <td>-56.564755</td>\n",
       "      <td>-52.872608</td>\n",
       "      <td>-50.994741</td>\n",
       "      <td>-51.168539</td>\n",
       "      <td>-52.508056</td>\n",
       "      <td>-54.186690</td>\n",
       "      <td>-55.543162</td>\n",
       "      <td>-56.344329</td>\n",
       "      <td>-56.958980</td>\n",
       "      <td>-57.861882</td>\n",
       "      <td>-59.209876</td>\n",
       "      <td>-61.062308</td>\n",
       "      <td>-63.164840</td>\n",
       "      <td>-65.059662</td>\n",
       "      <td>-66.386462</td>\n",
       "      <td>-66.852749</td>\n",
       "      <td>-66.428851</td>\n",
       "      <td>-65.182593</td>\n",
       "      <td>-63.109734</td>\n",
       "      <td>-60.057671</td>\n",
       "      <td>-55.869563</td>\n",
       "      <td>-50.202053</td>\n",
       "      <td>-42.961883</td>\n",
       "      <td>-34.191442</td>\n",
       "      <td>-24.094202</td>\n",
       "      <td>-13.229708</td>\n",
       "      <td>-2.263478</td>\n",
       "      <td>7.846479</td>\n",
       "      <td>16.290518</td>\n",
       "      <td>22.445510</td>\n",
       "      <td>26.188526</td>\n",
       "      <td>27.595866</td>\n",
       "      <td>26.930347</td>\n",
       "      <td>24.637061</td>\n",
       "      <td>20.995781</td>\n",
       "      <td>16.455838</td>\n",
       "      <td>11.402979</td>\n",
       "      <td>6.146649</td>\n",
       "      <td>1.042923</td>\n",
       "      <td>-3.641145</td>\n",
       "      <td>-7.782624</td>\n",
       "      <td>-11.428143</td>\n",
       "      <td>-14.548029</td>\n",
       "      <td>-17.307603</td>\n",
       "      <td>-19.863705</td>\n",
       "      <td>-22.279921</td>\n",
       "      <td>-24.891130</td>\n",
       "      <td>-27.731243</td>\n",
       "      <td>-30.834174</td>\n",
       "      <td>-34.212637</td>\n",
       "      <td>-37.951414</td>\n",
       "      <td>-41.855510</td>\n",
       "      <td>-45.865581</td>\n",
       "      <td>-49.765438</td>\n",
       "      <td>-53.194770</td>\n",
       "      <td>-55.950104</td>\n",
       "      <td>-57.671128</td>\n",
       "      <td>-58.167088</td>\n",
       "      <td>-57.264186</td>\n",
       "      <td>-55.081114</td>\n",
       "      <td>-51.677216</td>\n",
       "      <td>-47.294116</td>\n",
       "      <td>-42.139521</td>\n",
       "      <td>-36.395709</td>\n",
       "      <td>-30.287346</td>\n",
       "      <td>-23.869537</td>\n",
       "      <td>-17.320319</td>\n",
       "      <td>-10.783819</td>\n",
       "      <td>-4.353293</td>\n",
       "      <td>1.784743</td>\n",
       "      <td>7.490405</td>\n",
       "      <td>12.742495</td>\n",
       "      <td>17.600361</td>\n",
       "      <td>22.208128</td>\n",
       "      <td>26.904913</td>\n",
       "      <td>31.864514</td>\n",
       "      <td>37.048781</td>\n",
       "      <td>42.381412</td>\n",
       "      <td>47.561441</td>\n",
       "      <td>52.258226</td>\n",
       "      <td>56.247102</td>\n",
       "      <td>59.566219</td>\n",
       "      <td>62.300359</td>\n",
       "      <td>64.839505</td>\n",
       "      <td>67.446475</td>\n",
       "      <td>70.150941</td>\n",
       "      <td>72.779106</td>\n",
       "      <td>74.979135</td>\n",
       "      <td>76.344085</td>\n",
       "      <td>76.662008</td>\n",
       "      <td>76.293217</td>\n",
       "      <td>75.873558</td>\n",
       "      <td>76.327129</td>\n",
       "      <td>78.399988</td>\n",
       "      <td>82.511794</td>\n",
       "      <td>88.569290</td>\n",
       "      <td>96.190968</td>\n",
       "      <td>104.906302</td>\n",
       "      <td>114.282916</td>\n",
       "      <td>124.189402</td>\n",
       "      <td>134.850426</td>\n",
       "      <td>146.571193</td>\n",
       "      <td>159.728973</td>\n",
       "      <td>174.790053</td>\n",
       "      <td>192.195287</td>\n",
       "      <td>211.868373</td>\n",
       "      <td>232.948798</td>\n",
       "      <td>253.164473</td>\n",
       "      <td>269.170844</td>\n",
       "      <td>277.432608</td>\n",
       "      <td>275.478440</td>\n",
       "      <td>263.003135</td>\n",
       "      <td>242.113463</td>\n",
       "      <td>216.586353</td>\n",
       "      <td>190.486980</td>\n",
       "      <td>166.977621</td>\n",
       "      <td>147.550396</td>\n",
       "      <td>132.328235</td>\n",
       "      <td>120.569317</td>\n",
       "      <td>111.404652</td>\n",
       "      <td>104.096658</td>\n",
       "      <td>97.945904</td>\n",
       "      <td>92.274155</td>\n",
       "      <td>86.348067</td>\n",
       "      <td>79.718309</td>\n",
       "      <td>72.232278</td>\n",
       "      <td>64.212137</td>\n",
       "      <td>56.001241</td>\n",
       "      <td>47.709805</td>\n",
       "      <td>39.125879</td>\n",
       "      <td>29.656008</td>\n",
       "      <td>18.876293</td>\n",
       "      <td>6.841841</td>\n",
       "      <td>-5.853891</td>\n",
       "      <td>-18.418214</td>\n",
       "      <td>-30.266151</td>\n",
       "      <td>-41.270531</td>\n",
       "      <td>-51.906121</td>\n",
       "      <td>-62.974086</td>\n",
       "      <td>-75.186575</td>\n",
       "      <td>-88.827598</td>\n",
       "      <td>-103.659774</td>\n",
       "      <td>-119.254965</td>\n",
       "      <td>-135.146884</td>\n",
       "      <td>-150.818377</td>\n",
       "      <td>-165.307196</td>\n",
       "      <td>-177.112743</td>\n",
       "      <td>-184.759854</td>\n",
       "      <td>-187.778005</td>\n",
       "      <td>-187.307479</td>\n",
       "      <td>-185.895900</td>\n",
       "      <td>-186.548702</td>\n",
       "      <td>-190.753766</td>\n",
       "      <td>-197.324178</td>\n",
       "      <td>-202.754306</td>\n",
       "      <td>-203.140053</td>\n",
       "      <td>-196.561162</td>\n",
       "      <td>-184.577578</td>\n",
       "      <td>-171.123070</td>\n",
       "      <td>-159.919458</td>\n",
       "      <td>-152.543640</td>\n",
       "      <td>-147.253399</td>\n",
       "      <td>-140.581251</td>\n",
       "      <td>-129.530242</td>\n",
       "      <td>-113.515392</td>\n",
       "      <td>-94.490870</td>\n",
       "      <td>-75.975024</td>\n",
       "      <td>-60.930900</td>\n",
       "      <td>-50.062167</td>\n",
       "      <td>-41.791925</td>\n",
       "      <td>-33.106264</td>\n",
       "      <td>-21.279523</td>\n",
       "      <td>-5.205327</td>\n",
       "      <td>14.230376</td>\n",
       "      <td>34.522352</td>\n",
       "      <td>52.656689</td>\n",
       "      <td>65.911966</td>\n",
       "      <td>72.855408</td>\n",
       "      <td>73.749832</td>\n",
       "      <td>70.451909</td>\n",
       "      <td>65.526219</td>\n",
       "      <td>61.283005</td>\n",
       "      <td>59.371227</td>\n",
       "      <td>59.990117</td>\n",
       "      <td>62.257969</td>\n",
       "      <td>64.576689</td>\n",
       "      <td>65.407528</td>\n",
       "      <td>63.974754</td>\n",
       "      <td>60.731938</td>\n",
       "      <td>57.200871</td>\n",
       "      <td>55.051710</td>\n",
       "      <td>55.785053</td>\n",
       "      <td>59.782407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16356</th>\n",
       "      <td>7</td>\n",
       "      <td>9.078313</td>\n",
       "      <td>9.820562</td>\n",
       "      <td>12.418363</td>\n",
       "      <td>15.387361</td>\n",
       "      <td>16.686334</td>\n",
       "      <td>18.913083</td>\n",
       "      <td>19.840859</td>\n",
       "      <td>21.511029</td>\n",
       "      <td>25.778855</td>\n",
       "      <td>25.222277</td>\n",
       "      <td>26.150052</td>\n",
       "      <td>24.665553</td>\n",
       "      <td>25.778855</td>\n",
       "      <td>25.036606</td>\n",
       "      <td>26.150052</td>\n",
       "      <td>25.778855</td>\n",
       "      <td>27.820078</td>\n",
       "      <td>26.892302</td>\n",
       "      <td>26.706776</td>\n",
       "      <td>28.376801</td>\n",
       "      <td>27.634552</td>\n",
       "      <td>29.675774</td>\n",
       "      <td>26.706776</td>\n",
       "      <td>23.737778</td>\n",
       "      <td>22.624331</td>\n",
       "      <td>24.108830</td>\n",
       "      <td>0.171173</td>\n",
       "      <td>-19.127316</td>\n",
       "      <td>-3.354404</td>\n",
       "      <td>3.511368</td>\n",
       "      <td>5.923643</td>\n",
       "      <td>5.367065</td>\n",
       "      <td>7.222616</td>\n",
       "      <td>10.191614</td>\n",
       "      <td>12.232837</td>\n",
       "      <td>11.119390</td>\n",
       "      <td>5.738117</td>\n",
       "      <td>7.964866</td>\n",
       "      <td>15.201835</td>\n",
       "      <td>17.428584</td>\n",
       "      <td>17.799636</td>\n",
       "      <td>16.871861</td>\n",
       "      <td>13.902862</td>\n",
       "      <td>8.892642</td>\n",
       "      <td>9.820562</td>\n",
       "      <td>9.634891</td>\n",
       "      <td>10.562812</td>\n",
       "      <td>10.006088</td>\n",
       "      <td>9.634891</td>\n",
       "      <td>11.119390</td>\n",
       "      <td>12.975087</td>\n",
       "      <td>7.222616</td>\n",
       "      <td>12.047311</td>\n",
       "      <td>8.521589</td>\n",
       "      <td>-6.694599</td>\n",
       "      <td>3.696895</td>\n",
       "      <td>12.232837</td>\n",
       "      <td>14.645112</td>\n",
       "      <td>9.449365</td>\n",
       "      <td>5.738117</td>\n",
       "      <td>11.490587</td>\n",
       "      <td>16.686334</td>\n",
       "      <td>19.840859</td>\n",
       "      <td>19.284280</td>\n",
       "      <td>21.139832</td>\n",
       "      <td>19.655333</td>\n",
       "      <td>19.840859</td>\n",
       "      <td>20.397582</td>\n",
       "      <td>19.655333</td>\n",
       "      <td>20.954306</td>\n",
       "      <td>19.840859</td>\n",
       "      <td>14.830638</td>\n",
       "      <td>-14.302621</td>\n",
       "      <td>-10.962570</td>\n",
       "      <td>-20.240618</td>\n",
       "      <td>-10.777044</td>\n",
       "      <td>-7.622375</td>\n",
       "      <td>-4.282324</td>\n",
       "      <td>-2.797825</td>\n",
       "      <td>-1.127655</td>\n",
       "      <td>-1.127655</td>\n",
       "      <td>-3.540075</td>\n",
       "      <td>-4.653377</td>\n",
       "      <td>-5.766824</td>\n",
       "      <td>-5.024574</td>\n",
       "      <td>-5.210100</td>\n",
       "      <td>-5.766824</td>\n",
       "      <td>-3.911127</td>\n",
       "      <td>0.542370</td>\n",
       "      <td>-0.942129</td>\n",
       "      <td>-2.426628</td>\n",
       "      <td>-4.653377</td>\n",
       "      <td>-11.704820</td>\n",
       "      <td>-9.663598</td>\n",
       "      <td>-9.849124</td>\n",
       "      <td>-1.498852</td>\n",
       "      <td>2.769119</td>\n",
       "      <td>2.954645</td>\n",
       "      <td>-10.962570</td>\n",
       "      <td>-108.568605</td>\n",
       "      <td>-144.206722</td>\n",
       "      <td>-147.546917</td>\n",
       "      <td>-133.629702</td>\n",
       "      <td>-73.878395</td>\n",
       "      <td>-42.332716</td>\n",
       "      <td>-33.054524</td>\n",
       "      <td>-27.302054</td>\n",
       "      <td>-23.405280</td>\n",
       "      <td>-22.291833</td>\n",
       "      <td>-34.724550</td>\n",
       "      <td>-106.908573</td>\n",
       "      <td>-112.289991</td>\n",
       "      <td>-104.310772</td>\n",
       "      <td>-85.568862</td>\n",
       "      <td>-77.589643</td>\n",
       "      <td>-73.321672</td>\n",
       "      <td>-69.424898</td>\n",
       "      <td>-66.270373</td>\n",
       "      <td>-62.002402</td>\n",
       "      <td>-59.218930</td>\n",
       "      <td>-52.353013</td>\n",
       "      <td>-46.600687</td>\n",
       "      <td>-43.631689</td>\n",
       "      <td>-39.178047</td>\n",
       "      <td>-38.064745</td>\n",
       "      <td>-36.765772</td>\n",
       "      <td>-33.611248</td>\n",
       "      <td>-31.013302</td>\n",
       "      <td>-29.343277</td>\n",
       "      <td>-28.786553</td>\n",
       "      <td>-27.116528</td>\n",
       "      <td>-26.931002</td>\n",
       "      <td>-25.632029</td>\n",
       "      <td>-14.127088</td>\n",
       "      <td>-22.467511</td>\n",
       "      <td>-21.910788</td>\n",
       "      <td>-22.467511</td>\n",
       "      <td>-21.539591</td>\n",
       "      <td>-18.013869</td>\n",
       "      <td>-18.013869</td>\n",
       "      <td>-17.271619</td>\n",
       "      <td>-15.787120</td>\n",
       "      <td>-13.931569</td>\n",
       "      <td>-17.271619</td>\n",
       "      <td>-17.457290</td>\n",
       "      <td>-13.560372</td>\n",
       "      <td>-12.818122</td>\n",
       "      <td>-7.436849</td>\n",
       "      <td>-7.808046</td>\n",
       "      <td>-6.509073</td>\n",
       "      <td>-7.065652</td>\n",
       "      <td>-8.921348</td>\n",
       "      <td>-8.550296</td>\n",
       "      <td>-6.323402</td>\n",
       "      <td>-10.034795</td>\n",
       "      <td>-4.282324</td>\n",
       "      <td>-2.426628</td>\n",
       "      <td>-2.426628</td>\n",
       "      <td>2.398066</td>\n",
       "      <td>1.841343</td>\n",
       "      <td>0.913423</td>\n",
       "      <td>3.140171</td>\n",
       "      <td>7.779340</td>\n",
       "      <td>4.439144</td>\n",
       "      <td>4.253618</td>\n",
       "      <td>3.325842</td>\n",
       "      <td>4.253618</td>\n",
       "      <td>2.583593</td>\n",
       "      <td>-1.498852</td>\n",
       "      <td>-29.333283</td>\n",
       "      <td>-2.055576</td>\n",
       "      <td>-1.127655</td>\n",
       "      <td>1.470146</td>\n",
       "      <td>2.026869</td>\n",
       "      <td>6.851564</td>\n",
       "      <td>11.305061</td>\n",
       "      <td>10.933864</td>\n",
       "      <td>8.707115</td>\n",
       "      <td>5.738117</td>\n",
       "      <td>12.232837</td>\n",
       "      <td>15.387361</td>\n",
       "      <td>12.418363</td>\n",
       "      <td>25.964526</td>\n",
       "      <td>23.923304</td>\n",
       "      <td>28.562327</td>\n",
       "      <td>28.191275</td>\n",
       "      <td>31.345799</td>\n",
       "      <td>31.531326</td>\n",
       "      <td>7.408142</td>\n",
       "      <td>10.006088</td>\n",
       "      <td>23.366580</td>\n",
       "      <td>25.778855</td>\n",
       "      <td>25.036606</td>\n",
       "      <td>28.376801</td>\n",
       "      <td>28.376801</td>\n",
       "      <td>22.067607</td>\n",
       "      <td>18.356360</td>\n",
       "      <td>14.459586</td>\n",
       "      <td>19.655333</td>\n",
       "      <td>24.665553</td>\n",
       "      <td>22.253278</td>\n",
       "      <td>8.521589</td>\n",
       "      <td>5.552591</td>\n",
       "      <td>9.078313</td>\n",
       "      <td>0.171173</td>\n",
       "      <td>5.923643</td>\n",
       "      <td>15.944085</td>\n",
       "      <td>16.871861</td>\n",
       "      <td>19.098609</td>\n",
       "      <td>21.696555</td>\n",
       "      <td>22.253278</td>\n",
       "      <td>25.222277</td>\n",
       "      <td>24.108830</td>\n",
       "      <td>31.160273</td>\n",
       "      <td>34.129271</td>\n",
       "      <td>35.428244</td>\n",
       "      <td>38.211716</td>\n",
       "      <td>40.067268</td>\n",
       "      <td>43.964042</td>\n",
       "      <td>43.221792</td>\n",
       "      <td>43.778516</td>\n",
       "      <td>43.036266</td>\n",
       "      <td>30.046826</td>\n",
       "      <td>23.181054</td>\n",
       "      <td>21.696555</td>\n",
       "      <td>14.274059</td>\n",
       "      <td>24.294356</td>\n",
       "      <td>30.603550</td>\n",
       "      <td>33.015825</td>\n",
       "      <td>34.685995</td>\n",
       "      <td>35.242573</td>\n",
       "      <td>36.170494</td>\n",
       "      <td>34.314798</td>\n",
       "      <td>33.943745</td>\n",
       "      <td>34.500324</td>\n",
       "      <td>35.057047</td>\n",
       "      <td>33.201496</td>\n",
       "      <td>34.500324</td>\n",
       "      <td>33.572548</td>\n",
       "      <td>35.057047</td>\n",
       "      <td>1158.292377</td>\n",
       "      <td>1158.292377</td>\n",
       "      <td>1158.292377</td>\n",
       "      <td>1158.292377</td>\n",
       "      <td>1171.240743</td>\n",
       "      <td>1171.240743</td>\n",
       "      <td>1171.240743</td>\n",
       "      <td>1171.240743</td>\n",
       "      <td>1158.292377</td>\n",
       "      <td>...</td>\n",
       "      <td>11.814850</td>\n",
       "      <td>12.796686</td>\n",
       "      <td>14.033621</td>\n",
       "      <td>15.415903</td>\n",
       "      <td>16.756657</td>\n",
       "      <td>17.848244</td>\n",
       "      <td>18.566082</td>\n",
       "      <td>18.960596</td>\n",
       "      <td>19.260189</td>\n",
       "      <td>19.761489</td>\n",
       "      <td>20.681033</td>\n",
       "      <td>21.956529</td>\n",
       "      <td>23.208295</td>\n",
       "      <td>23.851976</td>\n",
       "      <td>23.410001</td>\n",
       "      <td>21.793384</td>\n",
       "      <td>19.408503</td>\n",
       "      <td>17.002857</td>\n",
       "      <td>15.276488</td>\n",
       "      <td>14.543819</td>\n",
       "      <td>14.597212</td>\n",
       "      <td>14.908670</td>\n",
       "      <td>14.935367</td>\n",
       "      <td>14.392539</td>\n",
       "      <td>13.283154</td>\n",
       "      <td>11.740694</td>\n",
       "      <td>9.845247</td>\n",
       "      <td>7.537488</td>\n",
       "      <td>4.669105</td>\n",
       "      <td>1.091783</td>\n",
       "      <td>-3.292365</td>\n",
       "      <td>-8.649449</td>\n",
       "      <td>-15.308726</td>\n",
       "      <td>-23.729968</td>\n",
       "      <td>-34.257262</td>\n",
       "      <td>-46.712631</td>\n",
       "      <td>-60.099410</td>\n",
       "      <td>-72.685295</td>\n",
       "      <td>-82.471021</td>\n",
       "      <td>-87.869633</td>\n",
       "      <td>-88.195923</td>\n",
       "      <td>-83.835505</td>\n",
       "      <td>-75.965990</td>\n",
       "      <td>-66.061613</td>\n",
       "      <td>-55.362276</td>\n",
       "      <td>-44.547254</td>\n",
       "      <td>-33.664008</td>\n",
       "      <td>-22.303192</td>\n",
       "      <td>-9.972406</td>\n",
       "      <td>3.538956</td>\n",
       "      <td>17.925367</td>\n",
       "      <td>32.382969</td>\n",
       "      <td>45.852803</td>\n",
       "      <td>57.433123</td>\n",
       "      <td>66.687887</td>\n",
       "      <td>73.729813</td>\n",
       "      <td>79.048335</td>\n",
       "      <td>83.129923</td>\n",
       "      <td>86.202980</td>\n",
       "      <td>88.178516</td>\n",
       "      <td>88.851859</td>\n",
       "      <td>88.119190</td>\n",
       "      <td>85.998307</td>\n",
       "      <td>82.435816</td>\n",
       "      <td>77.022373</td>\n",
       "      <td>68.897758</td>\n",
       "      <td>57.020811</td>\n",
       "      <td>40.664796</td>\n",
       "      <td>19.966161</td>\n",
       "      <td>-3.784766</td>\n",
       "      <td>-28.286159</td>\n",
       "      <td>-50.734894</td>\n",
       "      <td>-68.538449</td>\n",
       "      <td>-79.955624</td>\n",
       "      <td>-84.511815</td>\n",
       "      <td>-82.966388</td>\n",
       "      <td>-76.977488</td>\n",
       "      <td>-68.511752</td>\n",
       "      <td>-59.274786</td>\n",
       "      <td>-50.355211</td>\n",
       "      <td>-42.180170</td>\n",
       "      <td>-34.743730</td>\n",
       "      <td>-27.945038</td>\n",
       "      <td>-21.855285</td>\n",
       "      <td>-16.762199</td>\n",
       "      <td>-13.036563</td>\n",
       "      <td>-10.957208</td>\n",
       "      <td>-10.565660</td>\n",
       "      <td>-11.669113</td>\n",
       "      <td>-13.932377</td>\n",
       "      <td>-16.943141</td>\n",
       "      <td>-20.238668</td>\n",
       "      <td>-23.308758</td>\n",
       "      <td>-25.601685</td>\n",
       "      <td>-26.619115</td>\n",
       "      <td>-26.034760</td>\n",
       "      <td>-23.792260</td>\n",
       "      <td>-20.140781</td>\n",
       "      <td>-15.572724</td>\n",
       "      <td>-10.681344</td>\n",
       "      <td>-6.012435</td>\n",
       "      <td>-1.951611</td>\n",
       "      <td>1.308321</td>\n",
       "      <td>3.779224</td>\n",
       "      <td>5.618311</td>\n",
       "      <td>7.053986</td>\n",
       "      <td>8.317617</td>\n",
       "      <td>9.572350</td>\n",
       "      <td>10.889374</td>\n",
       "      <td>12.224196</td>\n",
       "      <td>13.440366</td>\n",
       "      <td>14.380674</td>\n",
       "      <td>14.905704</td>\n",
       "      <td>14.950198</td>\n",
       "      <td>14.540853</td>\n",
       "      <td>13.766656</td>\n",
       "      <td>12.752192</td>\n",
       "      <td>11.604245</td>\n",
       "      <td>10.379175</td>\n",
       "      <td>9.100713</td>\n",
       "      <td>7.751060</td>\n",
       "      <td>6.315385</td>\n",
       "      <td>4.787756</td>\n",
       "      <td>3.194868</td>\n",
       "      <td>1.581217</td>\n",
       "      <td>0.020959</td>\n",
       "      <td>-1.388019</td>\n",
       "      <td>-2.530033</td>\n",
       "      <td>-3.298297</td>\n",
       "      <td>-3.624587</td>\n",
       "      <td>-3.502970</td>\n",
       "      <td>-2.998704</td>\n",
       "      <td>-2.212642</td>\n",
       "      <td>-1.254537</td>\n",
       "      <td>-0.207444</td>\n",
       "      <td>0.887110</td>\n",
       "      <td>1.996495</td>\n",
       "      <td>3.079184</td>\n",
       "      <td>4.075851</td>\n",
       "      <td>4.900474</td>\n",
       "      <td>5.484829</td>\n",
       "      <td>5.796288</td>\n",
       "      <td>5.852647</td>\n",
       "      <td>5.733996</td>\n",
       "      <td>5.544155</td>\n",
       "      <td>5.372111</td>\n",
       "      <td>5.262359</td>\n",
       "      <td>5.188202</td>\n",
       "      <td>5.078450</td>\n",
       "      <td>4.850047</td>\n",
       "      <td>4.458500</td>\n",
       "      <td>3.930504</td>\n",
       "      <td>3.355047</td>\n",
       "      <td>2.827051</td>\n",
       "      <td>2.399908</td>\n",
       "      <td>2.049888</td>\n",
       "      <td>1.688003</td>\n",
       "      <td>1.207467</td>\n",
       "      <td>0.540056</td>\n",
       "      <td>-0.320162</td>\n",
       "      <td>-1.322761</td>\n",
       "      <td>-2.381720</td>\n",
       "      <td>-3.399151</td>\n",
       "      <td>-4.271234</td>\n",
       "      <td>-4.903050</td>\n",
       "      <td>-5.202643</td>\n",
       "      <td>-5.104756</td>\n",
       "      <td>-4.588625</td>\n",
       "      <td>-3.692811</td>\n",
       "      <td>-2.506303</td>\n",
       "      <td>-1.153684</td>\n",
       "      <td>0.237497</td>\n",
       "      <td>1.545622</td>\n",
       "      <td>2.666872</td>\n",
       "      <td>3.530057</td>\n",
       "      <td>4.084750</td>\n",
       "      <td>4.342815</td>\n",
       "      <td>4.354680</td>\n",
       "      <td>4.206367</td>\n",
       "      <td>4.007627</td>\n",
       "      <td>3.853381</td>\n",
       "      <td>3.797021</td>\n",
       "      <td>3.859313</td>\n",
       "      <td>4.007627</td>\n",
       "      <td>4.212299</td>\n",
       "      <td>4.428837</td>\n",
       "      <td>4.636476</td>\n",
       "      <td>4.838182</td>\n",
       "      <td>5.039889</td>\n",
       "      <td>5.238629</td>\n",
       "      <td>5.428470</td>\n",
       "      <td>5.558986</td>\n",
       "      <td>5.564919</td>\n",
       "      <td>5.372111</td>\n",
       "      <td>4.933103</td>\n",
       "      <td>4.241962</td>\n",
       "      <td>3.352081</td>\n",
       "      <td>2.385077</td>\n",
       "      <td>1.507061</td>\n",
       "      <td>0.901941</td>\n",
       "      <td>0.729898</td>\n",
       "      <td>1.085850</td>\n",
       "      <td>1.978698</td>\n",
       "      <td>3.322418</td>\n",
       "      <td>4.962766</td>\n",
       "      <td>6.698034</td>\n",
       "      <td>8.344314</td>\n",
       "      <td>9.729562</td>\n",
       "      <td>10.723263</td>\n",
       "      <td>11.242360</td>\n",
       "      <td>11.248293</td>\n",
       "      <td>10.758858</td>\n",
       "      <td>9.866011</td>\n",
       "      <td>8.712131</td>\n",
       "      <td>7.460365</td>\n",
       "      <td>6.267925</td>\n",
       "      <td>5.235663</td>\n",
       "      <td>4.411039</td>\n",
       "      <td>3.791089</td>\n",
       "      <td>3.346148</td>\n",
       "      <td>3.031724</td>\n",
       "      <td>2.800355</td>\n",
       "      <td>2.601614</td>\n",
       "      <td>2.388043</td>\n",
       "      <td>2.112180</td>\n",
       "      <td>1.747329</td>\n",
       "      <td>1.290523</td>\n",
       "      <td>0.741763</td>\n",
       "      <td>0.118846</td>\n",
       "      <td>-0.593059</td>\n",
       "      <td>-1.402851</td>\n",
       "      <td>-2.322394</td>\n",
       "      <td>-3.322028</td>\n",
       "      <td>-4.333526</td>\n",
       "      <td>-5.235272</td>\n",
       "      <td>-5.896750</td>\n",
       "      <td>-6.211175</td>\n",
       "      <td>-6.137018</td>\n",
       "      <td>-5.712842</td>\n",
       "      <td>-5.051363</td>\n",
       "      <td>-4.280133</td>\n",
       "      <td>-3.520768</td>\n",
       "      <td>-2.850391</td>\n",
       "      <td>-2.310529</td>\n",
       "      <td>-1.921948</td>\n",
       "      <td>-1.693545</td>\n",
       "      <td>-1.628287</td>\n",
       "      <td>-1.702444</td>\n",
       "      <td>-1.856690</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16357 rows  10082 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       EDA_0       EDA_1       EDA_2       EDA_3       EDA_4  \\\n",
       "0       8  279.693630  279.693630  277.263096  278.536233  277.263096   \n",
       "1       1  -26.705520   -8.030028    9.632398   26.103795   19.179108   \n",
       "2       4  424.691455  423.525565  424.108510  423.525565  423.525565   \n",
       "3       8  -96.859455  -96.743625  -96.743625  -95.932633  -95.932633   \n",
       "4       3 -110.091983 -109.777949 -111.034574 -110.406261 -109.463670   \n",
       "...    ..         ...         ...         ...         ...         ...   \n",
       "16352   7  -50.075140  -49.147074  -51.374547  -50.817679  -50.260811   \n",
       "16353   6   28.103361   35.282196   47.029379   70.523745   77.547103   \n",
       "16354   1  -42.740826  -42.740826  -42.652498  -42.784973  -42.784973   \n",
       "16355   5  -63.990278  -63.568335  -63.779307  -63.990278  -63.779307   \n",
       "16356   7    9.078313    9.820562   12.418363   15.387361   16.686334   \n",
       "\n",
       "            EDA_5       EDA_6       EDA_7       EDA_8       EDA_9      EDA_10  \\\n",
       "0      275.411261  274.253774  270.434365  275.642741  276.337179  273.906555   \n",
       "1        6.905367   16.839128   32.247524   -1.041654   22.086138   39.576458   \n",
       "2      422.651376  422.359904  422.068431  421.485487  421.194015  418.279519   \n",
       "3      -95.469222  -95.353392  -94.889891  -94.889891  -94.774061  -94.889891   \n",
       "4     -107.578732 -105.379515 -104.436924 -102.551986 -100.981081 -100.038490   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -50.632153  -50.817679  -50.260811  -50.260811  -50.446482  -50.260811   \n",
       "16353   49.484387   14.569084   55.847477   21.421672   32.516233    6.085007   \n",
       "16354  -42.784973  -42.784973  -42.784973  -42.784973  -42.784973  -42.740826   \n",
       "16355  -63.779307  -63.779307  -63.357364  -63.990278  -63.779307  -63.779307   \n",
       "16356   18.913083   19.840859   21.511029   25.778855   25.222277   26.150052   \n",
       "\n",
       "           EDA_11      EDA_12      EDA_13      EDA_14      EDA_15      EDA_16  \\\n",
       "0      274.832473  273.559336  272.864898  273.559336  272.517679  272.401939   \n",
       "1       19.983846   -0.202799  -17.869706    1.641230    9.588252   12.593879   \n",
       "2      417.988274  418.862464  417.113857  415.947968  415.656723  414.782307   \n",
       "3      -94.774061  -94.426480  -94.774061  -94.774061  -94.310650  -94.774061   \n",
       "4      -99.096143  -97.525239  -96.582648  -97.525239  -96.896927  -95.326022   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -50.817679  -50.632153  -50.260811  -50.260811  -50.817679  -50.817679   \n",
       "16353   -6.641171   -3.385711  -24.922214    5.587863   13.419319  -19.048622   \n",
       "16354  -42.740826  -42.740826  -42.784973  -42.784973  -42.740826  -42.652498   \n",
       "16355  -63.779307  -63.779307  -63.779307  -63.357364  -63.990278  -63.990278   \n",
       "16356   24.665553   25.778855   25.036606   26.150052   25.778855   27.820078   \n",
       "\n",
       "           EDA_17      EDA_18      EDA_19      EDA_20      EDA_21      EDA_22  \\\n",
       "0      271.823241  271.128803  270.550104  269.739836  269.045398  268.466700   \n",
       "1      -17.869706  -17.961445  -21.140247  -19.904037  -31.118154  -38.228698   \n",
       "2      419.153936  414.490834  408.370599  406.330520  403.416025  402.541835   \n",
       "3      -94.774061  -94.774061  -94.774061  -94.774061  -94.889891  -94.774061   \n",
       "4      -92.498493  -90.613310  -89.356684  -87.785780  -86.215121  -84.958251   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -50.260811  -50.632153  -49.889613  -51.003350  -50.446482  -50.817679   \n",
       "16353   36.097940   15.377183   45.234639   12.114076   16.845676   33.324332   \n",
       "16354  -42.740826  -42.740826  -42.696645  -42.740826  -42.740826  -42.696645   \n",
       "16355  -63.990278  -63.779307  -63.146557  -63.779307  -63.779307  -63.779307   \n",
       "16356   26.892302   26.706776   28.376801   27.634552   29.675774   26.706776   \n",
       "\n",
       "           EDA_23      EDA_24      EDA_25      EDA_26      EDA_27      EDA_28  \\\n",
       "0      267.772262  267.309303  268.235220  272.286200  272.980638  273.212117   \n",
       "1      -31.208860  -36.595168  -11.252977   -5.513496    6.321122  -11.512102   \n",
       "2      401.084474  400.210284  401.958891  420.028353  420.611070  420.028353   \n",
       "3      -94.658231  -94.774061  -94.774061  -94.774061  -94.774061  -94.658231   \n",
       "4      -84.958251  -84.329938  -84.329938  -84.958251  -85.272530  -84.644217   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -50.817679  -51.189021  -50.817679  -51.003350  -51.003350  -51.003350   \n",
       "16353   50.129362   22.719268   -6.975065   -1.264597  -12.196035   36.261191   \n",
       "16354  -42.740826  -42.740826  -42.740826  -42.696645  -42.784973  -42.740826   \n",
       "16355  -63.779307  -63.779307  -63.779307  -63.779307  -64.201250  -63.990278   \n",
       "16356   23.737778   22.624331   24.108830    0.171173  -19.127316   -3.354404   \n",
       "\n",
       "           EDA_29      EDA_30      EDA_31      EDA_32      EDA_33      EDA_34  \\\n",
       "0      273.443596  270.665844  273.443596  272.864898  272.401939  272.286200   \n",
       "1      -15.618053  -32.045303  -46.096628  -38.061313  -40.003904  -25.787572   \n",
       "2      418.570991  419.445408  425.565644  426.731533  424.399982  428.188667   \n",
       "3      -94.889891  -94.774061  -94.774061  -94.658231  -94.774061  -94.889891   \n",
       "4      -84.329938  -83.701626  -83.387347  -83.073313  -81.502409  -81.502409   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -50.817679  -50.817679  -51.003350  -50.817679  -51.003350  -51.189021   \n",
       "16353  -16.275014  -20.027491  -24.432716  -42.542989  -41.727244  -10.890793   \n",
       "16354  -42.740826  -42.696645  -42.740826  -42.740826  -42.740826  -42.740826   \n",
       "16355  -63.990278  -63.990278  -63.990278  -63.990278  -63.990278  -63.779307   \n",
       "16356    3.511368    5.923643    5.367065    7.222616   10.191614   12.232837   \n",
       "\n",
       "           EDA_35      EDA_36      EDA_37      EDA_38      EDA_39      EDA_40  \\\n",
       "0      272.170460  271.938981  272.054720  272.170460  272.517679  272.633419   \n",
       "1      -34.396863  -17.443196   -5.469314    9.632398    9.768250    4.249502   \n",
       "2      414.782307  412.159283  410.119205  410.410677  417.113857  436.640454   \n",
       "3      -94.889891  -94.658231  -94.774061  -94.774061  -94.889891  -94.889891   \n",
       "4      -81.188375  -81.502409  -81.502409  -81.502409  -80.559817  -81.502409   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.189021  -51.560218  -50.632153  -51.003350  -50.817679  -50.817679   \n",
       "16353   15.703557   21.577149   66.608018   44.426540   -4.520057   74.928971   \n",
       "16354  -42.696645  -42.696645  -42.696645  -42.652498  -42.696645  -42.696645   \n",
       "16355  -63.990278  -63.990278  -63.990278  -64.201250  -63.990278  -63.990278   \n",
       "16356   11.119390    5.738117    7.964866   15.201835   17.428584   17.799636   \n",
       "\n",
       "           EDA_41      EDA_42      EDA_43      EDA_44      EDA_45      EDA_46  \\\n",
       "0      272.980638  273.096377  272.980638  272.864898  272.633419  272.749158   \n",
       "1      -13.322253   26.103795   22.402057   -0.997508   -1.225134    9.591663   \n",
       "2      437.223398  443.343634  440.720611  442.760917  438.097815  437.806343   \n",
       "3      -95.700882  -95.353392  -96.048463  -96.396044  -96.164293  -95.237471   \n",
       "4      -80.874096  -81.188375  -80.559817  -80.559817  -79.931504  -80.245783   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -50.817679  -51.003350  -51.374547  -51.189021  -51.189021  -51.003350   \n",
       "16353   90.599528   64.168430   61.231570   60.252575   93.536388  101.857341   \n",
       "16354  -42.696645  -42.740826  -42.696645  -42.740826  -42.608352  -42.696645   \n",
       "16355  -63.990278  -64.201250  -63.990278  -63.990278  -63.779307  -63.990278   \n",
       "16356   16.871861   13.902862    8.892642    9.820562    9.634891   10.562812   \n",
       "\n",
       "           EDA_47      EDA_48      EDA_49      EDA_50      EDA_51      EDA_52  \\\n",
       "0      272.517679  272.286200  272.286200  272.170460  271.938981  271.823241   \n",
       "1        3.278172   26.059649   42.755260   60.866873   77.929098   95.090058   \n",
       "2      438.680532  438.680532  439.554949  443.343634  443.343634  442.469445   \n",
       "3      -95.816803  -95.700882  -96.048463  -95.469222  -95.932633  -96.048463   \n",
       "4      -80.245783  -80.559817  -80.874096  -80.874096  -81.502409  -81.816688   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.189021  -51.745889  -51.560218  -50.446482  -50.632153  -51.745889   \n",
       "16353  102.509963    0.537663    5.432386   60.742072    6.900752    8.042871   \n",
       "16354  -42.520024  -42.696645  -42.652498  -42.696645  -42.652498  -42.696645   \n",
       "16355  -63.990278  -63.990278  -63.990278  -63.990278  -63.990278  -63.990278   \n",
       "16356   10.006088    9.634891   11.119390   12.975087    7.222616   12.047311   \n",
       "\n",
       "           EDA_53      EDA_54      EDA_55      EDA_56      EDA_57      EDA_58  \\\n",
       "0      271.360282  271.591762  271.707501  272.170460  271.938981  271.938981   \n",
       "1       75.196691   68.309258   52.043706   34.176330   60.911019   70.114516   \n",
       "2      436.640454  447.715263  481.522637  500.175044  458.186650  450.026336   \n",
       "3      -96.164293  -96.280214  -95.700882  -95.816803  -95.932633  -96.164293   \n",
       "4      -81.816688  -81.816688  -82.130721  -82.445000  -82.130721  -82.445000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.374547  -51.189021  -51.189021  -51.560218  -51.374547  -50.817679   \n",
       "16353   80.647086   46.221281   40.673936   31.047740   64.331553  117.030754   \n",
       "16354  -42.652498  -42.696645  -42.696645  -42.696645  -42.696645  -42.696645   \n",
       "16355  -64.201250  -64.201250  -64.201250  -63.990278  -64.201250  -64.201250   \n",
       "16356    8.521589   -6.694599    3.696895   12.232837   14.645112    9.449365   \n",
       "\n",
       "           EDA_59      EDA_60      EDA_61      EDA_62      EDA_63      EDA_64  \\\n",
       "0      273.212117  273.096377  273.212117  273.675076  273.327857  272.749158   \n",
       "1       51.703939   34.088037   18.217841   41.872257   30.359142   27.621841   \n",
       "2      471.010066  485.290844  462.266807  457.020989  468.678743  449.734864   \n",
       "3      -96.048463  -96.048463  -95.932633  -96.048463  -95.932633  -95.585052   \n",
       "4      -82.759034  -82.759034  -82.759034  -81.502409  -82.759034  -83.073313   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.745889  -51.374547  -51.003350  -51.560218  -50.817679  -51.003350   \n",
       "16353  117.846627   66.289417   99.573103   99.409980   61.068447    1.516658   \n",
       "16354  -42.696645  -42.740826  -42.696645  -42.652498  -42.696645  -42.652498   \n",
       "16355  -64.201250  -64.412056  -64.201250  -64.201250  -63.568335  -64.201250   \n",
       "16356    5.738117   11.490587   16.686334   19.840859   19.284280   21.139832   \n",
       "\n",
       "           EDA_65      EDA_66      EDA_67      EDA_68      EDA_69      EDA_70  \\\n",
       "0      271.707501  271.823241  271.591762  271.476022  271.591762  272.517679   \n",
       "1       33.273029   24.398858   16.981665   -0.997508  -17.869706  -34.208662   \n",
       "2      440.408660  434.288425  433.414008  432.248346  432.248346  429.333851   \n",
       "3      -95.932633  -96.048463  -96.164293  -96.164293  -95.932633  -96.280214   \n",
       "4      -83.073313  -83.387347  -83.701626  -83.073313  -83.073313  -82.445000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.189021  -51.560218  -51.745889  -51.374547  -51.560218  -51.745889   \n",
       "16353  -16.430491  -40.748248  -40.422002   -3.548835  -21.495857    4.608995   \n",
       "16354  -42.652498  -42.520024  -42.652498  -42.652498  -42.652498  -42.652498   \n",
       "16355  -63.990278  -63.990278  -63.779307  -64.201250  -64.201250  -64.412056   \n",
       "16356   19.655333   19.840859   20.397582   19.655333   20.954306   19.840859   \n",
       "\n",
       "           EDA_71      EDA_72      EDA_73      EDA_74      EDA_75      EDA_76  \\\n",
       "0      272.864898  273.675076  274.369514  274.948212  275.179692  275.064042   \n",
       "1      -13.990312   -7.853442    9.632398   24.735145   25.845773   -0.997508   \n",
       "2      443.031683  429.042378  429.042378  422.922143  411.847333  405.144153   \n",
       "3      -96.164293  -96.280214  -96.164293  -96.280214  -96.164293  -96.280214   \n",
       "4      -82.445000  -83.073313  -82.445000  -83.073313  -82.445000  -82.759034   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.745889  -51.560218  -51.374547  -51.560218  -51.931560  -51.374547   \n",
       "16353   12.929948   -6.322443  -11.053916  -21.169610  -20.516988  -23.453848   \n",
       "16354  -42.652498  -42.652498  -42.652498  -42.652498  -42.652498  -42.608352   \n",
       "16355  -64.412056  -64.201250  -64.201250  -64.412056  -64.412056  -64.412056   \n",
       "16356   14.830638  -14.302621  -10.962570  -20.240618  -10.777044   -7.622375   \n",
       "\n",
       "           EDA_77      EDA_78      EDA_79      EDA_80      EDA_81      EDA_82  \\\n",
       "0      275.064042  275.179692  275.758480  274.716733  274.369514  270.897323   \n",
       "1      -17.516532  -24.407342  -10.941572    6.365269   26.059649   42.931846   \n",
       "2      401.063996  398.732445  396.400894  394.943760  394.069343  392.903681   \n",
       "3      -96.396044  -96.511874  -96.627704  -96.627704  -96.511874  -96.511874   \n",
       "4      -81.816688  -80.874096  -79.931504  -79.617471  -78.046567  -78.674879   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.745889  -52.117086  -51.189021  -51.931560  -51.745889  -52.859625   \n",
       "16353  -24.269593  -24.759091    6.240485   57.471320   58.613439   -3.548835   \n",
       "16354  -42.608352  -42.608352  -42.696645  -42.696645  -42.652498  -42.652498   \n",
       "16355  -64.412056  -64.201250  -64.201250  -64.201250  -64.412056  -64.412056   \n",
       "16356   -4.282324   -2.797825   -1.127655   -1.127655   -3.540075   -4.653377   \n",
       "\n",
       "           EDA_83      EDA_84      EDA_85      EDA_86      EDA_87      EDA_88  \\\n",
       "0      273.906555  273.443596  274.369514  274.716733  275.526911  273.559336   \n",
       "1       60.822692   34.043890   60.911019   67.200628   37.266839   60.866873   \n",
       "2      392.320737  391.446547  390.572130  389.697714  390.280658  388.240580   \n",
       "3      -96.627704  -96.511874  -96.511874  -96.511874  -96.627704  -96.627704   \n",
       "4      -79.617471  -79.617471  -78.989158  -79.303192  -79.617471  -79.617471   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.931560  -51.931560  -51.931560  -51.745889  -52.302757  -52.117086   \n",
       "16353    4.608995   43.440026   53.555592   79.497320   19.790055   -8.599035   \n",
       "16354  -42.652498  -42.652498  -42.696645  -42.608352  -42.696645  -42.652498   \n",
       "16355  -64.201250  -64.412056  -64.201250  -64.201250  -64.201250  -64.201250   \n",
       "16356   -5.766824   -5.024574   -5.210100   -5.766824   -3.911127    0.542370   \n",
       "\n",
       "           EDA_89      EDA_90      EDA_91      EDA_92      EDA_93      EDA_94  \\\n",
       "0      272.864898  270.318535  271.823241  272.054720  271.707501  271.476022   \n",
       "1       72.277875   72.719376   76.149652   78.931100   77.032655   81.889168   \n",
       "2      387.949107  387.949107  386.491973  386.200501  386.783446  385.617556   \n",
       "3      -96.627704  -96.511874  -96.164293  -96.511874  -96.627704  -96.627704   \n",
       "4      -79.303192  -78.360600  -79.303192  -79.617471  -78.674879  -78.360600   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.488428  -51.560218  -52.117086  -52.117086  -51.745889  -51.745889   \n",
       "16353  -50.700819  -57.390155  -52.658683  -78.763534  -77.375328    9.097057   \n",
       "16354  -42.608352  -42.652498  -42.696645  -42.652498  -42.652498  -42.652498   \n",
       "16355  -63.779307  -64.412056  -63.779307  -64.412056  -64.412056  -64.412056   \n",
       "16356   -0.942129   -2.426628   -4.653377  -11.704820   -9.663598   -9.849124   \n",
       "\n",
       "           EDA_95      EDA_96      EDA_97      EDA_98      EDA_99     EDA_100  \\\n",
       "0      270.434365  269.971316  270.665844  270.318535  269.855666  269.739836   \n",
       "1       82.154083   80.829579   81.138606   56.679452   51.116557   33.999743   \n",
       "2      386.783446  385.034612  384.451895  383.577478  383.286006  382.411816   \n",
       "3      -96.627704  -96.511874  -96.627704  -96.743625  -96.627704  -96.627704   \n",
       "4      -78.360600  -77.732288  -76.789941  -76.161629  -75.219037  -74.276445   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.931560  -51.931560  -51.560218  -51.374547  -52.302757  -51.745889   \n",
       "16353   52.576724    3.793123    8.198348   21.577149  -27.043201  -14.806520   \n",
       "16354  -42.696645  -42.652498  -42.652498  -42.652498  -42.740826  -42.652498   \n",
       "16355  -64.412056  -64.412056  -64.412056  -64.412056  -64.412056  -64.412056   \n",
       "16356   -1.498852    2.769119    2.954645  -10.962570 -108.568605 -144.206722   \n",
       "\n",
       "          EDA_101     EDA_102     EDA_103     EDA_104     EDA_105     EDA_106  \\\n",
       "0      269.508447  269.392617  271.013063  272.864898  272.054720  273.906555   \n",
       "1       60.911019   77.752511   81.756728   81.182787   80.741286   76.193799   \n",
       "2      382.703061  383.868950  381.245927  382.120344  381.828872  381.245927   \n",
       "3      -96.280214  -96.396044  -96.743625  -96.511874  -96.511874  -96.048463   \n",
       "4      -73.648133  -73.333854  -73.648133  -71.448916  -70.192291  -69.563978   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.189021  -52.302757  -51.374547  -52.117086  -52.302757  -52.302757   \n",
       "16353  -12.032912   -3.711958   44.255770   47.518877   48.824119   74.439601   \n",
       "16354  -43.933027  -45.699239  -45.831679  -45.831679  -45.787532  -45.566765   \n",
       "16355  -64.412056  -64.412056  -64.412056  -64.412056  -64.201250  -64.201250   \n",
       "16356 -147.546917 -133.629702  -73.878395  -42.332716  -33.054524  -27.302054   \n",
       "\n",
       "          EDA_107     EDA_108     EDA_109     EDA_110     EDA_111     EDA_112  \\\n",
       "0      274.138035  273.559336  274.716733  274.253774  274.369514  273.559336   \n",
       "1       69.924489   66.215892   51.160704   38.017401   60.866873   68.480951   \n",
       "2      379.788793  381.266405  381.557877  380.100744  380.100744  379.517799   \n",
       "3      -96.627704  -96.511874  -96.627704  -96.627704  -96.511874  -96.743625   \n",
       "4      -69.563978  -69.878012  -69.563978  -70.192291  -70.192291  -70.192291   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.931560  -52.302757  -51.560218  -52.488428  -52.302757  -52.117086   \n",
       "16353   -6.314797   79.497320  104.141452   95.820499  138.404134   43.627363   \n",
       "16354  -44.992754  -44.507038  -44.286270  -45.345997  -44.595331  -43.844700   \n",
       "16355  -64.201250  -64.201250  -64.201250  -63.779307  -64.201250  -64.201250   \n",
       "16356  -23.405280  -22.291833  -34.724550 -106.908573 -112.289991 -104.310772   \n",
       "\n",
       "          EDA_113     EDA_114     EDA_115     EDA_116     EDA_117     EDA_118  \\\n",
       "0      274.138035  272.401939  272.401939  270.897323  270.087146  269.624097   \n",
       "1       69.231513   69.099073   69.275660   69.187367   70.158662   72.101288   \n",
       "2      378.060665  379.226327  377.477720  377.477720  376.603303  375.437642   \n",
       "3      -96.627704  -96.743625  -96.743625  -96.511874  -96.396044  -96.164293   \n",
       "4      -70.506324  -70.820603  -70.820603  -77.103975  -76.789941  -77.732288   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.302757  -52.488428  -52.673954  -52.117086  -51.745889  -52.302757   \n",
       "16353   36.105587   21.911042   17.179569    7.553374   26.968889  102.183588   \n",
       "16354  -44.109649  -44.551184  -43.800553  -43.800553  -43.623932  -43.712225   \n",
       "16355  -64.201250  -64.201250  -64.412056  -64.412056  -64.412056  -64.412056   \n",
       "16356  -85.568862  -77.589643  -73.321672  -69.424898  -66.270373  -62.002402   \n",
       "\n",
       "          EDA_119     EDA_120     EDA_121     EDA_122     EDA_123     EDA_124  \\\n",
       "0      272.401939  271.476022  272.980638  272.401939  272.286200  271.013063   \n",
       "1       75.059322   77.606630   77.650777   77.959804   74.957589   56.149657   \n",
       "2      376.020586  376.020586  374.271753  373.689036  373.397563  373.689036   \n",
       "3      -95.700882  -96.048463  -96.048463  -96.396044  -96.396044  -96.859455   \n",
       "4      -77.732288  -78.046567  -78.046567  -77.103975  -75.847350  -75.847350   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.673954  -52.117086  -52.673954  -52.302757  -52.302757  -52.488428   \n",
       "16353   82.768073   98.594235   94.515256   80.810209   86.520677   72.815630   \n",
       "16354  -43.712225  -42.784973  -42.696645  -42.652498  -42.740826  -42.696645   \n",
       "16355  -64.412056  -64.412056  -64.412056  -64.412056  -64.412056  -64.412056   \n",
       "16356  -59.218930  -52.353013  -46.600687  -43.631689  -39.178047  -38.064745   \n",
       "\n",
       "          EDA_125     EDA_126     EDA_127     EDA_128     EDA_129     EDA_130  \\\n",
       "0      269.855666  269.739836  269.276968  268.235220  268.119481  267.772262   \n",
       "1       75.134210   51.160704   35.368394   17.025812    4.830301    0.238703   \n",
       "2      374.271753  373.106091  372.523146  373.397563  372.231674  372.814619   \n",
       "3      -96.975285  -96.627704  -96.511874  -96.511874  -96.511874  -96.511874   \n",
       "4      -76.475662  -75.847350  -75.847350  -75.219037  -75.219037  -74.904758   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.488428  -52.117086  -52.302757  -52.302757  -52.673954  -52.859625   \n",
       "16353   25.337399   -4.846431  -17.906503   45.234639   31.855966   -8.117184   \n",
       "16354  -42.740826  -42.740826  -42.696645  -42.696645  -42.696645  -42.740826   \n",
       "16355  -64.412056  -63.779307  -64.412056  -64.623028  -64.412056  -64.412056   \n",
       "16356  -36.765772  -33.611248  -31.013302  -29.343277  -28.786553  -27.116528   \n",
       "\n",
       "          EDA_131     EDA_132     EDA_133     EDA_134     EDA_135     EDA_136  \\\n",
       "0      267.425043  268.466700  271.476022  270.087146  269.508447  269.855666   \n",
       "1        0.320138  -17.781412  -20.080623  -27.232971    5.835474  -17.825559   \n",
       "2      372.231674  371.648957  372.231674  371.940202  371.648957  372.523146   \n",
       "3      -96.164293  -96.164293  -97.207035  -97.207035  -97.207035  -97.207035   \n",
       "4      -75.219037  -75.533071  -75.847350  -75.533071  -75.847350  -74.904758   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.117086  -52.302757  -52.859625  -52.302757  -52.302757  -52.673954   \n",
       "16353   -1.754094    5.914238  -21.006486  -50.537695  -27.532699  -19.048622   \n",
       "16354  -42.740826  -42.696645  -42.652498  -42.740826  -42.696645  -42.696645   \n",
       "16355  -64.412056  -64.623028  -64.201250  -64.623028  -64.623028  -64.412056   \n",
       "16356  -26.931002  -25.632029  -14.127088  -22.467511  -21.910788  -22.467511   \n",
       "\n",
       "          EDA_137     EDA_138     EDA_139     EDA_140     EDA_141     EDA_142  \\\n",
       "0      271.128803  273.790815  276.452828  276.337179  275.758480  275.989960   \n",
       "1      -11.912868  -24.319049  -19.948183  -27.895205  -34.296956  -12.754102   \n",
       "2      374.563225  376.020586  380.392216  379.809271  386.220979  382.723767   \n",
       "3      -96.743625  -97.322865  -97.322865  -97.207035  -97.322865  -97.322865   \n",
       "4      -75.847350  -75.847350  -75.847350  -76.161629  -75.847350  -76.161629   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.488428  -52.302757  -52.673954  -52.673954  -52.673954  -51.931560   \n",
       "16353   78.192077   96.962618   78.852345   97.125741   69.389400   80.320839   \n",
       "16354  -42.696645  -42.608352  -42.696645  -42.696645  -42.696645  -42.608352   \n",
       "16355  -64.623028  -64.412056  -64.623028  -64.623028  -63.990278  -63.990278   \n",
       "16356  -21.539591  -18.013869  -18.013869  -17.271619  -15.787120  -13.931569   \n",
       "\n",
       "          EDA_143     EDA_144     EDA_145     EDA_146     EDA_147     EDA_148  \\\n",
       "0      277.957534  274.485254  272.401939  273.906555  276.337179  274.948212   \n",
       "1       -5.513496  -27.541997  -33.193220  -39.023407  -46.096628  -46.096628   \n",
       "2      381.849350  381.266405  383.015239  396.130127  406.039048  383.889428   \n",
       "3      -97.091115  -97.322865  -97.438696  -97.438696  -97.091115  -96.627704   \n",
       "4      -74.590724  -75.219037  -75.219037  -75.219037  -75.219037  -75.219037   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.673954  -53.045296  -52.488428  -53.045296  -52.859625  -53.045296   \n",
       "16353   33.495102   98.757358   74.120873  102.020465   93.699511   99.409980   \n",
       "16354  -42.696645  -42.696645  -42.696645  -42.740826  -42.696645  -42.696645   \n",
       "16355  -64.623028  -64.623028  -64.623028  -64.623028  -64.623028  -64.623028   \n",
       "16356  -17.271619  -17.457290  -13.560372  -12.818122   -7.436849   -7.808046   \n",
       "\n",
       "          EDA_149     EDA_150     EDA_151     EDA_152     EDA_153     EDA_154  \\\n",
       "0      273.906555  272.980638  273.559336  272.980638  273.443596  272.286200   \n",
       "1      -46.096628  -21.681621   -6.970439  -10.411743    9.279190    6.015507   \n",
       "2      380.100744  383.015239  405.456103  408.370599  408.662071  406.913465   \n",
       "3      -97.322865  -97.322865  -97.322865  -97.207035  -96.975285  -97.207035   \n",
       "4      -75.219037  -75.533071  -74.276445  -73.962412  -71.448916  -72.391508   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.859625  -53.045296  -53.045296  -52.859625  -52.673954  -52.673954   \n",
       "16353  100.225724   43.937042    9.021740   91.741648   35.126719   48.342268   \n",
       "16354  -42.740826  -42.740826  -42.652498  -42.696645  -42.696645  -42.740826   \n",
       "16355  -64.623028  -64.623028  -64.623028  -64.623028  -64.412056  -64.412056   \n",
       "16356   -6.509073   -7.065652   -8.921348   -8.550296   -6.323402  -10.034795   \n",
       "\n",
       "          EDA_155     EDA_156     EDA_157     EDA_158     EDA_159     EDA_160  \\\n",
       "0      272.401939  273.327857  271.013063  269.045398  267.077824  267.540782   \n",
       "1       10.872020  -17.560679    9.588252   15.772680   25.088353   42.931846   \n",
       "2      406.330520  402.833307  402.250363  403.124552  401.667418  402.250363   \n",
       "3      -97.207035  -97.207035  -97.091115  -97.091115  -97.207035  -96.975285   \n",
       "4      -72.705541  -72.391508  -72.705541  -72.705541  -72.077229  -71.763195   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -53.045296  -52.859625  -52.673954  -53.230967  -52.488428  -52.859625   \n",
       "16353   41.163434   18.811186   40.837059   29.742497   35.126719   53.073740   \n",
       "16354  -42.652498  -42.784973  -42.696645  -42.696645  -42.564205  -42.696645   \n",
       "16355  -64.412056  -64.412056  -64.412056  -64.412056  -64.412056  -63.990278   \n",
       "16356   -4.282324   -2.426628   -2.426628    2.398066    1.841343    0.913423   \n",
       "\n",
       "          EDA_161     EDA_162     EDA_163     EDA_164     EDA_165     EDA_166  \\\n",
       "0      268.813919  266.267646  266.730604  267.309303  266.730604  266.036166   \n",
       "1       37.599748   48.328217   41.882320   16.981665   -0.997508    5.662299   \n",
       "2      399.627340  397.587261  398.752923  399.044395  397.004317  397.004317   \n",
       "3      -97.207035  -97.091115  -96.975285  -96.511874  -96.975285  -97.091115   \n",
       "4      -72.391508  -71.134882  -76.161629  -75.533071  -75.219037  -74.276445   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -53.045296  -54.715901  -52.859625  -52.859625  -52.859625  -52.673954   \n",
       "16353   49.647511    4.127144    8.695493   34.310974    8.695493   -8.272661   \n",
       "16354  -42.652498  -42.740826  -42.564205  -42.696645  -42.652498  -42.696645   \n",
       "16355  -63.990278  -64.833999  -64.833999  -64.623028  -64.412056  -64.412056   \n",
       "16356    3.140171    7.779340    4.439144    4.253618    3.325842    4.253618   \n",
       "\n",
       "          EDA_167     EDA_168     EDA_169     EDA_170     EDA_171     EDA_172  \\\n",
       "0      266.036166  264.415811  263.721373  263.258324  262.795365  263.026844   \n",
       "1       17.936005   -4.271496   24.955878   42.622785   44.972829   45.988271   \n",
       "2      397.295789  395.838655  395.547183  397.878733  395.255710  394.381293   \n",
       "3      -97.438696  -97.554526  -97.438696  -97.438696  -97.207035  -97.554526   \n",
       "4      -75.533071  -75.219037  -75.847350  -71.763195  -69.878012  -67.678795   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.302757  -52.859625  -53.045296  -52.673954  -52.673954  -52.302757   \n",
       "16353  -10.401295   22.066646   50.292485   -8.606682    7.219480   27.287617   \n",
       "16354  -42.652498  -42.696645  -42.652498  -42.608352  -42.564205  -42.696645   \n",
       "16355  -64.412056  -64.412056  -64.412056  -64.623028  -64.201250  -64.623028   \n",
       "16356    2.583593   -1.498852  -29.333283   -2.055576   -1.127655    1.470146   \n",
       "\n",
       "          EDA_173     EDA_174     EDA_175     EDA_176     EDA_177     EDA_178  \\\n",
       "0      262.911105  264.647290  264.531460  265.804687  266.962084  266.614865   \n",
       "1       46.959567   47.268628   47.356922   48.019191   48.548985   44.972829   \n",
       "2      384.763845  381.849350  379.517799  378.060665  378.060665  377.477720   \n",
       "3      -97.554526  -97.786276  -97.438696  -96.975285  -96.975285  -96.859455   \n",
       "4      -68.621386  -70.192291  -73.019820  -76.161629  -79.303192  -75.219037   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -53.230967  -52.302757  -52.673954  -52.859625  -52.859625  -53.045296   \n",
       "16353    8.524723   52.413473   25.656000   32.182213   50.292485   34.466451   \n",
       "16354  -42.740826  -42.784973  -42.696645  -42.696645  -42.652498  -42.740826   \n",
       "16355  -65.044971  -65.044971  -65.044971  -65.255777  -65.255777  -65.255777   \n",
       "16356    2.026869    6.851564   11.305061   10.933864    8.707115    5.738117   \n",
       "\n",
       "          EDA_179     EDA_180     EDA_181     EDA_182     EDA_183     EDA_184  \\\n",
       "0      266.267646  266.151906  266.730604  266.499125  266.267646  265.688947   \n",
       "1       35.524682   45.458477   23.250976   39.542374   41.308345   43.074349   \n",
       "2      377.186248  377.769193  375.729114  376.312059  375.146170  374.854697   \n",
       "3      -97.207035  -96.975285  -96.859455  -97.207035  -97.438696  -97.438696   \n",
       "4      -74.276445  -70.192291  -65.479578  -61.709702  -58.253615  -53.541147   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.673954  -53.230967  -52.488428  -53.045296  -51.931560  -53.045296   \n",
       "16353   63.018664   77.220856   80.483962   83.420694   53.073740   50.463255   \n",
       "16354  -42.696645  -42.784973  -42.740826  -42.784973  -42.740826  -42.740826   \n",
       "16355  -65.255777  -65.466749  -65.466749  -65.677720  -65.677720  -65.255777   \n",
       "16356   12.232837   15.387361   12.418363   25.964526   23.923304   28.562327   \n",
       "\n",
       "          EDA_185     EDA_186     EDA_187     EDA_188     EDA_189     EDA_190  \\\n",
       "0      265.804687  266.036166  265.920427  267.077824  266.846344  267.540782   \n",
       "1       40.160462   23.339269   16.981665   -0.997508  -17.516532  -34.208662   \n",
       "2      375.437642  373.397563  373.980508  375.437642  373.397563  374.271753   \n",
       "3      -97.554526  -97.786276  -97.670446  -97.670446  -97.554526  -97.786276   \n",
       "4      -51.970243  -51.656210  -51.027897  -49.771026  -48.828680  -49.142714   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.673954  -52.859625  -53.045296  -53.045296  -52.859625  -52.859625   \n",
       "16353   57.968464   46.221281   41.489681  -14.146253   -6.159320    1.509012   \n",
       "16354  -42.740826  -42.784973  -42.740826  -42.696645  -42.696645  -42.740826   \n",
       "16355  -65.255777  -65.044971  -64.833999  -64.623028  -64.623028  -64.833999   \n",
       "16356   28.191275   31.345799   31.531326    7.408142   10.006088   23.366580   \n",
       "\n",
       "          EDA_191     EDA_192     EDA_193     EDA_194     EDA_195     EDA_196  \\\n",
       "0      267.193563  266.962084  267.425043  266.962084  266.730604  267.425043   \n",
       "1      -36.595168  -38.758527  -46.096628  -46.096628  -46.096628  -46.096628   \n",
       "2      373.689036  373.689036  373.397563  373.397563  373.106091  373.397563   \n",
       "3      -97.902106  -97.786276  -97.091115  -97.207035  -97.438696  -97.438696   \n",
       "4      -48.828680  -48.828680  -49.142714  -50.399339  -52.912835  -55.740364   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -52.673954  -52.488428  -53.230967  -53.416494  -53.973362  -52.859625   \n",
       "16353    3.793123   23.208638   29.408604   12.277327   21.903396   64.160783   \n",
       "16354  -42.696645  -42.696645  -42.608352  -42.740826  -42.740826  -42.696645   \n",
       "16355  -64.412056  -64.833999  -64.833999  -65.044971  -64.833999  -64.833999   \n",
       "16356   25.778855   25.036606   28.376801   28.376801   22.067607   18.356360   \n",
       "\n",
       "          EDA_197     EDA_198     EDA_199     EDA_200     EDA_201     EDA_202  \\\n",
       "0      268.003741  268.929749  269.508447  270.318535  271.707501  271.938981   \n",
       "1      -45.964188  -45.875860  -45.787567  -45.743420  -45.787567  -45.699274   \n",
       "2      374.563225  372.523146  372.523146  372.231674  372.814619  369.900123   \n",
       "3      -97.207035  -97.322865  -97.322865  -96.743625  -97.438696  -97.670446   \n",
       "4      -56.368677  -56.996990  -57.939581  -56.682956  -57.311269  -58.567894   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -53.230967  -53.416494  -53.602165  -53.416494  -52.673954  -53.045296   \n",
       "16353   40.673936   35.126719   31.863612  -15.940993  -13.501278  -17.253882   \n",
       "16354  -42.740826  -42.740826  -42.608352  -42.696645  -42.564205  -42.740826   \n",
       "16355  -64.623028  -64.833999  -64.833999  -65.044971  -65.044971  -65.044971   \n",
       "16356   14.459586   19.655333   24.665553   22.253278    8.521589    5.552591   \n",
       "\n",
       "          EDA_203     EDA_204     EDA_205     EDA_206     EDA_207     EDA_208  \\\n",
       "0      271.360282  271.938981  271.591762  273.675076  273.327857  274.138035   \n",
       "1      -45.699274  -45.699274  -45.699274  -45.743420  -23.977420  -31.968589   \n",
       "2      373.397563  372.523146  373.397563  372.814619  371.940202  372.231674   \n",
       "3      -97.670446  -97.554526  -97.322865  -97.438696  -97.438696  -97.091115   \n",
       "4      -57.311269  -59.824519  -60.138798  -60.138798  -60.138798  -60.452832   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -53.602165  -53.416494  -52.859625  -53.602165  -52.488428  -54.159033   \n",
       "16353   13.419319  -21.332733  -28.674818  -15.622392    1.509012   25.819251   \n",
       "16354  -42.740826  -42.696645  -42.696645  -42.740826  -42.740826  -42.696645   \n",
       "16355  -65.044971  -64.201250  -64.833999  -64.833999  -64.833999  -64.623028   \n",
       "16356    9.078313    0.171173    5.923643   15.944085   16.871861   19.098609   \n",
       "\n",
       "          EDA_209     EDA_210     EDA_211     EDA_212     EDA_213     EDA_214  \\\n",
       "0      273.559336  271.476022  270.202885  268.466700  267.656522  267.888001   \n",
       "1      -30.909000  -32.807445  -25.390218  -17.487342  -46.096628  -46.096628   \n",
       "2      371.648957  371.940202  371.648957  372.231674  372.814619  372.523146   \n",
       "3      -97.091115  -97.207035  -96.859455  -97.322865  -97.438696  -97.438696   \n",
       "4      -60.452832  -60.452832  -61.081390  -60.767111  -61.081390  -61.709702   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -53.416494  -53.416494  -53.416494  -52.859625  -52.117086  -51.374547   \n",
       "16353   32.508587   31.692715  -11.869788  -64.568989   27.777115   18.150919   \n",
       "16354  -42.740826  -43.226508  -44.109649  -45.125194  -44.551184  -43.977174   \n",
       "16355  -64.623028  -63.990278  -64.833999  -64.833999  -64.833999  -64.833999   \n",
       "16356   21.696555   22.253278   25.222277   24.108830   31.160273   34.129271   \n",
       "\n",
       "          EDA_215     EDA_216     EDA_217     EDA_218     EDA_219     EDA_220  \\\n",
       "0      269.045398  272.749158  272.286200  272.054720  271.707501  270.897323   \n",
       "1      -46.096628  -46.096628  -45.920041  -45.787567  -45.787567  -45.699274   \n",
       "2      371.648957  372.523146  374.271753  374.271753  373.689036  375.146170   \n",
       "3      -97.322865  -97.438696  -97.438696  -97.670446  -97.322865  -97.670446   \n",
       "4      -60.767111  -60.138798  -59.824519  -59.824519  -60.767111  -60.767111   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.189021  -51.003350  -50.632153  -51.374547  -51.003350  -50.632153   \n",
       "16353   40.340043   56.818698   35.445319   -2.569839    9.014221   -2.733090   \n",
       "16354  -44.153795  -44.330416  -44.242089  -43.447311  -43.844700  -43.756406   \n",
       "16355  -64.623028  -64.833999  -64.623028  -64.623028  -64.833999  -64.623028   \n",
       "16356   35.428244   38.211716   40.067268   43.964042   43.221792   43.778516   \n",
       "\n",
       "          EDA_221     EDA_222     EDA_223     EDA_224     EDA_225     EDA_226  \\\n",
       "0      270.665844  270.550104  269.971316  268.813919  268.466700  268.119481   \n",
       "1      -45.699274  -22.255562   -5.469314    9.588252   -5.286938    0.408431   \n",
       "2      375.146170  376.020586  374.271753  373.689036  373.106091  373.689036   \n",
       "3      -97.670446  -97.438696  -97.670446  -97.554526  -97.554526  -97.670446   \n",
       "4      -61.395423  -59.510485  -57.625302  -54.797773  -51.027897  -46.629463   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.189021  -51.189021  -50.446482  -50.632153  -51.003350  -51.189021   \n",
       "16353  -18.722248   14.724561   31.529591  -59.511142   25.329753    1.998510   \n",
       "16354  -43.712225  -43.668078  -43.623932  -43.491457  -43.756406  -43.668078   \n",
       "16355  -64.201250  -64.412056  -64.412056  -64.412056  -64.412056  -64.201250   \n",
       "16356   43.036266   30.046826   23.181054   21.696555   14.274059   24.294356   \n",
       "\n",
       "          EDA_227     EDA_228     EDA_229     EDA_230     EDA_231     EDA_232  \\\n",
       "0      267.540782  267.540782  267.425043  267.772262  265.457468  268.929749   \n",
       "1      -17.737265   -7.144648  -25.290345  -34.076222  -46.096628  -46.096628   \n",
       "2      372.814619  372.231674  371.357485  371.940202  370.483068  371.357485   \n",
       "3      -97.670446  -97.322865  -96.859455  -96.975285  -97.786276  -97.786276   \n",
       "4      -43.801934  -42.545063  -39.717534  -39.089221  -39.089221  -38.146630   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.374547  -52.117086  -52.859625  -52.117086  -52.673954  -52.302757   \n",
       "16353   41.645285   11.950953   34.140077   61.060800   11.142854   -8.925282   \n",
       "16354  -44.374563  -45.743386  -45.743386  -45.743386  -45.743386  -45.787532   \n",
       "16355  -64.412056  -64.412056  -64.623028  -64.201250  -63.357364  -63.357364   \n",
       "16356   30.603550   33.015825   34.685995   35.242573   36.170494   34.314798   \n",
       "\n",
       "          EDA_233     EDA_234     EDA_235     EDA_236     EDA_237     EDA_238  \\\n",
       "0      265.573208  265.804687  265.573208  268.119481  267.888001  266.267646   \n",
       "1      -46.096628  -46.096628  -45.920041  -45.787567  -45.831714  -45.787567   \n",
       "2      369.900123  369.608878  369.900123  369.900123  369.608878  369.025934   \n",
       "3      -97.786276  -97.786276  -97.786276  -97.786276  -97.786276  -97.670446   \n",
       "4      -35.947413  -35.947413  -34.376754  -33.119883  -32.177537  -31.549224   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -51.560218  -47.847667  -47.290799  -47.290799  -46.919456  -46.919456   \n",
       "16353   49.313617   13.419319   31.529591   30.713847    1.019514   -6.811941   \n",
       "16354  -45.699239  -45.787532  -45.610911  -45.566765  -45.566765  -45.655058   \n",
       "16355  -63.990278  -63.990278  -63.990278  -63.990278  -63.990278  -64.201250   \n",
       "16356   33.943745   34.500324   35.057047   33.201496   34.500324   33.572548   \n",
       "\n",
       "          EDA_239       TEMP_0       TEMP_1       TEMP_2       TEMP_3  \\\n",
       "0      266.614865 -1831.016076 -1831.016076 -1831.016076 -1831.016076   \n",
       "1      -45.787567   237.318204   237.318204   237.318204   237.318204   \n",
       "2      369.900123 -2282.154114 -2282.154114 -2282.154114 -2282.154114   \n",
       "3      -97.670446   279.337205   279.337205   279.337205   279.337205   \n",
       "4      -31.234945   548.889178   548.889178   548.889178   548.889178   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "16352  -46.919456  1702.123736  1702.123736  1702.123736  1702.123736   \n",
       "16353  -35.364155   530.440199   530.440199   530.440199   530.440199   \n",
       "16354  -45.610911 -2104.362238 -2104.362238 -2104.362238 -2104.362238   \n",
       "16355  -63.990278  6819.633542  6819.633542  6819.633542  6819.633542   \n",
       "16356   35.057047  1158.292377  1158.292377  1158.292377  1158.292377   \n",
       "\n",
       "            TEMP_4       TEMP_5       TEMP_6       TEMP_7       TEMP_8  ...  \\\n",
       "0     -1831.016076 -1831.016076 -1831.016076 -1831.016076 -1843.006719  ...   \n",
       "1       252.045754   252.045754   252.045754   252.045754   266.773304  ...   \n",
       "2     -2282.154114 -2282.154114 -2282.154114 -2282.154114 -2310.992450  ...   \n",
       "3       267.346561   267.346561   267.346561   267.346561   291.327848  ...   \n",
       "4       548.889178   548.889178   548.889178   548.889178   565.200219  ...   \n",
       "...            ...          ...          ...          ...          ...  ...   \n",
       "16352  1689.175370  1689.175370  1689.175370  1689.175370  1702.123736  ...   \n",
       "16353   524.355934   524.355934   524.355934   524.355934   530.440199  ...   \n",
       "16354 -2089.634688 -2089.634688 -2089.634688 -2089.634688 -2089.634688  ...   \n",
       "16355  6761.305782  6761.305782  6761.305782  6761.305782  6761.305782  ...   \n",
       "16356  1171.240743  1171.240743  1171.240743  1171.240743  1158.292377  ...   \n",
       "\n",
       "         BVP_3591    BVP_3592    BVP_3593    BVP_3594    BVP_3595    BVP_3596  \\\n",
       "0      -62.958960  -66.060810  -70.568737  -75.758506  -80.517266  -83.643846   \n",
       "1       22.954543    8.774783   -2.938060  -11.064180  -15.794109  -18.508387   \n",
       "2      -51.019060  -46.956865  -40.932037  -33.693116  -25.915611  -17.864250   \n",
       "3       38.112586   49.364740   60.722880   72.013895   83.304911   94.627722   \n",
       "4      459.286333  473.555295  478.348699  470.880469  452.379188  426.917555   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    2.254561    2.132944    2.260493    2.560087    2.933837    3.271992   \n",
       "16353   29.748289   41.216813   52.225963   60.387782   63.825171   61.496617   \n",
       "16354  -63.431018  -63.588125  -63.544670  -63.424333  -63.347451  -63.431018   \n",
       "16355  -34.195681  -34.068512  -36.989166  -41.143362  -44.937245  -47.344983   \n",
       "16356   11.814850   12.796686   14.033621   15.415903   16.756657   17.848244   \n",
       "\n",
       "         BVP_3597    BVP_3598    BVP_3599    BVP_3600    BVP_3601    BVP_3602  \\\n",
       "0      -84.272695  -82.241301  -78.167915  -73.119461  -68.003882  -63.079077   \n",
       "1      -21.841065  -28.366028  -39.871623  -56.478189  -76.587911  -97.426342   \n",
       "2       -9.411234   -0.264450    9.786058   20.612491   31.758422   42.639626   \n",
       "3      106.010592  117.481783  128.949441  140.007288  149.440020  154.866491   \n",
       "4      373.382344  283.076742  161.539043   27.855797  -96.414771 -197.729239   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    3.455900    3.402507    3.082150    2.503727    1.688003    0.599382   \n",
       "16353   53.366479   40.325785   24.010067    6.502350  -10.023254  -23.681728   \n",
       "16354  -63.745233  -64.283408  -65.005433  -65.854480  -66.763696  -67.606059   \n",
       "16355  -48.027458  -47.506064  -46.700659  -46.526861  -47.866377  -51.079521   \n",
       "16356   18.566082   18.960596   19.260189   19.761489   20.681033   21.956529   \n",
       "\n",
       "         BVP_3603    BVP_3604    BVP_3605    BVP_3606    BVP_3607    BVP_3608  \\\n",
       "0      -57.723264  -50.692876  -40.758478  -27.241761  -10.255778    9.499966   \n",
       "1     -115.727660 -128.617136 -131.999955 -124.111168 -106.826033  -85.944146   \n",
       "2       52.754034   61.982975   70.499890   78.733822   87.122938   95.986737   \n",
       "3      153.241376  141.999820  120.399924   90.550803   57.168824   26.146795   \n",
       "4     -272.353493 -322.691489 -353.541469 -370.117650 -373.077855 -363.611970   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -0.842226   -2.773268   -5.264935   -8.257902  -11.491136  -14.561226   \n",
       "16353  -33.372156  -38.845050  -40.464741  -39.027216  -35.375980  -30.192175   \n",
       "16354  -68.264572  -68.585471  -68.451763  -67.813306  -66.713556  -65.276192   \n",
       "16355  -55.954343  -61.626092  -66.835793  -70.290558  -71.125636  -69.268965   \n",
       "16356   23.208295   23.851976   23.410001   21.793384   19.408503   17.002857   \n",
       "\n",
       "         BVP_3609    BVP_3610    BVP_3611    BVP_3612    BVP_3613    BVP_3614  \\\n",
       "0       31.025672   53.226154   74.797788   94.044800  108.995857  117.898660   \n",
       "1      -67.682941  -55.739451  -50.414519  -49.649040  -50.865785  -52.149384   \n",
       "2      105.334349  114.855402  123.755716  130.903353  135.002061  134.865133   \n",
       "3        2.589695  -10.905824  -14.763706  -11.513476   -4.309978    4.359657   \n",
       "4     -344.646343 -321.085626 -297.998929 -278.786618 -264.643416 -254.969543   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -17.035096  -18.601286  -19.179709  -18.897914  -17.981336  -16.655413   \n",
       "16353  -23.998538  -17.329686  -10.866761   -5.417628   -1.726791   -0.186302   \n",
       "16354  -63.651637  -61.990312  -60.322301  -58.584094  -56.585155  -54.138294   \n",
       "16355  -65.445409  -61.075025  -57.755907  -56.450303  -57.319293  -59.451498   \n",
       "16356   15.276488   14.543819   14.597212   14.908670   14.935367   14.392539   \n",
       "\n",
       "         BVP_3615    BVP_3616    BVP_3617    BVP_3618    BVP_3619    BVP_3620  \\\n",
       "0      119.961850  115.807208  107.388406   97.189044   87.233449   78.383639   \n",
       "1      -53.432983  -55.555602  -59.249292  -63.969194  -68.251201  -70.764916   \n",
       "2      129.807929  119.885221  105.845546   88.802587   69.906536   49.942448   \n",
       "3       13.361380   22.606871   32.502407   43.507261   55.833404   69.487902   \n",
       "4     -248.052724 -238.533634 -222.407288 -198.744996 -170.120006 -141.001650   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -15.083290  -13.395482  -11.746236  -10.349122   -9.402882   -9.014301   \n",
       "16353   -0.649637   -2.518816   -4.898852   -7.017519   -8.498606   -9.571800   \n",
       "16354  -51.096432  -47.432825  -43.214330  -38.598052  -33.774527  -28.930945   \n",
       "16355  -61.185239  -60.875793  -57.213319  -49.858696  -39.371470  -27.370931   \n",
       "16356   13.283154   11.740694    9.845247    7.537488    4.669105    1.091783   \n",
       "\n",
       "         BVP_3621    BVP_3622    BVP_3623    BVP_3624    BVP_3625    BVP_3626  \\\n",
       "0       70.346386   62.316199   53.805543   45.164171   37.512000   32.159720   \n",
       "1      -70.734832  -68.411651  -64.490656  -60.064913  -55.953384  -52.303149   \n",
       "2       29.439776    8.864077  -11.017855  -28.982796  -43.643210  -53.867160   \n",
       "3       84.612068  101.612183  120.965181  142.957225  166.934735  190.813325   \n",
       "4     -115.520669  -96.013305  -83.427597  -77.405611  -74.479265  -71.233680   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -9.091424   -9.373219   -9.536364   -9.340590   -8.732505   -7.821860   \n",
       "16353  -10.973685  -13.634889  -18.212795  -24.754922  -32.599932  -40.642947   \n",
       "16354  -24.241128  -19.872211  -15.967930  -12.635252   -9.837406   -7.420630   \n",
       "16355  -15.773093   -6.850050   -2.895086   -5.459666  -14.891386  -29.261514   \n",
       "16356   -3.292365   -8.649449  -15.308726  -23.729968  -34.257262  -46.712631   \n",
       "\n",
       "         BVP_3627    BVP_3628    BVP_3629    BVP_3630    BVP_3631    BVP_3632  \\\n",
       "0       29.990545   31.064534   34.600925   39.331422   43.849948   46.877608   \n",
       "1      -48.385497  -42.970312  -35.947286  -27.199423  -17.208074   -6.748746   \n",
       "2      -59.389919  -60.868740  -59.672903  -57.272101  -54.633956  -52.041455   \n",
       "3      211.197918  224.590984  228.565450  223.502864  212.522740  200.087078   \n",
       "4      -65.569628  -57.201728  -47.024814  -36.175566  -25.679414  -16.513420   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -6.789598   -5.745471   -4.698377   -3.594924   -2.381720   -1.082493   \n",
       "16353  -47.707811  -52.919337  -55.731026  -55.818149  -52.994579  -47.363280   \n",
       "16354   -5.080735   -2.503509    0.545040    4.101679    7.995932   11.926955   \n",
       "16355  -44.466719  -55.577074  -58.578269  -52.215566  -38.680517  -22.513065   \n",
       "16356  -60.099410  -72.685295  -82.471021  -87.869633  -88.195923  -83.835505   \n",
       "\n",
       "         BVP_3633    BVP_3634    BVP_3635    BVP_3636    BVP_3637    BVP_3638  \\\n",
       "0       47.464062   45.040521   39.458605   31.145789   21.186661   11.259329   \n",
       "1        3.483279   12.852885   20.818554   27.320118   32.531397   36.388880   \n",
       "2      -49.284640  -46.144426  -42.748614  -39.736200  -37.956138  -38.093066   \n",
       "3      189.824124  182.542903  176.000049  166.549653  152.096730  133.111148   \n",
       "4      -10.264098   -7.594109   -8.856549  -13.886963  -21.988832  -31.812649   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    0.184104    1.260860    2.029124    2.438470    2.521525    2.337616   \n",
       "16353  -39.383627  -29.934767  -20.117615  -11.025166   -3.322722    2.965958   \n",
       "16354   15.593904   18.822958   21.607433   24.087721   26.451014   28.861106   \n",
       "16355   -8.571074   -0.148229    1.933108   -1.161345   -7.651216  -16.082539   \n",
       "16356  -75.965990  -66.061613  -55.362276  -44.547254  -33.664008  -22.303192   \n",
       "\n",
       "         BVP_3639   BVP_3640   BVP_3641   BVP_3642   BVP_3643   BVP_3644  \\\n",
       "0        3.335128  -0.830112  -0.282519   4.727074  12.626546  20.829843   \n",
       "1       39.220153  41.392912  43.298255  45.952363  50.114033  55.756521   \n",
       "2      -40.302169 -44.218307 -49.156840 -54.405743 -59.472076 -64.109367   \n",
       "3      113.210557  96.867554  86.594002  81.340641  76.910437  68.438643   \n",
       "4      -41.399457 -48.577470 -51.605393 -50.594473 -46.642696 -41.505870   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "16352    1.949035   1.379511   0.649808  -0.183714  -0.996472  -1.595658   \n",
       "16353    8.466573  14.006789  20.196465  27.162327  34.381636  40.860402   \n",
       "16354   31.351422  33.845081  36.154891  38.093661  39.507626  40.306532   \n",
       "16355  -25.726208 -36.192239 -46.738810 -55.962821 -62.499321 -65.318240   \n",
       "16356   -9.972406   3.538956  17.925367  32.382969  45.852803  57.433123   \n",
       "\n",
       "        BVP_3645   BVP_3646   BVP_3647   BVP_3648   BVP_3649   BVP_3650  \\\n",
       "0      26.461219  27.238448  22.366636  12.845583   1.211880  -9.390228   \n",
       "1      62.191231  68.401980  73.626630  77.784957  81.294798  84.821354   \n",
       "2     -68.226333 -71.768201 -74.661944 -76.834534 -78.212941 -78.760653   \n",
       "3      53.084840  31.990142   7.987903 -15.102860 -35.226021 -52.759598   \n",
       "4     -36.896269 -34.100520 -33.790956 -35.895023 -39.667834 -44.030750   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "16352  -1.785500  -1.414716  -0.444745   1.047289   2.874511   4.802587   \n",
       "16353  45.533350  47.616376  46.903554  43.854257  39.442677  34.892492   \n",
       "16354  40.473668  40.005689  38.919309  37.161046  34.670729  31.418276   \n",
       "16355 -64.398382 -60.913944 -56.564755 -52.872608 -50.994741 -51.168539   \n",
       "16356  66.687887  73.729813  79.048335  83.129923  86.202980  88.178516   \n",
       "\n",
       "        BVP_3651   BVP_3652    BVP_3653    BVP_3654    BVP_3655    BVP_3656  \\\n",
       "0     -16.339360 -18.497937  -16.455945  -12.092865   -7.659127   -4.836373   \n",
       "1      90.169684  98.392742  109.353476  121.788344  133.822087  143.987257   \n",
       "2     -78.550697 -77.774772  -76.697606  -75.702596  -75.136628  -75.300941   \n",
       "3     -69.388762 -86.187504 -102.099498 -113.959304 -117.417972 -111.666479   \n",
       "4     -49.138555 -55.769995  -64.602241  -75.751379  -88.917519 -103.733055   \n",
       "...          ...        ...         ...         ...         ...         ...   \n",
       "16352   6.582349   8.015058    8.964264    9.382509    9.278689    8.715098   \n",
       "16353  31.344219  29.467120   29.277034   30.140341   31.047210   31.134333   \n",
       "16354  27.483911  23.051482   18.438547   13.989405    9.984843    6.615395   \n",
       "16355 -52.508056 -54.186690  -55.543162  -56.344329  -56.958980  -57.861882   \n",
       "16356  88.851859  88.119190   85.998307   82.435816   77.022373   68.897758   \n",
       "\n",
       "         BVP_3657    BVP_3658    BVP_3659    BVP_3660    BVP_3661    BVP_3662  \\\n",
       "0       -4.242853   -5.592405   -8.164326  -11.403957  -15.162919  -19.603722   \n",
       "1      151.916157  158.093478  163.532061  170.979611  182.669055  199.496240   \n",
       "2      -76.259437  -77.774772  -79.335750  -80.248603  -79.883462  -77.875186   \n",
       "3      -98.425325  -82.432075  -69.466485  -63.831576  -66.460023  -74.571465   \n",
       "4     -119.772336 -136.382376 -145.050165 -138.892746 -115.854418  -80.230382   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    7.786655    6.600147    5.280157    3.945335    2.711366    1.646475   \n",
       "16353   30.104700   28.405806   27.043523   27.031643   28.789938   31.870916   \n",
       "16354    3.891089    1.691588   -0.153586   -1.821596   -3.449494   -5.114162   \n",
       "16355  -59.209876  -61.062308  -63.164840  -65.059662  -66.386462  -66.852749   \n",
       "16356   57.020811   40.664796   19.966161   -3.784766  -28.286159  -50.734894   \n",
       "\n",
       "         BVP_3663    BVP_3664    BVP_3665    BVP_3666    BVP_3667    BVP_3668  \\\n",
       "0      -24.906542  -31.004256  -37.465854  -43.556502  -48.382386  -51.067359   \n",
       "1      220.438295  242.874541  263.328561  278.370740  285.066181  271.481422   \n",
       "2      -74.150747  -68.901844  -62.374947  -54.706985  -45.925342  -36.002633   \n",
       "3      -83.135114  -86.014394  -81.167312  -68.152262  -48.537832  -24.503796   \n",
       "4      -39.604954   -0.474139   33.611752   61.661145   84.177084  101.551359   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    0.762527    0.023925   -0.637553   -1.260470   -1.838892   -2.319428   \n",
       "16353   35.130100   37.256687   37.300249   34.888532   30.088860   23.170520   \n",
       "16354   -6.822285   -8.527065  -10.121536  -11.458619  -12.401262  -12.892640   \n",
       "16355  -66.428851  -65.182593  -63.109734  -60.057671  -55.869563  -50.202053   \n",
       "16356  -68.538449  -79.955624  -84.511815  -82.966388  -76.977488  -68.511752   \n",
       "\n",
       "         BVP_3669    BVP_3670    BVP_3671    BVP_3672    BVP_3673    BVP_3674  \\\n",
       "0      -50.954307  -47.792399  -41.949051  -34.459391  -26.888476  -20.917945   \n",
       "1      232.064228  169.859805   96.250067   25.164072  -33.510453  -76.584568   \n",
       "2      -25.121429  -13.756413   -2.619610    7.531312   16.212541   23.232378   \n",
       "3        2.568498   32.477677   65.516969   93.843427  122.908253  147.157793   \n",
       "4      114.461142  122.495293  125.305553  123.419148  118.233952  111.447731   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -2.657583   -2.841492   -2.894885   -2.868188   -2.740639   -2.381720   \n",
       "16353   14.489924    4.486646   -6.055208  -16.014925  -24.176744  -29.558555   \n",
       "16354  -12.989579  -12.926067  -13.043062  -13.671491  -15.005231  -17.007512   \n",
       "16355  -42.961883  -34.191442  -24.094202  -13.229708   -2.263478    7.846479   \n",
       "16356  -59.274786  -50.355211  -42.180170  -34.743730  -27.945038  -21.855285   \n",
       "\n",
       "         BVP_3675    BVP_3676    BVP_3677    BVP_3678    BVP_3679    BVP_3680  \\\n",
       "0      -17.812562  -17.957410  -20.702441  -24.602716  -27.877676  -28.976395   \n",
       "1     -105.569175 -124.004201 -135.105329 -139.521044 -137.331571 -129.536380   \n",
       "2       28.782522   33.173344   36.660441   39.307714   40.996491   41.562460   \n",
       "3      161.854484  164.094317  154.258839  135.958632  114.422328   94.415750   \n",
       "4      104.492216   98.242895   93.613947   89.908853   85.623328   79.509440   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -1.563029    0.000195    2.521525    6.060286   10.462231   15.377341   \n",
       "16353  -31.716823  -30.944599  -28.081428  -24.117342  -19.844366  -15.666434   \n",
       "16354  -19.414260  -21.804295  -23.736380  -24.862872  -25.033350  -24.321353   \n",
       "16355   16.290518   22.445510   26.188526   27.595866   26.930347   24.637061   \n",
       "16356  -16.762199  -13.036563  -10.957208  -10.565660  -11.669113  -13.932377   \n",
       "\n",
       "         BVP_3681    BVP_3682   BVP_3683   BVP_3684   BVP_3685   BVP_3686  \\\n",
       "0      -26.892008  -21.377217 -12.930152  -2.734323   7.496835  15.848512   \n",
       "1     -118.689298 -107.935811 -99.632529 -94.551615 -92.298631 -91.757112   \n",
       "2       40.905206   39.134272  36.486999  33.273758  29.750146  26.089607   \n",
       "3       78.641538   68.590556  62.295002  59.101298  59.578234  65.202544   \n",
       "4       71.068986   60.471259  48.117724  34.477563  20.242460   6.481376   \n",
       "...           ...         ...        ...        ...        ...        ...   \n",
       "16352   20.363642   24.967293  28.829377  31.727424  33.545747  34.207226   \n",
       "16353  -11.829072   -8.510486  -5.948285  -4.451357  -4.296913  -5.449309   \n",
       "16354  -22.964214  -21.332974 -19.781958 -18.575241 -17.796390 -17.345125   \n",
       "16355   20.995781   16.455838  11.402979   6.146649   1.042923  -3.641145   \n",
       "16356  -16.943141  -20.238668 -23.308758 -25.601685 -26.619115 -26.034760   \n",
       "\n",
       "        BVP_3687   BVP_3688    BVP_3689    BVP_3690    BVP_3691    BVP_3692  \\\n",
       "0      20.692061  21.176063   17.505423   10.824788    2.731009   -5.320375   \n",
       "1     -90.593850 -87.137492  -81.073823  -73.502593  -65.316305  -57.718333   \n",
       "2      22.383425  18.649858   14.907161   11.182723    7.449155    3.578660   \n",
       "3      77.051751  94.352159  113.807610  130.281329  139.618674  137.869909   \n",
       "4      -5.997920 -16.542441  -24.639473  -30.216460  -34.013455  -37.583114   \n",
       "...          ...        ...         ...         ...         ...         ...   \n",
       "16352  33.673297  31.946928   29.128971   25.468593   21.327680   17.091845   \n",
       "16353  -7.437292  -9.302512  -10.023254   -9.060944   -6.621506   -3.603891   \n",
       "16354 -16.987455 -16.449280  -15.506636  -14.089329  -12.277582  -10.225160   \n",
       "16355  -7.782624 -11.428143  -14.548029  -17.307603  -19.863705  -22.279921   \n",
       "16356 -23.792260 -20.140781  -15.572724  -10.681344   -6.012435   -1.951611   \n",
       "\n",
       "         BVP_3693    BVP_3694   BVP_3695   BVP_3696   BVP_3697    BVP_3698  \\\n",
       "0      -12.421420  -18.218841 -22.638447 -25.574252 -26.800154  -26.157174   \n",
       "1      -51.146572  -45.290151 -39.503926 -32.414045 -23.151406  -11.742749   \n",
       "2       -0.583949   -5.084312  -9.739861 -13.993755 -16.997040  -17.891636   \n",
       "3      124.823063  103.050056  76.398173  48.446197  21.246720   -4.854038   \n",
       "4      -42.768310  -50.904037 -62.353065 -76.815505 -93.599674 -111.849435   \n",
       "...           ...         ...        ...        ...        ...         ...   \n",
       "16352   13.063650    9.355812   5.926804   2.655007  -0.533733   -3.633486   \n",
       "16353   -1.283257   -0.784281  -2.633660  -6.498742 -11.108329  -14.834807   \n",
       "16354   -8.119255   -6.083547  -4.114693  -2.102384   0.097117    2.564034   \n",
       "16355  -24.891130  -27.731243 -30.834174 -34.212637 -37.951414  -41.855510   \n",
       "16356    1.308321    3.779224   5.618311   7.053986   8.317617    9.572350   \n",
       "\n",
       "         BVP_3699    BVP_3700    BVP_3701    BVP_3702    BVP_3703    BVP_3704  \\\n",
       "0      -23.878480  -20.734236  -17.865555  -16.325229  -16.600792  -18.342491   \n",
       "1        0.354505   11.836702   21.313275   28.326273   33.153141   36.405594   \n",
       "2      -16.184601  -11.930708   -5.750695    1.488227    9.037519   16.486397   \n",
       "3      -30.672167  -56.522092  -83.057391 -109.794064 -136.014939 -160.974583   \n",
       "4     -130.776366 -149.727483 -168.132026 -185.307986 -200.287978 -211.867604   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -6.567127   -9.275332  -11.764033  -14.119252  -16.400314  -18.610185   \n",
       "16353  -16.723787  -16.850511  -15.995124  -14.985292  -14.363552  -14.363552   \n",
       "16354    5.218143    7.792027    9.964786   11.475690   12.291310   12.612210   \n",
       "16355  -45.865581  -49.765438  -53.194770  -55.950104  -57.671128  -58.167088   \n",
       "16356   10.889374   12.224196   13.440366   14.380674   14.905704   14.950198   \n",
       "\n",
       "         BVP_3705    BVP_3706    BVP_3707    BVP_3708    BVP_3709    BVP_3710  \\\n",
       "0      -20.469272  -21.592721  -20.543462  -16.865756  -11.054204   -4.500752   \n",
       "1       39.113186   41.820778   44.752332   47.563548   49.846616   50.602068   \n",
       "2       23.780090   30.991626   37.974949   44.319275   49.431250   52.817934   \n",
       "3     -184.051214 -204.481735 -221.280476 -233.550093 -240.742993 -243.565747   \n",
       "4     -218.929531 -220.656317 -216.704540 -207.369253 -193.588821 -176.712751   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -20.615384  -22.226069  -23.276129  -23.697339  -23.507498  -22.789660   \n",
       "16353  -14.989252  -16.189170  -17.147521  -16.917833  -14.553638   -9.413395   \n",
       "16354   12.759289   13.060132   13.651792   14.484125   15.336516   15.988343   \n",
       "16355  -57.264186  -55.081114  -51.677216  -47.294116  -42.139521  -36.395709   \n",
       "16356   14.540853   13.766656   12.752192   11.604245   10.379175    9.100713   \n",
       "\n",
       "         BVP_3711    BVP_3712    BVP_3713    BVP_3714    BVP_3715    BVP_3716  \\\n",
       "0        0.883324    3.211478    1.169486   -5.595938  -16.427682  -29.827814   \n",
       "1       49.097850   45.962392   41.844177   37.592255   33.811654   30.666167   \n",
       "2       54.397169   54.552354   53.931614   53.201332   52.671877   52.215451   \n",
       "3     -243.180665 -240.986760 -238.100415 -235.259997 -232.864719 -231.243137   \n",
       "4     -158.196958 -139.178125 -120.309236 -101.822466  -83.722650  -66.004952   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -21.600186  -19.945007  -17.791495  -15.107020  -11.947942   -8.489271   \n",
       "16353   -1.560466    8.110162   18.145120   26.013890   29.791850   28.892902   \n",
       "16354   16.319271   16.346013   16.202276   16.031798   15.974972   16.101995   \n",
       "16355  -30.287346  -23.869537  -17.320319  -10.783819   -4.353293    1.784743   \n",
       "16356    7.751060    6.315385    4.787756    3.194868    1.581217    0.020959   \n",
       "\n",
       "         BVP_3717    BVP_3718    BVP_3719    BVP_3720    BVP_3721    BVP_3722  \\\n",
       "0      -43.945116  -57.066152  -68.081605  -76.708845  -83.403611  -89.003192   \n",
       "1       28.002030   25.568540   22.302716   17.602870   11.539201    4.753507   \n",
       "2       51.412141   49.832905   47.240404   43.780692   39.782397   35.574146   \n",
       "3     -230.635485 -231.038231 -232.091023 -232.543229 -230.635485 -224.573100   \n",
       "4      -48.785459  -32.359223  -17.118037   -3.477876    8.256531   18.002958   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -4.983139   -1.735073    0.973132    2.951634    4.108480    4.476297   \n",
       "16353   23.875422   15.788845    5.603402   -5.873043  -17.994988  -29.685279   \n",
       "16354   16.442951   16.984470   17.693124   18.485345   19.287595   20.073131   \n",
       "16355    7.490405   12.742495   17.600361   22.208128   26.904913   31.864514   \n",
       "16356   -1.388019   -2.530033   -3.298297   -3.624587   -3.502970   -2.998704   \n",
       "\n",
       "         BVP_3723    BVP_3724    BVP_3725    BVP_3726    BVP_3727    BVP_3728  \\\n",
       "0      -94.214158  -99.223751 -103.558569 -106.307132 -106.487308 -103.491444   \n",
       "1       -1.898479   -7.694732  -12.374521  -16.084925  -19.120102  -23.241659   \n",
       "2       31.311124   26.856403   21.899613   16.148641    9.430045    1.825982   \n",
       "3     -213.176099 -196.048802 -173.636348 -146.747763 -116.386377  -83.340020   \n",
       "4       25.950044   32.586321   38.477709   44.078882   49.525272   54.652425   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    4.203400    3.538956    2.779591    2.168539    1.824452    1.664273   \n",
       "16353  -39.288584  -45.086208  -46.183163  -43.054664  -37.328322  -31.114884   \n",
       "16354   20.845296   21.744484   22.984628   24.849858   27.530708   31.047236   \n",
       "16355   37.048781   42.381412   47.561441   52.258226   56.247102   59.566219   \n",
       "16356   -2.212642   -1.254537   -0.207444    0.887110    1.996495    3.079184   \n",
       "\n",
       "        BVP_3729   BVP_3730   BVP_3731   BVP_3732    BVP_3733    BVP_3734  \\\n",
       "0     -97.217088 -87.939802 -76.058799 -61.835511  -45.337062  -26.630577   \n",
       "1     -29.679712 -38.397490 -48.171563 -57.310523  -64.697904  -69.909183   \n",
       "2      -6.453591 -14.952250 -23.076639 -30.069090  -35.153680  -37.664025   \n",
       "3     -48.120955 -11.093066  27.400959  66.647483  105.643174  143.027882   \n",
       "4      59.174960  62.938096  66.077268  68.916550   71.760668   74.624135   \n",
       "...          ...        ...        ...        ...         ...         ...   \n",
       "16352   1.456634   0.910840  -0.133287  -1.619388   -3.259736   -4.653883   \n",
       "16353 -26.342933 -24.133183 -24.727201 -27.578492  -31.130725  -33.871132   \n",
       "16354  35.101939  39.163327  42.606314  44.882697   45.758487   45.337306   \n",
       "16355  62.300359  64.839505  67.446475  70.150941   72.779106   74.979135   \n",
       "16356   4.075851   4.900474   5.484829   5.796288    5.852647    5.733996   \n",
       "\n",
       "         BVP_3735    BVP_3736    BVP_3737    BVP_3738    BVP_3739    BVP_3740  \\\n",
       "0       -6.108203   15.141941   35.166182   51.420864   61.330532   63.068698   \n",
       "1      -73.348828  -75.879257  -80.645956  -89.878511 -103.603664 -119.822475   \n",
       "2      -37.326269  -34.414269  -29.749592  -24.464175  -19.580413  -15.646018   \n",
       "3      177.512113  208.099601  234.482987  257.050887  276.661784  294.591040   \n",
       "4       77.144178   78.687161   78.653303   76.747550   73.119847   68.268400   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -5.472574   -5.611988   -5.214508   -4.588625   -4.025034   -3.654250   \n",
       "16353  -34.397829  -31.859388  -26.303332  -18.739491  -10.680635   -3.283120   \n",
       "16354   43.970139   42.094880   40.039116   37.933211   35.787193   33.560951   \n",
       "16355   76.344085   76.662008   76.293217   75.873558   76.327129   78.399988   \n",
       "16356    5.544155    5.372111    5.262359    5.188202    5.078450    4.850047   \n",
       "\n",
       "         BVP_3741    BVP_3742    BVP_3743    BVP_3744    BVP_3745    BVP_3746  \\\n",
       "0       56.253814   42.253096   23.815108    4.183014  -14.025338  -29.644105   \n",
       "1     -135.720387 -149.458910 -160.473128 -169.251075 -176.808934 -186.984132   \n",
       "2      -12.606219  -10.059360   -7.603786   -5.294269   -3.642005   -3.404663   \n",
       "3      312.015098  330.135129  349.844946  371.367119  393.927953  415.435995   \n",
       "4       62.846195   57.409478   52.238793   47.377672   42.690681   38.027874   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -3.413982   -3.078793   -2.396551   -1.174448    0.617180    2.883410   \n",
       "16353    3.405532   10.632762   20.208346   33.447046   50.067694   67.868460   \n",
       "16354   31.261169   28.958044   26.725116   24.575756   22.463166   20.353918   \n",
       "16355   82.511794   88.569290   96.190968  104.906302  114.282916  124.189402   \n",
       "16356    4.458500    3.930504    3.355047    2.827051    2.399908    2.049888   \n",
       "\n",
       "         BVP_3747    BVP_3748    BVP_3749    BVP_3750    BVP_3751    BVP_3752  \\\n",
       "0      -43.068967  -55.649476  -68.660993  -82.389681  -95.807477 -106.946580   \n",
       "1     -202.534403 -223.904325 -249.158471 -275.278379 -299.539742 -319.595981   \n",
       "2       -5.175598   -9.036965  -14.459310  -20.557165  -26.381166  -31.274056   \n",
       "3      432.807059  442.720260  442.815647  432.457307  412.941797  386.989419   \n",
       "4       33.316698   28.537805   23.696032   18.670455   13.286945    7.342350   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    5.419571    7.982429   10.322816   12.206398   13.449265   13.926835   \n",
       "16353   83.439674   93.530075   96.484328   93.027139   85.780108   77.927179   \n",
       "16354   18.248013   16.245732   14.500839   13.163756   12.294653   11.826674   \n",
       "16355  134.850426  146.571193  159.728973  174.790053  192.195287  211.868373   \n",
       "16356    1.688003    1.207467    0.540056   -0.320162   -1.322761   -2.381720   \n",
       "\n",
       "         BVP_3753    BVP_3754    BVP_3755    BVP_3756    BVP_3757    BVP_3758  \\\n",
       "0     -113.779128 -115.065088 -110.762067 -101.876928  -89.872275  -76.041135   \n",
       "1     -333.311105 -322.069584 -276.418242 -200.572234 -111.405541  -27.720886   \n",
       "2      -34.980238  -37.536225  -39.088075  -39.763586  -39.562758  -38.467335   \n",
       "3      357.546577  326.842505  295.834606  264.565277  232.716558  200.108275   \n",
       "4        0.749606   -6.365527  -13.688649  -20.784434  -27.251418  -32.823569   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   13.582747   12.494126   10.835981    8.860445    6.828550    4.930137   \n",
       "16353   71.792944   67.884300   64.957767   60.993681   54.352550   44.563119   \n",
       "16354   11.555915   11.261756   10.797120   10.108523    9.279532    8.443855   \n",
       "16355  232.948798  253.164473  269.170844  277.432608  275.478440  263.003135   \n",
       "16356   -3.399151   -4.271234   -4.903050   -5.202643   -5.104756   -4.588625   \n",
       "\n",
       "         BVP_3759    BVP_3760    BVP_3761    BVP_3762    BVP_3763    BVP_3764  \\\n",
       "0      -61.093611  -45.255806  -28.598379  -11.389826    5.808129   22.356038   \n",
       "1       39.066388   86.332257  116.998245  135.847767  147.176199  151.164048   \n",
       "2      -36.550345  -33.985229  -31.137128  -28.480727  -26.445065  -25.340514   \n",
       "3      166.910005  133.708202  101.354284   70.735001   42.712368   17.639672   \n",
       "4      -37.413821  -41.123752  -44.166185  -46.797478  -49.264315  -51.706968   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    3.239363    1.741396    0.368013   -0.928247   -2.150351   -3.262702   \n",
       "16353   32.294650   18.663897    4.415364  -10.248982  -25.416263  -40.912236   \n",
       "16354    7.735201    7.243823    7.013177    7.046604    7.340762    7.878938   \n",
       "16355  242.113463  216.586353  190.486980  166.977621  147.550396  132.328235   \n",
       "16356   -3.692811   -2.506303   -1.153684    0.237497    1.545622    2.666872   \n",
       "\n",
       "         BVP_3765    BVP_3766    BVP_3767    BVP_3768    BVP_3769    BVP_3770  \\\n",
       "0       37.893549   52.639700   67.463574   83.552211  101.707570  121.745943   \n",
       "1      147.390132  136.997658  123.389500  110.847666  102.270281   98.800552   \n",
       "2      -25.313128  -26.198595  -27.549617  -28.708940  -28.882382  -27.339661   \n",
       "3       -4.574942  -24.574453  -43.157289  -61.241991  -79.623453  -98.672625   \n",
       "4      -54.120600  -56.311732  -57.975638  -58.768895  -58.439984  -56.959881   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -4.259369   -5.190778   -6.142951   -7.213774   -8.462574   -9.892317   \n",
       "16353  -55.794388  -68.534113  -77.448356  -81.329279  -79.879873  -73.805040   \n",
       "16354    8.567535    9.286217    9.884561   10.242231   10.359226   10.315771   \n",
       "16355  120.569317  111.404652  104.096658   97.945904   92.274155   86.348067   \n",
       "16356    3.530057    4.084750    4.342815    4.354680    4.206367    4.007627   \n",
       "\n",
       "         BVP_3771    BVP_3772    BVP_3773    BVP_3774    BVP_3775    BVP_3776  \\\n",
       "0      142.148200  160.374217  173.714290  180.331334  179.992179  174.085240   \n",
       "1       99.856847  103.878123  107.237543  107.418049  104.282590   99.983870   \n",
       "2      -23.587836  -17.590394   -9.703347   -0.583949    9.046647   18.613343   \n",
       "3     -118.629743 -139.445344 -161.345534 -184.567012 -209.219298 -235.023295   \n",
       "4      -54.565598  -51.673110  -48.799970  -46.357317  -44.591835  -43.527709   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -11.437743  -12.968339  -14.320958  -15.299828  -15.726970  -15.468905   \n",
       "16353  -64.415582  -53.327230  -42.120074  -32.085115  -24.473753  -19.939409   \n",
       "16354   10.279001   10.372597   10.623299   10.930828   11.158133   11.238357   \n",
       "16355   79.718309   72.232278   64.212137   56.001241   47.709805   39.125879   \n",
       "16356    3.853381    3.797021    3.859313    4.007627    4.212299    4.428837   \n",
       "\n",
       "         BVP_3777    BVP_3778    BVP_3779    BVP_3780    BVP_3781    BVP_3782  \\\n",
       "0      164.963400  154.855892  145.112268  136.036355  127.267801  118.449786   \n",
       "1       97.674060   99.091367  104.760597  114.063349  125.361697  134.925180   \n",
       "2       27.741870   36.240529   43.954134   50.690987   56.140717   60.056855   \n",
       "3     -261.540931 -288.662685 -316.960880 -347.431784 -380.622989 -415.810258   \n",
       "4      -42.990809  -42.652224  -42.086302  -40.819025  -38.381209  -34.424595   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -14.484103  -12.873418  -10.823725   -8.557495   -6.252703   -4.016135   \n",
       "16353  -17.975187  -16.676266  -13.116113   -4.475118   10.244669   29.423558   \n",
       "16354   11.151447   10.980969   10.783749   10.536389   10.111865    9.373127   \n",
       "16355   29.656008   18.876293    6.841841   -5.853891  -18.418214  -30.266151   \n",
       "16356    4.636476    4.838182    5.039889    5.238629    5.428470    5.558986   \n",
       "\n",
       "         BVP_3783    BVP_3784    BVP_3785    BVP_3786    BVP_3787    BVP_3788  \\\n",
       "0      109.656502  101.354284   94.051866   87.922357   82.626602   77.567549   \n",
       "1      138.869573  134.584224  121.785001  102.597866   80.532661   59.640746   \n",
       "2       62.348116   63.133169   62.749771   61.572191   59.929056   58.002937   \n",
       "3     -450.294489 -479.924573 -500.379823 -508.770361 -504.516800 -489.226589   \n",
       "4      -28.755705  -21.422910  -12.663218   -2.844238    7.666425   18.578553   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -1.886353    0.104015    1.865979    3.239363    4.034323    4.102547   \n",
       "16353   48.828175   62.724256   66.529937   58.966097   42.567216   22.445817   \n",
       "16354    8.256663    6.849384    5.425391    4.318955    3.830920    4.068252   \n",
       "16355  -41.270531  -51.906121  -62.974086  -75.186575  -88.827598 -103.659774   \n",
       "16356    5.564919    5.372111    4.933103    4.241962    3.352081    2.385077   \n",
       "\n",
       "         BVP_3789    BVP_3790    BVP_3791    BVP_3792    BVP_3793    BVP_3794  \\\n",
       "0       72.215268   66.386052   60.242411   54.112902   48.258955   42.691171   \n",
       "1       43.251457   33.263450   29.275601   29.703468   32.528055   36.054610   \n",
       "2       55.775576   53.146561   49.951576   46.154109   41.900216   37.472880   \n",
       "3     -465.453985 -435.378761 -400.484719 -361.471363 -319.137121 -274.951661   \n",
       "4       29.688996   40.871993   51.967925   62.763967   72.974739   82.242309   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    3.387676    1.957934   -0.035400   -2.363922   -4.799230   -7.115887   \n",
       "16353    4.062913   -9.243110  -16.997036  -21.060125  -24.469793  -29.328868   \n",
       "16354    4.890558    5.943511    6.805929    7.133514    6.805929    5.956881   \n",
       "16355 -119.254965 -135.146884 -150.818377 -165.307196 -177.112743 -184.759854   \n",
       "16356    1.507061    0.901941    0.729898    1.085850    1.978698    3.322418   \n",
       "\n",
       "         BVP_3795    BVP_3796    BVP_3797    BVP_3798    BVP_3799    BVP_3800  \\\n",
       "0       37.229371   31.636857   25.864166   20.130337   14.898174   10.683473   \n",
       "1       39.059703   40.466982   39.978947   37.578884   33.681288   28.877819   \n",
       "2       33.182472   29.248077   25.651438   22.100441   18.156917   13.382698   \n",
       "3     -230.805063 -188.537945 -149.309085 -113.330455  -79.817760  -47.697012   \n",
       "4       90.228091   96.641869  101.295001  104.177816  105.416071  105.275800   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -9.115154  -10.645749  -11.657247  -12.206007  -12.475938  -12.713240   \n",
       "16353  -35.775952  -42.369562  -47.260317  -49.216619  -48.582999  -46.915786   \n",
       "16354    4.867159    3.850976    3.098867    2.630888    2.370157    2.206365   \n",
       "16355 -187.778005 -187.307479 -185.895900 -186.548702 -190.753766 -197.324178   \n",
       "16356    4.962766    6.698034    8.344314    9.729562   10.723263   11.242360   \n",
       "\n",
       "         BVP_3801    BVP_3802    BVP_3803    BVP_3804    BVP_3805    BVP_3806  \\\n",
       "0        7.807727    6.228539    5.521967    5.052097    4.267803    2.960645   \n",
       "1       23.950670   19.708776   16.683626   15.172723   14.684688   14.303619   \n",
       "2        7.540440    0.666660   -6.800475  -14.130683  -20.401981  -24.646746   \n",
       "3      -16.063797   15.353912   45.937867   74.730663  100.810224  123.703147   \n",
       "4      104.129446  102.383312  100.409842   98.465394   96.695075   95.089212   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -13.152248  -13.908647  -14.940909  -16.068092  -17.049927  -17.649114   \n",
       "16353  -44.310023  -41.276567  -38.183709  -34.710679  -29.285306  -20.477986   \n",
       "16354    2.059286    1.858723    1.544509    1.026389    0.234168   -0.838841   \n",
       "16355 -202.754306 -203.140053 -196.561162 -184.577578 -171.123070 -159.919458   \n",
       "16356   11.248293   10.758858    9.866011    8.712131    7.460365    6.267925   \n",
       "\n",
       "         BVP_3807    BVP_3808    BVP_3809    BVP_3810    BVP_3811    BVP_3812  \\\n",
       "0        1.370859    0.038971   -0.434432    0.385191    2.621491    6.083692   \n",
       "1       12.899683    9.553633    3.981342   -3.162022  -10.723224  -17.645969   \n",
       "2      -26.235109  -24.893216  -20.967949  -15.134821   -8.315811   -1.177303   \n",
       "3      143.388234  160.271764  174.696425  186.782333  196.275124  202.817978   \n",
       "4       93.546229   91.906508   90.010429   87.712884   84.888113   81.439378   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -17.705473  -17.135949  -15.922744  -14.083657  -11.680978   -8.824459   \n",
       "16353   -5.643355   16.085855   43.414683   73.507679  101.977023  124.415096   \n",
       "16354   -2.099041   -3.362584   -4.425565   -5.134218   -5.391607   -5.167646   \n",
       "16355 -152.543640 -147.253399 -140.581251 -129.530242 -113.515392  -94.490870   \n",
       "16356    5.235663    4.411039    3.791089    3.346148    3.031724    2.800355   \n",
       "\n",
       "         BVP_3813    BVP_3814    BVP_3815    BVP_3816    BVP_3817    BVP_3818  \\\n",
       "0       10.266596   14.350580   17.198064   17.455963   13.820652    5.557296   \n",
       "1      -23.034411  -26.504141  -27.801111  -27.316418  -25.648408  -23.669526   \n",
       "2        5.869920   12.716315   19.307111   25.432353   30.717770   34.661294   \n",
       "3      206.382632  207.516679  207.431891  207.721585  210.187520  216.341760   \n",
       "4       77.323145   72.520067   67.083351   61.133919   54.826554   48.349896   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -5.677246   -2.470708    0.563787    3.197835    5.262359    6.680236   \n",
       "16353  137.590434  140.073433  132.121500  115.469171   92.547963   66.205207   \n",
       "16354   -4.479048   -3.379298   -2.015473   -0.624908    0.504927    1.086558   \n",
       "16355  -75.975024  -60.930900  -50.062167  -41.791925  -33.106264  -21.279523   \n",
       "16356    2.601614    2.388043    2.112180    1.747329    1.290523    0.741763   \n",
       "\n",
       "         BVP_3819    BVP_3820    BVP_3821    BVP_3822    BVP_3823    BVP_3824  \\\n",
       "0       -7.069140  -22.567790  -38.568105  -52.540560  -62.690463  -68.505548   \n",
       "1      -22.125195  -21.680615  -23.462278  -26.306921  -29.596144  -32.320450   \n",
       "2       36.934297   37.463751   36.550899   34.606523   32.087049   29.238949   \n",
       "3      227.353679  243.643689  264.784314  289.327081  314.643545  337.299765   \n",
       "4       41.849054   35.464298   29.268183   23.289729   17.504754   11.869723   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    7.463332    7.718431    7.584949    7.190435    6.588282    5.775524   \n",
       "16353   39.054584   12.505901  -12.553775  -35.724471  -57.065589  -76.794935   \n",
       "16354    0.976249    0.237511   -0.902352   -2.129125   -3.165364   -3.960929   \n",
       "16355   -5.205327   14.230376   34.522352   52.656689   65.911966   72.855408   \n",
       "16356    0.118846   -0.593059   -1.402851   -2.322394   -3.322028   -4.333526   \n",
       "\n",
       "         BVP_3825    BVP_3826    BVP_3827    BVP_3828    BVP_3829    BVP_3830  \\\n",
       "0      -70.816037  -71.225849  -71.271776  -71.755777  -72.483546  -72.511809   \n",
       "1      -33.914921  -34.346130  -34.078713  -33.737757  -33.911578  -35.613015   \n",
       "2       26.089607   22.575124   18.604215   14.186008    9.457431    4.719725   \n",
       "3      353.904200  362.167556  361.722416  354.271617  342.924076  330.947686   \n",
       "4        6.345941    0.894714   -4.440427   -9.620785  -14.578645  -19.323680   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352    4.725464    3.464799    2.103281    0.851515   -0.082860   -0.587126   \n",
       "16353  -94.983793 -111.319312 -124.910463 -134.549410 -139.547088 -139.186717   \n",
       "16354   -4.649526   -5.525315   -6.885797   -8.851308  -11.358338  -14.149498   \n",
       "16355   73.749832   70.451909   65.526219   61.283005   59.371227   59.990117   \n",
       "16356   -5.235272   -5.896750   -6.211175   -6.137018   -5.712842   -5.051363   \n",
       "\n",
       "         BVP_3831    BVP_3832    BVP_3833    BVP_3834    BVP_3835    BVP_3836  \\\n",
       "0      -70.688854  -66.219788  -58.931501  -49.297396  -38.197155  -26.658840   \n",
       "1      -39.189711  -44.564783  -51.019549  -57.948980  -65.329675  -73.666385   \n",
       "2        0.246747   -3.760676   -7.348187  -10.789642  -14.541466  -19.078344   \n",
       "3      320.811915  313.534227  308.846124  305.850260  303.458515  300.575702   \n",
       "4      -23.899421  -28.421957  -33.007372  -37.737896  -42.661897  -47.750354   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352   -0.723575   -0.711710   -0.815529   -1.254537   -2.085093   -3.218208   \n",
       "16353 -132.985160 -120.966178 -103.604987  -81.654010  -56.186441  -28.596244   \n",
       "16354  -16.913916  -19.377490  -21.413199  -23.027726  -24.297954  -25.327508   \n",
       "16355   62.257969   64.576689   65.407528   63.974754   60.731938   57.200871   \n",
       "16356   -4.280133   -3.520768   -2.850391   -2.310529   -1.921948   -1.693545   \n",
       "\n",
       "         BVP_3837    BVP_3838    BVP_3839  stress  \n",
       "0      -15.717577   -6.359035    0.593630     0.0  \n",
       "1      -83.570824  -90.660705  -92.425654     1.0  \n",
       "2      -24.665003  -31.201028  -38.184351     0.0  \n",
       "3      296.011249  288.440334  276.393287     0.0  \n",
       "4      -52.916202  -57.980475  -62.672303     0.0  \n",
       "...           ...         ...         ...     ...  \n",
       "16352   -4.493704   -5.736572   -6.810362     0.0  \n",
       "16353    0.348315   26.877198   46.408538     0.0  \n",
       "16354  -26.169870  -26.891894  -27.516981     0.0  \n",
       "16355   55.051710   55.785053   59.782407     0.0  \n",
       "16356   -1.628287   -1.702444   -1.856690     0.0  \n",
       "\n",
       "[16357 rows x 10082 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adarp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2093809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_features = adarp_dataset.loc[:, ~adarp_dataset.columns.isin(['dataset', 'stress'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58610fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_features = pd.DataFrame()\n",
    "adarp_features['id'] = adarp_dataset['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e99fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cols = [col for col in adarp_dataset.columns if 'ACC' in col]\n",
    "bvp_cols = [col for col in adarp_dataset.columns if 'BVP' in col]\n",
    "eda_cols = [col for col in adarp_dataset.columns if 'EDA' in col]\n",
    "temp_cols = [col for col in adarp_dataset.columns if 'TEMP' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295e7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_features[\"ACC\"] = adarp_dataset[acc_cols].apply(lambda x: statistics.mean(x), axis =1)\n",
    "adarp_features[\"BVP\"] = adarp_dataset[bvp_cols].apply(lambda x: statistics.mean(x), axis =1)\n",
    "adarp_features[\"EDA\"] = adarp_dataset[eda_cols].apply(lambda x: statistics.mean(x), axis =1)\n",
    "adarp_features[\"TEMP\"] = adarp_dataset[temp_cols].apply(lambda x: statistics.mean(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856012b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0831e3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ACC</th>\n",
       "      <th>BVP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>TEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>87.056141</td>\n",
       "      <td>-0.105964</td>\n",
       "      <td>271.004839</td>\n",
       "      <td>-1868.786603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24.218289</td>\n",
       "      <td>0.205137</td>\n",
       "      <td>9.872665</td>\n",
       "      <td>579.856470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>66.354776</td>\n",
       "      <td>0.171591</td>\n",
       "      <td>396.802050</td>\n",
       "      <td>-2609.949869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>-294.392913</td>\n",
       "      <td>1.178388</td>\n",
       "      <td>-96.544704</td>\n",
       "      <td>325.401261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>138.380651</td>\n",
       "      <td>0.345844</td>\n",
       "      <td>-73.417748</td>\n",
       "      <td>575.258694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16352</th>\n",
       "      <td>7</td>\n",
       "      <td>-132.704329</td>\n",
       "      <td>-0.177470</td>\n",
       "      <td>-51.834810</td>\n",
       "      <td>1708.921628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16353</th>\n",
       "      <td>6</td>\n",
       "      <td>83.817857</td>\n",
       "      <td>-0.158137</td>\n",
       "      <td>26.708240</td>\n",
       "      <td>563.751555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16354</th>\n",
       "      <td>1</td>\n",
       "      <td>369.445380</td>\n",
       "      <td>-0.191824</td>\n",
       "      <td>-43.091480</td>\n",
       "      <td>-2090.371065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16355</th>\n",
       "      <td>5</td>\n",
       "      <td>-156.766685</td>\n",
       "      <td>-0.033326</td>\n",
       "      <td>-64.335650</td>\n",
       "      <td>6672.842012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16356</th>\n",
       "      <td>7</td>\n",
       "      <td>91.189114</td>\n",
       "      <td>0.377453</td>\n",
       "      <td>0.700214</td>\n",
       "      <td>1146.099333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16357 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         ACC       BVP         EDA         TEMP\n",
       "0       8   87.056141 -0.105964  271.004839 -1868.786603\n",
       "1       1   24.218289  0.205137    9.872665   579.856470\n",
       "2       4   66.354776  0.171591  396.802050 -2609.949869\n",
       "3       8 -294.392913  1.178388  -96.544704   325.401261\n",
       "4       3  138.380651  0.345844  -73.417748   575.258694\n",
       "...    ..         ...       ...         ...          ...\n",
       "16352   7 -132.704329 -0.177470  -51.834810  1708.921628\n",
       "16353   6   83.817857 -0.158137   26.708240   563.751555\n",
       "16354   1  369.445380 -0.191824  -43.091480 -2090.371065\n",
       "16355   5 -156.766685 -0.033326  -64.335650  6672.842012\n",
       "16356   7   91.189114  0.377453    0.700214  1146.099333\n",
       "\n",
       "[16357 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adarp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23789dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ACC_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>187.167398</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>126.542680</td>\n",
       "      <td>510.635824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>225.810138</td>\n",
       "      <td>0.312767</td>\n",
       "      <td>223.377331</td>\n",
       "      <td>1631.651240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>122.693256</td>\n",
       "      <td>0.332713</td>\n",
       "      <td>129.439464</td>\n",
       "      <td>10.375996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>121.445170</td>\n",
       "      <td>0.365210</td>\n",
       "      <td>452.246350</td>\n",
       "      <td>1275.447248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>167.529967</td>\n",
       "      <td>0.399798</td>\n",
       "      <td>686.649171</td>\n",
       "      <td>5647.574597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>139.732682</td>\n",
       "      <td>0.427314</td>\n",
       "      <td>230.497774</td>\n",
       "      <td>3606.120156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>68.855065</td>\n",
       "      <td>0.259821</td>\n",
       "      <td>360.258644</td>\n",
       "      <td>604.655256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>116.649589</td>\n",
       "      <td>0.187998</td>\n",
       "      <td>1553.186876</td>\n",
       "      <td>1007.111861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>156.909637</td>\n",
       "      <td>0.322209</td>\n",
       "      <td>243.384879</td>\n",
       "      <td>1078.485286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>125.255298</td>\n",
       "      <td>0.340415</td>\n",
       "      <td>149.496755</td>\n",
       "      <td>859.069871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>261.016391</td>\n",
       "      <td>0.154993</td>\n",
       "      <td>211.208365</td>\n",
       "      <td>3603.631378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     ACC_std   BVP_std      EDA_std     TEMP_std\n",
       "0    0  187.167398  0.281347   126.542680   510.635824\n",
       "1    1  225.810138  0.312767   223.377331  1631.651240\n",
       "2    2  122.693256  0.332713   129.439464    10.375996\n",
       "3    3  121.445170  0.365210   452.246350  1275.447248\n",
       "4    4  167.529967  0.399798   686.649171  5647.574597\n",
       "5    5  139.732682  0.427314   230.497774  3606.120156\n",
       "6    6   68.855065  0.259821   360.258644   604.655256\n",
       "7    7  116.649589  0.187998  1553.186876  1007.111861\n",
       "8    8  156.909637  0.322209   243.384879  1078.485286\n",
       "9    9  125.255298  0.340415   149.496755   859.069871\n",
       "10  10  261.016391  0.154993   211.208365  3603.631378"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adarp_mean = adarp_features.groupby('id', as_index = False, group_keys = True).mean()\n",
    "adarp_mean.columns = map(lambda x: x + '_mean', adarp_mean.columns)\n",
    "adarp_mean = adarp_mean.rename({'id_mean': 'id'}, axis='columns')\n",
    "adarp_mean\n",
    "\n",
    "adarp_min = adarp_features.groupby('id', as_index = False, group_keys = True).min()\n",
    "adarp_min.columns = map(lambda x: x + '_min', adarp_min.columns)\n",
    "adarp_min = adarp_min.rename({'id_min': 'id'}, axis='columns')\n",
    "adarp_min\n",
    "\n",
    "adarp_std = adarp_features.groupby('id', as_index = False, group_keys = True).std()\n",
    "adarp_std.columns = map(lambda x: x + '_std', adarp_std.columns)\n",
    "adarp_std = adarp_std.rename({'id_std': 'id'}, axis='columns')\n",
    "adarp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b81b6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "adarp_mean.loc[:, adarp_mean.columns!='id'] = scaler.fit_transform(adarp_mean.loc[:, adarp_mean.columns!='id'])\n",
    "adarp_min.loc[:, adarp_min.columns!='id'] = scaler.fit_transform(adarp_min.loc[:, adarp_min.columns!='id'])\n",
    "adarp_std.loc[:, adarp_std.columns!='id'] = scaler.fit_transform(adarp_std.loc[:, adarp_std.columns!='id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3fbd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wesad_all_grouped = pd.merge(wesad_extra, wesad_mean, on='id')\n",
    "adarp_all_grouped = pd.merge(adarp_min, adarp_mean, on='id')\n",
    "adarp_all_grouped = pd.merge(adarp_std, adarp_all_grouped, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfea9cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ACC_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>ACC_min</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>EDA_min</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>ACC_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>TEMP_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.615693</td>\n",
       "      <td>0.463988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088743</td>\n",
       "      <td>0.798576</td>\n",
       "      <td>0.799142</td>\n",
       "      <td>0.980562</td>\n",
       "      <td>0.815866</td>\n",
       "      <td>0.893109</td>\n",
       "      <td>0.393920</td>\n",
       "      <td>0.043931</td>\n",
       "      <td>0.176999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.816788</td>\n",
       "      <td>0.579368</td>\n",
       "      <td>0.067876</td>\n",
       "      <td>0.287603</td>\n",
       "      <td>0.706703</td>\n",
       "      <td>0.687860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692886</td>\n",
       "      <td>0.213683</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0.256009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.280172</td>\n",
       "      <td>0.652613</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897139</td>\n",
       "      <td>0.258779</td>\n",
       "      <td>0.941305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.594706</td>\n",
       "      <td>0.096945</td>\n",
       "      <td>0.201203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.273677</td>\n",
       "      <td>0.771944</td>\n",
       "      <td>0.228301</td>\n",
       "      <td>0.224415</td>\n",
       "      <td>0.923178</td>\n",
       "      <td>0.143334</td>\n",
       "      <td>0.498146</td>\n",
       "      <td>0.771017</td>\n",
       "      <td>0.887556</td>\n",
       "      <td>0.580532</td>\n",
       "      <td>0.115397</td>\n",
       "      <td>0.042269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.898957</td>\n",
       "      <td>0.392604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.408612</td>\n",
       "      <td>0.300250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868684</td>\n",
       "      <td>0.440641</td>\n",
       "      <td>0.109819</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.368844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072867</td>\n",
       "      <td>0.637860</td>\n",
       "      <td>0.588477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863105</td>\n",
       "      <td>0.437029</td>\n",
       "      <td>0.459965</td>\n",
       "      <td>0.252217</td>\n",
       "      <td>0.227154</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384943</td>\n",
       "      <td>0.163822</td>\n",
       "      <td>0.105421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967761</td>\n",
       "      <td>0.244747</td>\n",
       "      <td>0.872943</td>\n",
       "      <td>0.593919</td>\n",
       "      <td>0.883874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.248721</td>\n",
       "      <td>0.121198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176814</td>\n",
       "      <td>0.804849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.679888</td>\n",
       "      <td>0.777135</td>\n",
       "      <td>0.815589</td>\n",
       "      <td>0.327821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.223379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.458233</td>\n",
       "      <td>0.614041</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.189475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344981</td>\n",
       "      <td>0.821914</td>\n",
       "      <td>0.664342</td>\n",
       "      <td>0.920866</td>\n",
       "      <td>0.414501</td>\n",
       "      <td>0.026126</td>\n",
       "      <td>0.131184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.293505</td>\n",
       "      <td>0.680894</td>\n",
       "      <td>0.016090</td>\n",
       "      <td>0.150552</td>\n",
       "      <td>0.768069</td>\n",
       "      <td>0.214407</td>\n",
       "      <td>0.666620</td>\n",
       "      <td>0.879314</td>\n",
       "      <td>0.587222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098305</td>\n",
       "      <td>0.180531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059346</td>\n",
       "      <td>0.637419</td>\n",
       "      <td>0.199553</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.953877</td>\n",
       "      <td>0.697553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417914</td>\n",
       "      <td>0.273745</td>\n",
       "      <td>0.503059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   ACC_std   BVP_std   EDA_std  TEMP_std   ACC_min   BVP_min   EDA_min  \\\n",
       "0    0  0.615693  0.463988  0.000000  0.088743  0.798576  0.799142  0.980562   \n",
       "1    1  0.816788  0.579368  0.067876  0.287603  0.706703  0.687860  1.000000   \n",
       "2    2  0.280172  0.652613  0.002030  0.000000  0.897139  0.258779  0.941305   \n",
       "3    3  0.273677  0.771944  0.228301  0.224415  0.923178  0.143334  0.498146   \n",
       "4    4  0.513500  0.898957  0.392604  1.000000  0.408612  0.300250  0.000000   \n",
       "5    5  0.368844  1.000000  0.072867  0.637860  0.588477  0.000000  0.863105   \n",
       "6    6  0.000000  0.384943  0.163822  0.105421  1.000000  0.967761  0.244747   \n",
       "7    7  0.248721  0.121198  1.000000  0.176814  0.804849  1.000000  0.679888   \n",
       "8    8  0.458233  0.614041  0.081900  0.189475  0.000000  0.344981  0.821914   \n",
       "9    9  0.293505  0.680894  0.016090  0.150552  0.768069  0.214407  0.666620   \n",
       "10  10  1.000000  0.000000  0.059346  0.637419  0.199553  0.445700  0.953877   \n",
       "\n",
       "    TEMP_min  ACC_mean  BVP_mean  EDA_mean  TEMP_mean  \n",
       "0   0.815866  0.893109  0.393920  0.043931   0.176999  \n",
       "1   0.692886  0.213683  1.000000  0.117380   0.256009  \n",
       "2   1.000000  1.000000  0.594706  0.096945   0.201203  \n",
       "3   0.771017  0.887556  0.580532  0.115397   0.042269  \n",
       "4   0.000000  0.868684  0.440641  0.109819   1.000000  \n",
       "5   0.437029  0.459965  0.252217  0.227154   0.000000  \n",
       "6   0.872943  0.593919  0.883874  0.000000   0.099551  \n",
       "7   0.777135  0.815589  0.327821  1.000000   0.223379  \n",
       "8   0.664342  0.920866  0.414501  0.026126   0.131184  \n",
       "9   0.879314  0.587222  0.000000  0.098305   0.180531  \n",
       "10  0.697553  0.000000  0.417914  0.273745   0.503059  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adarp_all_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbaf03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = adarp_all_grouped[\"id\"]\n",
    "adarp_all_grouped2 = adarp_all_grouped.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=9)\n",
    "adarp_all_grouped_pca = pca.fit_transform(adarp_all_grouped2)\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa65ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_all_grouped_pca = pd.DataFrame(adarp_all_grouped_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "112b949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqUAAAHmCAYAAABK/PdtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhpUlEQVR4nO3dd3hUZf4+/vtMMpn03jvpCRAIvaNYsAs2im3XdfWj62JfFBUL67II6/rza0FXV10LKIKogKCghhYQAiEJpPfee5vMzPn9ERkdM5EZSOaZJPfrurjcvOdkcufZADdz5jlHkmVZBhERERGRQArRAYiIiIiIWEqJiIiISDiWUiIiIiISjqWUiIiIiIRjKSUiIiIi4VhKiYiIiEg4llIiIiIiEs5WdIDzdfLkSciyDKVSKToKERERERnR29sLSZKQlJR0zmOH7SulsizDktf9l2UZarXaol9zuODaGMd1GRjXxjiui3Fcl4FxbYzjugzM0mtjTl8btq+Unn2FdPz48Rb5ep2dncjKykJUVBQcHR0t8jWHC66NcVyXgXFtjOO6GMd1GRjXxjiuy8AsvTYZGRkmHztsXyklIiIiopGDpZSIiIiIhGMpJSIiIiLhWEqJiIiISDiWUiIiIiISjqWUiIiIiIRjKSUiIiIi4VhKiYiIiEg4llIiIiIiEo6llIiIiIiEYyklIiIiIuFYSomIiIhIOJZSIiIiIhKOpZSIiIiIhGMpJSIiIiLhWEpNIMs6FNSnoqY3E53qFtFxiIiIiEYcllITpJbsQWrpTtRqsvBt1lto7aoXHYmIiIhoRGEpNUFVc77+f6u13UjO2QStTiMwEREREdHIwlJqgkD3aIOPG9orcLz4G0FpiIiIiEYellITJIZcDBeVt8Esq/IQShpOC0pERERENLKwlJpAaaPCzDE3QvrNch3K+xzt3c1iQhERERGNICylJnJ39EOAcqLBTK3pwv6cTdDptGJCEREREY0QLKVm8LSJQIh7gsGstq0EJ0u/E5SIiIiIaGRgKTWDJEmYHHYNXOw9DeYZ5T+ioilXUCoiIiKi4Y+l1Ex2NvaYH7scCsnGYH4g91N0qlsFpSIiIiIa3lhKz4O3SzAmh19pMOvu7cD+nM3QyTpBqYiIiIiGL5bS85QQOBshnvEGs+qWQqSXfS8oEREREdHwxVJ6niRJwpzom+GkcjOYnyrdh+qWQkGpiIiIiIYnltILoFI6Yl7sMoPrl8qQsT9nM7p72wUmIyIiIhpeWEovkJ9rOJLCLjOYdapbcSB3C2S+v5SIiIjIJCylg2B88HwEukcbzCqacnC64oCgRERERETDi9BSWlVVhXvvvReTJk3CggUL8P7774uMc94kSYG5MbfAQeliME8t2YPa1lJBqYiIiIiGD6Gl9KGHHoKjoyO2bduGVatW4ZVXXsF33w3PuyM52LlgbuwSAJJ+Jss6JOd8gh5Np7hgRERERMOAsFLa0tKCtLQ03HfffQgPD8ell16KuXPnIiUlRVSkCxboHoUJIRcbzDp6mnEobytkWRaUioiIiMj6CSul9vb2cHBwwLZt29Db24vCwkKcOHEC8fHx5/5kKzYh9BL4uY4xmJU2nEZ21RFBiYiIiIisn62oL6xSqbB69WqsWbMG//vf/6DVanHDDTfg5ptvNvk5ZFlGZ6dlTo13dXUZ/Pf3TA29Ht9mvQW19pdjjxXtgKudLzwcA4YsoyjmrM1ownUZGNfGOK6LcVyXgXFtjOO6DMzSayPLMiRJOveBACRZ4Hnl9evXo6qqCn/84x+Rl5eHNWvW4Pnnn8d11113zs/NyMiAWq22QMrz06qtQon6oMHMTnJGlOpS2EhKQamIiIiILMvOzg7jx48/53HCXilNSUnB559/juTkZNjb22P8+PGoqanBm2++aVIpBQClUomoqKghTtqnq6sLxcXFCA8Ph4ODgwmfEQ/7ci1yan95j6xabke7Qz6mhy82+V8Nw4H5azM6cF0GxrUxjutiHNdlYFwb47guA7P02uTn55t8rLBSmpmZibCwMNjb2+tnCQkJ2Lhxo8nPIUkSHB0dhyLegBwcHEz+mtOirkZDZznq28v0s9KmTIR4xSLaf+pQRRTGnLUZTbguA+PaGMd1MY7rMjCujXFcl4FZam3MeRFO2EYnX19flJSUGJyCLywsRHBwsKhIg85GYYv5ccugtLE3mB8p/ApNHTWCUhERERFZH2GldMGCBVAqlXj66adRVFSE77//Hhs3bsTtt98uKtKQcLH3xOzoGw1mWl0vknM+hkZrve+JJSIiIrIkYaXUxcUF77//Purq6nDTTTdh7dq1uO+++7BkyRJRkYZMuPd4xAXMMJg1d9biaOHXghIRERERWRdh7ykFgKioKLz33nsiI1jMlDFXo6a1BE0dVfpZXs0xBLhFIsJ3orhgRERERFZA6G1GRxNbhRIXxS2HrcLOYH64YBtau+oFpSIiIiKyDiylFuTm4IOZUYsNZhqtGj9mfwKtTiMoFREREZF4LKUWFumbhCjfyQazxo5KHC/aJSgRERERkXgspQJMj7webg6+BrOsqsMoqc8UlIiIiIhILJZSAZQ2drgobjlsFIb7zA7lfY627kZBqYiIiIjEYSkVxMPJH9MjDG+nqtZ2Y3/OZuh0WkGpiIiIiMRgKRUo2m8qxnhPMJjVtZXiRMm3ghIRERERicFSKpAkSZgZtRgu9l4G88yKZJQ35ghKRURERGR5LKWC2dnaY37cMigkG4P5gdzP0NnTKigVERERkWWxlFoBb+dgTBlzlcGsR9OB5JxN0Mk6QamIiIiILIel1ErEB8xCiGeCwaymtQinSvcJSkRERERkOSylVkKSJMyJvglOKneD+amy71HVXCAmFBEREZGFsJRaEZXSEfNjl0Ey+L9Fxv7czehStwvLRURERDTUWEqtjK9rGCaFX24w61K34UDuZ5D5/lIiIiIaoVhKrdC4oHkIdI8xmFU25yKzYr+gRERERERDi6XUCkmSAnNjboGDnYvB/ETxt6htLRGUioiIiGjosJRaKQc7Z8yLWQpA0s9k6JCcswk9vZ3ighERERENAZZSKxbgHokJIQsMZh09zTiU9zlkWRaUioiIiGjwsZRauQmhl8DPdYzBrLTxDLKrDgtKRERERDT4WEqtnEJSYF7sUqhsnQzmx4p2ob69XFAqIiIiosHFUjoMOKncMDfmFoOZTtYiOXsT1JpuQamIiIiIBg9L6TAR7BmLcUHzDGZt3Q1Iyf+C7y8lIiKiYY+ldBiZFLYQPi6hBrOi+lPIqzkmKBERERHR4GApHUYUChvMi10KOxt7g/nRwq/Q1FEtKBURERHRhWMpHWZc7D0xO/omg5lWp8GP2Z+gV6sWlIqIiIjowrCUDkNh3uMQFzDTYNbSVYufCr8SlIiIiIjowrCUDlNTxlwFT6dAg1lezXEU1J4UlIiIiIjo/LGUDlO2CiUuilsOWxs7g3lK/hdo6aoTlIqIiIjo/LCUDmOuDt6YFXmDwUyjU+PH7E+g0fUKSkVERERkPpbSYS7CdyKi/aYYzJo6qnC8aJegRERERETmYykdAaZHXAd3R1+DWXZVCorrMwQlIiIiIjIPS+kIYGtjh/mxt8JGoTSYH8rbirbuRkGpiIiIiEzHUjpCeDj5YXrEdQazXm03krM3QavTCEpFREREZBqW0hEk2m8KInwmGszq28twouRbMYGIiIiITMRSOoJIkoSZkYvhYu9lMD9dsR9ljdmCUhERERGdG0vpCKO0VeGiuOVQSDYG84O5n6Gjp0VQKiIiIqLfx1I6Ank5B2HqmKsNZj2aTuzP2QSdrBWUioiIiGhgLKUjVFzATIR6jTWY1bQW41TpPkGJiIiIiAbGUjpCSZKE2dE3wknlbjA/VfYDKpvzxYQiIiIiGgBL6QimsnXE/NjlkKRf/98s40DOp+hStwnLRURERPRbLKUjnK9rKCaHLTSYdfW24UDuZ5BlnaBURERERIZYSkeBsUFzEeQRazCrbM5DRnmyoEREREREhlhKRwFJUmBuzM1wtHM1mJ8s+Q41rcViQhERERH9CkvpKGGvdMa82KWQIOlnMnTYn7MJ3b0dApMRERERsZSOKv5uEZgQeonBrKOnBYfyPocsy4JSEREREbGUjjqJIQvg7xZhMCtrzMKZykOCEhERERGxlI46CkmBeTFLYa90MpinFn+D+rZyQamIiIhotGMpHYUcVa6YG7PEYKaTtUjO+QRqTbegVERERDSasZSOUkEeMRgXPN9g1tbdiMP52/j+UiIiIrI4ltJRbFLo5fBxCTWYFdenI7fmJ0GJiIiIaLRiKR3FFAobzI9dBjtbB4P5T4Vfo7GjSlAqIiIiGo1YSkc5Z3sPzI6+yWCm1WmQnP0JerVqQamIiIhotGEpJYR5jUV8wCyDWUtXHY4WfCkoEREREY02LKUEAJgy5ip4OQUZzPJrU5FfkyooEREREY0mLKUEALBR2GJ+3DIobVQG8yMFX6K5s1ZQKiIiIhotWEpJz9XBGzOjbjCYaXRqJGd/Ao22V1AqIiIiGg1YSslAhM8ExPhNM5g1dVbjWNFOQYmIiIhoNGAppX6mRVwDd0c/g1lO9REU16cLSkREREQjHUsp9WNrY4eL4pbDRqE0mB/K24q27gZBqYiIiGgkYyklo9wd/TAj8nqDWa+2B8nZm6DVaQSlIiIiopGKpZQGFOU7GRE+SQaz+vZynCjeLSgRERERjVQspTQgSZIwM3IRXO29DeanKw+irOGMoFREREQ0ErGU0u9S2qowP245FJKtwfxg3ufo6GkWE4qIiIhGHJZSOicv50BMi7jaYNaj6URyzmboZK2gVERERDSSsJSSSWL9ZyDMa5zBrLa1GGmlewUlIiIiopGEpZRMIkkSZkXfCGeVh8E8vexH1LQWiglFREREIwZLKZlMZeuA+XHLIEm//rGRcbT4C/TK3cJyERER0fDHUkpm8XEJxeSwKwxm3ZoOlKmPQifrBKUiIiKi4Y6llMw2NmgOgj1iDWYdulpk1xwSlIiIiIiGO5ZSMpskKTAn5hY42rkazE9X/oi6tlJBqYiIiGg4Yyml82KvdMK82KWQIOlnMmTsz9kMtYbvLyUiIiLzsJTSefN3i0BiyAKDWVt3I44WfCkoEREREQ1XLKV0QSaELoCXU7DBrKDuJApqTwpKRERERMMRSyldEIVkgxnhN0ABw9uQHinYjrbuBkGpiIiIaLhhKaUL5qRyR5ByssGsV9vTdxtSHW9DSkREROfGUkqDwt02FOGeEwxm9W1lSCvbJygRERERDScspTRokkKugIu9l8EsvewHVLfwNqRERET0+1hKadAobVSYH7u0321I9+d8ip7eTmG5iIiIyPqxlNKg8nYJwaSwyw1mneoWHM7fClmWBaUiIiIia8dSSoNuXNA8BLhFGsxKGk4jr+aYoERERERk7VhKadCdvQ2pytbRYP5T4ddo7qwVlIqIiIisGUspDQknlRtmR99oMNPoerE/ZxO0Oo2gVERERGSthJZStVqN559/HlOnTsWsWbPw8ssv832HI0io11jE+s8wmDV2VOFE8W5BiYiIiMha2Z77kKHz97//HUePHsW7776Ljo4OPPzwwwgMDMTSpUtFxqJBNHXM1ahpLTQ4bX+68iACPWIQ5BEjMBkRERFZE2GvlDY3N2Pr1q1Ys2YNEhMTMXPmTNx11104deqUqEg0BGxtlJgXuwwKyfDfPwdyP0OXul1QKiIiIrI2wkppamoqnJ2dMW3aNP3snnvuwdq1a0VFoiHi6RSAqWOuNJh197bjYN4Wvl2DiIiIAAg8fV9WVoagoCBs374dGzduRG9vL2644Qbcd999UChM68qyLKOz0zIXZe/q6jL4L/3ClLUJdZuIUtdsVLXm6WcVTTk4VfIjYnynD3lGEfgzMzCujXFcF+O4LgPj2hjHdRmYpddGlmVIkmTSsZIs6KWqN954A++++y6io6OxcuVK1NXVYfXq1bjnnntw1113nfPzMzIyoFarLZCUBotG7kFe97fQoFs/k6BApOoSOCjcxQUjIiKiIWNnZ4fx48ef8zhhr5Ta2tqivb0d//rXvxAUFAQAqKysxKZNm0wqpQCgVCoRFRU1lDH1urq6UFxcjPDwcDg4OFjkaw4X5qyNT6sL9ud/rP9Yhg610klcGvtn2CqUQx3VovgzMzCujXFcF+O4LgPj2hjHdRmYpdcmPz/f5GOFlVIfHx+oVCp9IQWAMWPGoKqqyuTnkCQJjo6O5z5wEDk4OFj8aw4XpqxNhON4NHTNxemKA/pZa3c9Tld/j5lRi4c6ohD8mRkY18Y4rotxXJeBcW2M47oMzFJrY+qpe0DgRqcJEyagp6cHRUVF+llhYaFBSaWRaVLYQng6BRrMcqqPoqThtKBEREREJJqwUhoREYGLLroITz75JLKzs3HgwAG8/fbbWLZsmahIZCE2ClvMj13W73T94byt6OhpEZSKiIiIRBJ6R6cNGzYgNDQUy5Ytw8qVK3Hrrbfi9ttvFxmJLMTN0QfTIq4zmPVoOnEg91PoZJ2gVERERCSK0Ds6ubi44KWXXhIZgQSK9puCiqZclDRk6GfVLYXILN+PxJCLxAUjIiIiixP6SimNbpIkYVb0Yjip3AzmJ0u/RV1bmaBUREREJAJLKQmlsnXEvJilkPDL7jxZ1mF/zmb0anoEJiMiIiJLYikl4fzcxiAx5GKDWVt3A44UfikoEREREVkaSylZhQmhl8DHJdRgVlB7AoW1aWICERERkUVdUCltampCSwsv4UMXTiHZYF7sUihtVAbzlIIv0NbdKCgVERERWYpZu+/b29vx2WefYd++fUhPT4dGowHQd0/TxMREXHLJJbjhhhvg6uo6JGFpZHOx98TMqMXYn7NZP+vV9mB/zmZcmXgvFJKNwHREREQ0lEwqpTqdDv/5z3/w9ttvIzAwEBdddBGWLFkCT09PaLVaNDY24vTp09i6dStef/11/PGPf8S9994LGxuWCDJPhM9EVDTloqD2hH5W11aKU6X7kBR2ucBkRERENJRMKqVLlixBVFQUNm/ejOjoaKPHLF7cd9/yjIwMfPDBB7jllluwdevWwUtKo8aMiOtR21pscNo+vewHBLhHwd8tQmAyIiIiGiomldIXXngB8fHxJj3h+PHjsWHDBpw5c+aCgtHopbRVYV7sMuxKfxPyz3d3kiHjQO6nuC7pQahsHQUnJCIiosFm0kanXxfS7du3Q61W9zums7MT77//vv7jhISEC09Ho5aPSwiSQg1P13f0tOBw3heQZVlQKiIiIhoqJpXSxsZGVFZWorKyEk8++STy8vL0H5/9dfjwYbz88stDnZdGkXHB8/qdri9pyEB+zXFBiYiIiGiomHT6fv/+/XjiiScgSRJkWcZNN93U7xhZljF//vxBD0ijl0JSYG7MEnx18v9Dj6ZTPz9a+BV8XcPh5ugjMB0RERENJpNK6aJFixAUFASdToc777wTr776KtzcfrlfuSRJcHR0RExMzJAFpdHJSeWGWdE34oesD/Uzja4XyTmbcPWE+2GjMOuqZkRERGSlTP4bferUqQCA//3vf5g0aRJsbVkGyDLCvMYi1n86cqqP6meNHZU4UfItpo65SmAyIiIiGixm39Fp2rRp+Oabb1BdXQ0AeOONN3DNNddg9erV6OnpGfSARAAwdczVcHPwNZidrtiPyqY8QYmIiIhoMJldSt944w089dRTqKysRGpqKl599VUkJSXh6NGj2LBhw1BkJIKtjR3mxy7td1enA7mfobu3XVAqIiIiGixml9KtW7di3bp1mDRpEvbs2YOJEydizZo1ePHFF7F79+6hyEgEAPB0DsSU8CsNZl29bTiY+zkvE0VERDTMmV1Ka2trkZSUBAA4fPgw5syZAwAICAhAa2vr4KYj+o34wNkI8og1mJU3ZSO7KkVQIiIiIhoMZpdSf39/FBUVoaSkBPn5+Zg9ezYA4Pjx4/D39x/0gES/JkkS5kTfDHuls8H8WNEuNHVUC0pFREREF8rsUrp06VI89NBDuO222xAbG4ukpCR8/PHHWL16NW655ZahyEhkwMHOGXNibjaY6WQNknM2QaPtFZSKiIiILoTZ13X605/+hDFjxqCsrAzXXXcdAMDV1RXPPPOM0YvqEw2FYI9YJATOwZnKg/pZc2cNjhfvxIzIReKCERER0Xk5r4uNLliwAEDf7UdbW1tx7bXXDmooIlNMDr8C1S0FaOyo0s+yq44g0D0GoV4JApMRERGRucw+fQ/0XUB/zpw5mD17NqZPn465c+fi/fffH+RoRL/PRmGLebHLYKNQGswP5X2Ozh5uuiMiIhpOzH6ldPPmzVi/fj2WL1+OqVOnQpZlHDt2DC+//DKcnZ15Cp8syt3RF9MjrsXh/G36WY+mEwdyP8Pl4+6CJJ3Xv7uIiIjIwswupe+//z5WrlyJ2267TT+77LLLEBYWhg8++ICllCwu2m8qKppyUNJwWj+raslHZsUBjA+eLzAZERERmcrsl5EqKysxb968fvO5c+eipKRkUEIRmUOSJMyKuhGOdm4G8xMle1DfVi4oFREREZnD7FIaGBiIzMzMfvOMjAx4e3sPSigic6mUjpgXuwSApJ/Jsg7JOZvQq+0RF4yIiIhMYvbp+6VLl+L5559Hc3MzJk2aBABITU3Fq6++ijvuuGPQAxKZyt8tAokhFyG97Af9rK27AUcLvup3XVMiIiKyLmaX0jvuuAMVFRX4xz/+Aa1WC1mWYWtri6VLl+K+++4bioxEJpsYcikqm/NR31amn+XXpiLQIwYRPhMEJiMiIqLfY3YpVSgUeOqpp/Dggw+isLAQABAREQFnZ+dzfCbR0FMobDA/dim+OvmqwWn7lPwv4OMSAhd7T4HpiIiIaCBmvaf01KlT6O7uBgA4OzsjMTER1dXVyM/PH5JwROfDxd6r312derXdOJD7KXSyVkwoIiIi+l0ml9LnnnsOS5cuRVpamsF8y5YtWLZsGdauXTvY2YjOW6RvEiJ8kgxmta0lOFX6vaBERERE9HtMKqVbtmzBl19+ibVr12Lq1KkGj7311lv4xz/+gc2bN2P79u1DkZHovMyIvL7f6fr0su9R01IsJhARERENyKRSumnTJvztb3/DokWLYGNjY/gECgUWL16M+++/H5988smQhCQ6H3a29pgXuxTSr37MZcjYn7sZPZougcmIiIjot0wqpcXFxZg9e/bvHnPppZfqNz4RWQsfl1AkhV1mMOvoaUZK/heQZVlQKiIiIvotk0qpnZ2dfoPT7/ntq6hE1mBc8Hz4uY4xmBXXpyO/NlVQIiIiIvotk0rp2LFj8eOPP/7uMfv27UNERMRgZCIaVApJgXmxS2Bn62AwP1rwFVq66gSlIiIiol8zqZQuX74cb775Jn744Qejj3///fd44403sGTJkkENRzRYnFTumB11o8FMo1Njf85maHUaQamIiIjoLJMunn/JJZfo79gUHx+PSZMmwdXVFc3NzThx4gRyc3OxZMkSLFq0aIjjEp2/MO9xiPGfhtzqn/SzhvYKnCz5DlPGXCkwGREREZl8R6eVK1dixowZ2LRpE/bs2YOWlhZ4enoiKSkJK1euxKxZs4YyJ9GgmDrmGtS0FBmcts+sSEagRxQC3aMFJiMiIhrdzLrN6Pz58zF//vyhykI05JQ2dpgXuww7T71ucHenA7mf4fqkB2Gv5O1yiYiIRDDpPaUffPABtFrTb8+o0Wjw3nvvnXcooqHk5RyIyeGGp+u71G04lLeVl4kiIiISxKRSWl5ejmuvvRabNm1CY2PjgMc1NTXhvffew5VXXony8vJBC0k02BICZyHII8ZgVtaYhZzqI4ISERERjW4mnb5/6qmnkJqaildeeQV///vfMXbsWMTExMDLywtarRaNjY04c+YM8vLyMHHiRLz44ouYNm3aUGcnOm+SpMCc6Jvx5cn/D9297fr5saKd8HMdAw8nf4HpiIiIRh+T31M6efJkfPjhh0hPT8e+fftw6tQppKWlQZIk+Pr64uKLL8aLL76IsWPHDmVeokHjYOeCOdE3Y++ZX95qotVpkJyzCddMeAC2NkqB6YiIiEYXszY6AUBiYiISExOHIguRxQV7xiIhcDbOVB7Sz5o7a3C8eBdmRF4vMBkREdHoYtJ7SolGssnhV8LDKcBgll2VgrLGLEGJiIiIRh+WUhr1bBS2mB+7FDYKw9P1B3M/R6e6VVAqIiKi0YWllAiAu6Mfpo25xmDWo+nAgdzPIMs6QamIiIhGD7NLaUdHx1DkIBIuxn8aQr0MN+pVNefjdMVBQYmIiIhGD7NL6aJFi3D69OmhyEIklCRJmBV1AxztXA3mJ0r2oL6d190lIiIaSmaX0q6uLjg4OAxFFiLh7JVOmBuzBICkn+lkLfZnb0avtkdcMCIiohHO7EtC3XHHHXjggQdw6623IjQ0FPb29gaPT506ddDCEYkQ4B6J8cHzkVH+o37W2l2Pnwq/xuzom8QFIyIiGsHMLqUvv/wyAGDNmjX9HpMkCVlZvIwODX9JoZehqrkA9e1l+llezXEEusdgjA+v00tERDTYzC6l+/btG4ocRFZFobDBvNil+Crt/4NGq9bPD+dvg49LCJztPQSmIyIiGnnMfk9pUFAQgoKC4ObmhoaGBrS2tsLNzU0/JxopXB28MDNykcGsV9uN/bmfQsfLRBEREQ0qs18p1el0WLduHT755BNoNBrIsgw7OzssWbIEq1atgiRJ534SomEiwicJFU25KKxL089qW4uRXvY9JoZeKi4YERHRCGN2KX3rrbewdetWPP7445g2bRp0Oh2OHTuG119/HX5+frj77ruHIieREJIkYUbkItS2lqK9p1E/P1W6DwHuUfBzDRcXjoiIaAQx+/T9li1b8Oyzz+KOO+5AXFwcEhIScOedd+KZZ57BZ599NhQZiYSys7XHvNglkH7120WGjP05m6HWdAtMRkRENHKYXUobGhowYcKEfvMJEyagqqpqUEIRWRtf1zBMDL3EYNbR04yU/C8gy7KgVERERCOH2aU0PDwchw8f7jc/dOgQNzrRiDY+5GL4uY4xmBXVn0JB7QlBiYiIiEYOs99T+sc//hGrV69GWVkZJk2aBABITU3Fxx9/jL/97W+DHpDIWigkBebFLsGXJ16BWvvLafsjBV/C1zUMrg7eAtMRERENb2aX0kWLFqG5uRnvvPMO3n33XQCAt7c3HnroIdx6662DHpDImjip3DEr+kb8mP2xfqbRqZGcsxlXJf4fbBRm/5YiIiIinEcp3bFjBxYvXow//OEPaGxshCzL8PLyGopsRFYp3Hs8ov2mIq/mmH7W0F6OtNK9mBx+hcBkREREw5fZ7yl94YUXUFdXBwDw9PRkIaVRaVrEtXB18DGYZZQno6o5X1AiIiKi4e28Njrl5uYORRaiYUNpY4f5sUuhkGx+NZVxIPczdPd2CMtFREQ0XJl9+j4uLg6PPfYY3nnnHYSHh0OlUhk8vnbt2kELR2TNvJyDMDn8Chwr2qmfdapbcShvKxbE3y4wGRER0fBjdiktKirC5MmTAUB/Gp9otEoInI2KplxUNufpZ2WNZ5BTfRShbokCkxEREQ0vZpfSBx98EImJibCzsxuKPETDiiQpMCfmFnx18hWD0/bHinbALdZfYDIiIqLhxez3lP71r39FXl7euQ8kGiUc7VwwJ/pmg5lWp8GRom3QyVpBqYiIiIYXs0upp6cn2trahiIL0bAV7BmH+IBZBrOW7lpU96YLSkRERDS8mH36ft68ebj33nsxf/58hIWF9dvo9MADDwxaOKLhZPKYK1HdUoimzmr9rEGbj8qWXEQ5ThQXjIiIaBgwu5Tu2bMHXl5eyMzMRGZmpsFjkiSxlNKoZatQYn7cMnyd9v+g1Wn08yNF26BSqRDiGS8wHRERkXUzu5R+//33Q5GDaERwd/TD1DHX4EjBdv1Mo1Nj35n/YUr4lRgbNBeSJIkLSEREZKVMek9pc3PzOY9Rq9X49ttvLzQP0bAX6z8dYV5jfzOVcbx4Fw7nbzV4FZWIiIj6mFRKZ86ciYaGBoPZypUrDWatra148MEHBzcd0TAkSRLmxi5FqMf4fo/l1RzHt5nv8q5PREREv2FSKZVlud/su+++Q2dn5zmPIxqNbBVKTA9fBD/bcf0eq2ktws5Tr6O5s0ZAMiIiIutk9iWhzjJWQPleOaJfSJIEX2U8Zo25GbYKpcFjbd2N2HnqDZQ35ghKR0REZF3Ou5QSkWmCPeJxZeL/wdHO1WDeq+3BvjPv40zFQZ5lICKiUY+llMgCvJyDcM3EB+DtHGwwlyHjp6IdSCn4Ajod7/5ERESjl8mllKfmiS6Mo50rrhh/L8K9E/s9llv9E749zQ1QREQ0epl8ndK///3vBndv6u3txfr16+Hk5AQA6OnpGfx0RCOMrY0S82OXwd3RF2mlew0eq24pxM5Tb+CShDvh7ugrKCEREZEYJpXSqVOnoq6uzmCWlJSEpqYmNDU16WdTpkw57yD33HMPPD098c9//vO8n4NoOJAkCRNDL4W7oy8O5G6BVterf6ytuwE7T72Bi+KWI8gjRmBKIiIiyzKplH744YdDGmLnzp1ITk7G4sWLh/TrEFmTcO9EOKs88X3W/9CpbtXPe7Xd2Hv6fUyLuAbxgbMEJiQiIrIc4Rudmpub8dJLL2H8+P4XGica6bxdgnH1hL/AyynIYC5Dh6OFXyElfzs3QBER0aggvJSuW7cO119/PaKiokRHIRLCSeWGKxPvRZhX/3+Y5VQfwXen30OPptPIZxIREY0cJm90GgopKSk4fvw4vv76azz33HNmf74sy/3uKjVUurq6DP5Lv+DaGGfuukwLXQQnpQfOVO83mFe15GPHydcxJ3IpXOy9Bj2nCPyZMY7rYhzXZWBcG+O4LgOz9NrIsmzyFZwkWdBVu3t6enDttdfimWeewdy5c/HEE08AgMkbnTIyMqBWq4cyIpEQzZpSlPcegwydwVwBJcLsZsLZxk9QMiIiIvPZ2dmZ9DZNYa+Uvvbaaxg3bhzmzp173s+hVCotdtq/q6sLxcXFCA8Ph4ODg0W+5nDBtTHu/NclHg0dY3Go4DN0a9r1Ux16Uaw+gKSQKxHlc/5XurAG/JkxjutiHNdlYFwb47guA7P02uTn55t8rEml9MknnzT5CdeuXWvScTt37kR9fT2SkpIAQP+q5549e3Dy5EmTnkOSJDg6OpqcbTA4ODhY/GsOF1wb485nXRwdY3Ct6wPYd+YDNHZU6ecyZJwo24VOTROmRVwDhWQz2HEtij8zxnFdjOO6DIxrYxzXZWCWWhtzbr5kUiktLy/X/29ZlnH8+HF4e3sjISEBtra2yM7ORk1NDS655BKTv/CHH34IjUaj/3jDhg0AgMcee8zk5yAayZxU7rgy8T4cyP0UpQ2nDR7LrkpBa1c95scth8qWrwIQEdHwZ/Z1Sjds2AA/Pz+sXbsWdnZ2AACtVovVq1eb1YaDggwvgXP2zlBhYWEmPwfRSKe0scPFcbfiZMl3SC//weCxyuY87Pr5DlCuDt6CEhIREQ0Osy8J9emnn+L+++/XF1IAsLGxwZ/+9Cfs2rVrUMMRESBJCkwKX4i5MUugkAz/HdnSVYedp95AVXOBoHRERESDw+yNTkqlEpWVlYiMjDSYFxQUXNB7E3h7UaLfF+mbBBd7T3yf9SG6e3/ZANWj6cS3p9/FzMhFiPGfJjAhERHR+TP7ldJrrrkGTz31FLZt24bc3FxkZ2fjk08+werVq7FkyZKhyEhEP/N1DcM1E/4CD0d/g7ks63A4fxt+KvwaOlk3wGcTERFZL7NfKX3sscfQ3d2NZ599FhqNBrIsQ6VS4bbbbsMDDzwwFBmJ6Fec7T1w1YT7sD9nM8oaswweO1N5CC1d9Zgfuwx2tvaCEhIREZnP7FJqZ2eHF154AStXrkRRUREkScKYMWN4yQUiC1LaqLAg/nakluxBZnmywWMVTTnYld63AWqk3AGKiIhGPrNP3wNAd3c3vvvuO+zZswdBQUHIzMxEU1PTYGcjot8hSQpMCb8Sc6Jv7ne90ubOWuxIex3VLYWC0hEREZnH7FJaX1+Pq6++Gs899xzeffddtLW14b///S+uvfZaFBRwBzCRpUX5TcbC8X+GvdLJYN6j6cS3me8ir/qYoGRERESmM7uU/vOf/0R0dDRSUlKgUqkAAOvWrUN0dDTWr18/6AGJ6Nz8XMNx9YS/wN3Rz2Cuk7U4lL8Vx4p2cQMUERFZNbNL6ZEjR7BixQqD+6W6ublh5cqVOHHixKCGIyLTudh74qrE+xDsEdfvsdMV+/H9mf9BrekWkIyIiOjczC6lHR0dA25q+vVtQ4nI8uxs7bEg4Q6MDZrX77HypmzsSn8Tbd2NApIRERH9PrNL6dSpU7Fp0yaDWW9vL958801MmjRp0IIR0flRSApMHXMVZkfdaGQDVA12nnodNa3FYsIRERENwOxLQq1cuRK33norfvrpJ/T29uK5555DYWEh2tra8NFHHw1FRiI6D9H+U+Hi4IUfsj5Cj6ZTP+/u7cCejP9gVtQNiPKbLDAhERHRL8x+pTQyMhJfffUVLrroIsyePRsKhQJXXnkltm/fjri4/u9lIyJx/N0icM3Ev8Dd0ddgrpO1OJi3BceLv4HMDVBERGQFzH6l9IEHHsDDDz+MBx98cCjyENEgc7H3wlWJ9yM5ZxMqmnIMHsssT0ZLZx3mxS6B0kYlKCEREdF57r4/eykoIhoe7GztcUnCnUgInN3vsbLGM9iVvhHt3c2WD0ZERPQzs0vp4sWLsWHDBuTl5UGtVg9FJiIaAgpJgWkR12JW1A2QJMPf+k0dVdhx6jXUtpYISkdERKOd2afvk5OTUVpaij179hh9PCsr64JDEdHQifGfBhd7L/yY/fFvNkC1Y3fGfzA7+kZE+iYJTEhERKOR2aX0vvvuG4ocRGRBAe6RuHrC/dh35gO0dNXp5zpZgwO5n6KlsxZJYZf1e0WViIhoqJhdShcvXjwUOYjIwlwdvHHVhPuRnP0JKpvzDB5LL/8BLV21mBOzBEobO0EJiYhoNDG7lALAvn37kJubC61Wq5+p1WpkZGTgvffeG7RwRDS0VLYOuHTsH3CscCeyqg4bPFbScBpt6RtxScIdcFK5iwlIRESjhtmldMOGDXjnnXfg7e2NhoYG+Pn5ob6+HlqtFldfffVQZCSiIaSQbDA98jq4OfriaMFXkPHLdUsbOyqxI+11LEi4Az4uIQJTEhHRSGf2G8a+/vprrFq1CgcPHoSvry8++eQTHDx4EJMmTUJICP/SIhqu4gJm4LJxd8HOxt5g3tXbht0Zb6Gw7pSgZERENBqYXUobGhqwYMECAEBsbCzS09Ph7u6Ohx9+GLt27Rr0gERkOYHuUbh64l/gau9tMNfqNNifswknS77jHaCIiGhImF1KXV1d0dnZdxmZ0NBQ5OfnAwACAwNRU1MzuOmIyOLcHHxw9cT7EeAW1e+xU2X7kJyzCRotr1FMRESDy+xSOn36dGzYsAE1NTWYMGECdu/ejcbGRuzZsweenp5DkZGILExl64jLxv4Rsf4z+j1WXJ+BbzLeQkdPi4BkREQ0UpldSv/2t7+htrYW33zzDRYuXAg7OzvMnj0bL730Eu68886hyEhEAigUNpgZtQjTI66H9Js/KhraK7Dj1GuobysXlI6IiEYas3ffBwQEYPv27ejp6YGdnR0+/vhjHDhwAP7+/khMTByKjEQkUHzgTLg6eOHH7E/Qq+3Wz7vUbfgm4y3MjbkZ4d78vU9ERBfmvG/XolKpAAAODg64/PLLWUiJRrAgjxhcPeF+uNh7Gcy1ul78mP0J0kr3QpZlQemIiGgkMPuV0ri4OEiSNODjWVlZFxSIiKyTu6Mvrp5wP37M/hjVLYUGj6WV7kVLZx1mR98EWxuloIRERDScmV1K//GPfxiUUo1Gg+LiYmzfvh1/+9vfBjUcEVkXe6UTLh/7Jxwp+BK5NT8ZPFZUfwpt3Q1YkHAHHO1cBSUkIqLhyuxSesMNNxidjxs3Dlu2bMH1119/waGIyHr1bYBaDHdHXxwr2gkZv5y2r28vx46013FJwh3wcg4SmJKIiIab835P6W8lJiYiNTV1sJ6OiKyYJElICJqDS8b+AUoblcFjneoWfJO+ESX1mYLSERHRcDQopbSjowMfffQRvL29z30wEY0YwR6xP2+AMrxGsUbXix+yP8Kpsu+5AYqIiEwyaBudJEnC888/PyihiGj4cHf0w9UT/oIfsj5CTWuRwWMnS75FS2ctZkXfCFsFN0AREdHALnijEwAolUpMmDABISEhgxaMiIYPe6UTLh/3Jxwp2I68muMGjxXWpaGtuxEL4m+Hg52LoIRERGTtBm2jExGNbjYKW8yKuhHujn44VrQL+NUGqLq2Uuw49Rouib8Tns6B4kISEZHVMruUvvbaayYf+8ADD5j79EQ0jEmShLFBc+Hq4IPknE+g0ar1j3X0tGBX+kbMi10Cb4cxAlMSEZE1MruUHj16FOnp6dDpdAgPD4dSqURxcTG6uroQEBCgP06SJJZSolEqxDMOVyfej31nPkB7T5N+rtGp8X3WRxgfuACy7CEwIRERWRuzS+ns2bOh1Wrx73//G35+fgCA9vZ2rFy5EpGRkXjkkUcGPSQRDT8eTv59G6CyP0Rta8mvHpGRUbkP7jahGNMbCkc4CstIRETWw+xLQn344Yd45pln9IUUAJydnfHQQw/h008/HdRwRDS8Odg5Y+G4PyPSd1K/x5q1pdiV+SqOFe5Al7pNQDoiIrImZpdStVqNzs7OfvO6urpBCUREI4uNwhZzom/GlPArARheuUMra3C68iA+P/4SjhXtRJe6XUxIIiISzuxSeumll+Lpp5/GkSNH0NHRgfb2diQnJ2P16tW47rrrhiIjEQ1zkiRhXPB8LIi/HbY2dv0e1+p6cbriALYeX4djRbtYTomIRiGz31P61FNP4a9//Sv+8Ic/6K9XKssyrrrqKjz++OODHpCIRo5QrwQsSnoYqUXfoqghDb++bBTQdyeo0xX7kVOVgriAWRgXPBf2SmchWYmIyLLMLqXOzs547733UFBQgLy8PABAQkICQkNDBz0cEY08zvYemBp2Leza/aF2rkJxQzpk6AyO0eh6kVmRjOyqFMQFzsS4oHmwVzoJSkxERJZg9un7syIjIzFt2jQoFArU19cPZiYiGgXsFM6YGnYdFk9+FFG+kyEZ+eNIo1MjszwZnx9bh9Ti3eju7RCQlIiILMHkUvr6669j+vTpKCnpu7TLiRMncPnll2PFihVYvnw5/vjHP6K7u3vIghLRyOTq4IU5MTdj8eRHEOk7CdJvNkMBfeU0o/xHfH6c5ZSIaKQyqZR++umn2LhxI2655RZ4eXkBAFatWgV7e3vs2LEDycnJ6OjowNtvvz2kYYlo5HJ18MbcmFuwePKjA5dT7S/l9ETxHvT09r8SCBERDU8mldItW7bgiSeewKOPPgpnZ2dkZGSguLgYt99+O6KiouDn54f77rsPO3fuHOq8RDTCGZRTn6QBy2l6+Q995bTkW5ZTIqIRwKRSWlBQgNmzZ+s/PnLkCCRJwvz58/WzqKgoVFZWDn5CIhqVXB28MTd2CRZNegQRA5TTXm0P0su+x+fH1+Fkybfo0bCcEhENVya/p/Ts5Z8A4Pjx43Bzc0NcXJx+1tHRAQcHh8FNR0SjnpujD+bFLsH1kx5GhM9E/PYC/EBfOT1V9j0+P7YOJ0u+Q4+my+I5iYjowphUSmNiYnDixAkAQGtrK44ePWrwyikAfPPNN4iJiRn8hEREANwdfTEvdikWTXoYY3wmYOByuo/llIhoGDLpOqW33nornn32WWRlZeHkyZNQq9W48847AQA1NTX4+uuv8e677+LFF18c0rBERO6OvpgfuwwTQi7BqdJ9KKpPx28vwt+r7capsn04U3kIY4PmICFwDuxs7cUEJiIik5hUSq+77jqo1Wps2rQJCoUC//73v5GYmAgAeOutt/DZZ5/hz3/+M66//vohDUtEdJa7oy/mxy3DhM4FSCvdh+L6DBgrp2mle3Gm4iDGBs1FfOBsllMiIitl8h2dbrrpJtx000395vfeey/++te/wsPDY1CDERGZwt3RDxfFLUdTRw1Ole1DcX16v2PU2m6cLP0OpysPYmzgHJZTIiIrZPZtRn/Lz89vMHIQEV0QD6ez5XTBz+U0o98xak3XL+U0aC7iA2axnBIRWYkLLqVERNbEw8kfF8XdiqaOaqSV7kVJQ2a/Y9SaLpws+faX0/oBs6C0VQlIS0REZ7GUEtGI5OHkj4vjb0NjRxVOle5FScPpfsf0aDpxomQPTlccwNigeYgPmMlySkQkCEspEY1onk4BuDj+djS2VyKtbB9KByynu3G6Yj/GBc9DXMBMKG1YTomILImllIhGBU/nQCyIvx0N7ZU4VboXpY1n+h3To+lEavFuZJYf+FU5tROQloho9GEpJaJRxcs5EAsS7kBDewXSSvehzGg57UBq8Td9r5wGzUMsyykR0ZBjKSWiUcnLOQiX6MvpXpQ1ZvU7pru3A8eLv0FmxX6MC5qPuIAZsGU5JSIaEiylRDSq9ZXTO1HfXo60kr0ob8rud0xfOd2FzIr9GB88H7H+01lOiYgGGUspEREAb+dgXDr2D6hvK0da6UDltB3HinYiozyZ5ZSIaJCxlBIR/Yq3S185rWsrw6nSvShvyul3zNlymlm+H+P05VQpIC0R0cjBUkpEZISPSwguHftH1LWVIq10HyqMlNOu3jYcK9qBzJ9fOY1hOSUiOm8spUREv8PHJRSXjf0jaltLkVa6F5XNuf2O6eptw09FO5BRkYzxwRchxn8abBUsp0RE5mApJSIyga9rKC4fdxdqW0t+Lqd5/Y7pUrfhp8Kvf37l9CJE+09lOSUiMpFCdAAiouHE1zUMl4/7E65KvA+B7tFGj+lUt+Jo4VfYdnw9sipToNH1WjglEdHww1JKRHQezpbTKxP/DwHuUUaP6SunX2Lb8fXIrkqBVqexcEoiouGDpZSI6AL4uYZj4bi7ceX4exHgNnA5PVLwJbYeX4/sqiMsp0RERrCUEhENAj+3MVg4/m5cMf4e+LtFGD2mU92CIwXbsS11PXKqjrKcEhH9CkspEdEg8neLwBXj7/ndctrR04KUgi+wLXUDcqpZTomIAO6+JyIaEmfLaVVzAdJK96KmtajfMR09zUjJ/wLpZT9gQsgCBLrEC0hKRGQdWEqJiIZQgHskAtwjfy6n36GmtbjfMR09zTicvw1Odu5wlyOh1Rnf1U9ENJKxlBIRWUCAeyT83SJQ3VKAk6V7UWusnKqb0YFU7MjMQmzAdMT6T4eTyt3iWYmIRGApJSKyEEmSEOAeBX+3SFS19L1yWtta0u+4Hk0n0st+QEZZMkK94hEXMAv+bhGQJElAaiIiy2ApJSKyMEmSEOgehQC3SFQ15+Nk6Xeoayvtd5wMHUoaTqOk4TTcHX0RFzATkT6ToLRVCUhNRDS0WEqJiASRJAmBHtEIcI/qK6cle1HX3v+VUwBo7qzFkYIvkVq8G5G+kxAXMBPujr4WTkxENHRYSomIBDtbTt1VQTh5+gh0Lo0oaUyHRqfud2yvtgfZVSnIrkpBgFsU4gJmIMQrHgrJRkByIqLBw1JKRGRF7BVuiA+dgelR16CgNhVZVUfQ2lVn9NiqlnxUteTDSeWGWP8ZiPGfCnuls4UTExENDpZSIiIrZGdrj/jA2YgLmIWqlnxkVaagvDELMuR+x3b0tOBEyR6kle7FGO9ExAXOgo9LiIDURETnj6WUiMiK9W2KikagezTau5uQU30UudU/oUfT2e9YnaxFQd1JFNSdhJdzMOIDZiLcJxG2CqWA5ERE5mEpJSIaJpztPTA5/ApMCL0ExXXpyK5KQX17udFjG9rLcTBvC44V7US0/1TE+k+Hi72nhRMTEZmOpZSIaJixVSgR5TcZUX6TUddWhuyqFBTVpUMna/od26PpRGZ5MjLL9yPEMw5xATMR6B4FSVIISE5ENDChpbSmpgYvvvgijhw5ApVKhauuugqPPPIIVCpeg4+IyBQ+LiHwcQnB1DFXIbf6OHKqj6Cjp9nIkTLKGrNQ1pgFVwdvxAXMRJTvZNjZ2ls6MhGRUcJKqSzLWLFiBVxdXfHxxx+jpaUFq1atgkKhwMqVK0XFIiIaluyVzkgMuQjjguehvDELWVUpqGrON3psa1c9fir8GieK9yDSNwlxATPh4eRv4cRERIaEldLCwkKkpaXh0KFD8Pb2BgCsWLEC69atYyklIjpPCkmBUK+xCPUai+bOWuRUHUF+bSp6tT39jtXo1MipPoqc6qPwcx2D+MCZCPUcC4WC1zwlIssTVkp9fHzwzjvv6AvpWe3t7YISERGNLO6OvpgeeR0mhS1EQd1JZFcdRnNnrdFja1qLUNNaBEc7V8T4T0OM/3Q42rlYODERjWbCSqmrqyvmzp2r/1in0+Gjjz7CjBkzTH4OWZbR2dn/sihDoaury+C/9AuujXFcl4FxbYwbynUJdUtEiOt41LWXIL/uGCqas41e87RT3Yq00r1IL/sewe4JiPKZAi+nEEiSNOiZTMWfl4FxbYzjugzM0msjy7LJf35Isiz3/1NJgHXr1uHjjz/G559/jpiYmHMen5GRAbW6/y34iIjo3HrlTjRoCtGkKYQG/U/t/5q95A4v2yi424RAIfGiLURkHjs7O4wfP/6cx1nFny7r16/HBx98gH//+98mFdKzlEoloqKihjDZL7q6ulBcXIzw8HA4ODhY5GsOF1wb47guA+PaGGf5dZkMrU6D8uYs5NcdQ0OH8WuedsvNqOg9jjpdJsK9JiLKZwqcVZa75il/XgbGtTGO6zIwS69Nfr7xDZfGCC+la9aswaZNm7B+/XosXLjQrM+VJAmOjo5DlMw4BwcHi3/N4YJrYxzXZWBcG+MsvS7xztMRHzwdDe0VyK5KQWFdGrS6/tc8VWu7kVt7BLm1RxHsEYO4gJkI8oix2DVP+fMyMK6NcVyXgVlqbcx564/QUvraa69h8+bNePnll3HFFVeIjEJENOp5OQdhdvRNmBJ+FfJqjiO76gjaexqNHCmjvCkH5U05cLH3Qqz/dET7TYFKyb/8iej8CSulBQUFeOONN3DPPfdg8uTJqKur0z/m4+MjKhYR0ainUjpiXPA8JATNQUVTLrKrDqOiKdfosW3dDThevAsnS79DhM9ExAXMhJdzoIUTE9FIIKyU7tu3D1qtFm+++SbefPNNg8dycnIEpSIiorMUkgIhnnEI8YxDa1c9squOIL/mONTa7n7HanW9yKs5hryaY/B1DUNcwEyEeY2DjUL4u8SIaJgQ9qfFPffcg3vuuUfUlyciIjO4OnhjWsQ1SAq7HEV1aciqPIymzmqjx9a2lqC2tQT2SmfE/nzNUyeVm4UTE9Fww3/CEhGRyZQ2dojxn4Zov6mobS1BVtVhlDRkQpZ1/Y7t7m3HqbLvkV72I0K9xiI+cCb8XMcIveYpEVkvllIiIjKbJEnwcwuHn1s4OtWtyK3+CTnVR9Glbut3rAwdShoyUNKQAXdHP8QFzESkbxKUNioByYnIWrGUEhHRBXG0c8XE0EuRGHwxShpOI7sqBTWtRUaPbe6swZGC7Ugt/gZRflMQFzADbg7c3EpELKVERDRIFAobjPFJxBifRDR2VPVd87T2JDS63n7H9mp7kFV5CFmVhxDoHo24gJkI9oyDwkLXPCUi68NSSkREg87TKQCzom7A5PArkV+TipyqI2jtrjd6bGVzHiqb8+CkckdcwAxE+02FvdLJwomJSDSWUiIiGjIqWweMDZqDhMBZqGzOR3ZVCsoaswHI/Y7t6GlGavFunCzZizE+iYgPmAVvl2DLhyYiIVhKiYhoyEmSAkEeMQjyiEFbdyNyqo4gr+Y4ejSd/Y7VyRoU1J5AQe0JeLuEIMJzMnQyT+sTjXQspUREZFEu9p6YMuYqTAy9DEX1p5BdmYKGjgqjx9a3laG+rQw2UEFXUYtxoXN5zVOiEYqllIiIhLC1USLabwqifCejrq0M2VUpKK5Ph07W9jtWix5k1RxEds1hhHmPRXzALPi6hvOap0QjCEspEREJJUkSfF1D4esaiqljrkZezTFkVx1Bp7ql37EydCiuz0BxfQY8nAIQHzALET4TYWujFJCciAYTSykREVkNBztnJIZcjHHB81DWkIXsqhRUtRQYPbapowqH87citfgbRPtPRZz/TDjbu1s2MBENGpZSIiKyOgrJBmHe4xDmPQ5VDSU4lvstWuVSo9c87dF0IrM8GafL9yPUKwFxAbPg7xbBU/tEwwxLKRERWTU3Bx8E2U3CvOgbUdHWd8eotu7GfsfJkFHScBolDafh4eiP+MCzp/btBKQmInOxlBIR0bBgZ2uPsUFzkRA4G+VNuciqPITK5jyjxzZ1VuNw/jYcL/4GMX5TERswAy72nhZOTETmYCklIqJhRZIUCPGMQ4hnHJo7a5FdlYL82lRotOp+x6o1Xcis2I/TFQcQ4hmP+MBZ8HeL5Kl9IivEUkpERMOWu6MvZkRej0lhC5Ffm4qsysNo627od5wMGaWNZ1DaeAbujn59u/Z9k6DkqX0iq8FSSkREw56drT0SAmcjPmAmKprykFV1CBVNuUaPbe6sQUrBF3279v2mIC5wJlzsvSycmIh+i6WUiIhGDElSINgzFsGesWjpqkN2Zd+p/V5tT79j1dpunK48iNOVhxDiGYf4wFkIcIviqX0iQVhKiYhoRHJz8MH0yOt+ObVflYLWrjojR8ooa8xCWWMW3Bx8ER84E5G+k6C0UVk8M9FoxlJKREQjmtJWhfjAWYgLmIHK5nxkVR5GeVMOALnfsS1dtThS8CVSi/cg2m8y4gJmwtXB2/KhiUYhllIiIhoVJEmBII8YBHnEoLWrHtlVKcirOW701H6vthtnKg/hTOVhBHvEIj5wFgLdoyBJCgHJiUYHllIiIhp1XB28MS3iWiSFXY6C2pPIqjyMlq5aI0fKKG/KRnlTNlwdfBAfMBNRvpOhtOWpfaLBxlJKRESjltJGhbiAGYj1n46qlr5T+2WN2TB2ar+1qw5HC7/CiZI9iPKdjPjAWTy1TzSIWEqJiGjUkyQJge7RCHSPRlt3A7KrjiC3+hh6td39ju3V9iCr6jCyqg4jyCMW8QGzEOQRzVP7RBeIpZSIiOhXXOy9MHXM1ZgYehkKa08iq+oQmjuNndoHKppyUNGUA1d7b8QF9p3at7O1t3BiopGBpZSIiMgIpY0dYgOmI8Z/GqpbCn4+tZ8F2dip/e56/FT4tf7UflzATLg7+gpITTR8sZQSERH9DkmSEOAehQD3KLR1NyKn6ghya45Brenqd6xGq0Z2VQqyq1IQ6B6N+MBZCPaI5al9IhOwlBIREZnIxd4TU8ZchYmhl6KwLg1ZlYfR1Flt9NjK5jxUNufBxd4LcQEzEOU3BSpbBwsnJho+WEqJiIjMZGtjhxj/aYj2m4qa1iJkVR5CacMZo6f227obcKxoJ06WfIdI30mID5wJd0c/AamJrBtLKRER0XmSJAn+bhHwd4tAe3cTsquPIK/6GHo0nf2O1ejUyKk+gpzqIwhwj0J8wCwEe8ZBwVP7RABYSomIiAaFs70HpoRfiYkhl6KoLg1nqg6jqaPK6LFVzfmoas6Hs8oTcQEzEO0/BSpbRwsnJrIuLKVERESDyNZGiWj/qYjym4La1mJkVR1GSf1pyND1O7a9pxHHi3chrfQ7RPgmIT5gFjyc/AWkJhKPpZSIiGgISJIEP7cx8HMbg46eZuRUHUVO9U/o0XT0O1aj60Vu9U/Irf4J/m4RiA+cjRDPeJ7ap1GFpZSIiGiIOancMSl8IRJDF6Co7hSyKg+jsaPS6LHVLYWobimEk8od8QEzEe03FSolT+3TyMdSSkREZCG2CiWi/aYgyncyattKkFV5GCUNmZDl/qf2O3qacbz4G5ws3YsIn4mID5wFT6cAAamJLIOllIiIyMIkSYKfazj8XMPR0dOCnOqjyK0+iu7e/qf2tbpe5NUcQ17NMfi5jkFC4GyEeMVDIdkISE40dFhKiYiIBHJSuWFS2OWYELIAxfXpOFN5GA3t5UaPrWktQk1rEZxUboj1n4kQt3EWTks0dFhKiYiIrICNwhaRvpMQ4ZOEurYyZFUdQnF9xgCn9ltwomQ3Tkl74aoIhnuLEuH2CbBR8K91Gr7400tERGRFJEmCr2sofF1DMXXM1T/v2j+K7t72fsdqZQ2atMU4UFCMI8UqBHnEItRrLII9YmFnay8gPdH5YyklIiKyUo52rkgKuwyJIRejuD4DWVWHUd9WZvTYXm0PiuvTUVyfDoVkA3+3SIR6JSDUMwGOKlcLJycyH0spERGRles7tZ+ESN+fT+1XHkZxfTp0stbo8TpZi8rmXFQ25+JIwXZ4u4Qg1HMsQr0S4O7oa+H0RKZhKSUiIhpGfFxC4BO7BFPGXIXsiqPIrzqJTl3D735OfVsZ6tvKcKJkN1wdfH5+BXUsfFyCIfEC/WQlWEqJiIiGIUc7F8T5zYLc6IHwqBA0dBWjtOE0KpvzB3wFFQBau+qQWZ6MzPJkOChdEOIVj1DPsQhwj+RGKRKKP31ERETDnIPSGTFu0xDjPw29mh5UNOegtOEMyhqz0avtHvDzunrb9Lc3Vdqc3SiVgGCPOG6UIotjKSUiIhpBlLYqhHsnItw7EVqdBjUtRShtPI3ShjPoVLcO+Hn9N0pFINRrLEI84+GkcrPgd0CjFUspERHRCGWjsEWgRzQCPaIxPeI6NLRXoLThDEobT6O5s3bAz+vbKJWHyua8vo1SziF970P1Ggs3Bx9IkmTB74JGC5ZSIiKiUUCSFPB2CYG3SwgmhS9ES1cdyhrOoLThDGrbSgHIA35ufXsZ6tvLcKJkD1wdvPU7+X1cQrhRigYNSykREdEo5ObgA7fg+RgXPB+d6jaUN2aZuFGqHpkVycisSIa90lm/k58bpehC8aeHiIholHO0c0GMv/kbpbp72/UbpWxt7BDsEceNUnTeWEqJiIhI73w3Smm0am6UogvCUkpERERGcaMUWRJLKREREZ0TN0rRUGMpJSIiIrMN2kYpzwSEeiXA3z0StgqlBb8DsjYspURERHRBLmijVM1PyK05u1EqFqGeYxHkGQuVrYMFvwOyBiylRERENGiMb5Q6g9KG0yZslMpAcX0GJEmBALdIhHolIMQzgRulRgmWUiIiIhoSRjdK/byTv7mzZsDPk2XdrzZKfQlv52CEevW9D9XNwZcbpUYollIiIiIacpIkwdslGN4uwZgUthCtXfX6nfy1refaKFWO+vbyvo1S9t76nfzcKDWysJQSERGRxbk6eGNc8DyMC56HLnUbyhqzUNpwBpXNeb+/Uaq7HpkV+5FZsZ8bpUYYllIiIiISyqHfRqlclDacPu+NUp72oRZMT4OFpZSIiIisRt9GqfEI9x7ft1GqtajvNL85G6WggJPCB6huRLBXNLycg2Brw1dRrR1LKREREVklG4UtAt2jEehu5kYp6NCuq0FGZQ0yKvdBIdnAyzkIvi6h8HENg69rGBztXC34nZApWEqJiIjI6l3IRimdrEVdWynq2kqByoMAAGeVJ3xdQ+HrGgZflzC4O/lDwU1TQrGUEhER0bAz8EapfOhkzTk/v72nEe11jSisSwMA2NrYwcclFL4ufa+k+riEws7Wfoi/C/o1llIiIiIa1n67UaqwJgM5pSehs2tHS1ct5N95FfUsjVaNquZ8VDXn/zyR4OHo1/dKqmsYfFzC4GLvyWukDiGWUiIiIhoxlLYqhHgkoL1aQnx8PGztFKhvK0NtWwlqW0tQ11aKXm2PCc8ko6mzGk2d1cipPgoAsFc660/3+7qGwcs5CDYKVqnBwpUkIiKiEcvO1l5/VykA0Mk6NHfWoLb1bEktQVt3o0nP1d3bjtKG0yhtOA0AUEg28HYO7nsl9eey6mDnPGTfy0jHUkpERESjhkJSwNMpAJ5OAYgLmAEA6FS3oa61RP9qakN7xe9ewP8snazt+5y2EqCib+Zi72Xwaqq7oy/vOmUillIiIiIa1RztXBDmPQ5h3uMAABpdLxraK/qK6s9ltbu3w6TnautuQFt3AwpqTwAAlDb28HEJhd/Pr6b6OIdAaasasu9lOGMpJSIiIvoVW4USfq7h8HMNBwDIsoy27gZ9Qa1tLUFzZy1+7zJUZ/Vqu1HZnIvK5lwAgAQJHk7+8HUNh69LGHxcQ+Gs8uAGKrCUEhEREf0uSZLg6uANVwdvRPlNBgD0aLr6rn3aWoLa1r5roGp06nM+lwwZjR1VaOyoQnZVCgDA0c4VPi5hP183NRyeTgGjcgPV6PuOiYiIiC6QytYBwR6xCPaIBdD3/tKmjuqfC2oJalqL0dHTbNJzdapbUdKQgZKGDAB9d7Lq20AVrr8Llb3Saai+FavBUkpERER0gc7eytTLOQjxmAkA6OhpQV1bKWpbi1HbWoqGjgrIsu6cz6XVaVDTWoya1mL9zNXBB74uP9+ByjUcbg7eI24DFUspERER0RBwUrnBSTUe4d7jAQAabS/q28v0r6bWtpagR9Np0nO1dtWhtasO+bWpAAA7Wwf9q6i+LmHwdgmB0sZuyL4XS2ApJSIiIrIAWxsl/N0i4O8WAaBvA1VrV71+81RtawlaumpNei61pgvlTTkob8oBAEhQwNM5QH8pKl/XMDip3IfqWxkSLKVEREREAkiSBDdHH7g5+iDabwoAoKe38+dT/n07/evayqDV9Z7zuWTo0NBegYb2CmRVHQbQ90qtz69KqqdjwJB+PxeKpZSIiIjISqiUjgj2jEOwZxwAQKfTorGjyuDV1E51i0nP1dHTgo6edBTXpwPou9SVh2MgbDUe0MkxQ/Y9nC+WUiIiIiIrpVDYwNslGN4uwUgInA0A6Ohp1hfU2rYSNLZXQca5N1BpdL2oay8BUIK0ciXmxN44xOnNw1JKRERENIw4qdwxxscdY3wmAAB6tWrUt5X1ne7/uayqtd2/+xw1rYWWiGoWllIiIiKiYUxpY4cA90gEuEcCAGRZh5auOoNXU1u76g0+J8AtWkTU38VSSkRERDSCSJIC7o5+cHf0Q4z/NABAd29H36uoLeVoqmvHhKDLBKfsj6WUiIiIaISzVzohxCsBXg7hyGrKgiRJoiP1M7JuBUBEREREwxJLKREREREJJ7SU9vT0YNWqVZgyZQrmzJmD//73vyLjEBEREZEgQt9T+tJLLyEzMxMffPABKisrsXLlSgQGBuKKK64QGYuIiIiILExYKe3s7MSWLVvwn//8B2PHjsXYsWORl5eHjz/+mKWUiIiIaJQRVkqzs7Oh0WiQlJSkn02ePBkbN26ETqeDQnHudxbIsozOzs6hjKnX1dVl8F/6BdfGOK7LwLg2xnFdjOO6DIxrYxzXZWCWXhtZlk3e6S+slNbV1cHDwwN2dnb6mbe3N3p6etDc3AxPT89zPkdvby+ysrKGMmY/xcXFFv16wwnXxjiuy8C4NsZxXYzjugyMa2Mc12VgllybX3e93yOslHZ1dfULefZjtVpt0nMolUpERUUNejZjurq6UFxcjPDwcDg4OFjkaw4XXBvjuC4D49oYx3UxjusyMK6NcVyXgVl6bfLz800+VlgpValU/crn2Y/t7e1Neg5JkuDo6Djo2X6Pg4ODxb/mcMG1MY7rMjCujXFcF+O4LgPj2hjHdRmYpdbGnIv0C7sklJ+fH5qamqDRaPSzuro62Nvbw9XVVVQsIiIiIhJAWCmNj4+Hra0t0tLS9LPU1FSMHz/epE1ORERERDRyCGt/Dg4OWLRoEZ577jmkp6dj7969+O9//4s77rhDVCQiIiIiEkToxfOffPJJPPfcc7jzzjvh7OyMv/71r7j88stFRiIiIiIiAYSWUgcHB6xbtw7r1q0TGYOIiIiIBOObN4mIiIhIOEmWZVl0iPNx4sQJyLJs8gVZL5Qsy+jt7YVSqTTr8gajAdfGOK7LwLg2xnFdjOO6DIxrYxzXZWCWXhu1Wg1JkjBp0qRzHiv09P2FsPQPmSRJFivAww3Xxjiuy8C4NsZxXYzjugyMa2Mc12Vgll4bSZJM7mzD9pVSIiIiIho5+J5SIiIiIhKOpZSIiIiIhGMpJSIiIiLhWEqJiIiISDiWUiIiIiISjqWUiIiIiIRjKSUiIiIi4VhKiYiIiEg4llIT1NTUYMWKFZg2bRrmzp2LtWvXoqenR3Qs4UpKSvCnP/0JSUlJuOiii/DOO++IjmR17rnnHjzxxBOiY1iN7777DrGxsQa/VqxYITqWVVCr1Xj++ecxdepUzJo1Cy+//DJG+71Ntm3b1u/nJTY2FnFxcaKjWYWqqirce++9mDRpEhYsWID3339fdCSr0NDQgBUrVmDKlCm47LLLsG3bNtGRhFOr1bjmmmtw9OhR/aysrAx/+MMfMHHiRFx11VU4ePCgwIR9hu1tRi1FlmWsWLECrq6u+Pjjj9HS0oJVq1ZBoVBg5cqVouMJo9PpcM8992D8+PH44osvUFJSgkceeQR+fn649tprRcezCjt37kRycjIWL14sOorVyM/Px8UXX4w1a9boZyqVSmAi6/H3v/8dR48exbvvvouOjg48/PDDCAwMxNKlS0VHE+aqq67C3Llz9R9rNBrceeeduOiii8SFsiIPPfQQAgMDsW3bNuTn5+Oxxx5DUFAQLrvsMtHRhJFlGX/5y1+g0+nwv//9DzU1NVi5ciWcnZ1x+eWXi44nRE9PDx599FHk5eXpZ2fXKSYmBlu3bsXevXvxwAMPYNeuXQgMDBSWla+UnkNhYSHS0tKwdu1aREdHY8qUKVixYgV27NghOppQ9fX1iI+Px3PPPYfw8HDMnz8fM2fORGpqquhoVqG5uRkvvfQSxo8fLzqKVSkoKEBMTAx8fHz0v1xdXUXHEq65uRlbt27FmjVrkJiYiJkzZ+Kuu+7CqVOnREcTyt7e3uBn5auvvoIsy3jsscdERxOupaUFaWlpuO+++xAeHo5LL70Uc+fORUpKiuhoQmVmZuLkyZP417/+hYSEBFx88cW4++678e6774qOJkR+fj5uueUWlJaWGsyPHDmCsrIyvPDCC4iMjMS9996LiRMnYuvWrYKS9mEpPQcfHx+888478Pb2Npi3t7cLSmQdfH198corr8DZ2RmyLCM1NRXHjh3DtGnTREezCuvWrcP111+PqKgo0VGsSkFBAcLDw0XHsDqpqalwdnY2+P1zzz33YO3atQJTWZfm5mb85z//waOPPgo7OzvRcYSzt7eHg4MDtm3bht7eXhQWFuLEiROIj48XHU2osrIyeHp6IiQkRD+LjY1FZmYment7BSYT46effsL06dPx6aefGsxPnTqFhIQEODo66meTJ09GWlqahRMaYik9B1dXV4PTRzqdDh999BFmzJghMJV1WbBgAZYvX46kpCQsXLhQdBzhUlJScPz4cdx///2io1gVWZZRVFSEgwcPYuHChbj00kuxYcMGqNVq0dGEKysrQ1BQELZv344rrrgCl1xyCV5//XXodDrR0azGpk2b4OvriyuuuEJ0FKugUqmwevVqfPrpp5gwYQKuvPJKzJs3DzfffLPoaEJ5e3ujra0NXV1d+ll1dTU0Gg3a2toEJhNj+fLlWLVqFRwcHAzmdXV18PX1NZh5eXmhurrakvH6YSk10/r163HmzBk8/PDDoqNYjVdffRUbN25EVlbWqH9lp6enB88++yxWr14Ne3t70XGsSmVlJbq6umBnZ4dXXnkFK1euxNdff42XXnpJdDThOjs7UVJSgs2bN2Pt2rVYuXIlPvzwQ25c+Zksy9iyZQtuu+020VGsSkFBAS6++GJ8+umnWLt2LXbv3o2vvvpKdCyhJkyYAF9fX6xZs0b/++q9994DgFH5SulAzv5Z/Gt2dnbCXyTgRiczrF+/Hh988AH+/e9/IyYmRnQcq3H2fZM9PT147LHH8Le//W3Unl577bXXMG7cOINX16lPUFAQjh49Cjc3N0iShPj4eOh0Ojz++ON48sknYWNjIzqiMLa2tmhvb8e//vUvBAUFAegr8Zs2bcJdd90lOJ14GRkZqKmpwdVXXy06itVISUnB559/juTkZNjb22P8+PGoqanBm2++ieuuu050PGFUKhVeeeUVPPTQQ5g8eTK8vLxw9913Y+3atXB2dhYdz2qoVCo0NzcbzNRqtfAXU1hKTbRmzRps2rQJ69ev5ylq9G10SktLw6WXXqqfRUVFobe3F+3t7fD09BSYTpydO3eivr4eSUlJAKD/V+eePXtw8uRJkdGsgru7u8HHkZGR6OnpQUtLy6j9mQH63ruuUqn0hRQAxowZg6qqKoGprMeBAwcwZcoUuLm5iY5iNTIzMxEWFmZQIhISErBx40aBqaxDYmIivv/+e9TV1cHDwwOHDh2Ch4cHnJycREezGn5+fsjPzzeY1dfX9zulb2k8fW+C1157DZs3b8bLL7/Mf6n/rLy8HA888ABqamr0s8zMTHh6eo7qcvHhhx/i66+/xvbt27F9+3YsWLAACxYswPbt20VHE+7AgQOYPn26wXu9srKy4O7uPqp/ZoC+U449PT0oKirSzwoLCw1K6miWnp6OSZMmiY5hVXx9fVFSUmJwurWwsBDBwcECU4nX3NyMZcuWoampCT4+PrC1tcWPP/7ITbi/MWHCBJw+fRrd3d36WWpqKiZMmCAwFUvpORUUFOCNN97An//8Z0yePBl1dXX6X6PZ+PHjMXbsWKxatQr5+flITk7G+vXr8X//93+iowkVFBSEsLAw/S8nJyc4OTkhLCxMdDThkpKSoFKp8PTTT6OwsBDJycl46aWXcPfdd4uOJlxERAQuuugiPPnkk8jOzsaBAwfw9ttvY9myZaKjWYW8vDxeyeI3FixYAKVSiaeffhpFRUX4/vvvsXHjRtx+++2iownl7u6Ozs5OrF+/HmVlZdiyZQu2bt3KP2d+Y9q0aQgICMCTTz6JvLw8vP3220hPT8dNN90kNJckj/ZbhpzD22+/jX/9619GH8vJybFwGutSU1ODNWvWICUlBQ4ODrjttttw7733QpIk0dGsxtm7Of3zn/8UnMQ65OXl4R//+AfS0tLg5OSEpUuX4i9/+Qt/ZgC0tbVhzZo1+O677+Dg4IDly5dzbX6WmJiI119/ne/V/o38/Hy8+OKLSE9Ph6enJ2699Vbceeedo/5nprCwEM8++ywyMjIQHByMRx99FBdffLHoWMLFxsbif//7H6ZPnw6g766MTz31FE6dOoWwsDCsWrUKs2bNEpqRpZSIiIiIhOPpeyIiIiISjqWUiIiIiIRjKSUiIiIi4VhKiYiIiEg4llIiIiIiEo6llIiIiIiEYyklIiIiIuFYSomIiIhIOJZSIhr1br/9dtxwww0DPv70009j4cKF53ye//f//h8WLFgwmNHOy9atWzFnzhwkJibiu+++6/f4E088YfR2lLt27UJCQgKeeeYZ6HQ6S0QlItJjKSWiUe+mm27C6dOnUVBQ0O+xnp4e7N69W/g9oc2xbt06zJ07F9988w3mzJlj0ufs2rULjz/+OJYtW4YXXngBCgX/eiAiy+KfOkQ06i1cuBAuLi74+uuv+z22d+9edHV1YdGiRZYPdp5aWlowZcoUBAUFwcHB4ZzH7969G48//jhuv/12PPPMM6P+3ulEJAZLKRGNevb29rj66quxY8eOfo998cUXmD9/Pnx8fJCbm4t7770XU6dOxbhx43DJJZfgv//974DPGxsbi23btv3u7IcffsANN9yAxMREXHbZZXjllVegVqsHfE6tVov3338fCxcuxPjx47Fw4UJs2rQJAFBeXo7Y2FgAwKpVq0x6K8GePXvw6KOP4k9/+hOeeOKJcx5PRDRUWEqJiADceOONKCsrw8mTJ/Wzuro6HD58GDfffDO6urpw1113wd3dHZs3b8aOHTtwxRVXYN26dcjKyjqvr7l//3489NBDuOWWW7Bjxw48++yz+Oabb/D4448P+Dn//Oc/8cYbb+CBBx7A119/jVtvvRUvvvgi3n//fQQEBODgwYMA+krp559//rtf/9tvv8UjjzyCiRMn4pFHHjmv74GIaLCwlBIRAUhMTERMTIzBKfyvvvoKXl5emDdvHrq6unDHHXdg9erViIyMRHh4OFasWAEAyMnJOa+vuXHjRtxyyy1YunQpQkNDMWfOHDz//PPYvXs3ysvL+x3f3t6OTZs2YcWKFbj22msRHh6OO+64A8uXL8fbb78NhUIBHx8fAICLiws8PT0H/Np5eXl45JFHMH36dBw/fhx79+49r++BiGiw2IoOQERkLW688Ua89dZbWLVqFWxtbbF9+3YsXrwYNjY28PT0xPLly7Fjxw6cOXMGpaWlyM7OBoDz3ql+5swZpKenG7yiKcsyAKCgoADBwcEGxxcWFqK3txeTJ082mE+bNg0ffPABGhoa4O3tbdLXbmpqwuOPP467774bf/7zn/HUU09h3Lhx8Pf3P6/vhYjoQrGUEhH97LrrrsOGDRtw6NAh+Pj4IC8vD6+99hqAvlP5S5YsgaenJxYsWIA5c+Zg/PjxmD9/vsnPr9FoDD7W6XS4++67sXjx4n7Hnn3F89fOFtbfOluKbW1N/yN90qRJuPvuuwEA//jHP3DNNdfgsccewwcffAAbGxuTn4eIaLDw9D0R0c/OFs5du3Zh586dmDp1KsLCwgAAO3bsQHNzMzZt2oT7778fl112GVpaWgAMXBaVSiXa29v1H5eUlBg8Hh0djaKiIoSFhel/VVdX46WXXkJHR0e/54uMjIRSqURqaqrB/Pjx4/Dx8YGbm5vJ3+uvC6yPjw/WrFmDY8eO4Y033jD5OYiIBhNLKRHRr9x000344YcfsGfPHoNrk/r7+6Orqwu7d+9GZWUlDh48qN8cNNBu+YkTJ2LLli3IysrCmTNn8Nxzz8HOzk7/+J///Gfs2bMHr732GoqKipCSkoInn3wSbW1tRl8pdXZ2xpIlS/Dqq69ix44dKCkpwccff4xPPvkEd9111wVdyunyyy/H4sWL8eabb+LYsWPn/TxEROeLp++JiH5lzpw5cHR0RHNzs8FdnK644gqcPn0a//znP9He3o6goCDcfPPN2LdvHzIyMrBs2bJ+z/Xcc8/hueeewy233AJfX188+OCDqK6uNnjOf//733jrrbewceNGuLu7Y8GCBXjssccGzPfkk0/Cw8MDGzZsQH19PcLDw7F69WrccsstF/y9P/300/jpp5/w2GOP4csvv4S7u/sFPycRkakkeaDzTkREREREFsLT90REREQkHEspEREREQnHUkpEREREwrGUEhEREZFwLKVEREREJBxLKREREREJx1JKRERERMKxlBIRERGRcCylRERERCQcSykRERERCcdSSkRERETC/f/iacgZTm0mJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering(adarp_all_grouped2, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b43901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "silhouette(adarp_all_grouped2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07c6ad3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster\n",
       "0          7\n",
       "2          2\n",
       "1          1\n",
       "3          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 4, max_iter = 500, random_state = 0)\n",
    "y = kmeans.fit_predict(adarp_all_grouped2)\n",
    "y = pd.DataFrame(y, columns=[\"Cluster\"])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81643117",
   "metadata": {},
   "source": [
    "####  Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a11f879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e7aa119880>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4, max_iter = 500, random_state = 0)\n",
    "model = kmeans.fit(adarp_all_grouped2)\n",
    "tsne = TSNE().fit_transform(adarp_all_grouped2)\n",
    "plt.scatter(x = tsne[:, 0], y = tsne[:, 1], c=model.labels_, cmap='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ba11737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Cluster\n",
       "0    0        0\n",
       "1    1        1\n",
       "2    2        0\n",
       "3    3        0\n",
       "4    4        4\n",
       "5    5        0\n",
       "6    6        2\n",
       "7    7        3\n",
       "8    8        0\n",
       "9    9        0\n",
       "10  10        1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = pd.concat([ids, y], axis=1)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f089b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EDA_0</th>\n",
       "      <th>EDA_1</th>\n",
       "      <th>EDA_2</th>\n",
       "      <th>EDA_3</th>\n",
       "      <th>EDA_4</th>\n",
       "      <th>EDA_5</th>\n",
       "      <th>EDA_6</th>\n",
       "      <th>EDA_7</th>\n",
       "      <th>EDA_8</th>\n",
       "      <th>...</th>\n",
       "      <th>BVP_3832</th>\n",
       "      <th>BVP_3833</th>\n",
       "      <th>BVP_3834</th>\n",
       "      <th>BVP_3835</th>\n",
       "      <th>BVP_3836</th>\n",
       "      <th>BVP_3837</th>\n",
       "      <th>BVP_3838</th>\n",
       "      <th>BVP_3839</th>\n",
       "      <th>stress</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>279.693630</td>\n",
       "      <td>279.693630</td>\n",
       "      <td>277.263096</td>\n",
       "      <td>278.536233</td>\n",
       "      <td>277.263096</td>\n",
       "      <td>275.411261</td>\n",
       "      <td>274.253774</td>\n",
       "      <td>270.434365</td>\n",
       "      <td>275.642741</td>\n",
       "      <td>...</td>\n",
       "      <td>-66.219788</td>\n",
       "      <td>-58.931501</td>\n",
       "      <td>-49.297396</td>\n",
       "      <td>-38.197155</td>\n",
       "      <td>-26.658840</td>\n",
       "      <td>-15.717577</td>\n",
       "      <td>-6.359035</td>\n",
       "      <td>0.593630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>-96.859455</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-96.743625</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-95.932633</td>\n",
       "      <td>-95.469222</td>\n",
       "      <td>-95.353392</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>-94.889891</td>\n",
       "      <td>...</td>\n",
       "      <td>313.534227</td>\n",
       "      <td>308.846124</td>\n",
       "      <td>305.850260</td>\n",
       "      <td>303.458515</td>\n",
       "      <td>300.575702</td>\n",
       "      <td>296.011249</td>\n",
       "      <td>288.440334</td>\n",
       "      <td>276.393287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>24.728516</td>\n",
       "      <td>25.423045</td>\n",
       "      <td>24.149637</td>\n",
       "      <td>22.991969</td>\n",
       "      <td>24.959996</td>\n",
       "      <td>26.349233</td>\n",
       "      <td>25.770445</td>\n",
       "      <td>21.486901</td>\n",
       "      <td>19.403044</td>\n",
       "      <td>...</td>\n",
       "      <td>1.823065</td>\n",
       "      <td>-12.470880</td>\n",
       "      <td>-34.141434</td>\n",
       "      <td>-59.157604</td>\n",
       "      <td>-82.926676</td>\n",
       "      <td>-100.810005</td>\n",
       "      <td>-108.801331</td>\n",
       "      <td>-104.466513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>419.906014</td>\n",
       "      <td>398.356009</td>\n",
       "      <td>393.374048</td>\n",
       "      <td>332.315761</td>\n",
       "      <td>366.119795</td>\n",
       "      <td>360.442673</td>\n",
       "      <td>371.217676</td>\n",
       "      <td>389.639364</td>\n",
       "      <td>401.600156</td>\n",
       "      <td>...</td>\n",
       "      <td>107.038653</td>\n",
       "      <td>98.545662</td>\n",
       "      <td>87.632662</td>\n",
       "      <td>74.988562</td>\n",
       "      <td>60.662821</td>\n",
       "      <td>44.114913</td>\n",
       "      <td>25.055141</td>\n",
       "      <td>3.780268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>109.262338</td>\n",
       "      <td>107.989201</td>\n",
       "      <td>104.979970</td>\n",
       "      <td>103.012305</td>\n",
       "      <td>99.308544</td>\n",
       "      <td>97.340970</td>\n",
       "      <td>97.109491</td>\n",
       "      <td>98.266887</td>\n",
       "      <td>98.961325</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.339360</td>\n",
       "      <td>-16.282835</td>\n",
       "      <td>-16.590193</td>\n",
       "      <td>-17.151918</td>\n",
       "      <td>-17.766635</td>\n",
       "      <td>-18.261235</td>\n",
       "      <td>-18.550930</td>\n",
       "      <td>-18.600390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16352</th>\n",
       "      <td>5</td>\n",
       "      <td>-87.831367</td>\n",
       "      <td>-85.512658</td>\n",
       "      <td>-86.988140</td>\n",
       "      <td>-84.880238</td>\n",
       "      <td>-81.929273</td>\n",
       "      <td>-80.453790</td>\n",
       "      <td>-91.836201</td>\n",
       "      <td>-83.193948</td>\n",
       "      <td>-81.507824</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.504330</td>\n",
       "      <td>-58.658809</td>\n",
       "      <td>-84.355479</td>\n",
       "      <td>-110.043671</td>\n",
       "      <td>-136.744978</td>\n",
       "      <td>-165.633597</td>\n",
       "      <td>-196.654420</td>\n",
       "      <td>-221.490578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16353</th>\n",
       "      <td>5</td>\n",
       "      <td>-79.606117</td>\n",
       "      <td>-78.340947</td>\n",
       "      <td>-76.232550</td>\n",
       "      <td>-74.123989</td>\n",
       "      <td>-71.593814</td>\n",
       "      <td>-70.961229</td>\n",
       "      <td>-70.117837</td>\n",
       "      <td>-70.117837</td>\n",
       "      <td>-70.117837</td>\n",
       "      <td>...</td>\n",
       "      <td>26.235155</td>\n",
       "      <td>20.902523</td>\n",
       "      <td>12.140561</td>\n",
       "      <td>3.285341</td>\n",
       "      <td>-1.559808</td>\n",
       "      <td>-0.432241</td>\n",
       "      <td>5.625255</td>\n",
       "      <td>12.941727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16354</th>\n",
       "      <td>5</td>\n",
       "      <td>-57.442094</td>\n",
       "      <td>-57.231122</td>\n",
       "      <td>-57.653065</td>\n",
       "      <td>-57.442094</td>\n",
       "      <td>-57.231122</td>\n",
       "      <td>-57.020151</td>\n",
       "      <td>-57.020151</td>\n",
       "      <td>-57.020151</td>\n",
       "      <td>-57.020151</td>\n",
       "      <td>...</td>\n",
       "      <td>10.478882</td>\n",
       "      <td>9.546308</td>\n",
       "      <td>8.834160</td>\n",
       "      <td>8.172880</td>\n",
       "      <td>7.439537</td>\n",
       "      <td>6.549352</td>\n",
       "      <td>5.544715</td>\n",
       "      <td>4.455298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16355</th>\n",
       "      <td>5</td>\n",
       "      <td>66.717804</td>\n",
       "      <td>64.820873</td>\n",
       "      <td>63.766839</td>\n",
       "      <td>66.717804</td>\n",
       "      <td>65.663935</td>\n",
       "      <td>64.820873</td>\n",
       "      <td>60.605068</td>\n",
       "      <td>38.683473</td>\n",
       "      <td>-81.885464</td>\n",
       "      <td>...</td>\n",
       "      <td>122.481095</td>\n",
       "      <td>132.408776</td>\n",
       "      <td>130.683513</td>\n",
       "      <td>114.414324</td>\n",
       "      <td>85.830912</td>\n",
       "      <td>51.016206</td>\n",
       "      <td>17.443519</td>\n",
       "      <td>-9.202681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16356</th>\n",
       "      <td>5</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.568335</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.779307</td>\n",
       "      <td>-63.357364</td>\n",
       "      <td>-63.990278</td>\n",
       "      <td>...</td>\n",
       "      <td>64.576689</td>\n",
       "      <td>65.407528</td>\n",
       "      <td>63.974754</td>\n",
       "      <td>60.731938</td>\n",
       "      <td>57.200871</td>\n",
       "      <td>55.051710</td>\n",
       "      <td>55.785053</td>\n",
       "      <td>59.782407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16357 rows  10083 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       EDA_0       EDA_1       EDA_2       EDA_3       EDA_4  \\\n",
       "0       8  279.693630  279.693630  277.263096  278.536233  277.263096   \n",
       "1       8  -96.859455  -96.743625  -96.743625  -95.932633  -95.932633   \n",
       "2       8   24.728516   25.423045   24.149637   22.991969   24.959996   \n",
       "3       8  419.906014  398.356009  393.374048  332.315761  366.119795   \n",
       "4       8  109.262338  107.989201  104.979970  103.012305   99.308544   \n",
       "...    ..         ...         ...         ...         ...         ...   \n",
       "16352   5  -87.831367  -85.512658  -86.988140  -84.880238  -81.929273   \n",
       "16353   5  -79.606117  -78.340947  -76.232550  -74.123989  -71.593814   \n",
       "16354   5  -57.442094  -57.231122  -57.653065  -57.442094  -57.231122   \n",
       "16355   5   66.717804   64.820873   63.766839   66.717804   65.663935   \n",
       "16356   5  -63.990278  -63.568335  -63.779307  -63.990278  -63.779307   \n",
       "\n",
       "            EDA_5       EDA_6       EDA_7       EDA_8  ...    BVP_3832  \\\n",
       "0      275.411261  274.253774  270.434365  275.642741  ...  -66.219788   \n",
       "1      -95.469222  -95.353392  -94.889891  -94.889891  ...  313.534227   \n",
       "2       26.349233   25.770445   21.486901   19.403044  ...    1.823065   \n",
       "3      360.442673  371.217676  389.639364  401.600156  ...  107.038653   \n",
       "4       97.340970   97.109491   98.266887   98.961325  ...  -16.339360   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "16352  -80.453790  -91.836201  -83.193948  -81.507824  ...  -32.504330   \n",
       "16353  -70.961229  -70.117837  -70.117837  -70.117837  ...   26.235155   \n",
       "16354  -57.020151  -57.020151  -57.020151  -57.020151  ...   10.478882   \n",
       "16355   64.820873   60.605068   38.683473  -81.885464  ...  122.481095   \n",
       "16356  -63.779307  -63.779307  -63.357364  -63.990278  ...   64.576689   \n",
       "\n",
       "         BVP_3833    BVP_3834    BVP_3835    BVP_3836    BVP_3837    BVP_3838  \\\n",
       "0      -58.931501  -49.297396  -38.197155  -26.658840  -15.717577   -6.359035   \n",
       "1      308.846124  305.850260  303.458515  300.575702  296.011249  288.440334   \n",
       "2      -12.470880  -34.141434  -59.157604  -82.926676 -100.810005 -108.801331   \n",
       "3       98.545662   87.632662   74.988562   60.662821   44.114913   25.055141   \n",
       "4      -16.282835  -16.590193  -17.151918  -17.766635  -18.261235  -18.550930   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "16352  -58.658809  -84.355479 -110.043671 -136.744978 -165.633597 -196.654420   \n",
       "16353   20.902523   12.140561    3.285341   -1.559808   -0.432241    5.625255   \n",
       "16354    9.546308    8.834160    8.172880    7.439537    6.549352    5.544715   \n",
       "16355  132.408776  130.683513  114.414324   85.830912   51.016206   17.443519   \n",
       "16356   65.407528   63.974754   60.731938   57.200871   55.051710   55.785053   \n",
       "\n",
       "         BVP_3839  stress  Cluster  \n",
       "0        0.593630     0.0        0  \n",
       "1      276.393287     0.0        0  \n",
       "2     -104.466513     0.0        0  \n",
       "3        3.780268     0.0        0  \n",
       "4      -18.600390     0.0        0  \n",
       "...           ...     ...      ...  \n",
       "16352 -221.490578     0.0        0  \n",
       "16353   12.941727     0.0        0  \n",
       "16354    4.455298     0.0        0  \n",
       "16355   -9.202681     0.0        0  \n",
       "16356   59.782407     0.0        0  \n",
       "\n",
       "[16357 rows x 10083 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adarp_grouped_all = pd.merge(adarp_dataset, clusters, on = \"id\")\n",
    "adarp_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5de8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_grouped_all.to_csv(\"Final_CSVs/adarp_clusters_4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "478244e4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['dataset'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop 'dataset' to run pycaret tests based on \"Cluster\".\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m adarp_grouped_all \u001b[38;5;241m=\u001b[39m adarp_grouped_all\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m adarp_grouped_all\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\env_python8\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\env_python8\\lib\\site-packages\\pandas\\core\\frame.py:4957\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4809\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   4810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   4811\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4818\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4819\u001b[0m ):\n\u001b[0;32m   4820\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4821\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4822\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4955\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4956\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4963\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4964\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\env_python8\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\env_python8\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\env_python8\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6661\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6661\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6662\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['dataset'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop 'dataset' to run pycaret tests based on \"Cluster\".\n",
    "\n",
    "adarp_grouped_all = adarp_grouped_all.drop('dataset', axis = 1)\n",
    "adarp_grouped_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6da72364",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_participants = adarp_grouped_all[\"Cluster\"].unique()\n",
    "adarp_group = adarp_grouped_all.groupby('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88a9c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression     0.681  0.4992  0.2689  0.0355  0.0626 -0.0093   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr -0.0149     1.738  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier     0.735  0.5650  0.3412  0.0542  0.0936  0.0256   \n",
       "lr      Logistic Regression     0.681  0.4992  0.2689  0.0355  0.0626 -0.0093   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.0416     0.627  \n",
       "lr  -0.0149     1.738  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542  0.0936  0.0256   \n",
       "nb              Naive Bayes    0.5006  0.4809  0.4726  0.0386  0.0713 -0.0038   \n",
       "lr      Logistic Regression    0.6810  0.4992  0.2689  0.0355  0.0626 -0.0093   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.0416     0.627  \n",
       "nb  -0.0101     0.077  \n",
       "lr  -0.0149     1.738  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542  0.0936   \n",
       "dt   Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487  0.0775   \n",
       "nb                Naive Bayes    0.5006  0.4809  0.4726  0.0386  0.0713   \n",
       "lr        Logistic Regression    0.6810  0.4992  0.2689  0.0355  0.0626   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.0256  0.0416     0.627  \n",
       "dt   0.0140  0.0180     0.517  \n",
       "nb  -0.0038 -0.0101     0.077  \n",
       "lr  -0.0093 -0.0149     1.738  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542  0.0936   \n",
       "dt   Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487  0.0775   \n",
       "svm       SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416  0.0769   \n",
       "nb                Naive Bayes    0.5006  0.4809  0.4726  0.0386  0.0713   \n",
       "lr        Logistic Regression    0.6810  0.4992  0.2689  0.0355  0.0626   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.0256  0.0416     0.627  \n",
       "dt   0.0140  0.0180     0.517  \n",
       "svm  0.0022  0.0050     0.304  \n",
       "nb  -0.0038 -0.0101     0.077  \n",
       "lr  -0.0093 -0.0149     1.738  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542  0.0936   \n",
       "dt     Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487  0.0775   \n",
       "svm         SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416  0.0769   \n",
       "nb                  Naive Bayes    0.5006  0.4809  0.4726  0.0386  0.0713   \n",
       "ridge          Ridge Classifier    0.6796  0.0000  0.3000  0.0393  0.0694   \n",
       "lr          Logistic Regression    0.6810  0.4992  0.2689  0.0355  0.0626   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.0256  0.0416     0.627  \n",
       "dt     0.0140  0.0180     0.517  \n",
       "svm    0.0022  0.0050     0.304  \n",
       "nb    -0.0038 -0.0101     0.077  \n",
       "ridge -0.0021 -0.0026     0.077  \n",
       "lr    -0.0093 -0.0149     1.738  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542  0.0936   \n",
       "dt     Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487  0.0775   \n",
       "svm         SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416  0.0769   \n",
       "nb                  Naive Bayes    0.5006  0.4809  0.4726  0.0386  0.0713   \n",
       "ridge          Ridge Classifier    0.6796  0.0000  0.3000  0.0393  0.0694   \n",
       "lr          Logistic Regression    0.6810  0.4992  0.2689  0.0355  0.0626   \n",
       "rf     Random Forest Classifier    0.9521  0.5198  0.0115  0.0733  0.0198   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.0256  0.0416     0.627  \n",
       "dt     0.0140  0.0180     0.517  \n",
       "svm    0.0022  0.0050     0.304  \n",
       "nb    -0.0038 -0.0101     0.077  \n",
       "ridge -0.0021 -0.0026     0.077  \n",
       "lr    -0.0093 -0.0149     1.738  \n",
       "rf     0.0064  0.0111     1.350  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542   \n",
       "dt            Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487   \n",
       "svm                SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416   \n",
       "nb                         Naive Bayes    0.5006  0.4809  0.4726  0.0386   \n",
       "ridge                 Ridge Classifier    0.6796  0.0000  0.3000  0.0393   \n",
       "lr                 Logistic Regression    0.6810  0.4992  0.2689  0.0355   \n",
       "qda    Quadratic Discriminant Analysis    0.8254  0.4825  0.1089  0.0303   \n",
       "rf            Random Forest Classifier    0.9521  0.5198  0.0115  0.0733   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0936  0.0256  0.0416     0.627  \n",
       "dt     0.0775  0.0140  0.0180     0.517  \n",
       "svm    0.0769  0.0022  0.0050     0.304  \n",
       "nb     0.0713 -0.0038 -0.0101     0.077  \n",
       "ridge  0.0694 -0.0021 -0.0026     0.077  \n",
       "lr     0.0626 -0.0093 -0.0149     1.738  \n",
       "qda    0.0473 -0.0167 -0.0205     0.136  \n",
       "rf     0.0198  0.0064  0.0111     1.350  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542   \n",
       "dt            Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487   \n",
       "svm                SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416   \n",
       "ada               Ada Boost Classifier    0.6988  0.5227  0.2948  0.0420   \n",
       "nb                         Naive Bayes    0.5006  0.4809  0.4726  0.0386   \n",
       "ridge                 Ridge Classifier    0.6796  0.0000  0.3000  0.0393   \n",
       "lr                 Logistic Regression    0.6810  0.4992  0.2689  0.0355   \n",
       "qda    Quadratic Discriminant Analysis    0.8254  0.4825  0.1089  0.0303   \n",
       "rf            Random Forest Classifier    0.9521  0.5198  0.0115  0.0733   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0936  0.0256  0.0416     0.627  \n",
       "dt     0.0775  0.0140  0.0180     0.517  \n",
       "svm    0.0769  0.0022  0.0050     0.304  \n",
       "ada    0.0735  0.0029  0.0047     1.713  \n",
       "nb     0.0713 -0.0038 -0.0101     0.077  \n",
       "ridge  0.0694 -0.0021 -0.0026     0.077  \n",
       "lr     0.0626 -0.0093 -0.0149     1.738  \n",
       "qda    0.0473 -0.0167 -0.0205     0.136  \n",
       "rf     0.0198  0.0064  0.0111     1.350  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>0.5234</td>\n",
       "      <td>0.1937</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>7.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542   \n",
       "gbc       Gradient Boosting Classifier    0.8241  0.5234  0.1937  0.0519   \n",
       "dt            Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487   \n",
       "svm                SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416   \n",
       "ada               Ada Boost Classifier    0.6988  0.5227  0.2948  0.0420   \n",
       "nb                         Naive Bayes    0.5006  0.4809  0.4726  0.0386   \n",
       "ridge                 Ridge Classifier    0.6796  0.0000  0.3000  0.0393   \n",
       "lr                 Logistic Regression    0.6810  0.4992  0.2689  0.0355   \n",
       "qda    Quadratic Discriminant Analysis    0.8254  0.4825  0.1089  0.0303   \n",
       "rf            Random Forest Classifier    0.9521  0.5198  0.0115  0.0733   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0936  0.0256  0.0416     0.627  \n",
       "gbc    0.0818  0.0193  0.0245     7.537  \n",
       "dt     0.0775  0.0140  0.0180     0.517  \n",
       "svm    0.0769  0.0022  0.0050     0.304  \n",
       "ada    0.0735  0.0029  0.0047     1.713  \n",
       "nb     0.0713 -0.0038 -0.0101     0.077  \n",
       "ridge  0.0694 -0.0021 -0.0026     0.077  \n",
       "lr     0.0626 -0.0093 -0.0149     1.738  \n",
       "qda    0.0473 -0.0167 -0.0205     0.136  \n",
       "rf     0.0198  0.0064  0.0111     1.350  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>0.5234</td>\n",
       "      <td>0.1937</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>7.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542   \n",
       "gbc       Gradient Boosting Classifier    0.8241  0.5234  0.1937  0.0519   \n",
       "dt            Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487   \n",
       "svm                SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416   \n",
       "ada               Ada Boost Classifier    0.6988  0.5227  0.2948  0.0420   \n",
       "nb                         Naive Bayes    0.5006  0.4809  0.4726  0.0386   \n",
       "ridge                 Ridge Classifier    0.6796  0.0000  0.3000  0.0393   \n",
       "lda       Linear Discriminant Analysis    0.6796  0.4974  0.3000  0.0393   \n",
       "lr                 Logistic Regression    0.6810  0.4992  0.2689  0.0355   \n",
       "qda    Quadratic Discriminant Analysis    0.8254  0.4825  0.1089  0.0303   \n",
       "rf            Random Forest Classifier    0.9521  0.5198  0.0115  0.0733   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0936  0.0256  0.0416     0.627  \n",
       "gbc    0.0818  0.0193  0.0245     7.537  \n",
       "dt     0.0775  0.0140  0.0180     0.517  \n",
       "svm    0.0769  0.0022  0.0050     0.304  \n",
       "ada    0.0735  0.0029  0.0047     1.713  \n",
       "nb     0.0713 -0.0038 -0.0101     0.077  \n",
       "ridge  0.0694 -0.0021 -0.0026     0.077  \n",
       "lda    0.0694 -0.0021 -0.0026     0.240  \n",
       "lr     0.0626 -0.0093 -0.0149     1.738  \n",
       "qda    0.0473 -0.0167 -0.0205     0.136  \n",
       "rf     0.0198  0.0064  0.0111     1.350  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>0.5234</td>\n",
       "      <td>0.1937</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>7.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.5113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0080</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542   \n",
       "gbc       Gradient Boosting Classifier    0.8241  0.5234  0.1937  0.0519   \n",
       "dt            Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487   \n",
       "svm                SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416   \n",
       "ada               Ada Boost Classifier    0.6988  0.5227  0.2948  0.0420   \n",
       "nb                         Naive Bayes    0.5006  0.4809  0.4726  0.0386   \n",
       "ridge                 Ridge Classifier    0.6796  0.0000  0.3000  0.0393   \n",
       "lda       Linear Discriminant Analysis    0.6796  0.4974  0.3000  0.0393   \n",
       "lr                 Logistic Regression    0.6810  0.4992  0.2689  0.0355   \n",
       "qda    Quadratic Discriminant Analysis    0.8254  0.4825  0.1089  0.0303   \n",
       "rf            Random Forest Classifier    0.9521  0.5198  0.0115  0.0733   \n",
       "et              Extra Trees Classifier    0.9575  0.5113  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0936  0.0256  0.0416     0.627  \n",
       "gbc    0.0818  0.0193  0.0245     7.537  \n",
       "dt     0.0775  0.0140  0.0180     0.517  \n",
       "svm    0.0769  0.0022  0.0050     0.304  \n",
       "ada    0.0735  0.0029  0.0047     1.713  \n",
       "nb     0.0713 -0.0038 -0.0101     0.077  \n",
       "ridge  0.0694 -0.0021 -0.0026     0.077  \n",
       "lda    0.0694 -0.0021 -0.0026     0.240  \n",
       "lr     0.0626 -0.0093 -0.0149     1.738  \n",
       "qda    0.0473 -0.0167 -0.0205     0.136  \n",
       "rf     0.0198  0.0064  0.0111     1.350  \n",
       "et     0.0000 -0.0038 -0.0080     0.488  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>0.5234</td>\n",
       "      <td>0.1937</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>7.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9336</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.5113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0080</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542   \n",
       "gbc          Gradient Boosting Classifier    0.8241  0.5234  0.1937  0.0519   \n",
       "dt               Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487   \n",
       "svm                   SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416   \n",
       "ada                  Ada Boost Classifier    0.6988  0.5227  0.2948  0.0420   \n",
       "nb                            Naive Bayes    0.5006  0.4809  0.4726  0.0386   \n",
       "ridge                    Ridge Classifier    0.6796  0.0000  0.3000  0.0393   \n",
       "lda          Linear Discriminant Analysis    0.6796  0.4974  0.3000  0.0393   \n",
       "lr                    Logistic Regression    0.6810  0.4992  0.2689  0.0355   \n",
       "qda       Quadratic Discriminant Analysis    0.8254  0.4825  0.1089  0.0303   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9336  0.5324  0.0271  0.0440   \n",
       "rf               Random Forest Classifier    0.9521  0.5198  0.0115  0.0733   \n",
       "et                 Extra Trees Classifier    0.9575  0.5113  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.0936  0.0256  0.0416     0.627  \n",
       "gbc       0.0818  0.0193  0.0245     7.537  \n",
       "dt        0.0775  0.0140  0.0180     0.517  \n",
       "svm       0.0769  0.0022  0.0050     0.304  \n",
       "ada       0.0735  0.0029  0.0047     1.713  \n",
       "nb        0.0713 -0.0038 -0.0101     0.077  \n",
       "ridge     0.0694 -0.0021 -0.0026     0.077  \n",
       "lda       0.0694 -0.0021 -0.0026     0.240  \n",
       "lr        0.0626 -0.0093 -0.0149     1.738  \n",
       "qda       0.0473 -0.0167 -0.0205     0.136  \n",
       "lightgbm  0.0328  0.0001  0.0007     0.952  \n",
       "rf        0.0198  0.0064  0.0111     1.350  \n",
       "et        0.0000 -0.0038 -0.0080     0.488  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>0.5234</td>\n",
       "      <td>0.1937</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>7.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9336</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.5113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0080</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542   \n",
       "gbc          Gradient Boosting Classifier    0.8241  0.5234  0.1937  0.0519   \n",
       "dt               Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487   \n",
       "svm                   SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416   \n",
       "ada                  Ada Boost Classifier    0.6988  0.5227  0.2948  0.0420   \n",
       "nb                            Naive Bayes    0.5006  0.4809  0.4726  0.0386   \n",
       "ridge                    Ridge Classifier    0.6796  0.0000  0.3000  0.0393   \n",
       "lda          Linear Discriminant Analysis    0.6796  0.4974  0.3000  0.0393   \n",
       "lr                    Logistic Regression    0.6810  0.4992  0.2689  0.0355   \n",
       "qda       Quadratic Discriminant Analysis    0.8254  0.4825  0.1089  0.0303   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9336  0.5324  0.0271  0.0440   \n",
       "rf               Random Forest Classifier    0.9521  0.5198  0.0115  0.0733   \n",
       "et                 Extra Trees Classifier    0.9575  0.5113  0.0000  0.0000   \n",
       "dummy                    Dummy Classifier    0.9595  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.0936  0.0256  0.0416     0.627  \n",
       "gbc       0.0818  0.0193  0.0245     7.537  \n",
       "dt        0.0775  0.0140  0.0180     0.517  \n",
       "svm       0.0769  0.0022  0.0050     0.304  \n",
       "ada       0.0735  0.0029  0.0047     1.713  \n",
       "nb        0.0713 -0.0038 -0.0101     0.077  \n",
       "ridge     0.0694 -0.0021 -0.0026     0.077  \n",
       "lda       0.0694 -0.0021 -0.0026     0.240  \n",
       "lr        0.0626 -0.0093 -0.0149     1.738  \n",
       "qda       0.0473 -0.0167 -0.0205     0.136  \n",
       "lightgbm  0.0328  0.0001  0.0007     0.952  \n",
       "rf        0.0198  0.0064  0.0111     1.350  \n",
       "et        0.0000 -0.0038 -0.0080     0.488  \n",
       "dummy     0.0000  0.0000  0.0000     0.040  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>0.5234</td>\n",
       "      <td>0.1937</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>7.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0149</td>\n",
       "      <td>1.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9336</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.5113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>-0.0080</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.7350  0.5650  0.3412  0.0542   \n",
       "gbc          Gradient Boosting Classifier    0.8241  0.5234  0.1937  0.0519   \n",
       "dt               Decision Tree Classifier    0.8164  0.5166  0.1903  0.0487   \n",
       "svm                   SVM - Linear Kernel    0.5013  0.0000  0.5120  0.0416   \n",
       "ada                  Ada Boost Classifier    0.6988  0.5227  0.2948  0.0420   \n",
       "nb                            Naive Bayes    0.5006  0.4809  0.4726  0.0386   \n",
       "ridge                    Ridge Classifier    0.6796  0.0000  0.3000  0.0393   \n",
       "lda          Linear Discriminant Analysis    0.6796  0.4974  0.3000  0.0393   \n",
       "lr                    Logistic Regression    0.6810  0.4992  0.2689  0.0355   \n",
       "qda       Quadratic Discriminant Analysis    0.8254  0.4825  0.1089  0.0303   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9336  0.5324  0.0271  0.0440   \n",
       "rf               Random Forest Classifier    0.9521  0.5198  0.0115  0.0733   \n",
       "et                 Extra Trees Classifier    0.9575  0.5113  0.0000  0.0000   \n",
       "dummy                    Dummy Classifier    0.9595  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.0936  0.0256  0.0416     0.627  \n",
       "gbc       0.0818  0.0193  0.0245     7.537  \n",
       "dt        0.0775  0.0140  0.0180     0.517  \n",
       "svm       0.0769  0.0022  0.0050     0.304  \n",
       "ada       0.0735  0.0029  0.0047     1.713  \n",
       "nb        0.0713 -0.0038 -0.0101     0.077  \n",
       "ridge     0.0694 -0.0021 -0.0026     0.077  \n",
       "lda       0.0694 -0.0021 -0.0026     0.240  \n",
       "lr        0.0626 -0.0093 -0.0149     1.738  \n",
       "qda       0.0473 -0.0167 -0.0205     0.136  \n",
       "lightgbm  0.0328  0.0001  0.0007     0.952  \n",
       "rf        0.0198  0.0064  0.0111     1.350  \n",
       "et        0.0000 -0.0038 -0.0080     0.488  \n",
       "dummy     0.0000  0.0000  0.0000     0.040  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Participant:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression     0.637  0.5454  0.4468  0.1648  0.2405  0.0645   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.0787     1.195  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110  0.3055  0.1460   \n",
       "lr      Logistic Regression    0.6370  0.5454  0.4468  0.1648  0.2405  0.0645   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.1760     0.345  \n",
       "lr   0.0787     1.195  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110  0.3055  0.1460   \n",
       "lr      Logistic Regression    0.6370  0.5454  0.4468  0.1648  0.2405  0.0645   \n",
       "nb              Naive Bayes    0.3569  0.4038  0.4329  0.0924  0.1513 -0.0789   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.1760     0.345  \n",
       "lr   0.0787     1.195  \n",
       "nb  -0.1564     0.037  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110  0.3055   \n",
       "dt   Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913  0.2409   \n",
       "lr        Logistic Regression    0.6370  0.5454  0.4468  0.1648  0.2405   \n",
       "nb                Naive Bayes    0.3569  0.4038  0.4329  0.0924  0.1513   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.1460  0.1760     0.345  \n",
       "dt   0.0938  0.0988     0.134  \n",
       "lr   0.0645  0.0787     1.195  \n",
       "nb  -0.0789 -0.1564     0.037  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110  0.3055   \n",
       "dt   Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913  0.2409   \n",
       "lr        Logistic Regression    0.6370  0.5454  0.4468  0.1648  0.2405   \n",
       "svm       SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361  0.2144   \n",
       "nb                Naive Bayes    0.3569  0.4038  0.4329  0.0924  0.1513   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.1460  0.1760     0.345  \n",
       "dt   0.0938  0.0988     0.134  \n",
       "lr   0.0645  0.0787     1.195  \n",
       "svm  0.0117  0.0063     0.071  \n",
       "nb  -0.0789 -0.1564     0.037  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110  0.3055   \n",
       "ridge          Ridge Classifier    0.6224  0.0000  0.4775  0.1657  0.2459   \n",
       "dt     Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913  0.2409   \n",
       "lr          Logistic Regression    0.6370  0.5454  0.4468  0.1648  0.2405   \n",
       "svm         SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361  0.2144   \n",
       "nb                  Naive Bayes    0.3569  0.4038  0.4329  0.0924  0.1513   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.1460  0.1760     0.345  \n",
       "ridge  0.0674  0.0842     0.035  \n",
       "dt     0.0938  0.0988     0.134  \n",
       "lr     0.0645  0.0787     1.195  \n",
       "svm    0.0117  0.0063     0.071  \n",
       "nb    -0.0789 -0.1564     0.037  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110  0.3055   \n",
       "ridge          Ridge Classifier    0.6224  0.0000  0.4775  0.1657  0.2459   \n",
       "dt     Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913  0.2409   \n",
       "lr          Logistic Regression    0.6370  0.5454  0.4468  0.1648  0.2405   \n",
       "svm         SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361  0.2144   \n",
       "rf     Random Forest Classifier    0.8085  0.6833  0.1855  0.2234  0.2008   \n",
       "nb                  Naive Bayes    0.3569  0.4038  0.4329  0.0924  0.1513   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.1460  0.1760     0.345  \n",
       "ridge  0.0674  0.0842     0.035  \n",
       "dt     0.0938  0.0988     0.134  \n",
       "lr     0.0645  0.0787     1.195  \n",
       "svm    0.0117  0.0063     0.071  \n",
       "rf     0.0935  0.0948     0.302  \n",
       "nb    -0.0789 -0.1564     0.037  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.1553</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110   \n",
       "ridge                 Ridge Classifier    0.6224  0.0000  0.4775  0.1657   \n",
       "dt            Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913   \n",
       "lr                 Logistic Regression    0.6370  0.5454  0.4468  0.1648   \n",
       "svm                SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361   \n",
       "rf            Random Forest Classifier    0.8085  0.6833  0.1855  0.2234   \n",
       "nb                         Naive Bayes    0.3569  0.4038  0.4329  0.0924   \n",
       "qda    Quadratic Discriminant Analysis    0.4442  0.3163  0.1854  0.0502   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.3055  0.1460  0.1760     0.345  \n",
       "ridge  0.2459  0.0674  0.0842     0.035  \n",
       "dt     0.2409  0.0938  0.0988     0.134  \n",
       "lr     0.2405  0.0645  0.0787     1.195  \n",
       "svm    0.2144  0.0117  0.0063     0.071  \n",
       "rf     0.2008  0.0935  0.0948     0.302  \n",
       "nb     0.1513 -0.0789 -0.1564     0.037  \n",
       "qda    0.0789 -0.1553 -0.2230     0.049  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>0.2065</td>\n",
       "      <td>0.2801</td>\n",
       "      <td>0.1277</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.1553</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110   \n",
       "ada               Ada Boost Classifier    0.7119  0.6819  0.4363  0.2065   \n",
       "ridge                 Ridge Classifier    0.6224  0.0000  0.4775  0.1657   \n",
       "dt            Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913   \n",
       "lr                 Logistic Regression    0.6370  0.5454  0.4468  0.1648   \n",
       "svm                SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361   \n",
       "rf            Random Forest Classifier    0.8085  0.6833  0.1855  0.2234   \n",
       "nb                         Naive Bayes    0.3569  0.4038  0.4329  0.0924   \n",
       "qda    Quadratic Discriminant Analysis    0.4442  0.3163  0.1854  0.0502   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.3055  0.1460  0.1760     0.345  \n",
       "ada    0.2801  0.1277  0.1420     0.500  \n",
       "ridge  0.2459  0.0674  0.0842     0.035  \n",
       "dt     0.2409  0.0938  0.0988     0.134  \n",
       "lr     0.2405  0.0645  0.0787     1.195  \n",
       "svm    0.2144  0.0117  0.0063     0.071  \n",
       "rf     0.2008  0.0935  0.0948     0.302  \n",
       "nb     0.1513 -0.0789 -0.1564     0.037  \n",
       "qda    0.0789 -0.1553 -0.2230     0.049  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>0.2065</td>\n",
       "      <td>0.2801</td>\n",
       "      <td>0.1277</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>2.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.1553</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110   \n",
       "ada               Ada Boost Classifier    0.7119  0.6819  0.4363  0.2065   \n",
       "gbc       Gradient Boosting Classifier    0.7331  0.6893  0.3506  0.1995   \n",
       "ridge                 Ridge Classifier    0.6224  0.0000  0.4775  0.1657   \n",
       "dt            Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913   \n",
       "lr                 Logistic Regression    0.6370  0.5454  0.4468  0.1648   \n",
       "svm                SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361   \n",
       "rf            Random Forest Classifier    0.8085  0.6833  0.1855  0.2234   \n",
       "nb                         Naive Bayes    0.3569  0.4038  0.4329  0.0924   \n",
       "qda    Quadratic Discriminant Analysis    0.4442  0.3163  0.1854  0.0502   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.3055  0.1460  0.1760     0.345  \n",
       "ada    0.2801  0.1277  0.1420     0.500  \n",
       "gbc    0.2535  0.1065  0.1129     2.194  \n",
       "ridge  0.2459  0.0674  0.0842     0.035  \n",
       "dt     0.2409  0.0938  0.0988     0.134  \n",
       "lr     0.2405  0.0645  0.0787     1.195  \n",
       "svm    0.2144  0.0117  0.0063     0.071  \n",
       "rf     0.2008  0.0935  0.0948     0.302  \n",
       "nb     0.1513 -0.0789 -0.1564     0.037  \n",
       "qda    0.0789 -0.1553 -0.2230     0.049  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>0.2065</td>\n",
       "      <td>0.2801</td>\n",
       "      <td>0.1277</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>2.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.5721</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.1553</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110   \n",
       "ada               Ada Boost Classifier    0.7119  0.6819  0.4363  0.2065   \n",
       "gbc       Gradient Boosting Classifier    0.7331  0.6893  0.3506  0.1995   \n",
       "ridge                 Ridge Classifier    0.6224  0.0000  0.4775  0.1657   \n",
       "lda       Linear Discriminant Analysis    0.6224  0.5721  0.4775  0.1657   \n",
       "dt            Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913   \n",
       "lr                 Logistic Regression    0.6370  0.5454  0.4468  0.1648   \n",
       "svm                SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361   \n",
       "rf            Random Forest Classifier    0.8085  0.6833  0.1855  0.2234   \n",
       "nb                         Naive Bayes    0.3569  0.4038  0.4329  0.0924   \n",
       "qda    Quadratic Discriminant Analysis    0.4442  0.3163  0.1854  0.0502   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.3055  0.1460  0.1760     0.345  \n",
       "ada    0.2801  0.1277  0.1420     0.500  \n",
       "gbc    0.2535  0.1065  0.1129     2.194  \n",
       "ridge  0.2459  0.0674  0.0842     0.035  \n",
       "lda    0.2459  0.0674  0.0842     0.058  \n",
       "dt     0.2409  0.0938  0.0988     0.134  \n",
       "lr     0.2405  0.0645  0.0787     1.195  \n",
       "svm    0.2144  0.0117  0.0063     0.071  \n",
       "rf     0.2008  0.0935  0.0948     0.302  \n",
       "nb     0.1513 -0.0789 -0.1564     0.037  \n",
       "qda    0.0789 -0.1553 -0.2230     0.049  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>0.2065</td>\n",
       "      <td>0.2801</td>\n",
       "      <td>0.1277</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>2.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.5721</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.6912</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.1553</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110   \n",
       "ada               Ada Boost Classifier    0.7119  0.6819  0.4363  0.2065   \n",
       "gbc       Gradient Boosting Classifier    0.7331  0.6893  0.3506  0.1995   \n",
       "ridge                 Ridge Classifier    0.6224  0.0000  0.4775  0.1657   \n",
       "lda       Linear Discriminant Analysis    0.6224  0.5721  0.4775  0.1657   \n",
       "dt            Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913   \n",
       "lr                 Logistic Regression    0.6370  0.5454  0.4468  0.1648   \n",
       "svm                SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361   \n",
       "rf            Random Forest Classifier    0.8085  0.6833  0.1855  0.2234   \n",
       "et              Extra Trees Classifier    0.8214  0.6912  0.1374  0.2033   \n",
       "nb                         Naive Bayes    0.3569  0.4038  0.4329  0.0924   \n",
       "qda    Quadratic Discriminant Analysis    0.4442  0.3163  0.1854  0.0502   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.3055  0.1460  0.1760     0.345  \n",
       "ada    0.2801  0.1277  0.1420     0.500  \n",
       "gbc    0.2535  0.1065  0.1129     2.194  \n",
       "ridge  0.2459  0.0674  0.0842     0.035  \n",
       "lda    0.2459  0.0674  0.0842     0.058  \n",
       "dt     0.2409  0.0938  0.0988     0.134  \n",
       "lr     0.2405  0.0645  0.0787     1.195  \n",
       "svm    0.2144  0.0117  0.0063     0.071  \n",
       "rf     0.2008  0.0935  0.0948     0.302  \n",
       "et     0.1621  0.0684  0.0699     0.132  \n",
       "nb     0.1513 -0.0789 -0.1564     0.037  \n",
       "qda    0.0789 -0.1553 -0.2230     0.049  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>0.2065</td>\n",
       "      <td>0.2801</td>\n",
       "      <td>0.1277</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>2.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.5721</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.1754</td>\n",
       "      <td>0.2009</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.6912</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.1553</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110   \n",
       "ada                  Ada Boost Classifier    0.7119  0.6819  0.4363  0.2065   \n",
       "gbc          Gradient Boosting Classifier    0.7331  0.6893  0.3506  0.1995   \n",
       "ridge                    Ridge Classifier    0.6224  0.0000  0.4775  0.1657   \n",
       "lda          Linear Discriminant Analysis    0.6224  0.5721  0.4775  0.1657   \n",
       "dt               Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913   \n",
       "lr                    Logistic Regression    0.6370  0.5454  0.4468  0.1648   \n",
       "svm                   SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361   \n",
       "rf               Random Forest Classifier    0.8085  0.6833  0.1855  0.2234   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8018  0.6950  0.1754  0.2009   \n",
       "et                 Extra Trees Classifier    0.8214  0.6912  0.1374  0.2033   \n",
       "nb                            Naive Bayes    0.3569  0.4038  0.4329  0.0924   \n",
       "qda       Quadratic Discriminant Analysis    0.4442  0.3163  0.1854  0.0502   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.3055  0.1460  0.1760     0.345  \n",
       "ada       0.2801  0.1277  0.1420     0.500  \n",
       "gbc       0.2535  0.1065  0.1129     2.194  \n",
       "ridge     0.2459  0.0674  0.0842     0.035  \n",
       "lda       0.2459  0.0674  0.0842     0.058  \n",
       "dt        0.2409  0.0938  0.0988     0.134  \n",
       "lr        0.2405  0.0645  0.0787     1.195  \n",
       "svm       0.2144  0.0117  0.0063     0.071  \n",
       "rf        0.2008  0.0935  0.0948     0.302  \n",
       "lightgbm  0.1867  0.0747  0.0751     0.829  \n",
       "et        0.1621  0.0684  0.0699     0.132  \n",
       "nb        0.1513 -0.0789 -0.1564     0.037  \n",
       "qda       0.0789 -0.1553 -0.2230     0.049  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>0.2065</td>\n",
       "      <td>0.2801</td>\n",
       "      <td>0.1277</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>2.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.5721</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.1754</td>\n",
       "      <td>0.2009</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.6912</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.1553</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110   \n",
       "ada                  Ada Boost Classifier    0.7119  0.6819  0.4363  0.2065   \n",
       "gbc          Gradient Boosting Classifier    0.7331  0.6893  0.3506  0.1995   \n",
       "ridge                    Ridge Classifier    0.6224  0.0000  0.4775  0.1657   \n",
       "lda          Linear Discriminant Analysis    0.6224  0.5721  0.4775  0.1657   \n",
       "dt               Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913   \n",
       "lr                    Logistic Regression    0.6370  0.5454  0.4468  0.1648   \n",
       "svm                   SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361   \n",
       "rf               Random Forest Classifier    0.8085  0.6833  0.1855  0.2234   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8018  0.6950  0.1754  0.2009   \n",
       "et                 Extra Trees Classifier    0.8214  0.6912  0.1374  0.2033   \n",
       "nb                            Naive Bayes    0.3569  0.4038  0.4329  0.0924   \n",
       "qda       Quadratic Discriminant Analysis    0.4442  0.3163  0.1854  0.0502   \n",
       "dummy                    Dummy Classifier    0.8710  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.3055  0.1460  0.1760     0.345  \n",
       "ada       0.2801  0.1277  0.1420     0.500  \n",
       "gbc       0.2535  0.1065  0.1129     2.194  \n",
       "ridge     0.2459  0.0674  0.0842     0.035  \n",
       "lda       0.2459  0.0674  0.0842     0.058  \n",
       "dt        0.2409  0.0938  0.0988     0.134  \n",
       "lr        0.2405  0.0645  0.0787     1.195  \n",
       "svm       0.2144  0.0117  0.0063     0.071  \n",
       "rf        0.2008  0.0935  0.0948     0.302  \n",
       "lightgbm  0.1867  0.0747  0.0751     0.829  \n",
       "et        0.1621  0.0684  0.0699     0.132  \n",
       "nb        0.1513 -0.0789 -0.1564     0.037  \n",
       "qda       0.0789 -0.1553 -0.2230     0.049  \n",
       "dummy     0.0000  0.0000  0.0000     0.025  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>0.2065</td>\n",
       "      <td>0.2801</td>\n",
       "      <td>0.1277</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>2.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.5721</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.2409</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.4468</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2008</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.1754</td>\n",
       "      <td>0.2009</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.6912</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.3569</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.1553</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.6742  0.6920  0.5570  0.2110   \n",
       "ada                  Ada Boost Classifier    0.7119  0.6819  0.4363  0.2065   \n",
       "gbc          Gradient Boosting Classifier    0.7331  0.6893  0.3506  0.1995   \n",
       "ridge                    Ridge Classifier    0.6224  0.0000  0.4775  0.1657   \n",
       "lda          Linear Discriminant Analysis    0.6224  0.5721  0.4775  0.1657   \n",
       "dt               Decision Tree Classifier    0.7354  0.5610  0.3261  0.1913   \n",
       "lr                    Logistic Regression    0.6370  0.5454  0.4468  0.1648   \n",
       "svm                   SVM - Linear Kernel    0.4973  0.0000  0.5146  0.1361   \n",
       "rf               Random Forest Classifier    0.8085  0.6833  0.1855  0.2234   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8018  0.6950  0.1754  0.2009   \n",
       "et                 Extra Trees Classifier    0.8214  0.6912  0.1374  0.2033   \n",
       "nb                            Naive Bayes    0.3569  0.4038  0.4329  0.0924   \n",
       "qda       Quadratic Discriminant Analysis    0.4442  0.3163  0.1854  0.0502   \n",
       "dummy                    Dummy Classifier    0.8710  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.3055  0.1460  0.1760     0.345  \n",
       "ada       0.2801  0.1277  0.1420     0.500  \n",
       "gbc       0.2535  0.1065  0.1129     2.194  \n",
       "ridge     0.2459  0.0674  0.0842     0.035  \n",
       "lda       0.2459  0.0674  0.0842     0.058  \n",
       "dt        0.2409  0.0938  0.0988     0.134  \n",
       "lr        0.2405  0.0645  0.0787     1.195  \n",
       "svm       0.2144  0.0117  0.0063     0.071  \n",
       "rf        0.2008  0.0935  0.0948     0.302  \n",
       "lightgbm  0.1867  0.0747  0.0751     0.829  \n",
       "et        0.1621  0.0684  0.0699     0.132  \n",
       "nb        0.1513 -0.0789 -0.1564     0.037  \n",
       "qda       0.0789 -0.1553 -0.2230     0.049  \n",
       "dummy     0.0000  0.0000  0.0000     0.025  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Participant:  4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression    0.8232  0.5077  0.2667  0.0299  0.0538  0.0202   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.0364     0.896  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337  0.0605  0.0271   \n",
       "lr      Logistic Regression    0.8232  0.5077  0.2667  0.0299  0.0538  0.0202   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.0478     0.302  \n",
       "lr   0.0364     0.896  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337  0.0605  0.0271   \n",
       "lr      Logistic Regression    0.8232  0.5077  0.2667  0.0299  0.0538  0.0202   \n",
       "nb              Naive Bayes    0.5003  0.4548  0.5000  0.0194  0.0374  0.0005   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.0478     0.302  \n",
       "lr   0.0364     0.896  \n",
       "nb   0.0001     0.029  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337  0.0605   \n",
       "lr        Logistic Regression    0.8232  0.5077  0.2667  0.0299  0.0538   \n",
       "nb                Naive Bayes    0.5003  0.4548  0.5000  0.0194  0.0374   \n",
       "dt   Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167  0.0222   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.0271  0.0478     0.302  \n",
       "lr   0.0202  0.0364     0.896  \n",
       "nb   0.0005  0.0001     0.029  \n",
       "dt  -0.0060 -0.0090     0.104  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337  0.0605   \n",
       "lr        Logistic Regression    0.8232  0.5077  0.2667  0.0299  0.0538   \n",
       "nb                Naive Bayes    0.5003  0.4548  0.5000  0.0194  0.0374   \n",
       "svm       SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164  0.0318   \n",
       "dt   Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167  0.0222   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.0271  0.0478     0.302  \n",
       "lr   0.0202  0.0364     0.896  \n",
       "nb   0.0005  0.0001     0.029  \n",
       "svm -0.0054 -0.0226     0.049  \n",
       "dt  -0.0060 -0.0090     0.104  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337  0.0605   \n",
       "lr          Logistic Regression    0.8232  0.5077  0.2667  0.0299  0.0538   \n",
       "nb                  Naive Bayes    0.5003  0.4548  0.5000  0.0194  0.0374   \n",
       "ridge          Ridge Classifier    0.8335  0.0000  0.1667  0.0208  0.0370   \n",
       "svm         SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164  0.0318   \n",
       "dt     Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167  0.0222   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.0271  0.0478     0.302  \n",
       "lr     0.0202  0.0364     0.896  \n",
       "nb     0.0005  0.0001     0.029  \n",
       "ridge  0.0032  0.0052     0.029  \n",
       "svm   -0.0054 -0.0226     0.049  \n",
       "dt    -0.0060 -0.0090     0.104  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337  0.0605   \n",
       "lr          Logistic Regression    0.8232  0.5077  0.2667  0.0299  0.0538   \n",
       "nb                  Naive Bayes    0.5003  0.4548  0.5000  0.0194  0.0374   \n",
       "ridge          Ridge Classifier    0.8335  0.0000  0.1667  0.0208  0.0370   \n",
       "svm         SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164  0.0318   \n",
       "dt     Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167  0.0222   \n",
       "rf     Random Forest Classifier    0.9809  0.6102  0.0000  0.0000  0.0000   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.0271  0.0478     0.302  \n",
       "lr     0.0202  0.0364     0.896  \n",
       "nb     0.0005  0.0001     0.029  \n",
       "ridge  0.0032  0.0052     0.029  \n",
       "svm   -0.0054 -0.0226     0.049  \n",
       "dt    -0.0060 -0.0090     0.104  \n",
       "rf     0.0000  0.0000     0.229  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337   \n",
       "lr                 Logistic Regression    0.8232  0.5077  0.2667  0.0299   \n",
       "nb                         Naive Bayes    0.5003  0.4548  0.5000  0.0194   \n",
       "ridge                 Ridge Classifier    0.8335  0.0000  0.1667  0.0208   \n",
       "svm                SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164   \n",
       "dt            Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167   \n",
       "rf            Random Forest Classifier    0.9809  0.6102  0.0000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9809  0.5000  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0605  0.0271  0.0478     0.302  \n",
       "lr     0.0538  0.0202  0.0364     0.896  \n",
       "nb     0.0374  0.0005  0.0001     0.029  \n",
       "ridge  0.0370  0.0032  0.0052     0.029  \n",
       "svm    0.0318 -0.0054 -0.0226     0.049  \n",
       "dt     0.0222 -0.0060 -0.0090     0.104  \n",
       "rf     0.0000  0.0000  0.0000     0.229  \n",
       "qda    0.0000  0.0000  0.0000     0.041  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>0.4522</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337   \n",
       "lr                 Logistic Regression    0.8232  0.5077  0.2667  0.0299   \n",
       "nb                         Naive Bayes    0.5003  0.4548  0.5000  0.0194   \n",
       "ridge                 Ridge Classifier    0.8335  0.0000  0.1667  0.0208   \n",
       "svm                SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164   \n",
       "dt            Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167   \n",
       "ada               Ada Boost Classifier    0.9381  0.4522  0.0333  0.0111   \n",
       "rf            Random Forest Classifier    0.9809  0.6102  0.0000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9809  0.5000  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0605  0.0271  0.0478     0.302  \n",
       "lr     0.0538  0.0202  0.0364     0.896  \n",
       "nb     0.0374  0.0005  0.0001     0.029  \n",
       "ridge  0.0370  0.0032  0.0052     0.029  \n",
       "svm    0.0318 -0.0054 -0.0226     0.049  \n",
       "dt     0.0222 -0.0060 -0.0090     0.104  \n",
       "ada    0.0167 -0.0090 -0.0093     0.388  \n",
       "rf     0.0000  0.0000  0.0000     0.229  \n",
       "qda    0.0000  0.0000  0.0000     0.041  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>0.4522</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>1.657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337   \n",
       "lr                 Logistic Regression    0.8232  0.5077  0.2667  0.0299   \n",
       "nb                         Naive Bayes    0.5003  0.4548  0.5000  0.0194   \n",
       "ridge                 Ridge Classifier    0.8335  0.0000  0.1667  0.0208   \n",
       "svm                SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164   \n",
       "dt            Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167   \n",
       "ada               Ada Boost Classifier    0.9381  0.4522  0.0333  0.0111   \n",
       "rf            Random Forest Classifier    0.9809  0.6102  0.0000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9809  0.5000  0.0000  0.0000   \n",
       "gbc       Gradient Boosting Classifier    0.9738  0.5634  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0605  0.0271  0.0478     0.302  \n",
       "lr     0.0538  0.0202  0.0364     0.896  \n",
       "nb     0.0374  0.0005  0.0001     0.029  \n",
       "ridge  0.0370  0.0032  0.0052     0.029  \n",
       "svm    0.0318 -0.0054 -0.0226     0.049  \n",
       "dt     0.0222 -0.0060 -0.0090     0.104  \n",
       "ada    0.0167 -0.0090 -0.0093     0.388  \n",
       "rf     0.0000  0.0000  0.0000     0.229  \n",
       "qda    0.0000  0.0000  0.0000     0.041  \n",
       "gbc    0.0000 -0.0093 -0.0103     1.657  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>0.4522</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>1.657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337   \n",
       "lr                 Logistic Regression    0.8232  0.5077  0.2667  0.0299   \n",
       "nb                         Naive Bayes    0.5003  0.4548  0.5000  0.0194   \n",
       "ridge                 Ridge Classifier    0.8335  0.0000  0.1667  0.0208   \n",
       "lda       Linear Discriminant Analysis    0.8335  0.5612  0.1667  0.0208   \n",
       "svm                SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164   \n",
       "dt            Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167   \n",
       "ada               Ada Boost Classifier    0.9381  0.4522  0.0333  0.0111   \n",
       "rf            Random Forest Classifier    0.9809  0.6102  0.0000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9809  0.5000  0.0000  0.0000   \n",
       "gbc       Gradient Boosting Classifier    0.9738  0.5634  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0605  0.0271  0.0478     0.302  \n",
       "lr     0.0538  0.0202  0.0364     0.896  \n",
       "nb     0.0374  0.0005  0.0001     0.029  \n",
       "ridge  0.0370  0.0032  0.0052     0.029  \n",
       "lda    0.0370  0.0032  0.0052     0.049  \n",
       "svm    0.0318 -0.0054 -0.0226     0.049  \n",
       "dt     0.0222 -0.0060 -0.0090     0.104  \n",
       "ada    0.0167 -0.0090 -0.0093     0.388  \n",
       "rf     0.0000  0.0000  0.0000     0.229  \n",
       "qda    0.0000  0.0000  0.0000     0.041  \n",
       "gbc    0.0000 -0.0093 -0.0103     1.657  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>0.4522</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>1.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5982</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337   \n",
       "lr                 Logistic Regression    0.8232  0.5077  0.2667  0.0299   \n",
       "nb                         Naive Bayes    0.5003  0.4548  0.5000  0.0194   \n",
       "ridge                 Ridge Classifier    0.8335  0.0000  0.1667  0.0208   \n",
       "lda       Linear Discriminant Analysis    0.8335  0.5612  0.1667  0.0208   \n",
       "svm                SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164   \n",
       "dt            Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167   \n",
       "ada               Ada Boost Classifier    0.9381  0.4522  0.0333  0.0111   \n",
       "rf            Random Forest Classifier    0.9809  0.6102  0.0000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9809  0.5000  0.0000  0.0000   \n",
       "gbc       Gradient Boosting Classifier    0.9738  0.5634  0.0000  0.0000   \n",
       "et              Extra Trees Classifier    0.9809  0.5982  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.0605  0.0271  0.0478     0.302  \n",
       "lr     0.0538  0.0202  0.0364     0.896  \n",
       "nb     0.0374  0.0005  0.0001     0.029  \n",
       "ridge  0.0370  0.0032  0.0052     0.029  \n",
       "lda    0.0370  0.0032  0.0052     0.049  \n",
       "svm    0.0318 -0.0054 -0.0226     0.049  \n",
       "dt     0.0222 -0.0060 -0.0090     0.104  \n",
       "ada    0.0167 -0.0090 -0.0093     0.388  \n",
       "rf     0.0000  0.0000  0.0000     0.229  \n",
       "qda    0.0000  0.0000  0.0000     0.041  \n",
       "gbc    0.0000 -0.0093 -0.0103     1.657  \n",
       "et     0.0000  0.0000  0.0000     0.095  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>0.4522</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>1.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5982</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337   \n",
       "lr                    Logistic Regression    0.8232  0.5077  0.2667  0.0299   \n",
       "nb                            Naive Bayes    0.5003  0.4548  0.5000  0.0194   \n",
       "ridge                    Ridge Classifier    0.8335  0.0000  0.1667  0.0208   \n",
       "lda          Linear Discriminant Analysis    0.8335  0.5612  0.1667  0.0208   \n",
       "svm                   SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164   \n",
       "dt               Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167   \n",
       "ada                  Ada Boost Classifier    0.9381  0.4522  0.0333  0.0111   \n",
       "rf               Random Forest Classifier    0.9809  0.6102  0.0000  0.0000   \n",
       "qda       Quadratic Discriminant Analysis    0.9809  0.5000  0.0000  0.0000   \n",
       "gbc          Gradient Boosting Classifier    0.9738  0.5634  0.0000  0.0000   \n",
       "et                 Extra Trees Classifier    0.9809  0.5982  0.0000  0.0000   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9796  0.5679  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.0605  0.0271  0.0478     0.302  \n",
       "lr        0.0538  0.0202  0.0364     0.896  \n",
       "nb        0.0374  0.0005  0.0001     0.029  \n",
       "ridge     0.0370  0.0032  0.0052     0.029  \n",
       "lda       0.0370  0.0032  0.0052     0.049  \n",
       "svm       0.0318 -0.0054 -0.0226     0.049  \n",
       "dt        0.0222 -0.0060 -0.0090     0.104  \n",
       "ada       0.0167 -0.0090 -0.0093     0.388  \n",
       "rf        0.0000  0.0000  0.0000     0.229  \n",
       "qda       0.0000  0.0000  0.0000     0.041  \n",
       "gbc       0.0000 -0.0093 -0.0103     1.657  \n",
       "et        0.0000  0.0000  0.0000     0.095  \n",
       "lightgbm  0.0000 -0.0019 -0.0022     0.806  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>0.4522</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>1.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5982</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337   \n",
       "lr                    Logistic Regression    0.8232  0.5077  0.2667  0.0299   \n",
       "nb                            Naive Bayes    0.5003  0.4548  0.5000  0.0194   \n",
       "ridge                    Ridge Classifier    0.8335  0.0000  0.1667  0.0208   \n",
       "lda          Linear Discriminant Analysis    0.8335  0.5612  0.1667  0.0208   \n",
       "svm                   SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164   \n",
       "dt               Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167   \n",
       "ada                  Ada Boost Classifier    0.9381  0.4522  0.0333  0.0111   \n",
       "rf               Random Forest Classifier    0.9809  0.6102  0.0000  0.0000   \n",
       "qda       Quadratic Discriminant Analysis    0.9809  0.5000  0.0000  0.0000   \n",
       "gbc          Gradient Boosting Classifier    0.9738  0.5634  0.0000  0.0000   \n",
       "et                 Extra Trees Classifier    0.9809  0.5982  0.0000  0.0000   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9796  0.5679  0.0000  0.0000   \n",
       "dummy                    Dummy Classifier    0.9809  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.0605  0.0271  0.0478     0.302  \n",
       "lr        0.0538  0.0202  0.0364     0.896  \n",
       "nb        0.0374  0.0005  0.0001     0.029  \n",
       "ridge     0.0370  0.0032  0.0052     0.029  \n",
       "lda       0.0370  0.0032  0.0052     0.049  \n",
       "svm       0.0318 -0.0054 -0.0226     0.049  \n",
       "dt        0.0222 -0.0060 -0.0090     0.104  \n",
       "ada       0.0167 -0.0090 -0.0093     0.388  \n",
       "rf        0.0000  0.0000  0.0000     0.229  \n",
       "qda       0.0000  0.0000  0.0000     0.041  \n",
       "gbc       0.0000 -0.0093 -0.0103     1.657  \n",
       "et        0.0000  0.0000  0.0000     0.095  \n",
       "lightgbm  0.0000 -0.0019 -0.0022     0.806  \n",
       "dummy     0.0000  0.0000  0.0000     0.022  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.4893</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>0.4522</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>1.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5982</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.8200  0.5871  0.3000  0.0337   \n",
       "lr                    Logistic Regression    0.8232  0.5077  0.2667  0.0299   \n",
       "nb                            Naive Bayes    0.5003  0.4548  0.5000  0.0194   \n",
       "ridge                    Ridge Classifier    0.8335  0.0000  0.1667  0.0208   \n",
       "lda          Linear Discriminant Analysis    0.8335  0.5612  0.1667  0.0208   \n",
       "svm                   SVM - Linear Kernel    0.4199  0.0000  0.5000  0.0164   \n",
       "dt               Decision Tree Classifier    0.9279  0.4893  0.0333  0.0167   \n",
       "ada                  Ada Boost Classifier    0.9381  0.4522  0.0333  0.0111   \n",
       "rf               Random Forest Classifier    0.9809  0.6102  0.0000  0.0000   \n",
       "qda       Quadratic Discriminant Analysis    0.9809  0.5000  0.0000  0.0000   \n",
       "gbc          Gradient Boosting Classifier    0.9738  0.5634  0.0000  0.0000   \n",
       "et                 Extra Trees Classifier    0.9809  0.5982  0.0000  0.0000   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9796  0.5679  0.0000  0.0000   \n",
       "dummy                    Dummy Classifier    0.9809  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.0605  0.0271  0.0478     0.302  \n",
       "lr        0.0538  0.0202  0.0364     0.896  \n",
       "nb        0.0374  0.0005  0.0001     0.029  \n",
       "ridge     0.0370  0.0032  0.0052     0.029  \n",
       "lda       0.0370  0.0032  0.0052     0.049  \n",
       "svm       0.0318 -0.0054 -0.0226     0.049  \n",
       "dt        0.0222 -0.0060 -0.0090     0.104  \n",
       "ada       0.0167 -0.0090 -0.0093     0.388  \n",
       "rf        0.0000  0.0000  0.0000     0.229  \n",
       "qda       0.0000  0.0000  0.0000     0.041  \n",
       "gbc       0.0000 -0.0093 -0.0103     1.657  \n",
       "et        0.0000  0.0000  0.0000     0.095  \n",
       "lightgbm  0.0000 -0.0019 -0.0022     0.806  \n",
       "dummy     0.0000  0.0000  0.0000     0.022  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Participant:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.     F1   Kappa  \\\n",
       "lr  Logistic Regression    0.6423  0.5079    0.31  0.0759  0.121  0.0052   \n",
       "\n",
       "      MCC  TT (Sec)  \n",
       "lr -0.005     0.603  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.6571  0.6021   0.535  0.1094  0.1812  0.0717   \n",
       "lr      Logistic Regression    0.6423  0.5079   0.310  0.0759  0.1210  0.0052   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.1095     0.359  \n",
       "lr  -0.0050     0.603  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.6571  0.6021   0.535  0.1094  0.1812  0.0717   \n",
       "lr      Logistic Regression    0.6423  0.5079   0.310  0.0759  0.1210  0.0052   \n",
       "nb              Naive Bayes    0.5323  0.4442   0.440  0.0658  0.1133 -0.0099   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.1095     0.359  \n",
       "lr  -0.0050     0.603  \n",
       "nb  -0.0101     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.6571  0.6021   0.535  0.1094  0.1812   \n",
       "dt   Decision Tree Classifier    0.7994  0.5366   0.230  0.0880  0.1269   \n",
       "lr        Logistic Regression    0.6423  0.5079   0.310  0.0759  0.1210   \n",
       "nb                Naive Bayes    0.5323  0.4442   0.440  0.0658  0.1133   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.0717  0.1095     0.359  \n",
       "dt   0.0336  0.0423     0.026  \n",
       "lr   0.0052 -0.0050     0.603  \n",
       "nb  -0.0099 -0.0101     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.6571  0.6021   0.535  0.1094  0.1812   \n",
       "svm       SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788  0.1410   \n",
       "dt   Decision Tree Classifier    0.7994  0.5366   0.230  0.0880  0.1269   \n",
       "lr        Logistic Regression    0.6423  0.5079   0.310  0.0759  0.1210   \n",
       "nb                Naive Bayes    0.5323  0.4442   0.440  0.0658  0.1133   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.0717  0.1095     0.359  \n",
       "svm  0.0157  0.0329     0.013  \n",
       "dt   0.0336  0.0423     0.026  \n",
       "lr   0.0052 -0.0050     0.603  \n",
       "nb  -0.0099 -0.0101     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.6571  0.6021   0.535  0.1094  0.1812   \n",
       "svm         SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788  0.1410   \n",
       "dt     Decision Tree Classifier    0.7994  0.5366   0.230  0.0880  0.1269   \n",
       "lr          Logistic Regression    0.6423  0.5079   0.310  0.0759  0.1210   \n",
       "nb                  Naive Bayes    0.5323  0.4442   0.440  0.0658  0.1133   \n",
       "ridge          Ridge Classifier    0.6617  0.0000   0.200  0.0499  0.0788   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.0717  0.1095     0.359  \n",
       "svm    0.0157  0.0329     0.013  \n",
       "dt     0.0336  0.0423     0.026  \n",
       "lr     0.0052 -0.0050     0.603  \n",
       "nb    -0.0099 -0.0101     0.011  \n",
       "ridge -0.0395 -0.0568     0.011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.6571  0.6021   0.535  0.1094  0.1812   \n",
       "svm         SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788  0.1410   \n",
       "dt     Decision Tree Classifier    0.7994  0.5366   0.230  0.0880  0.1269   \n",
       "lr          Logistic Regression    0.6423  0.5079   0.310  0.0759  0.1210   \n",
       "nb                  Naive Bayes    0.5323  0.4442   0.440  0.0658  0.1133   \n",
       "ridge          Ridge Classifier    0.6617  0.0000   0.200  0.0499  0.0788   \n",
       "rf     Random Forest Classifier    0.9256  0.5568   0.000  0.0000  0.0000   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.0717  0.1095     0.359  \n",
       "svm    0.0157  0.0329     0.013  \n",
       "dt     0.0336  0.0423     0.026  \n",
       "lr     0.0052 -0.0050     0.603  \n",
       "nb    -0.0099 -0.0101     0.011  \n",
       "ridge -0.0395 -0.0568     0.011  \n",
       "rf    -0.0055 -0.0076     0.091  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6571  0.6021   0.535  0.1094   \n",
       "svm                SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788   \n",
       "dt            Decision Tree Classifier    0.7994  0.5366   0.230  0.0880   \n",
       "lr                 Logistic Regression    0.6423  0.5079   0.310  0.0759   \n",
       "nb                         Naive Bayes    0.5323  0.4442   0.440  0.0658   \n",
       "ridge                 Ridge Classifier    0.6617  0.0000   0.200  0.0499   \n",
       "rf            Random Forest Classifier    0.9256  0.5568   0.000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9288  0.5000   0.000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.1812  0.0717  0.1095     0.359  \n",
       "svm    0.1410  0.0157  0.0329     0.013  \n",
       "dt     0.1269  0.0336  0.0423     0.026  \n",
       "lr     0.1210  0.0052 -0.0050     0.603  \n",
       "nb     0.1133 -0.0099 -0.0101     0.011  \n",
       "ridge  0.0788 -0.0395 -0.0568     0.011  \n",
       "rf     0.0000 -0.0055 -0.0076     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.4537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0931</td>\n",
       "      <td>-0.0974</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6571  0.6021   0.535  0.1094   \n",
       "svm                SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788   \n",
       "dt            Decision Tree Classifier    0.7994  0.5366   0.230  0.0880   \n",
       "lr                 Logistic Regression    0.6423  0.5079   0.310  0.0759   \n",
       "nb                         Naive Bayes    0.5323  0.4442   0.440  0.0658   \n",
       "ridge                 Ridge Classifier    0.6617  0.0000   0.200  0.0499   \n",
       "rf            Random Forest Classifier    0.9256  0.5568   0.000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9288  0.5000   0.000  0.0000   \n",
       "ada               Ada Boost Classifier    0.8172  0.4537   0.000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.1812  0.0717  0.1095     0.359  \n",
       "svm    0.1410  0.0157  0.0329     0.013  \n",
       "dt     0.1269  0.0336  0.0423     0.026  \n",
       "lr     0.1210  0.0052 -0.0050     0.603  \n",
       "nb     0.1133 -0.0099 -0.0101     0.011  \n",
       "ridge  0.0788 -0.0395 -0.0568     0.011  \n",
       "rf     0.0000 -0.0055 -0.0076     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  \n",
       "ada    0.0000 -0.0931 -0.0974     0.136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.4537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0931</td>\n",
       "      <td>-0.0974</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.5313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0345</td>\n",
       "      <td>-0.0377</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6571  0.6021   0.535  0.1094   \n",
       "svm                SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788   \n",
       "dt            Decision Tree Classifier    0.7994  0.5366   0.230  0.0880   \n",
       "lr                 Logistic Regression    0.6423  0.5079   0.310  0.0759   \n",
       "nb                         Naive Bayes    0.5323  0.4442   0.440  0.0658   \n",
       "ridge                 Ridge Classifier    0.6617  0.0000   0.200  0.0499   \n",
       "rf            Random Forest Classifier    0.9256  0.5568   0.000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9288  0.5000   0.000  0.0000   \n",
       "ada               Ada Boost Classifier    0.8172  0.4537   0.000  0.0000   \n",
       "gbc       Gradient Boosting Classifier    0.9029  0.5313   0.000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.1812  0.0717  0.1095     0.359  \n",
       "svm    0.1410  0.0157  0.0329     0.013  \n",
       "dt     0.1269  0.0336  0.0423     0.026  \n",
       "lr     0.1210  0.0052 -0.0050     0.603  \n",
       "nb     0.1133 -0.0099 -0.0101     0.011  \n",
       "ridge  0.0788 -0.0395 -0.0568     0.011  \n",
       "rf     0.0000 -0.0055 -0.0076     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  \n",
       "ada    0.0000 -0.0931 -0.0974     0.136  \n",
       "gbc    0.0000 -0.0345 -0.0377     0.549  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.4551</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.4537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0931</td>\n",
       "      <td>-0.0974</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.5313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0345</td>\n",
       "      <td>-0.0377</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6571  0.6021   0.535  0.1094   \n",
       "svm                SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788   \n",
       "dt            Decision Tree Classifier    0.7994  0.5366   0.230  0.0880   \n",
       "lr                 Logistic Regression    0.6423  0.5079   0.310  0.0759   \n",
       "nb                         Naive Bayes    0.5323  0.4442   0.440  0.0658   \n",
       "ridge                 Ridge Classifier    0.6617  0.0000   0.200  0.0499   \n",
       "lda       Linear Discriminant Analysis    0.6617  0.4551   0.200  0.0499   \n",
       "rf            Random Forest Classifier    0.9256  0.5568   0.000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9288  0.5000   0.000  0.0000   \n",
       "ada               Ada Boost Classifier    0.8172  0.4537   0.000  0.0000   \n",
       "gbc       Gradient Boosting Classifier    0.9029  0.5313   0.000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.1812  0.0717  0.1095     0.359  \n",
       "svm    0.1410  0.0157  0.0329     0.013  \n",
       "dt     0.1269  0.0336  0.0423     0.026  \n",
       "lr     0.1210  0.0052 -0.0050     0.603  \n",
       "nb     0.1133 -0.0099 -0.0101     0.011  \n",
       "ridge  0.0788 -0.0395 -0.0568     0.011  \n",
       "lda    0.0788 -0.0395 -0.0568     0.074  \n",
       "rf     0.0000 -0.0055 -0.0076     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  \n",
       "ada    0.0000 -0.0931 -0.0974     0.136  \n",
       "gbc    0.0000 -0.0345 -0.0377     0.549  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.4551</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.4537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0931</td>\n",
       "      <td>-0.0974</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.5313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0345</td>\n",
       "      <td>-0.0377</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>0.5531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0028</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.6571  0.6021   0.535  0.1094   \n",
       "svm                SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788   \n",
       "dt            Decision Tree Classifier    0.7994  0.5366   0.230  0.0880   \n",
       "lr                 Logistic Regression    0.6423  0.5079   0.310  0.0759   \n",
       "nb                         Naive Bayes    0.5323  0.4442   0.440  0.0658   \n",
       "ridge                 Ridge Classifier    0.6617  0.0000   0.200  0.0499   \n",
       "lda       Linear Discriminant Analysis    0.6617  0.4551   0.200  0.0499   \n",
       "rf            Random Forest Classifier    0.9256  0.5568   0.000  0.0000   \n",
       "qda    Quadratic Discriminant Analysis    0.9288  0.5000   0.000  0.0000   \n",
       "ada               Ada Boost Classifier    0.8172  0.4537   0.000  0.0000   \n",
       "gbc       Gradient Boosting Classifier    0.9029  0.5313   0.000  0.0000   \n",
       "et              Extra Trees Classifier    0.9272  0.5531   0.000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.1812  0.0717  0.1095     0.359  \n",
       "svm    0.1410  0.0157  0.0329     0.013  \n",
       "dt     0.1269  0.0336  0.0423     0.026  \n",
       "lr     0.1210  0.0052 -0.0050     0.603  \n",
       "nb     0.1133 -0.0099 -0.0101     0.011  \n",
       "ridge  0.0788 -0.0395 -0.0568     0.011  \n",
       "lda    0.0788 -0.0395 -0.0568     0.074  \n",
       "rf     0.0000 -0.0055 -0.0076     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  \n",
       "ada    0.0000 -0.0931 -0.0974     0.136  \n",
       "gbc    0.0000 -0.0345 -0.0377     0.549  \n",
       "et     0.0000 -0.0028 -0.0038     0.054  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.4551</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.4537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0931</td>\n",
       "      <td>-0.0974</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.5313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0345</td>\n",
       "      <td>-0.0377</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>0.5531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0028</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9208</td>\n",
       "      <td>0.4859</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>-0.0138</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.6571  0.6021   0.535  0.1094   \n",
       "svm                   SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788   \n",
       "dt               Decision Tree Classifier    0.7994  0.5366   0.230  0.0880   \n",
       "lr                    Logistic Regression    0.6423  0.5079   0.310  0.0759   \n",
       "nb                            Naive Bayes    0.5323  0.4442   0.440  0.0658   \n",
       "ridge                    Ridge Classifier    0.6617  0.0000   0.200  0.0499   \n",
       "lda          Linear Discriminant Analysis    0.6617  0.4551   0.200  0.0499   \n",
       "rf               Random Forest Classifier    0.9256  0.5568   0.000  0.0000   \n",
       "qda       Quadratic Discriminant Analysis    0.9288  0.5000   0.000  0.0000   \n",
       "ada                  Ada Boost Classifier    0.8172  0.4537   0.000  0.0000   \n",
       "gbc          Gradient Boosting Classifier    0.9029  0.5313   0.000  0.0000   \n",
       "et                 Extra Trees Classifier    0.9272  0.5531   0.000  0.0000   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9208  0.4859   0.000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.1812  0.0717  0.1095     0.359  \n",
       "svm       0.1410  0.0157  0.0329     0.013  \n",
       "dt        0.1269  0.0336  0.0423     0.026  \n",
       "lr        0.1210  0.0052 -0.0050     0.603  \n",
       "nb        0.1133 -0.0099 -0.0101     0.011  \n",
       "ridge     0.0788 -0.0395 -0.0568     0.011  \n",
       "lda       0.0788 -0.0395 -0.0568     0.074  \n",
       "rf        0.0000 -0.0055 -0.0076     0.091  \n",
       "qda       0.0000  0.0000  0.0000     0.015  \n",
       "ada       0.0000 -0.0931 -0.0974     0.136  \n",
       "gbc       0.0000 -0.0345 -0.0377     0.549  \n",
       "et        0.0000 -0.0028 -0.0038     0.054  \n",
       "lightgbm  0.0000 -0.0118 -0.0138     0.656  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.4551</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.4537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0931</td>\n",
       "      <td>-0.0974</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.5313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0345</td>\n",
       "      <td>-0.0377</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>0.5531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0028</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9208</td>\n",
       "      <td>0.4859</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>-0.0138</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.6571  0.6021   0.535  0.1094   \n",
       "svm                   SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788   \n",
       "dt               Decision Tree Classifier    0.7994  0.5366   0.230  0.0880   \n",
       "lr                    Logistic Regression    0.6423  0.5079   0.310  0.0759   \n",
       "nb                            Naive Bayes    0.5323  0.4442   0.440  0.0658   \n",
       "ridge                    Ridge Classifier    0.6617  0.0000   0.200  0.0499   \n",
       "lda          Linear Discriminant Analysis    0.6617  0.4551   0.200  0.0499   \n",
       "rf               Random Forest Classifier    0.9256  0.5568   0.000  0.0000   \n",
       "qda       Quadratic Discriminant Analysis    0.9288  0.5000   0.000  0.0000   \n",
       "ada                  Ada Boost Classifier    0.8172  0.4537   0.000  0.0000   \n",
       "gbc          Gradient Boosting Classifier    0.9029  0.5313   0.000  0.0000   \n",
       "et                 Extra Trees Classifier    0.9272  0.5531   0.000  0.0000   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9208  0.4859   0.000  0.0000   \n",
       "dummy                    Dummy Classifier    0.9288  0.5000   0.000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.1812  0.0717  0.1095     0.359  \n",
       "svm       0.1410  0.0157  0.0329     0.013  \n",
       "dt        0.1269  0.0336  0.0423     0.026  \n",
       "lr        0.1210  0.0052 -0.0050     0.603  \n",
       "nb        0.1133 -0.0099 -0.0101     0.011  \n",
       "ridge     0.0788 -0.0395 -0.0568     0.011  \n",
       "lda       0.0788 -0.0395 -0.0568     0.074  \n",
       "rf        0.0000 -0.0055 -0.0076     0.091  \n",
       "qda       0.0000  0.0000  0.0000     0.015  \n",
       "ada       0.0000 -0.0931 -0.0974     0.136  \n",
       "gbc       0.0000 -0.0345 -0.0377     0.549  \n",
       "et        0.0000 -0.0028 -0.0038     0.054  \n",
       "lightgbm  0.0000 -0.0118 -0.0138     0.656  \n",
       "dummy     0.0000  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6617</td>\n",
       "      <td>0.4551</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.4537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0931</td>\n",
       "      <td>-0.0974</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.5313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0345</td>\n",
       "      <td>-0.0377</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>0.5531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0028</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.9208</td>\n",
       "      <td>0.4859</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>-0.0138</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.6571  0.6021   0.535  0.1094   \n",
       "svm                   SVM - Linear Kernel    0.4062  0.0000   0.675  0.0788   \n",
       "dt               Decision Tree Classifier    0.7994  0.5366   0.230  0.0880   \n",
       "lr                    Logistic Regression    0.6423  0.5079   0.310  0.0759   \n",
       "nb                            Naive Bayes    0.5323  0.4442   0.440  0.0658   \n",
       "ridge                    Ridge Classifier    0.6617  0.0000   0.200  0.0499   \n",
       "lda          Linear Discriminant Analysis    0.6617  0.4551   0.200  0.0499   \n",
       "rf               Random Forest Classifier    0.9256  0.5568   0.000  0.0000   \n",
       "qda       Quadratic Discriminant Analysis    0.9288  0.5000   0.000  0.0000   \n",
       "ada                  Ada Boost Classifier    0.8172  0.4537   0.000  0.0000   \n",
       "gbc          Gradient Boosting Classifier    0.9029  0.5313   0.000  0.0000   \n",
       "et                 Extra Trees Classifier    0.9272  0.5531   0.000  0.0000   \n",
       "lightgbm  Light Gradient Boosting Machine    0.9208  0.4859   0.000  0.0000   \n",
       "dummy                    Dummy Classifier    0.9288  0.5000   0.000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.1812  0.0717  0.1095     0.359  \n",
       "svm       0.1410  0.0157  0.0329     0.013  \n",
       "dt        0.1269  0.0336  0.0423     0.026  \n",
       "lr        0.1210  0.0052 -0.0050     0.603  \n",
       "nb        0.1133 -0.0099 -0.0101     0.011  \n",
       "ridge     0.0788 -0.0395 -0.0568     0.011  \n",
       "lda       0.0788 -0.0395 -0.0568     0.074  \n",
       "rf        0.0000 -0.0055 -0.0076     0.091  \n",
       "qda       0.0000  0.0000  0.0000     0.015  \n",
       "ada       0.0000 -0.0931 -0.0974     0.136  \n",
       "gbc       0.0000 -0.0345 -0.0377     0.549  \n",
       "et        0.0000 -0.0028 -0.0038     0.054  \n",
       "lightgbm  0.0000 -0.0118 -0.0138     0.656  \n",
       "dummy     0.0000  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Participant:  3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "lr  Logistic Regression     0.582  0.5398     0.5  0.1304  0.2055  0.0391   \n",
       "\n",
       "       MCC  TT (Sec)  \n",
       "lr  0.0578     0.636  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267  0.2077  0.0313   \n",
       "lr      Logistic Regression    0.5820  0.5398  0.5000  0.1304  0.2055  0.0391   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.0478     0.342  \n",
       "lr   0.0578     0.636  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy     AUC  Recall   Prec.      F1   Kappa  \\\n",
       "knn  K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267  0.2077  0.0313   \n",
       "lr      Logistic Regression    0.5820  0.5398  0.5000  0.1304  0.2055  0.0391   \n",
       "nb              Naive Bayes    0.4764  0.4671  0.4429  0.0963  0.1578 -0.0291   \n",
       "\n",
       "        MCC  TT (Sec)  \n",
       "knn  0.0478     0.342  \n",
       "lr   0.0578     0.636  \n",
       "nb  -0.0485     0.010  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267  0.2077   \n",
       "lr        Logistic Regression    0.5820  0.5398  0.5000  0.1304  0.2055   \n",
       "nb                Naive Bayes    0.4764  0.4671  0.4429  0.0963  0.1578   \n",
       "dt   Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165  0.1466   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.0313  0.0478     0.342  \n",
       "lr   0.0391  0.0578     0.636  \n",
       "nb  -0.0291 -0.0485     0.010  \n",
       "dt   0.0083  0.0075     0.024  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn    K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267  0.2077   \n",
       "lr        Logistic Regression    0.5820  0.5398  0.5000  0.1304  0.2055   \n",
       "svm       SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186  0.2011   \n",
       "nb                Naive Bayes    0.4764  0.4671  0.4429  0.0963  0.1578   \n",
       "dt   Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165  0.1466   \n",
       "\n",
       "      Kappa     MCC  TT (Sec)  \n",
       "knn  0.0313  0.0478     0.342  \n",
       "lr   0.0391  0.0578     0.636  \n",
       "svm  0.0162  0.0343     0.012  \n",
       "nb  -0.0291 -0.0485     0.010  \n",
       "dt   0.0083  0.0075     0.024  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267  0.2077   \n",
       "lr          Logistic Regression    0.5820  0.5398  0.5000  0.1304  0.2055   \n",
       "svm         SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186  0.2011   \n",
       "ridge          Ridge Classifier    0.6250  0.0000  0.4000  0.1257  0.1898   \n",
       "nb                  Naive Bayes    0.4764  0.4671  0.4429  0.0963  0.1578   \n",
       "dt     Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165  0.1466   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.0313  0.0478     0.342  \n",
       "lr     0.0391  0.0578     0.636  \n",
       "svm    0.0162  0.0343     0.012  \n",
       "ridge  0.0269  0.0351     0.010  \n",
       "nb    -0.0291 -0.0485     0.010  \n",
       "dt     0.0083  0.0075     0.024  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy     AUC  Recall   Prec.      F1  \\\n",
       "knn      K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267  0.2077   \n",
       "lr          Logistic Regression    0.5820  0.5398  0.5000  0.1304  0.2055   \n",
       "svm         SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186  0.2011   \n",
       "ridge          Ridge Classifier    0.6250  0.0000  0.4000  0.1257  0.1898   \n",
       "nb                  Naive Bayes    0.4764  0.4671  0.4429  0.0963  0.1578   \n",
       "dt     Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165  0.1466   \n",
       "rf     Random Forest Classifier    0.8735  0.5129  0.0286  0.0667  0.0400   \n",
       "\n",
       "        Kappa     MCC  TT (Sec)  \n",
       "knn    0.0313  0.0478     0.342  \n",
       "lr     0.0391  0.0578     0.636  \n",
       "svm    0.0162  0.0343     0.012  \n",
       "ridge  0.0269  0.0351     0.010  \n",
       "nb    -0.0291 -0.0485     0.010  \n",
       "dt     0.0083  0.0075     0.024  \n",
       "rf     0.0085  0.0047     0.091  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267   \n",
       "lr                 Logistic Regression    0.5820  0.5398  0.5000  0.1304   \n",
       "svm                SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186   \n",
       "ridge                 Ridge Classifier    0.6250  0.0000  0.4000  0.1257   \n",
       "nb                         Naive Bayes    0.4764  0.4671  0.4429  0.0963   \n",
       "dt            Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165   \n",
       "rf            Random Forest Classifier    0.8735  0.5129  0.0286  0.0667   \n",
       "qda    Quadratic Discriminant Analysis    0.8892  0.5000  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.2077  0.0313  0.0478     0.342  \n",
       "lr     0.2055  0.0391  0.0578     0.636  \n",
       "svm    0.2011  0.0162  0.0343     0.012  \n",
       "ridge  0.1898  0.0269  0.0351     0.010  \n",
       "nb     0.1578 -0.0291 -0.0485     0.010  \n",
       "dt     0.1466  0.0083  0.0075     0.024  \n",
       "rf     0.0400  0.0085  0.0047     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267   \n",
       "lr                 Logistic Regression    0.5820  0.5398  0.5000  0.1304   \n",
       "svm                SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186   \n",
       "ridge                 Ridge Classifier    0.6250  0.0000  0.4000  0.1257   \n",
       "nb                         Naive Bayes    0.4764  0.4671  0.4429  0.0963   \n",
       "ada               Ada Boost Classifier    0.7532  0.4895  0.2000  0.1223   \n",
       "dt            Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165   \n",
       "rf            Random Forest Classifier    0.8735  0.5129  0.0286  0.0667   \n",
       "qda    Quadratic Discriminant Analysis    0.8892  0.5000  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.2077  0.0313  0.0478     0.342  \n",
       "lr     0.2055  0.0391  0.0578     0.636  \n",
       "svm    0.2011  0.0162  0.0343     0.012  \n",
       "ridge  0.1898  0.0269  0.0351     0.010  \n",
       "nb     0.1578 -0.0291 -0.0485     0.010  \n",
       "ada    0.1497  0.0162  0.0176     0.135  \n",
       "dt     0.1466  0.0083  0.0075     0.024  \n",
       "rf     0.0400  0.0085  0.0047     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8244</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>-0.0396</td>\n",
       "      <td>-0.0405</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267   \n",
       "lr                 Logistic Regression    0.5820  0.5398  0.5000  0.1304   \n",
       "svm                SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186   \n",
       "ridge                 Ridge Classifier    0.6250  0.0000  0.4000  0.1257   \n",
       "nb                         Naive Bayes    0.4764  0.4671  0.4429  0.0963   \n",
       "ada               Ada Boost Classifier    0.7532  0.4895  0.2000  0.1223   \n",
       "dt            Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165   \n",
       "gbc       Gradient Boosting Classifier    0.8244  0.4993  0.0429  0.0676   \n",
       "rf            Random Forest Classifier    0.8735  0.5129  0.0286  0.0667   \n",
       "qda    Quadratic Discriminant Analysis    0.8892  0.5000  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.2077  0.0313  0.0478     0.342  \n",
       "lr     0.2055  0.0391  0.0578     0.636  \n",
       "svm    0.2011  0.0162  0.0343     0.012  \n",
       "ridge  0.1898  0.0269  0.0351     0.010  \n",
       "nb     0.1578 -0.0291 -0.0485     0.010  \n",
       "ada    0.1497  0.0162  0.0176     0.135  \n",
       "dt     0.1466  0.0083  0.0075     0.024  \n",
       "gbc    0.0510 -0.0396 -0.0405     0.583  \n",
       "rf     0.0400  0.0085  0.0047     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.5494</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8244</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>-0.0396</td>\n",
       "      <td>-0.0405</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267   \n",
       "lr                 Logistic Regression    0.5820  0.5398  0.5000  0.1304   \n",
       "svm                SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186   \n",
       "ridge                 Ridge Classifier    0.6250  0.0000  0.4000  0.1257   \n",
       "lda       Linear Discriminant Analysis    0.6250  0.5494  0.4000  0.1257   \n",
       "nb                         Naive Bayes    0.4764  0.4671  0.4429  0.0963   \n",
       "ada               Ada Boost Classifier    0.7532  0.4895  0.2000  0.1223   \n",
       "dt            Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165   \n",
       "gbc       Gradient Boosting Classifier    0.8244  0.4993  0.0429  0.0676   \n",
       "rf            Random Forest Classifier    0.8735  0.5129  0.0286  0.0667   \n",
       "qda    Quadratic Discriminant Analysis    0.8892  0.5000  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.2077  0.0313  0.0478     0.342  \n",
       "lr     0.2055  0.0391  0.0578     0.636  \n",
       "svm    0.2011  0.0162  0.0343     0.012  \n",
       "ridge  0.1898  0.0269  0.0351     0.010  \n",
       "lda    0.1898  0.0269  0.0351     0.017  \n",
       "nb     0.1578 -0.0291 -0.0485     0.010  \n",
       "ada    0.1497  0.0162  0.0176     0.135  \n",
       "dt     0.1466  0.0083  0.0075     0.024  \n",
       "gbc    0.0510 -0.0396 -0.0405     0.583  \n",
       "rf     0.0400  0.0085  0.0047     0.091  \n",
       "qda    0.0000  0.0000  0.0000     0.015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.5494</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8244</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>-0.0396</td>\n",
       "      <td>-0.0405</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0064</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn             K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267   \n",
       "lr                 Logistic Regression    0.5820  0.5398  0.5000  0.1304   \n",
       "svm                SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186   \n",
       "ridge                 Ridge Classifier    0.6250  0.0000  0.4000  0.1257   \n",
       "lda       Linear Discriminant Analysis    0.6250  0.5494  0.4000  0.1257   \n",
       "nb                         Naive Bayes    0.4764  0.4671  0.4429  0.0963   \n",
       "ada               Ada Boost Classifier    0.7532  0.4895  0.2000  0.1223   \n",
       "dt            Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165   \n",
       "gbc       Gradient Boosting Classifier    0.8244  0.4993  0.0429  0.0676   \n",
       "rf            Random Forest Classifier    0.8735  0.5129  0.0286  0.0667   \n",
       "et              Extra Trees Classifier    0.8750  0.5205  0.0143  0.0500   \n",
       "qda    Quadratic Discriminant Analysis    0.8892  0.5000  0.0000  0.0000   \n",
       "\n",
       "           F1   Kappa     MCC  TT (Sec)  \n",
       "knn    0.2077  0.0313  0.0478     0.342  \n",
       "lr     0.2055  0.0391  0.0578     0.636  \n",
       "svm    0.2011  0.0162  0.0343     0.012  \n",
       "ridge  0.1898  0.0269  0.0351     0.010  \n",
       "lda    0.1898  0.0269  0.0351     0.017  \n",
       "nb     0.1578 -0.0291 -0.0485     0.010  \n",
       "ada    0.1497  0.0162  0.0176     0.135  \n",
       "dt     0.1466  0.0083  0.0075     0.024  \n",
       "gbc    0.0510 -0.0396 -0.0405     0.583  \n",
       "rf     0.0400  0.0085  0.0047     0.091  \n",
       "et     0.0222 -0.0064 -0.0127     0.058  \n",
       "qda    0.0000  0.0000  0.0000     0.015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.5494</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8545</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>-0.0089</td>\n",
       "      <td>0.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8244</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>-0.0396</td>\n",
       "      <td>-0.0405</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0064</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267   \n",
       "lr                    Logistic Regression    0.5820  0.5398  0.5000  0.1304   \n",
       "svm                   SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186   \n",
       "ridge                    Ridge Classifier    0.6250  0.0000  0.4000  0.1257   \n",
       "lda          Linear Discriminant Analysis    0.6250  0.5494  0.4000  0.1257   \n",
       "nb                            Naive Bayes    0.4764  0.4671  0.4429  0.0963   \n",
       "ada                  Ada Boost Classifier    0.7532  0.4895  0.2000  0.1223   \n",
       "dt               Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8545  0.5003  0.0429  0.0833   \n",
       "gbc          Gradient Boosting Classifier    0.8244  0.4993  0.0429  0.0676   \n",
       "rf               Random Forest Classifier    0.8735  0.5129  0.0286  0.0667   \n",
       "et                 Extra Trees Classifier    0.8750  0.5205  0.0143  0.0500   \n",
       "qda       Quadratic Discriminant Analysis    0.8892  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.2077  0.0313  0.0478     0.342  \n",
       "lr        0.2055  0.0391  0.0578     0.636  \n",
       "svm       0.2011  0.0162  0.0343     0.012  \n",
       "ridge     0.1898  0.0269  0.0351     0.010  \n",
       "lda       0.1898  0.0269  0.0351     0.017  \n",
       "nb        0.1578 -0.0291 -0.0485     0.010  \n",
       "ada       0.1497  0.0162  0.0176     0.135  \n",
       "dt        0.1466  0.0083  0.0075     0.024  \n",
       "lightgbm  0.0554 -0.0041 -0.0089     0.683  \n",
       "gbc       0.0510 -0.0396 -0.0405     0.583  \n",
       "rf        0.0400  0.0085  0.0047     0.091  \n",
       "et        0.0222 -0.0064 -0.0127     0.058  \n",
       "qda       0.0000  0.0000  0.0000     0.015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.5494</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8545</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>-0.0089</td>\n",
       "      <td>0.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8244</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>-0.0396</td>\n",
       "      <td>-0.0405</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0064</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267   \n",
       "lr                    Logistic Regression    0.5820  0.5398  0.5000  0.1304   \n",
       "svm                   SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186   \n",
       "ridge                    Ridge Classifier    0.6250  0.0000  0.4000  0.1257   \n",
       "lda          Linear Discriminant Analysis    0.6250  0.5494  0.4000  0.1257   \n",
       "nb                            Naive Bayes    0.4764  0.4671  0.4429  0.0963   \n",
       "ada                  Ada Boost Classifier    0.7532  0.4895  0.2000  0.1223   \n",
       "dt               Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8545  0.5003  0.0429  0.0833   \n",
       "gbc          Gradient Boosting Classifier    0.8244  0.4993  0.0429  0.0676   \n",
       "rf               Random Forest Classifier    0.8735  0.5129  0.0286  0.0667   \n",
       "et                 Extra Trees Classifier    0.8750  0.5205  0.0143  0.0500   \n",
       "qda       Quadratic Discriminant Analysis    0.8892  0.5000  0.0000  0.0000   \n",
       "dummy                    Dummy Classifier    0.8892  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.2077  0.0313  0.0478     0.342  \n",
       "lr        0.2055  0.0391  0.0578     0.636  \n",
       "svm       0.2011  0.0162  0.0343     0.012  \n",
       "ridge     0.1898  0.0269  0.0351     0.010  \n",
       "lda       0.1898  0.0269  0.0351     0.017  \n",
       "nb        0.1578 -0.0291 -0.0485     0.010  \n",
       "ada       0.1497  0.0162  0.0176     0.135  \n",
       "dt        0.1466  0.0083  0.0075     0.024  \n",
       "lightgbm  0.0554 -0.0041 -0.0089     0.683  \n",
       "gbc       0.0510 -0.0396 -0.0405     0.583  \n",
       "rf        0.0400  0.0085  0.0047     0.091  \n",
       "et        0.0222 -0.0064 -0.0127     0.058  \n",
       "qda       0.0000  0.0000  0.0000     0.015  \n",
       "dummy     0.0000  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5509</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM - Linear Kernel</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.5494</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.0485</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "      <td>0.8545</td>\n",
       "      <td>0.5003</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>-0.0041</td>\n",
       "      <td>-0.0089</td>\n",
       "      <td>0.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.8244</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>-0.0396</td>\n",
       "      <td>-0.0405</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.0064</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qda</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
       "knn                K Neighbors Classifier    0.5002  0.5509  0.5857  0.1267   \n",
       "lr                    Logistic Regression    0.5820  0.5398  0.5000  0.1304   \n",
       "svm                   SVM - Linear Kernel    0.4097  0.0000  0.6714  0.1186   \n",
       "ridge                    Ridge Classifier    0.6250  0.0000  0.4000  0.1257   \n",
       "lda          Linear Discriminant Analysis    0.6250  0.5494  0.4000  0.1257   \n",
       "nb                            Naive Bayes    0.4764  0.4671  0.4429  0.0963   \n",
       "ada                  Ada Boost Classifier    0.7532  0.4895  0.2000  0.1223   \n",
       "dt               Decision Tree Classifier    0.7406  0.5040  0.2000  0.1165   \n",
       "lightgbm  Light Gradient Boosting Machine    0.8545  0.5003  0.0429  0.0833   \n",
       "gbc          Gradient Boosting Classifier    0.8244  0.4993  0.0429  0.0676   \n",
       "rf               Random Forest Classifier    0.8735  0.5129  0.0286  0.0667   \n",
       "et                 Extra Trees Classifier    0.8750  0.5205  0.0143  0.0500   \n",
       "qda       Quadratic Discriminant Analysis    0.8892  0.5000  0.0000  0.0000   \n",
       "dummy                    Dummy Classifier    0.8892  0.5000  0.0000  0.0000   \n",
       "\n",
       "              F1   Kappa     MCC  TT (Sec)  \n",
       "knn       0.2077  0.0313  0.0478     0.342  \n",
       "lr        0.2055  0.0391  0.0578     0.636  \n",
       "svm       0.2011  0.0162  0.0343     0.012  \n",
       "ridge     0.1898  0.0269  0.0351     0.010  \n",
       "lda       0.1898  0.0269  0.0351     0.017  \n",
       "nb        0.1578 -0.0291 -0.0485     0.010  \n",
       "ada       0.1497  0.0162  0.0176     0.135  \n",
       "dt        0.1466  0.0083  0.0075     0.024  \n",
       "lightgbm  0.0554 -0.0041 -0.0089     0.683  \n",
       "gbc       0.0510 -0.0396 -0.0405     0.583  \n",
       "rf        0.0400  0.0085  0.0047     0.091  \n",
       "et        0.0222 -0.0064 -0.0127     0.058  \n",
       "qda       0.0000  0.0000  0.0000     0.015  \n",
       "dummy     0.0000  0.0000  0.0000     0.008  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1scores = []\n",
    "\n",
    "for participant in unique_participants:\n",
    "    print(\"Participant: \",participant)\n",
    "    part_df = adarp_group.get_group(participant)\n",
    "    grid = setup(data=part_df, target='stress', pca = True, pca_components = 200, fix_imbalance = True, html=False, silent=True, verbose=False) #fix_imbalance = True,\n",
    "    best = compare_models(sort=\"F1\")\n",
    "    accuracies.append(pull()['Accuracy'][0])\n",
    "    precision.append(pull()['Prec.'][0])\n",
    "    recall.append(pull()['Recall'][0])\n",
    "    f1scores.append(pull()['F1'][0])\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f49de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = statistics.mean(accuracies)\n",
    "mean_prec = statistics.mean(precision)\n",
    "mean_rec = statistics.mean(recall)\n",
    "mean_f1 = statistics.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9542dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy ADARP - Cluster All Features:  0.6773\n",
      "Mean Precision ADARP- Cluster All Features:  0.107\n",
      "Mean Recall ADARP- Cluster All Features:  0.46378\n",
      "Mean F1-score ADARP- Cluster All Features:  0.1697\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy ADARP - Cluster All Features: \", mean_acc)\n",
    "print(\"Mean Precision ADARP- Cluster All Features: \", mean_prec)\n",
    "print(\"Mean Recall ADARP- Cluster All Features: \", mean_rec)\n",
    "print(\"Mean F1-score ADARP- Cluster All Features: \", mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7c6eb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Cluster\n",
       "0    0        0\n",
       "1    1        2\n",
       "2    2        0\n",
       "3    3        0\n",
       "4    4        1\n",
       "5    5        0\n",
       "6    6        0\n",
       "7    7        3\n",
       "8    8        0\n",
       "9    9        0\n",
       "10  10        2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f13bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_dataset_new = pd.read_csv(\"Final_CSVs/adarp_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9468de96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>stress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.513261</td>\n",
       "      <td>20.935081</td>\n",
       "      <td>-47.196465</td>\n",
       "      <td>-3.260985</td>\n",
       "      <td>4.507111</td>\n",
       "      <td>0.716637</td>\n",
       "      <td>0.303609</td>\n",
       "      <td>-0.124521</td>\n",
       "      <td>1.554185</td>\n",
       "      <td>0.872075</td>\n",
       "      <td>...</td>\n",
       "      <td>4.887624</td>\n",
       "      <td>3.015592</td>\n",
       "      <td>-2.245096</td>\n",
       "      <td>1.452785</td>\n",
       "      <td>-1.587706</td>\n",
       "      <td>-1.760503</td>\n",
       "      <td>2.011873</td>\n",
       "      <td>8</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.417520</td>\n",
       "      <td>-21.996034</td>\n",
       "      <td>14.541838</td>\n",
       "      <td>1.649816</td>\n",
       "      <td>-5.103676</td>\n",
       "      <td>-2.204086</td>\n",
       "      <td>-12.916046</td>\n",
       "      <td>-8.806577</td>\n",
       "      <td>20.147872</td>\n",
       "      <td>0.889388</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.509922</td>\n",
       "      <td>3.669814</td>\n",
       "      <td>1.374393</td>\n",
       "      <td>-2.458410</td>\n",
       "      <td>-0.429740</td>\n",
       "      <td>1.754223</td>\n",
       "      <td>-0.090012</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-33.102149</td>\n",
       "      <td>-25.279355</td>\n",
       "      <td>18.484651</td>\n",
       "      <td>5.258636</td>\n",
       "      <td>-3.633260</td>\n",
       "      <td>-1.777928</td>\n",
       "      <td>0.609462</td>\n",
       "      <td>-1.762000</td>\n",
       "      <td>-9.085495</td>\n",
       "      <td>-2.921152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258656</td>\n",
       "      <td>0.035151</td>\n",
       "      <td>-0.303729</td>\n",
       "      <td>-0.385087</td>\n",
       "      <td>-0.506989</td>\n",
       "      <td>-0.520648</td>\n",
       "      <td>-0.201086</td>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.193078</td>\n",
       "      <td>-47.192845</td>\n",
       "      <td>-18.580473</td>\n",
       "      <td>3.686088</td>\n",
       "      <td>-4.581467</td>\n",
       "      <td>-3.020590</td>\n",
       "      <td>-7.064863</td>\n",
       "      <td>1.519770</td>\n",
       "      <td>-8.749378</td>\n",
       "      <td>4.872315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376895</td>\n",
       "      <td>0.058002</td>\n",
       "      <td>-0.757182</td>\n",
       "      <td>-0.087456</td>\n",
       "      <td>-0.128169</td>\n",
       "      <td>0.339075</td>\n",
       "      <td>-0.838829</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.319440</td>\n",
       "      <td>22.393429</td>\n",
       "      <td>48.325996</td>\n",
       "      <td>-0.647691</td>\n",
       "      <td>-0.581191</td>\n",
       "      <td>-1.557754</td>\n",
       "      <td>-18.945942</td>\n",
       "      <td>0.094177</td>\n",
       "      <td>-18.148259</td>\n",
       "      <td>9.633245</td>\n",
       "      <td>...</td>\n",
       "      <td>2.074177</td>\n",
       "      <td>1.789060</td>\n",
       "      <td>0.861509</td>\n",
       "      <td>3.254957</td>\n",
       "      <td>-0.424161</td>\n",
       "      <td>-2.378359</td>\n",
       "      <td>0.299103</td>\n",
       "      <td>9</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27103</th>\n",
       "      <td>-14.183324</td>\n",
       "      <td>69.585348</td>\n",
       "      <td>32.609356</td>\n",
       "      <td>-3.750749</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.031458</td>\n",
       "      <td>-0.639752</td>\n",
       "      <td>0.862550</td>\n",
       "      <td>-0.214938</td>\n",
       "      <td>1.032628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173204</td>\n",
       "      <td>-0.016553</td>\n",
       "      <td>-0.101481</td>\n",
       "      <td>0.192474</td>\n",
       "      <td>0.151399</td>\n",
       "      <td>0.389866</td>\n",
       "      <td>-0.139952</td>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27104</th>\n",
       "      <td>6.289888</td>\n",
       "      <td>-32.050495</td>\n",
       "      <td>87.843617</td>\n",
       "      <td>4.951944</td>\n",
       "      <td>-9.923721</td>\n",
       "      <td>16.940356</td>\n",
       "      <td>0.339763</td>\n",
       "      <td>-3.149450</td>\n",
       "      <td>8.660654</td>\n",
       "      <td>-0.037309</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482223</td>\n",
       "      <td>-1.560710</td>\n",
       "      <td>0.559244</td>\n",
       "      <td>-3.376335</td>\n",
       "      <td>-0.160281</td>\n",
       "      <td>1.505873</td>\n",
       "      <td>-0.992808</td>\n",
       "      <td>8</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27105</th>\n",
       "      <td>-52.526937</td>\n",
       "      <td>117.548782</td>\n",
       "      <td>-19.696015</td>\n",
       "      <td>9.568756</td>\n",
       "      <td>-21.686070</td>\n",
       "      <td>3.745054</td>\n",
       "      <td>2.493357</td>\n",
       "      <td>2.038464</td>\n",
       "      <td>-3.945271</td>\n",
       "      <td>3.929809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237621</td>\n",
       "      <td>-0.553347</td>\n",
       "      <td>0.353389</td>\n",
       "      <td>0.205311</td>\n",
       "      <td>1.381038</td>\n",
       "      <td>0.067580</td>\n",
       "      <td>2.151312</td>\n",
       "      <td>10</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27106</th>\n",
       "      <td>-28.249098</td>\n",
       "      <td>-26.997105</td>\n",
       "      <td>54.082033</td>\n",
       "      <td>0.550082</td>\n",
       "      <td>-1.863535</td>\n",
       "      <td>-0.258588</td>\n",
       "      <td>0.668584</td>\n",
       "      <td>-0.656412</td>\n",
       "      <td>-1.946633</td>\n",
       "      <td>-0.691243</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.055964</td>\n",
       "      <td>-1.498716</td>\n",
       "      <td>1.744852</td>\n",
       "      <td>0.092225</td>\n",
       "      <td>1.233039</td>\n",
       "      <td>-1.125368</td>\n",
       "      <td>-0.894168</td>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27107</th>\n",
       "      <td>-26.644633</td>\n",
       "      <td>42.590907</td>\n",
       "      <td>-24.583798</td>\n",
       "      <td>0.828119</td>\n",
       "      <td>2.140673</td>\n",
       "      <td>1.620500</td>\n",
       "      <td>-0.330326</td>\n",
       "      <td>6.011797</td>\n",
       "      <td>6.592117</td>\n",
       "      <td>-0.954890</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.040036</td>\n",
       "      <td>-4.081151</td>\n",
       "      <td>-2.638535</td>\n",
       "      <td>4.867645</td>\n",
       "      <td>-0.197978</td>\n",
       "      <td>5.922384</td>\n",
       "      <td>-2.331666</td>\n",
       "      <td>9</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27108 rows  253 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1          2         3          4          5  \\\n",
       "0      20.513261   20.935081 -47.196465 -3.260985   4.507111   0.716637   \n",
       "1      92.417520  -21.996034  14.541838  1.649816  -5.103676  -2.204086   \n",
       "2     -33.102149  -25.279355  18.484651  5.258636  -3.633260  -1.777928   \n",
       "3      35.193078  -47.192845 -18.580473  3.686088  -4.581467  -3.020590   \n",
       "4      44.319440   22.393429  48.325996 -0.647691  -0.581191  -1.557754   \n",
       "...          ...         ...        ...       ...        ...        ...   \n",
       "27103 -14.183324   69.585348  32.609356 -3.750749   0.883306   0.031458   \n",
       "27104   6.289888  -32.050495  87.843617  4.951944  -9.923721  16.940356   \n",
       "27105 -52.526937  117.548782 -19.696015  9.568756 -21.686070   3.745054   \n",
       "27106 -28.249098  -26.997105  54.082033  0.550082  -1.863535  -0.258588   \n",
       "27107 -26.644633   42.590907 -24.583798  0.828119   2.140673   1.620500   \n",
       "\n",
       "               6         7          8         9  ...       243       244  \\\n",
       "0       0.303609 -0.124521   1.554185  0.872075  ...  4.887624  3.015592   \n",
       "1     -12.916046 -8.806577  20.147872  0.889388  ... -1.509922  3.669814   \n",
       "2       0.609462 -1.762000  -9.085495 -2.921152  ...  0.258656  0.035151   \n",
       "3      -7.064863  1.519770  -8.749378  4.872315  ... -0.376895  0.058002   \n",
       "4     -18.945942  0.094177 -18.148259  9.633245  ...  2.074177  1.789060   \n",
       "...          ...       ...        ...       ...  ...       ...       ...   \n",
       "27103  -0.639752  0.862550  -0.214938  1.032628  ... -0.173204 -0.016553   \n",
       "27104   0.339763 -3.149450   8.660654 -0.037309  ... -0.482223 -1.560710   \n",
       "27105   2.493357  2.038464  -3.945271  3.929809  ... -0.237621 -0.553347   \n",
       "27106   0.668584 -0.656412  -1.946633 -0.691243  ... -1.055964 -1.498716   \n",
       "27107  -0.330326  6.011797   6.592117 -0.954890  ... -1.040036 -4.081151   \n",
       "\n",
       "            245       246       247       248       249  id  dataset  stress  \n",
       "0     -2.245096  1.452785 -1.587706 -1.760503  2.011873   8    Train     0.0  \n",
       "1      1.374393 -2.458410 -0.429740  1.754223 -0.090012   1    Train     1.0  \n",
       "2     -0.303729 -0.385087 -0.506989 -0.520648 -0.201086   3    Train     1.0  \n",
       "3     -0.757182 -0.087456 -0.128169  0.339075 -0.838829   2    Train     0.0  \n",
       "4      0.861509  3.254957 -0.424161 -2.378359  0.299103   9    Train     1.0  \n",
       "...         ...       ...       ...       ...       ...  ..      ...     ...  \n",
       "27103 -0.101481  0.192474  0.151399  0.389866 -0.139952   1     Test     0.0  \n",
       "27104  0.559244 -3.376335 -0.160281  1.505873 -0.992808   8     Test     0.0  \n",
       "27105  0.353389  0.205311  1.381038  0.067580  2.151312  10     Test     0.0  \n",
       "27106  1.744852  0.092225  1.233039 -1.125368 -0.894168   2     Test     0.0  \n",
       "27107 -2.638535  4.867645 -0.197978  5.922384 -2.331666   9     Test     0.0  \n",
       "\n",
       "[27108 rows x 253 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adarp_dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1af5fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_dataset_new = pd.merge(adarp_dataset_new, clusters, on = \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75ec3d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15727\n",
       "2     5824\n",
       "1     3021\n",
       "3     2536\n",
       "Name: Cluster, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adarp_dataset_new[\"Cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f576167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adarp_dataset_new.to_csv(\"Final_CSVs/adarp_clusters4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d0789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpyth8",
   "language": "python",
   "name": "env_python8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
